{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-07T02:46:15.379380Z",
     "start_time": "2019-02-07T02:46:01.322165Z"
    },
    "code_folding": [
     0,
     32,
     34,
     61,
     69,
     93,
     96
    ]
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "if (!(\"Notification\" in window)) {\n",
       "    alert(\"This browser does not support desktop notifications, so the %%notify magic will not work.\");\n",
       "} else if (Notification.permission !== 'granted' && Notification.permission !== 'denied') {\n",
       "    Notification.requestPermission(function (permission) {\n",
       "        if(!('permission' in Notification)) {\n",
       "            Notification.permission = permission;\n",
       "        }\n",
       "    })\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import Library and some random image data set\n",
    "import tensorflow as tf\n",
    "import numpy      as np\n",
    "import seaborn    as sns \n",
    "import pandas     as pd\n",
    "import os,sys\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "np.random.seed(78); tf.set_random_seed(78)\n",
    "\n",
    "# get some of the STL data set\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from skimage import util \n",
    "from skimage.transform import resize\n",
    "from skimage.io import imread\n",
    "import warnings\n",
    "from numpy import inf\n",
    "\n",
    "from scipy.stats import kurtosis,skew\n",
    "\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import gc\n",
    "from IPython.display import display, clear_output\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from matplotlib import animation\n",
    "%load_ext jupyternotify\n",
    "\n",
    "# Def: Read STL 10 images\n",
    "def read_STL10_data():\n",
    "    # read all of the data (STL 10) https://github.com/mttk/STL10\n",
    "    def read_all_images(path_to_data):\n",
    "        \"\"\"\n",
    "        :param path_to_data: the file containing the binary images from the STL-10 dataset\n",
    "        :return: an array containing all the images\n",
    "        \"\"\"\n",
    "\n",
    "        with open(path_to_data, 'rb') as f:\n",
    "            # read whole file in uint8 chunks\n",
    "            everything = np.fromfile(f, dtype=np.uint8)\n",
    "\n",
    "            # We force the data into 3x96x96 chunks, since the\n",
    "            # images are stored in \"column-major order\", meaning\n",
    "            # that \"the first 96*96 values are the red channel,\n",
    "            # the next 96*96 are green, and the last are blue.\"\n",
    "            # The -1 is since the size of the pictures depends\n",
    "            # on the input file, and this way numpy determines\n",
    "            # the size on its own.\n",
    "\n",
    "            images = np.reshape(everything, (-1, 3, 96, 96))\n",
    "\n",
    "            # Now transpose the images into a standard image format\n",
    "            # readable by, for example, matplotlib.imshow\n",
    "            # You might want to comment this line or reverse the shuffle\n",
    "            # if you will use a learning algorithm like CNN, since they like\n",
    "            # their channels separated.\n",
    "            images = np.transpose(images, (0, 3, 2, 1))\n",
    "            return images\n",
    "    def read_labels(path_to_labels):\n",
    "        \"\"\"\n",
    "        :param path_to_labels: path to the binary file containing labels from the STL-10 dataset\n",
    "        :return: an array containing the labels\n",
    "        \"\"\"\n",
    "        with open(path_to_labels, 'rb') as f:\n",
    "            labels = np.fromfile(f, dtype=np.uint8)\n",
    "            return labels\n",
    "    def show_images(data,row=1,col=1):\n",
    "        fig=plt.figure(figsize=(10,10))\n",
    "        columns = col; rows = row\n",
    "        for i in range(1, columns*rows +1):\n",
    "            fig.add_subplot(rows, columns, i)\n",
    "            plt.imshow(data[i-1])\n",
    "        plt.show()\n",
    "\n",
    "    train_images = read_all_images(\"../../../DataSet/STL10/stl10_binary/train_X.bin\") / 255.0\n",
    "    train_labels = read_labels    (\"../../../DataSet/STL10/stl10_binary/train_Y.bin\")\n",
    "    test_images  = read_all_images(\"../../../DataSet/STL10/stl10_binary/test_X.bin\")  / 255.0\n",
    "    test_labels  = read_labels    (\"../../../DataSet/STL10/stl10_binary/test_y.bin\")\n",
    "\n",
    "    label_encoder= OneHotEncoder(sparse=False,categories='auto')\n",
    "    train_labels = label_encoder.fit_transform(train_labels.reshape((-1,1)))\n",
    "    test_labels  = label_encoder.fit_transform(test_labels.reshape((-1,1)))\n",
    "\n",
    "    print(train_images.shape,train_images.max(),train_images.min())\n",
    "    print(train_labels.shape,train_labels.max(),train_labels.min())\n",
    "    print(test_images.shape,test_images.max(),test_images.min())\n",
    "    print(test_labels.shape,test_labels.max(),test_labels.min())\n",
    "    return train_images,train_labels,test_images,test_labels\n",
    "\n",
    "# Def: Read CIFAR 10 images\n",
    "def read_CIFAR10_data():\n",
    "    # ====== miscellaneous =====\n",
    "    # code from: https://github.com/tensorflow/tensorflow/issues/8246\n",
    "    def tf_repeat(tensor, repeats):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "\n",
    "        input: A Tensor. 1-D or higher.\n",
    "        repeats: A list. Number of repeat for each dimension, length must be the same as the number of dimensions in input\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        A Tensor. Has the same type as input. Has the shape of tensor.shape * repeats\n",
    "        \"\"\"\n",
    "        expanded_tensor = tf.expand_dims(tensor, -1)\n",
    "        multiples = [1] + repeats\n",
    "        tiled_tensor = tf.tile(expanded_tensor, multiples = multiples)\n",
    "        repeated_tesnor = tf.reshape(tiled_tensor, tf.shape(tensor) * repeats)\n",
    "        return repeated_tesnor\n",
    "    def unpickle(file):\n",
    "        import pickle\n",
    "        with open(file, 'rb') as fo:\n",
    "            dict = pickle.load(fo, encoding='bytes')\n",
    "        return dict\n",
    "    # ====== miscellaneous =====\n",
    "\n",
    "    # data\n",
    "    PathDicom = \"../../../Dataset/cifar-10-batches-py/\"\n",
    "    lstFilesDCM = []  # create an empty list\n",
    "    for dirName, subdirList, fileList in os.walk(PathDicom):\n",
    "        for filename in fileList:\n",
    "            if not \".html\" in filename.lower() and not  \".meta\" in filename.lower():  # check whether the file's DICOM\n",
    "                lstFilesDCM.append(os.path.join(dirName,filename))\n",
    "\n",
    "    # Read the data traind and Test\n",
    "    batch0 = unpickle(lstFilesDCM[0])\n",
    "    batch1 = unpickle(lstFilesDCM[1])\n",
    "    batch2 = unpickle(lstFilesDCM[2])\n",
    "    batch3 = unpickle(lstFilesDCM[3])\n",
    "    batch4 = unpickle(lstFilesDCM[4])\n",
    "\n",
    "    onehot_encoder = OneHotEncoder(sparse=True)\n",
    "    train_batch = np.vstack((batch0[b'data'],batch1[b'data'],batch2[b'data'],batch3[b'data'],batch4[b'data']))\n",
    "    train_label = np.expand_dims(np.hstack((batch0[b'labels'],batch1[b'labels'],batch2[b'labels'],batch3[b'labels'],batch4[b'labels'])).T,axis=1).astype(np.float64)\n",
    "    train_label = onehot_encoder.fit_transform(train_label).toarray().astype(np.float64)\n",
    "\n",
    "    test_batch = unpickle(lstFilesDCM[5])[b'data']\n",
    "    test_label = np.expand_dims(np.array(unpickle(lstFilesDCM[5])[b'labels']),axis=0).T.astype(np.float64)\n",
    "    test_label = onehot_encoder.fit_transform(test_label).toarray().astype(np.float64)\n",
    "\n",
    "    # reshape data\n",
    "    train_batch = np.reshape(train_batch,(len(train_batch),3,32,32)); test_batch = np.reshape(test_batch,(len(test_batch),3,32,32))\n",
    "    # rotate data\n",
    "    train_batch = np.rot90(np.rot90(train_batch,1,axes=(1,3)),3,axes=(1,2)).astype(np.float64); test_batch = np.rot90(np.rot90(test_batch,1,axes=(1,3)),3,axes=(1,2)).astype(np.float64)\n",
    "    # normalize\n",
    "    train_batch= train_batch/255.0; test_batch = test_batch/255.0\n",
    "\n",
    "    # print out the data shape and the max and min value\n",
    "    print(train_batch.shape,train_batch.max(),train_batch.min())\n",
    "    print(train_label.shape,train_label.max(),train_label.min())\n",
    "    print(test_batch.shape,test_batch.max(),test_batch.min())\n",
    "    print(test_label.shape,test_label.max(),test_label.min())\n",
    "    return train_batch,train_label,test_batch,test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-07T02:46:15.461160Z",
     "start_time": "2019-02-07T02:46:15.395338Z"
    },
    "code_folding": [
     0,
     7
    ]
   },
   "outputs": [],
   "source": [
    "# create the layers\n",
    "def tf_softmax(x): return tf.nn.softmax(x)\n",
    "def tf_relu(x):   return tf.nn.relu(x)\n",
    "def d_tf_relu(x): return tf.cast(tf.greater(x,0),tf.float32)\n",
    "\n",
    "class FNN():\n",
    "\n",
    "    def __init__(self,inc,outc,act=tf_relu,d_act=d_tf_relu,special_init=False,which_reg=0.0):\n",
    "        if special_init:\n",
    "            interval = np.sqrt(6.0 / (inc + outc + 1.0))\n",
    "            self.w = tf.Variable(tf.random_uniform(shape=(inc, outc),minval=-interval,maxval=interval,dtype=tf.float32,seed=2))\n",
    "            self.b = tf.Variable(tf.random_uniform(shape=(outc),minval=-interval,maxval=interval,dtype=tf.float32,seed=2))\n",
    "        else:\n",
    "            self.w = tf.Variable(tf.random_normal([inc,outc], stddev=0.05,seed=2,dtype=tf.float32))\n",
    "            self.b = tf.Variable(tf.random_normal([outc], stddev=0.05,seed=2,dtype=tf.float32))\n",
    "\n",
    "        self.m,self.v = tf.Variable(tf.zeros_like(self.w)),tf.Variable(tf.zeros_like(self.w))\n",
    "        self.m_b,self.v_b = tf.Variable(tf.zeros_like(self.b)),tf.Variable(tf.zeros_like(self.b))\n",
    "        self.act,self.d_act = act,d_act\n",
    "        self.which_reg = which_reg\n",
    "\n",
    "    def getw(self): return self.w\n",
    "    def feedforward(self,input=None):\n",
    "        self.input = input\n",
    "        self.layer = tf.matmul(input,self.w) + self.b\n",
    "        self.layerA = self.act(self.layer)\n",
    "        return self.layerA\n",
    "\n",
    "    def backprop(self,gradient=None,which_reg=0):\n",
    "        grad_part_1 = gradient\n",
    "        grad_part_2 = self.d_act(self.layer)\n",
    "        grad_part_3 = self.input\n",
    "\n",
    "        grad_middle = grad_part_1 * grad_part_2\n",
    "        grad  = tf.matmul(tf.transpose(grad_part_3),grad_middle)/batch_size\n",
    "        grad_b= tf.reduce_mean(grad_middle,axis=0)\n",
    "        grad_pass = tf.matmul(grad_middle,tf.transpose(self.w))\n",
    "\n",
    "        # === Reg ===\n",
    "        if self.which_reg == 0:\n",
    "            grad  = grad\n",
    "            grad_b= grad_b\n",
    "\n",
    "        if self.which_reg == 0.5:\n",
    "            grad  = grad + lamda * (tf.sqrt(tf.abs(self.w))) * (1.0/tf.sqrt(tf.abs(self.w)+ 10e-5)) * tf.sign(self.w)\n",
    "            grad_b= grad_b+lamda * (tf.sqrt(tf.abs(self.b))) * (1.0/tf.sqrt(tf.abs(self.b)+ 10e-5)) * tf.sign(self.b)\n",
    "\n",
    "        if self.which_reg == 1:\n",
    "            grad = grad   + lamda * tf.sign(self.w)\n",
    "            grad_b=grad_b + lamda * tf.sign(self.b)\n",
    "\n",
    "        if self.which_reg == 1.5:\n",
    "            grad = grad   + lamda * 1.0/(tf.sqrt(tf.square(self.w) + 10e-5)) * self.w\n",
    "            grad_b=grad_b + lamda * 1.0/(tf.sqrt(tf.square(self.b) + 10e-5)) * self.b\n",
    "\n",
    "        if self.which_reg == 2:\n",
    "            grad = grad  + lamda * (1.0/tf.sqrt(tf.square(tf.abs(self.w))+ 10e-5)) * tf.abs(self.w) * tf.sign(self.w)\n",
    "            grad_b=grad_b+ lamda * (1.0/tf.sqrt(tf.square(tf.abs(self.b))+ 10e-5)) * tf.abs(self.b) * tf.sign(self.b)\n",
    "\n",
    "        if self.which_reg == 2.5:\n",
    "            grad = grad   + lamda * 2.0 * self.w\n",
    "            grad_b=grad_b + lamda * 2.0 * self.b\n",
    "\n",
    "        if self.which_reg == 3:\n",
    "            grad = grad   + lamda * tf.pow(tf.pow(tf.abs(self.w),3)+ 10e-5,-0.66) * tf.pow(tf.abs(self.w),2) * tf.sign(self.w)\n",
    "            grad_b=grad_b + lamda * tf.pow(tf.pow(tf.abs(self.b),3)+ 10e-5,-0.66) * tf.pow(tf.abs(self.b),2) * tf.sign(self.b)\n",
    "\n",
    "        if self.which_reg == 4:\n",
    "            grad = grad   + lamda * tf.pow(tf.pow(tf.abs(self.w),4)+ 10e-5,-0.75) * tf.pow(tf.abs(self.w),3) * tf.sign(self.w)\n",
    "            grad_b=grad_b + lamda * tf.pow(tf.pow(tf.abs(self.b),4)+ 10e-5,-0.75) * tf.pow(tf.abs(self.b),3) * tf.sign(self.b)\n",
    "\n",
    "        update_w = []\n",
    "\n",
    "        # Update the Weight First\n",
    "        update_w.append(tf.assign( self.m,self.m*beta1 + (1-beta1) * (grad)   ))\n",
    "        update_w.append(tf.assign( self.v,self.v*beta2 + (1-beta2) * (grad ** 2)   ))\n",
    "        m_hat = self.m / (1-beta1)\n",
    "        v_hat = self.v / (1-beta2)\n",
    "        adam_middle = m_hat *  learning_rate/(tf.sqrt(v_hat) + adam_e)\n",
    "        update_w.append(tf.assign(self.w,tf.subtract(self.w,adam_middle )))\n",
    "\n",
    "        # Update the Bias later\n",
    "        update_w.append(tf.assign(self.m_b,self.m_b*beta1 + (1-beta1) * (grad_b)   ))\n",
    "        update_w.append(tf.assign(self.v_b,self.v_b*beta2 + (1-beta2) * (grad_b ** 2)   ))\n",
    "        m_hat_b = self.m_b / (1-beta1)\n",
    "        v_hat_b = self.v_b / (1-beta2)\n",
    "        adam_middle_b = m_hat_b *  learning_rate/(tf.sqrt(v_hat_b) + adam_e)\n",
    "        update_w.append(tf.assign(self.b,tf.subtract(self.b,adam_middle_b )))\n",
    "\n",
    "        return grad_pass,update_w\n",
    "class CNN():\n",
    "\n",
    "    def __init__(self,k,inc,out, stddev=0.05,which_reg=0,act=tf_relu,d_act=d_tf_relu):\n",
    "        self.w              = tf.Variable(tf.random_normal([k,k,inc,out],stddev=stddev,seed=2,dtype=tf.float32))\n",
    "        self.m,self.v       = tf.Variable(tf.zeros_like(self.w)),tf.Variable(tf.zeros_like(self.w))\n",
    "        self.act,self.d_act = act,d_act\n",
    "        self.regularizer    = which_reg\n",
    "    def getw(self): return self.w\n",
    "    def feedforward(self,input,stride=1,padding='SAME'):\n",
    "        self.input  = input\n",
    "        self.layer  = tf.nn.conv2d(input,self.w,strides=[1,stride,stride,1],padding=padding) \n",
    "        self.layerA = self.act(self.layer)\n",
    "        return self.layer, self.layerA\n",
    "    \n",
    "    def backprop(self,gradient,stride=1,padding='SAME'):\n",
    "        grad_part_1 = gradient\n",
    "        grad_part_2 = self.d_act(self.layer)\n",
    "        grad_part_3 = self.input\n",
    "\n",
    "        grad_middle = grad_part_1 * grad_part_2\n",
    "        grad        = tf.nn.conv2d_backprop_filter(input = grad_part_3,filter_sizes = tf.shape(self.w),  out_backprop = grad_middle,strides=[1,stride,stride,1],padding=padding) / batch_size\n",
    "        grad_pass   = tf.nn.conv2d_backprop_input (input_sizes = tf.shape(self.input),filter= self.w,out_backprop = grad_middle,strides=[1,stride,stride,1],padding=padding)\n",
    "\n",
    "        if self.regularizer == 'A': grad = grad + lamda * tf.sign(self.w)\n",
    "        if self.regularizer == 'B': grad = grad + lamda * 2.0 * self.w\n",
    "        if self.regularizer == 'C': grad = grad + lamda * (1.0/tf.sqrt(tf.square(self.w)+ 10e-8)) * self.w\n",
    "        if self.regularizer == 'D': grad = grad + lamda * -(2*self.w)/(1 + self.w**2)\n",
    "        if self.regularizer == 'E': grad = grad + lamda * -(1-tf.tanh(self.w) ** 2)\n",
    "        if self.regularizer == 'F': grad = grad + lamda * -(1-tf.tanh(self.w** 2) ** 2) * 2.0 * self.w \n",
    "        if self.regularizer == 'G': grad = grad + lamda * -(1-tf.tanh(tf.abs(self.w)) ** 2) * tf.sign(self.w)\n",
    "        if self.regularizer == 'H': grad = grad + lamda * -(1-tf.tanh(tf.abs(self.w)** 2) ** 2) * 2.0 * tf.abs(self.w) *  tf.sign(self.w)\n",
    "        if self.regularizer == 'I': grad = grad + lamda * tf.cos(self.w)\n",
    "        if self.regularizer == 'J': grad = grad + lamda * tf.sign(tf.sin(self.w)) * tf.cos(self.w)\n",
    "        if self.regularizer == 'K': grad = grad + lamda * (2)/(self.w + 10e-8)\n",
    "        if self.regularizer == 'L': grad = grad + lamda * (tf.log(self.w**2) + 2.0)\n",
    "        \n",
    "        update_w = []\n",
    "        update_w.append(tf.assign( self.m,self.m*beta1 + (1-beta1) * (grad)   ))\n",
    "        update_w.append(tf.assign( self.v,self.v*beta2 + (1-beta2) * (grad ** 2)   ))\n",
    "        m_hat = self.m / (1-beta1) ; v_hat = self.v / (1-beta2)\n",
    "        adam_middle = m_hat * learning_rate/(tf.sqrt(v_hat) + adam_e)\n",
    "        update_w.append(tf.assign(self.w,tf.subtract(self.w,adam_middle  )))\n",
    "        return grad_pass,grad,update_w\n",
    "class tf_batch_norm_layer():\n",
    "    \n",
    "    def __init__(self,vector_shape,axis):\n",
    "        self.moving_mean = tf.Variable(tf.zeros(shape=[1,1,1,vector_shape],dtype=tf.float32))\n",
    "        self.moving_vari = tf.Variable(tf.zeros(shape=[1,1,1,vector_shape],dtype=tf.float32))\n",
    "        self.axis        = axis\n",
    "    def feedforward(self,input,training_phase=True,eps = 1e-8):\n",
    "        self.input = input\n",
    "        self.input_size          = self.input.shape\n",
    "        self.batch,self.h,self.w,self.c = self.input_size[0].value,self.input_size[1].value,self.input_size[2].value,self.input_size[3].value\n",
    "\n",
    "        # Training Moving Average Mean         \n",
    "        def training_fn():\n",
    "            self.mean    = tf.reduce_mean(self.input,axis=self.axis ,keepdims=True)\n",
    "            self.var     = tf.reduce_mean(tf.square(self.input-self.mean),axis=self.axis,keepdims=True)\n",
    "            centered_data= (self.input - self.mean)/tf.sqrt(self.var + eps)\n",
    "            \n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean*0.9 + 0.1 * self.mean ))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari*0.9 + 0.1 * self.var  ))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        # Testing Moving Average Mean        \n",
    "        def  testing_fn():\n",
    "            centered_data   = (self.input - self.moving_mean)/tf.sqrt(self.moving_vari + eps)\n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        self.output,update_variable = tf.cond(training_phase,true_fn=training_fn,false_fn=testing_fn)\n",
    "        return self.output,update_variable\n",
    "    def backprop(self,grad,eps = 1e-8):\n",
    "        change_parts = 1.0 /(self.batch * self.h * self.w)\n",
    "        grad_sigma   = tf.reduce_sum( grad *  (self.input-self.mean)     ,axis=self.axis,keepdims=True) * -0.5 * (self.var+eps) ** -1.5\n",
    "        grad_mean    = tf.reduce_sum( grad *  (-1./tf.sqrt(self.var+eps)),axis=self.axis,keepdims=True) + grad_sigma * change_parts * 2.0 * tf.reduce_sum((self.input-self.mean),axis=self.axis,keepdims=True) * -1\n",
    "        grad_x       = grad * 1/(tf.sqrt(self.var+eps)) + grad_sigma * change_parts * 2.0 * (self.input-self.mean) + grad_mean * change_parts\n",
    "        return grad_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-07T02:46:17.518681Z",
     "start_time": "2019-02-07T02:46:15.477117Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3) 1.0 0.0\n",
      "(50000, 10) 1.0 0.0\n",
      "(10000, 32, 32, 3) 1.0 0.0\n",
      "(10000, 10) 1.0 0.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# read the data\n",
    "train_images,train_labels,test_images,test_labels = read_CIFAR10_data()\n",
    "rand_choice = np.random.choice(len(train_images))\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(train_images[rand_choice])\n",
    "plt.title(str(train_labels[rand_choice]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-07T02:47:35.636477Z",
     "start_time": "2019-02-07T02:47:35.632487Z"
    }
   },
   "outputs": [],
   "source": [
    "# define learning rate\n",
    "num_eps   = 10; num_epoch = 200; learning_rate = 0.0008; batch_size = 50;  beta1,beta2,adam_e = 0.9,0.999,1e-9; print_iter = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-07T03:02:20.770903Z",
     "start_time": "2019-02-07T03:02:20.735022Z"
    },
    "code_folding": [
     26,
     47,
     78
    ]
   },
   "outputs": [],
   "source": [
    "# create the layers\n",
    "def tf_softmax(x): return tf.nn.softmax(x)\n",
    "def tf_relu(x):   return tf.nn.relu(x)\n",
    "def d_tf_relu(x): return tf.cast(tf.greater(x,0),tf.float32)\n",
    "def tf_iden(x):   return x\n",
    "def d_tf_iden(x): return tf.ones_like(x)\n",
    "\n",
    "class FNN():\n",
    "\n",
    "    def __init__(self,inc,outc,act=tf_iden,d_act=d_tf_iden,special_init=False):\n",
    "        if special_init:\n",
    "            interval = np.sqrt(6.0 / (inc + outc + 1.0))\n",
    "            self.w = tf.Variable(tf.random_uniform(shape=(inc, outc),minval=-interval,maxval=interval,dtype=tf.float32,seed=2))\n",
    "        else:\n",
    "            self.w = tf.Variable(tf.random_normal([inc,outc], stddev=0.05,seed=2,dtype=tf.float32))\n",
    "\n",
    "        self.m,self.v = tf.Variable(tf.zeros_like(self.w)),tf.Variable(tf.zeros_like(self.w))\n",
    "        self.act,self.d_act = act,d_act\n",
    "\n",
    "    def getw(self): return self.w\n",
    "    def feedforward(self,input=None):\n",
    "        self.input = input\n",
    "        self.layer = tf.matmul(input,self.w) \n",
    "        self.layerA = self.act(self.layer)\n",
    "        return self.layer,self.layerA\n",
    "\n",
    "    def backprop(self,gradient=None,which_reg=0):\n",
    "        grad_part_1 = gradient\n",
    "        grad_part_2 = self.d_act(self.layer)\n",
    "        grad_part_3 = self.input\n",
    "\n",
    "        grad_middle = grad_part_1 * grad_part_2\n",
    "        grad  = tf.matmul(tf.transpose(grad_part_3),grad_middle)\n",
    "        grad_b= tf.reduce_mean(grad_middle,axis=0)\n",
    "        grad_pass = tf.matmul(grad_middle,tf.transpose(self.w))\n",
    "\n",
    "        update_w = []\n",
    "\n",
    "        # Update the Weight First\n",
    "        update_w.append(tf.assign( self.m,self.m*beta1 + (1-beta1) * (grad)   ))\n",
    "        update_w.append(tf.assign( self.v,self.v*beta2 + (1-beta2) * (grad ** 2)   ))\n",
    "        m_hat = self.m / (1-beta1)\n",
    "        v_hat = self.v / (1-beta2)\n",
    "        adam_middle = m_hat *  learning_rate/(tf.sqrt(v_hat) + adam_e)\n",
    "        update_w.append(tf.assign(self.w,tf.subtract(self.w,adam_middle )))\n",
    "\n",
    "        return grad_pass,update_w\n",
    "class CNN():\n",
    "\n",
    "    def __init__(self,k,inc,out, stddev=0.05,act=tf_relu,d_act=d_tf_relu):\n",
    "        self.w              = tf.Variable(tf.random_normal([k,k,inc,out],stddev=stddev,seed=2,dtype=tf.float32))\n",
    "        self.m,self.v       = tf.Variable(tf.zeros_like(self.w)),tf.Variable(tf.zeros_like(self.w))\n",
    "        self.act,self.d_act = act,d_act\n",
    "\n",
    "    def getw(self): return self.w\n",
    "    def feedforward(self,input,stride=1,padding='VALID'):\n",
    "        self.input  = input\n",
    "        self.layer  = tf.nn.conv2d(input,self.w,strides=[1,stride,stride,1],padding=padding) \n",
    "        self.layerA = self.act(self.layer)\n",
    "        return self.layer, self.layerA\n",
    "    \n",
    "    def backprop(self,gradient,stride=1,padding='VALID'):\n",
    "        grad_part_1 = gradient\n",
    "        grad_part_2 = self.d_act(self.layer)\n",
    "        grad_part_3 = self.input\n",
    "\n",
    "        grad_middle = grad_part_1 * grad_part_2\n",
    "        grad        = tf.nn.conv2d_backprop_filter(input = grad_part_3,filter_sizes = tf.shape(self.w),  out_backprop = grad_middle,strides=[1,stride,stride,1],padding=padding) \n",
    "        grad_pass   = tf.nn.conv2d_backprop_input (input_sizes = tf.shape(self.input),filter= self.w,out_backprop = grad_middle,strides=[1,stride,stride,1],padding=padding)\n",
    "\n",
    "        update_w = []\n",
    "        update_w.append(tf.assign( self.m,self.m*beta1 + (1-beta1) * (grad)   ))\n",
    "        update_w.append(tf.assign( self.v,self.v*beta2 + (1-beta2) * (grad ** 2)   ))\n",
    "        m_hat = self.m / (1-beta1) ; v_hat = self.v / (1-beta2)\n",
    "        adam_middle = m_hat * learning_rate/(tf.sqrt(v_hat) + adam_e)\n",
    "        update_w.append(tf.assign(self.w,tf.subtract(self.w,adam_middle  )))\n",
    "        return grad_pass,grad,update_w\n",
    "    \n",
    "class tf_batch_norm_layer():\n",
    "    \n",
    "    def __init__(self,vector_shape,axis):\n",
    "        self.moving_mean = tf.Variable(tf.zeros(shape=[1,1,1,vector_shape],dtype=tf.float32))\n",
    "        self.moving_vari = tf.Variable(tf.zeros(shape=[1,1,1,vector_shape],dtype=tf.float32))\n",
    "        self.axis        = axis\n",
    "    def feedforward(self,input,training_phase=True,eps = 1e-8):\n",
    "        self.input = input\n",
    "        self.input_size          = self.input.shape\n",
    "        self.batch,self.h,self.w,self.c = self.input_size[0].value,self.input_size[1].value,self.input_size[2].value,self.input_size[3].value\n",
    "\n",
    "        # Training Moving Average Mean         \n",
    "        def training_fn():\n",
    "            self.mean    = tf.reduce_mean(self.input,axis=self.axis ,keepdims=True)\n",
    "            self.var     = tf.reduce_mean(tf.square(self.input-self.mean),axis=self.axis,keepdims=True)\n",
    "            centered_data= (self.input - self.mean)/tf.sqrt(self.var + eps)\n",
    "            \n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean*0.9 + 0.1 * self.mean ))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari*0.9 + 0.1 * self.var  ))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        # Testing Moving Average Mean        \n",
    "        def  testing_fn():\n",
    "            centered_data   = (self.input - self.moving_mean)/tf.sqrt(self.moving_vari + eps)\n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        self.output,update_variable = tf.cond(training_phase,true_fn=training_fn,false_fn=testing_fn)\n",
    "        return self.output,update_variable\n",
    "    def backprop(self,grad,eps = 1e-8):\n",
    "        change_parts = 1.0 /(self.batch * self.h * self.w)\n",
    "        grad_sigma   = tf.reduce_sum( grad *  (self.input-self.mean)     ,axis=self.axis,keepdims=True) * -0.5 * (self.var+eps) ** -1.5\n",
    "        grad_mean    = tf.reduce_sum( grad *  (-1./tf.sqrt(self.var+eps)),axis=self.axis,keepdims=True) + grad_sigma * change_parts * 2.0 * tf.reduce_sum((self.input-self.mean),axis=self.axis,keepdims=True) * -1\n",
    "        grad_x       = grad * 1/(tf.sqrt(self.var+eps)) + grad_sigma * change_parts * 2.0 * (self.input-self.mean) + grad_mean * change_parts\n",
    "        return grad_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-07T03:02:24.026043Z",
     "start_time": "2019-02-07T03:02:23.898358Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"MatMul_3:0\", shape=(50, 64), dtype=float32)\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# build encoder \n",
    "x = tf.placeholder(tf.float32,(batch_size,32,32,3))\n",
    "y = tf.placeholder(tf.float32,(batch_size,10))\n",
    "\n",
    "def tf_encoder():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "\n",
    "# create layers\n",
    "l1 = CNN(4,3, 64); \n",
    "l2 = CNN(4,64,128); \n",
    "l3 = CNN(4,128,256); \n",
    "l4 = CNN(4,256,512); \n",
    "l5 = FNN(512*20*20,64); \n",
    "\n",
    "\n",
    "layer1, layer1a = l1. feedforward(x,      stride=1)\n",
    "layer2, layer2a = l2. feedforward(layer1a,stride=1)\n",
    "layer3, layer3a = l3. feedforward(layer2a,stride=1)\n",
    "layer4, layer4a = l4. feedforward(layer3a,stride=1)\n",
    "layer5_input    = tf.reshape(layer4a,[batch_size,-1])\n",
    "layer5, layer5a = l5. feedforward(layer5_input)\n",
    "\n",
    "print(layer5a)\n",
    "sys.exit()\n",
    "\n",
    "\n",
    "final_layer   = tf.reduce_mean(layer6a,(1,2))\n",
    "final_softmax = tf_softmax(final_layer)\n",
    "cost          = -tf.reduce_mean(y * tf.log(final_softmax + 1e-8))\n",
    "correct_prediction = tf.equal(tf.argmax(final_softmax, 1), tf.argmax(y, 1))\n",
    "accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "gradient = tf.tile((final_softmax-y)[:,None,None,:],[1,6,6,1])/batch_size\n",
    "grad6p,grad6w,grad6_up = l6.backprop(gradient,std_value=std_value)\n",
    "grad5p,grad5w,grad5_up = l5.backprop(grad6p,std_value=std_value)\n",
    "grad4p,grad4w,grad4_up = l4.backprop(grad5p,stride=2,std_value=std_value)\n",
    "grad3p,grad3w,grad3_up = l3.backprop(grad4p,stride=2,std_value=std_value)\n",
    "grad2p,grad2w,grad2_up = l2.backprop(grad3p,stride=2,std_value=std_value)\n",
    "grad1p,grad1w,grad1_up = l1.backprop(grad2p,stride=2,std_value=std_value)\n",
    "\n",
    "gradient_update = grad6_up + grad5_up + grad4_up + grad3_up + grad2_up + grad1_up \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-07T02:18:36.811605Z",
     "start_time": "2019-02-07T02:18:35.307630Z"
    },
    "code_folding": [
     39,
     69,
     81,
     97
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                                                               | 0/781 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import nn as nn\n",
    "\n",
    "import torch\n",
    "from torchvision.datasets.cifar import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import statistics as stats\n",
    "import argparse\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.c0 = nn.Conv2d(3, 64, kernel_size=4, stride=1)\n",
    "        self.c1 = nn.Conv2d(64, 128, kernel_size=4, stride=1)\n",
    "        self.c2 = nn.Conv2d(128, 256, kernel_size=4, stride=1)\n",
    "        self.c3 = nn.Conv2d(256, 512, kernel_size=4, stride=1)\n",
    "        self.l1 = nn.Linear(512*20*20, 64)\n",
    "\n",
    "        self.b1 = nn.BatchNorm2d(128)\n",
    "        self.b2 = nn.BatchNorm2d(256)\n",
    "        self.b3 = nn.BatchNorm2d(512)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.c0(x))                      # (64, 64, 29, 29)\n",
    "        features = F.relu(self.b1(self.c1(h)))      # (64, 128, 26, 26)\n",
    "        h = F.relu(self.b2(self.c2(features)))      # (64, 256, 23, 23)\n",
    "        h = F.relu(self.b3(self.c3(h)))             # (64, 512, 20, 20)\n",
    "        encoded = self.l1(h.view(x.shape[0], -1))   # (batch,64)\n",
    "        return encoded, features\n",
    "\n",
    "class GlobalDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.c0 = nn.Conv2d(128, 64, kernel_size=3) # (64, 64, 24, 24)\n",
    "        self.c1 = nn.Conv2d(64, 32, kernel_size=3)  # (64, 32, 22, 22)\n",
    "        self.l0 = nn.Linear(32 * 22 * 22 + 64, 512) # (64, 512)\n",
    "        self.l1 = nn.Linear(512, 512)               # (512, 512)\n",
    "        self.l2 = nn.Linear(512, 1)                 # (512, 1)\n",
    "\n",
    "    def forward(self, y, M):\n",
    "        h = F.relu(self.c0(M))\n",
    "        h = self.c1(h)\n",
    "        h = h.view(y.shape[0], -1)\n",
    "        h = torch.cat((y, h), dim=1)\n",
    "        h = F.relu(self.l0(h))\n",
    "        h = F.relu(self.l1(h))\n",
    "        return self.l2(h)\n",
    "    \n",
    "class LocalDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.c0 = nn.Conv2d(192, 512, kernel_size=1)\n",
    "        self.c1 = nn.Conv2d(512, 512, kernel_size=1)\n",
    "        self.c2 = nn.Conv2d(512, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.c0(x))\n",
    "        h = F.relu(self.c1(h))\n",
    "        return self.c2(h)\n",
    "    \n",
    "class PriorDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l0 = nn.Linear(64, 1000)\n",
    "        self.l1 = nn.Linear(1000, 200)\n",
    "        self.l2 = nn.Linear(200, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.l0(x))\n",
    "        h = F.relu(self.l1(h))\n",
    "        return torch.sigmoid(self.l2(h))\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(64, 15)\n",
    "        self.bn1 = nn.BatchNorm1d(15)\n",
    "        self.l2 = nn.Linear(15, 10)\n",
    "        self.bn2 = nn.BatchNorm1d(10)\n",
    "        self.l3 = nn.Linear(10, 10)\n",
    "        self.bn3 = nn.BatchNorm1d(10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded, _ = x[0], x[1]\n",
    "        clazz = F.relu(self.bn1(self.l1(encoded)))\n",
    "        clazz = F.relu(self.bn2(self.l2(clazz)))\n",
    "        clazz = F.softmax(self.bn3(self.l3(clazz)), dim=1)\n",
    "        return clazz\n",
    "class DeepInfoAsLatent(nn.Module):\n",
    "    def __init__(self, run, epoch):\n",
    "        super().__init__()\n",
    "        model_path = Path(r'c:/data/deepinfomax/models') / Path(str(run)) / Path('encoder' + str(epoch) + '.wgt')\n",
    "        self.encoder = Encoder()\n",
    "        self.encoder.load_state_dict(torch.load(str(model_path)))\n",
    "        self.classifier = Classifier()\n",
    "\n",
    "    def forward(self, x):\n",
    "        z, features = self.encoder(x)\n",
    "        z = z.detach()\n",
    "        return self.classifier((z, features))\n",
    "    \n",
    "class DeepInfoMaxLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.5, beta=1.0, gamma=0.1):\n",
    "        super().__init__()\n",
    "        self.global_d = GlobalDiscriminator()\n",
    "        self.local_d = LocalDiscriminator()\n",
    "        self.prior_d = PriorDiscriminator()\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, y, M, M_prime):\n",
    "\n",
    "        # see appendix 1A of https://arxiv.org/pdf/1808.06670.pdf\n",
    "\n",
    "        y_exp = y.unsqueeze(-1).unsqueeze(-1)\n",
    "        y_exp = y_exp.expand(-1, -1, 26, 26)\n",
    "        y_M = torch.cat((M, y_exp), dim=1)\n",
    "        y_M_prime = torch.cat((M_prime, y_exp), dim=1)\n",
    "        Ej = -F.softplus(-self.local_d(y_M)).mean()\n",
    "        Em = F.softplus(self.local_d(y_M_prime)).mean()\n",
    "        LOCAL = (Em - Ej) * self.beta\n",
    "        \n",
    "        # global         \n",
    "        Ej = -F.softplus(-self.global_d(y, M)).mean()\n",
    "        Em = F.softplus(self.global_d(y, M_prime)).mean()\n",
    "        GLOBAL = (Em - Ej) * self.alpha\n",
    "\n",
    "        prior = torch.rand_like(y)\n",
    "        term_a = torch.log(self.prior_d(prior)).mean()\n",
    "        term_b = torch.log(1.0 - self.prior_d(y)).mean()\n",
    "        PRIOR = - (term_a + term_b) * self.gamma\n",
    "\n",
    "        return LOCAL + GLOBAL + PRIOR\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "batch_size = 64\n",
    "\n",
    "# image size 3, 32, 32 batch size must be an even numbershuffle must be True\n",
    "cifar_10_train_dt = CIFAR10(r'c:\\data\\tv',  download=True, transform=ToTensor())\n",
    "cifar_10_train_l  = DataLoader(cifar_10_train_dt, batch_size=batch_size, shuffle=True, drop_last=True,pin_memory=torch.cuda.is_available())\n",
    "\n",
    "encoder    = Encoder().to(device)\n",
    "loss_fn    = DeepInfoMaxLoss().to(device)\n",
    "optim      = Adam(encoder.parameters(), lr=1e-4)\n",
    "loss_optim = Adam(loss_fn.parameters(), lr=1e-4)\n",
    "\n",
    "for epoch in range(100):\n",
    "    batch = tqdm(cifar_10_train_l, total=len(cifar_10_train_dt) // batch_size)\n",
    "    train_loss = []\n",
    "    \n",
    "    for x, target in batch:\n",
    "        x = x.to(device)\n",
    "\n",
    "        optim.zero_grad(); loss_optim.zero_grad()\n",
    "        y, M = encoder(x)\n",
    "        # y - > (64, 128, 26, 26)\n",
    "        # M - > (batch,64)\n",
    "        \n",
    "        # rotate images to create pairs for comparison (ROTATING)\n",
    "        M_prime = torch.cat((M[1:], M[0].unsqueeze(0)), dim=0)\n",
    "        loss = loss_fn(y, M, M_prime) # ()\n",
    "        sys.exit()\n",
    "        \n",
    "        train_loss.append(loss.item())\n",
    "        batch.set_description(str(epoch) + ' Loss: ' + str(stats.mean(train_loss[-20:])))\n",
    "        loss.backward()\n",
    "        optim.step(); loss_optim.step()\n",
    "        sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-07T01:49:41.457723Z",
     "start_time": "2019-02-07T01:49:41.329296Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 32, 32, 3)\n",
      "(64,)\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# batch = tqdm(cifar_10_train_l, total=len(cifar_10_train_dt) // batch_size)\n",
    "for x, target in batch:\n",
    "    temp = np.swapaxes(np.swapaxes(x.numpy(),1,3),2,1)\n",
    "    plt.imshow(temp[0])\n",
    "    plt.show()\n",
    "    print(temp.shape)\n",
    "    print(target.numpy().shape)\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
