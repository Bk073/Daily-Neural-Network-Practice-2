{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-11T22:49:15.461188Z",
     "start_time": "2019-02-11T22:49:01.531536Z"
    },
    "code_folding": [
     0,
     32,
     34,
     61,
     69,
     93,
     96
    ]
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "if (!(\"Notification\" in window)) {\n",
       "    alert(\"This browser does not support desktop notifications, so the %%notify magic will not work.\");\n",
       "} else if (Notification.permission !== 'granted' && Notification.permission !== 'denied') {\n",
       "    Notification.requestPermission(function (permission) {\n",
       "        if(!('permission' in Notification)) {\n",
       "            Notification.permission = permission;\n",
       "        }\n",
       "    })\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import Library and some random image data set\n",
    "import tensorflow as tf\n",
    "import numpy      as np\n",
    "import seaborn    as sns \n",
    "import pandas     as pd\n",
    "import os,sys\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "np.random.seed(78); tf.set_random_seed(78)\n",
    "\n",
    "# get some of the STL data set\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from skimage import util \n",
    "from skimage.transform import resize\n",
    "from skimage.io import imread\n",
    "import warnings\n",
    "from numpy import inf\n",
    "\n",
    "from scipy.stats import kurtosis,skew\n",
    "\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import gc\n",
    "from IPython.display import display, clear_output\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from matplotlib import animation\n",
    "%load_ext jupyternotify\n",
    "\n",
    "# Def: Read STL 10 images\n",
    "def read_STL10_data():\n",
    "    # read all of the data (STL 10) https://github.com/mttk/STL10\n",
    "    def read_all_images(path_to_data):\n",
    "        \"\"\"\n",
    "        :param path_to_data: the file containing the binary images from the STL-10 dataset\n",
    "        :return: an array containing all the images\n",
    "        \"\"\"\n",
    "\n",
    "        with open(path_to_data, 'rb') as f:\n",
    "            # read whole file in uint8 chunks\n",
    "            everything = np.fromfile(f, dtype=np.uint8)\n",
    "\n",
    "            # We force the data into 3x96x96 chunks, since the\n",
    "            # images are stored in \"column-major order\", meaning\n",
    "            # that \"the first 96*96 values are the red channel,\n",
    "            # the next 96*96 are green, and the last are blue.\"\n",
    "            # The -1 is since the size of the pictures depends\n",
    "            # on the input file, and this way numpy determines\n",
    "            # the size on its own.\n",
    "\n",
    "            images = np.reshape(everything, (-1, 3, 96, 96))\n",
    "\n",
    "            # Now transpose the images into a standard image format\n",
    "            # readable by, for example, matplotlib.imshow\n",
    "            # You might want to comment this line or reverse the shuffle\n",
    "            # if you will use a learning algorithm like CNN, since they like\n",
    "            # their channels separated.\n",
    "            images = np.transpose(images, (0, 3, 2, 1))\n",
    "            return images\n",
    "    def read_labels(path_to_labels):\n",
    "        \"\"\"\n",
    "        :param path_to_labels: path to the binary file containing labels from the STL-10 dataset\n",
    "        :return: an array containing the labels\n",
    "        \"\"\"\n",
    "        with open(path_to_labels, 'rb') as f:\n",
    "            labels = np.fromfile(f, dtype=np.uint8)\n",
    "            return labels\n",
    "    def show_images(data,row=1,col=1):\n",
    "        fig=plt.figure(figsize=(10,10))\n",
    "        columns = col; rows = row\n",
    "        for i in range(1, columns*rows +1):\n",
    "            fig.add_subplot(rows, columns, i)\n",
    "            plt.imshow(data[i-1])\n",
    "        plt.show()\n",
    "\n",
    "    train_images = read_all_images(\"../../../DataSet/STL10/stl10_binary/train_X.bin\") / 255.0\n",
    "    train_labels = read_labels    (\"../../../DataSet/STL10/stl10_binary/train_Y.bin\")\n",
    "    test_images  = read_all_images(\"../../../DataSet/STL10/stl10_binary/test_X.bin\")  / 255.0\n",
    "    test_labels  = read_labels    (\"../../../DataSet/STL10/stl10_binary/test_y.bin\")\n",
    "\n",
    "    label_encoder= OneHotEncoder(sparse=False,categories='auto')\n",
    "    train_labels = label_encoder.fit_transform(train_labels.reshape((-1,1)))\n",
    "    test_labels  = label_encoder.fit_transform(test_labels.reshape((-1,1)))\n",
    "\n",
    "    print(train_images.shape,train_images.max(),train_images.min())\n",
    "    print(train_labels.shape,train_labels.max(),train_labels.min())\n",
    "    print(test_images.shape,test_images.max(),test_images.min())\n",
    "    print(test_labels.shape,test_labels.max(),test_labels.min())\n",
    "    return train_images,train_labels,test_images,test_labels\n",
    "\n",
    "# Def: Read CIFAR 10 images\n",
    "def read_CIFAR10_data():\n",
    "    # ====== miscellaneous =====\n",
    "    # code from: https://github.com/tensorflow/tensorflow/issues/8246\n",
    "    def tf_repeat(tensor, repeats):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "\n",
    "        input: A Tensor. 1-D or higher.\n",
    "        repeats: A list. Number of repeat for each dimension, length must be the same as the number of dimensions in input\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        A Tensor. Has the same type as input. Has the shape of tensor.shape * repeats\n",
    "        \"\"\"\n",
    "        expanded_tensor = tf.expand_dims(tensor, -1)\n",
    "        multiples = [1] + repeats\n",
    "        tiled_tensor = tf.tile(expanded_tensor, multiples = multiples)\n",
    "        repeated_tesnor = tf.reshape(tiled_tensor, tf.shape(tensor) * repeats)\n",
    "        return repeated_tesnor\n",
    "    def unpickle(file):\n",
    "        import pickle\n",
    "        with open(file, 'rb') as fo:\n",
    "            dict = pickle.load(fo, encoding='bytes')\n",
    "        return dict\n",
    "    # ====== miscellaneous =====\n",
    "\n",
    "    # data\n",
    "    PathDicom = \"../../../Dataset/cifar-10-batches-py/\"\n",
    "    lstFilesDCM = []  # create an empty list\n",
    "    for dirName, subdirList, fileList in os.walk(PathDicom):\n",
    "        for filename in fileList:\n",
    "            if not \".html\" in filename.lower() and not  \".meta\" in filename.lower():  # check whether the file's DICOM\n",
    "                lstFilesDCM.append(os.path.join(dirName,filename))\n",
    "\n",
    "    # Read the data traind and Test\n",
    "    batch0 = unpickle(lstFilesDCM[0])\n",
    "    batch1 = unpickle(lstFilesDCM[1])\n",
    "    batch2 = unpickle(lstFilesDCM[2])\n",
    "    batch3 = unpickle(lstFilesDCM[3])\n",
    "    batch4 = unpickle(lstFilesDCM[4])\n",
    "\n",
    "    onehot_encoder = OneHotEncoder(sparse=True)\n",
    "    train_batch = np.vstack((batch0[b'data'],batch1[b'data'],batch2[b'data'],batch3[b'data'],batch4[b'data']))\n",
    "    train_label = np.expand_dims(np.hstack((batch0[b'labels'],batch1[b'labels'],batch2[b'labels'],batch3[b'labels'],batch4[b'labels'])).T,axis=1).astype(np.float64)\n",
    "    train_label = onehot_encoder.fit_transform(train_label).toarray().astype(np.float64)\n",
    "\n",
    "    test_batch = unpickle(lstFilesDCM[5])[b'data']\n",
    "    test_label = np.expand_dims(np.array(unpickle(lstFilesDCM[5])[b'labels']),axis=0).T.astype(np.float64)\n",
    "    test_label = onehot_encoder.fit_transform(test_label).toarray().astype(np.float64)\n",
    "\n",
    "    # reshape data\n",
    "    train_batch = np.reshape(train_batch,(len(train_batch),3,32,32)); test_batch = np.reshape(test_batch,(len(test_batch),3,32,32))\n",
    "    # rotate data\n",
    "    train_batch = np.rot90(np.rot90(train_batch,1,axes=(1,3)),3,axes=(1,2)).astype(np.float64); test_batch = np.rot90(np.rot90(test_batch,1,axes=(1,3)),3,axes=(1,2)).astype(np.float64)\n",
    "    # normalize\n",
    "    train_batch= train_batch/255.0; test_batch = test_batch/255.0\n",
    "\n",
    "    # print out the data shape and the max and min value\n",
    "    print(train_batch.shape,train_batch.max(),train_batch.min())\n",
    "    print(train_label.shape,train_label.max(),train_label.min())\n",
    "    print(test_batch.shape,test_batch.max(),test_batch.min())\n",
    "    print(test_label.shape,test_label.max(),test_label.min())\n",
    "    return train_batch,train_label,test_batch,test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-11T23:30:25.994780Z",
     "start_time": "2019-02-11T23:30:23.583430Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3) 1.0 0.0\n",
      "(50000, 10) 1.0 0.0\n",
      "(10000, 32, 32, 3) 1.0 0.0\n",
      "(10000, 10) 1.0 0.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# read the data\n",
    "train_images,train_labels,test_images,test_labels = read_CIFAR10_data()\n",
    "rand_choice = np.random.choice(len(train_images))\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(train_images[rand_choice])\n",
    "plt.title(str(train_labels[rand_choice]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-11T23:30:37.176082Z",
     "start_time": "2019-02-11T23:30:37.170091Z"
    }
   },
   "outputs": [],
   "source": [
    "# define learning rate\n",
    "num_eps   = 10; num_epoch = 200; learning_rate = 0.0008; batch_size = 50;  beta1,beta2,adam_e = 0.9,0.999,1e-9; print_iter = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-11T23:27:08.731172Z",
     "start_time": "2019-02-11T23:27:08.689308Z"
    },
    "code_folding": [
     7,
     47,
     78
    ]
   },
   "outputs": [],
   "source": [
    "# create the layers\n",
    "def tf_softmax(x):    return tf.nn.softmax(x)\n",
    "def tf_relu(x):       return tf.nn.relu(x)\n",
    "def d_tf_relu(x):     return tf.cast(tf.greater(x,0),tf.float32)\n",
    "def tf_iden(x):       return x\n",
    "def d_tf_iden(x):     return tf.ones_like(x)\n",
    "def tf_sigmoid(x):    return tf.nn.sigmoid(x)\n",
    "def d_tf_sigmoid(x):  return tf_sigmoid(x) * (1.0-tf_sigmoid(x))\n",
    "def tf_softplus(x):   return tf.nn.softplus(x)\n",
    "def d_tf_softplus(x): return tf.nn.sigmoid(x)\n",
    "\n",
    "class FNN():\n",
    "\n",
    "    def __init__(self,inc,outc,act=tf_iden,d_act=d_tf_iden,special_init=False):\n",
    "        if special_init:\n",
    "            interval = np.sqrt(6.0 / (inc + outc + 1.0))\n",
    "            self.w = tf.Variable(tf.random_uniform(shape=(inc, outc),minval=-interval,maxval=interval,dtype=tf.float32,seed=2))\n",
    "        else:\n",
    "            self.w = tf.Variable(tf.random_normal([inc,outc], stddev=0.05,seed=2,dtype=tf.float32))\n",
    "\n",
    "        self.m,self.v = tf.Variable(tf.zeros_like(self.w)),tf.Variable(tf.zeros_like(self.w))\n",
    "        self.act,self.d_act = act,d_act\n",
    "\n",
    "    def getw(self): return self.w\n",
    "    def feedforward(self,input=None):\n",
    "        self.input = input\n",
    "        self.layer = tf.matmul(input,self.w) \n",
    "        self.layerA = self.act(self.layer)\n",
    "        return self.layer,self.layerA\n",
    "\n",
    "    def backprop(self,gradient=None,which_reg=0):\n",
    "        grad_part_1 = gradient\n",
    "        grad_part_2 = self.d_act(self.layer)\n",
    "        grad_part_3 = self.input\n",
    "\n",
    "        grad_middle = grad_part_1 * grad_part_2\n",
    "        grad  = tf.matmul(tf.transpose(grad_part_3),grad_middle)\n",
    "        grad_b= tf.reduce_mean(grad_middle,axis=0)\n",
    "        grad_pass = tf.matmul(grad_middle,tf.transpose(self.w))\n",
    "\n",
    "        update_w = []\n",
    "\n",
    "        # Update the Weight First\n",
    "        update_w.append(tf.assign( self.m,self.m*beta1 + (1-beta1) * (grad)   ))\n",
    "        update_w.append(tf.assign( self.v,self.v*beta2 + (1-beta2) * (grad ** 2)   ))\n",
    "        m_hat = self.m / (1-beta1)\n",
    "        v_hat = self.v / (1-beta2)\n",
    "        adam_middle = m_hat *  learning_rate/(tf.sqrt(v_hat) + adam_e)\n",
    "        update_w.append(tf.assign(self.w,tf.subtract(self.w,adam_middle )))\n",
    "\n",
    "        return grad_pass,update_w\n",
    "class CNN():\n",
    "\n",
    "    def __init__(self,k,inc,out, stddev=0.05,act=tf_relu,d_act=d_tf_relu):\n",
    "        self.w              = tf.Variable(tf.random_normal([k,k,inc,out],stddev=stddev,seed=2,dtype=tf.float32))\n",
    "        self.m,self.v       = tf.Variable(tf.zeros_like(self.w)),tf.Variable(tf.zeros_like(self.w))\n",
    "        self.act,self.d_act = act,d_act\n",
    "\n",
    "    def getw(self): return self.w\n",
    "    def feedforward(self,input,stride=1,padding='VALID'):\n",
    "        self.input  = input\n",
    "        self.layer  = tf.nn.conv2d(input,self.w,strides=[1,stride,stride,1],padding=padding) \n",
    "        self.layerA = self.act(self.layer)\n",
    "        return self.layer, self.layerA\n",
    "    \n",
    "    def backprop(self,gradient,stride=1,padding='VALID'):\n",
    "        grad_part_1 = gradient\n",
    "        grad_part_2 = self.d_act(self.layer)\n",
    "        grad_part_3 = self.input\n",
    "\n",
    "        grad_middle = grad_part_1 * grad_part_2\n",
    "        grad        = tf.nn.conv2d_backprop_filter(input = grad_part_3,filter_sizes = tf.shape(self.w),  out_backprop = grad_middle,strides=[1,stride,stride,1],padding=padding) \n",
    "        grad_pass   = tf.nn.conv2d_backprop_input (input_sizes = tf.shape(self.input),filter= self.w,out_backprop = grad_middle,strides=[1,stride,stride,1],padding=padding)\n",
    "\n",
    "        update_w = []\n",
    "        update_w.append(tf.assign( self.m,self.m*beta1 + (1-beta1) * (grad)   ))\n",
    "        update_w.append(tf.assign( self.v,self.v*beta2 + (1-beta2) * (grad ** 2)   ))\n",
    "        m_hat = self.m / (1-beta1) ; v_hat = self.v / (1-beta2)\n",
    "        adam_middle = m_hat * learning_rate/(tf.sqrt(v_hat) + adam_e)\n",
    "        update_w.append(tf.assign(self.w,tf.subtract(self.w,adam_middle  )))\n",
    "        return grad_pass,grad,update_w\n",
    "    \n",
    "class tf_batch_norm_layer():\n",
    "    \n",
    "    def __init__(self,vector_shape,axis):\n",
    "        self.moving_mean = tf.Variable(tf.zeros(shape=[1,1,1,vector_shape],dtype=tf.float32))\n",
    "        self.moving_vari = tf.Variable(tf.zeros(shape=[1,1,1,vector_shape],dtype=tf.float32))\n",
    "        self.axis        = axis\n",
    "    def feedforward(self,input,training_phase=True,eps = 1e-8):\n",
    "        self.input = input\n",
    "        self.input_size          = self.input.shape\n",
    "        self.batch,self.h,self.w,self.c = self.input_size[0].value,self.input_size[1].value,self.input_size[2].value,self.input_size[3].value\n",
    "\n",
    "        # Training Moving Average Mean         \n",
    "        def training_fn():\n",
    "            self.mean    = tf.reduce_mean(self.input,axis=self.axis ,keepdims=True)\n",
    "            self.var     = tf.reduce_mean(tf.square(self.input-self.mean),axis=self.axis,keepdims=True)\n",
    "            centered_data= (self.input - self.mean)/tf.sqrt(self.var + eps)\n",
    "            \n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean*0.9 + 0.1 * self.mean ))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari*0.9 + 0.1 * self.var  ))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        # Testing Moving Average Mean        \n",
    "        def  testing_fn():\n",
    "            centered_data   = (self.input - self.moving_mean)/tf.sqrt(self.moving_vari + eps)\n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        self.output,update_variable = tf.cond(training_phase,true_fn=training_fn,false_fn=testing_fn)\n",
    "        return self.output,update_variable\n",
    "    def backprop(self,grad,eps = 1e-8):\n",
    "        change_parts = 1.0 /(self.batch * self.h * self.w)\n",
    "        grad_sigma   = tf.reduce_sum( grad *  (self.input-self.mean)     ,axis=self.axis,keepdims=True) * -0.5 * (self.var+eps) ** -1.5\n",
    "        grad_mean    = tf.reduce_sum( grad *  (-1./tf.sqrt(self.var+eps)),axis=self.axis,keepdims=True) + grad_sigma * change_parts * 2.0 * tf.reduce_sum((self.input-self.mean),axis=self.axis,keepdims=True) * -1\n",
    "        grad_x       = grad * 1/(tf.sqrt(self.var+eps)) + grad_sigma * change_parts * 2.0 * (self.input-self.mean) + grad_mean * change_parts\n",
    "        return grad_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-11T23:27:11.800202Z",
     "start_time": "2019-02-11T23:27:11.772277Z"
    },
    "code_folding": [
     1,
     2,
     18,
     28,
     51,
     66
    ]
   },
   "outputs": [],
   "source": [
    "# create the modules\n",
    "class Encoder():\n",
    "    def __init__(self):\n",
    "        self.l1 = CNN(4,3, 64); \n",
    "        self.l2 = CNN(4,64,128); \n",
    "        self.l3 = CNN(4,128,256); \n",
    "        self.l4 = CNN(4,256,512); \n",
    "        self.l5 = FNN(512*20*20,64); \n",
    "        \n",
    "    def feedforward(self,input):\n",
    "        layer1, layer1a = self.l1. feedforward(input,stride=1)\n",
    "        layer2, layer2a = self.l2. feedforward(layer1a,stride=1)\n",
    "        layer3, layer3a = self.l3. feedforward(layer2a,stride=1)\n",
    "        layer4, layer4a = self.l4. feedforward(layer3a,stride=1)\n",
    "        layer5_input    = tf.reshape(layer4a,[batch_size,-1])\n",
    "        layer5, layer5a = self.l5. feedforward(layer5_input)\n",
    "        return layer5a,layer2a\n",
    "    \n",
    "    def backprop(self,grad):\n",
    "        gradient = tf.tile((final_softmax-y)[:,None,None,:],[1,6,6,1])/batch_size\n",
    "        grad6p,grad6w,grad6_up = l6.backprop(gradient,std_value=std_value)\n",
    "        grad5p,grad5w,grad5_up = l5.backprop(grad6p,std_value=std_value)\n",
    "        grad4p,grad4w,grad4_up = l4.backprop(grad5p,stride=2,std_value=std_value)\n",
    "        grad3p,grad3w,grad3_up = l3.backprop(grad4p,stride=2,std_value=std_value)\n",
    "        grad2p,grad2w,grad2_up = l2.backprop(grad3p,stride=2,std_value=std_value)\n",
    "        grad1p,grad1w,grad1_up = l1.backprop(grad2p,stride=2,std_value=std_value)\n",
    "        gradient_update = grad6_up + grad5_up + grad4_up + grad3_up + grad2_up + grad1_up \n",
    "        return None \n",
    "class GlobalDiscriminator():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.c0 = CNN(3,128,64)\n",
    "        self.c1 = CNN(3,64, 32,               act=tf_iden,d_act=tf_iden)\n",
    "        self.l0 = FNN(32*22*22+64,512,act=tf_relu,d_act=d_tf_relu)\n",
    "        self.l1 = FNN(512, 512               ,act=tf_iden,d_act=tf_iden)\n",
    "        self.l2 = FNN(512, 1)\n",
    "\n",
    "    def feedforward(self, encoded, features):\n",
    "        layer1,layer1a = self.c0.feedforward(features)\n",
    "        layer2,layer2a = self.c1.feedforward(layer1a)\n",
    "        \n",
    "        layer2a_reshape= tf.reshape(layer2a,[batch_size,-1])\n",
    "        layer3_input   = tf.concat([layer2a_reshape,encoded],axis=1)\n",
    "        \n",
    "        layer3,layer3a = self.l0.feedforward(layer3_input)\n",
    "        layer4,layer4a = self.l1.feedforward(layer3a)\n",
    "        layer5,layer5a = self.l2.feedforward(layer4a)\n",
    "        return layer5a\n",
    "    \n",
    "    def backprop(self):\n",
    "        return None\n",
    "class LocalDiscriminator():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.l1 = CNN(1,192,512); \n",
    "        self.l2 = CNN(1,512,512); \n",
    "        self.l3 = CNN(1,512,1); \n",
    "        \n",
    "    def feedforward(self,input):\n",
    "        layer1, layer1a = self.l1. feedforward(input,  stride=1)\n",
    "        layer2, layer2a = self.l2. feedforward(layer1a,stride=1)  \n",
    "        layer3, layer3a = self.l3. feedforward(layer2a,stride=1)  \n",
    "        return layer3a\n",
    "    \n",
    "    def backprop(self):\n",
    "        return None  \n",
    "class PriorDiscriminator():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.l1 = FNN(64,1000, act=tf_relu,d_act=d_tf_relu)\n",
    "        self.l2 = FNN(1000,200,act=tf_relu,d_act=d_tf_relu)\n",
    "        self.l3 = FNN(200,1,   act=tf_sigmoid,d_act=d_tf_sigmoid)\n",
    "\n",
    "    def feedforward(self, x):\n",
    "        layer1,layer1a = self.l1.feedforward(x)\n",
    "        layer2,layer2a = self.l2.feedforward(layer1a)\n",
    "        layer3,layer3a = self.l3.feedforward(layer2a)\n",
    "        return layer3a\n",
    "    \n",
    "    def backprop(self,grad):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-11T23:27:13.302690Z",
     "start_time": "2019-02-11T23:27:12.922347Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# build encoder \n",
    "alpha = 0.5 ; beta = 1.0 ; gamma = 0.1\n",
    "x = tf.placeholder(tf.float32,(batch_size,32,32,3))\n",
    "y = tf.placeholder(tf.float32,(batch_size,10))\n",
    "\n",
    "# 0. create models\n",
    "MI_encoder = Encoder()\n",
    "Global_Dis = GlobalDiscriminator()\n",
    "Local_Dis  = LocalDiscriminator()\n",
    "Prior_Dis  = PriorDiscriminator()\n",
    "\n",
    "# 1. generate encoding\n",
    "encoded,features = MI_encoder.feedforward(x)\n",
    "\n",
    "# 2. create prime variables\n",
    "features_prime = tf.transpose(features,[0,2,1,3])\n",
    "encoded_expand = tf.tile(encoded[:,None,None,:],[1,26,26,1])\n",
    "encoded_M        = tf.concat([features,encoded_expand],3)\n",
    "encoded_M_prime  = tf.concat([features_prime,encoded_expand],3)\n",
    "\n",
    "# 3. global loss\n",
    "Ej  = -tf.reduce_mean(tf_softplus(-Global_Dis.feedforward(encoded,features)))\n",
    "Em  = tf.reduce_mean(tf_softplus (Global_Dis.feedforward(encoded,features_prime)))\n",
    "GLOBAL_LOSS = (Em - Ej) * alpha \n",
    "\n",
    "# 4.local loss\n",
    "Ej  = -tf.reduce_mean(tf_softplus(-Local_Dis.feedforward(encoded_M)))\n",
    "Em  = tf.reduce_mean(tf_softplus (Local_Dis.feedforward(encoded_M_prime)))\n",
    "LOCAL_LOSS = (Em - Ej) * beta \n",
    "\n",
    "# 5. Prior\n",
    "prior = tf.random_uniform(tf.shape(encoded))\n",
    "term_a= tf.reduce_mean(tf.log(Prior_Dis.feedforward(prior)))\n",
    "term_b= tf.reduce_mean(tf.log(1.0 - Prior_Dis.feedforward(encoded)))\n",
    "PRIOR = - (term_a + term_b) * gamma\n",
    "\n",
    "TOTAL_LOSS = GLOBAL_LOSS + LOCAL_LOSS + PRIOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-11T23:30:42.690052Z",
     "start_time": "2019-02-11T23:30:41.215736Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n",
      "1000\n",
      "1050\n",
      "1100\n",
      "1150\n",
      "1200\n",
      "1250\n",
      "1300\n",
      "1350\n",
      "1400\n",
      "1450\n",
      "1500\n",
      "1550\n",
      "1600\n",
      "1650\n",
      "1700\n",
      "1750\n",
      "1800\n",
      "1850\n",
      "1900\n",
      "1950\n",
      "2000\n",
      "2050\n",
      "2100\n",
      "2150\n",
      "2200\n",
      "2250\n",
      "2300\n",
      "2350\n",
      "2400\n",
      "2450\n",
      "2500\n",
      "2550\n",
      "2600\n",
      "2650\n",
      "2700\n",
      "2750\n",
      "2800\n",
      "2850\n",
      "2900\n",
      "2950\n",
      "3000\n",
      "3050\n",
      "3100\n",
      "3150\n",
      "3200\n",
      "3250\n",
      "3300\n",
      "3350\n",
      "3400\n",
      "3450\n",
      "3500\n",
      "3550\n",
      "3600\n",
      "3650\n",
      "3700\n",
      "3750\n",
      "3800\n",
      "3850\n",
      "3900\n",
      "3950\n",
      "4000\n",
      "4050\n",
      "4100\n",
      "4150\n",
      "4200\n",
      "4250\n",
      "4300\n",
      "4350\n",
      "4400\n",
      "4450\n",
      "4500\n",
      "4550\n",
      "4600\n",
      "4650\n",
      "4700\n",
      "4750\n",
      "4800\n",
      "4850\n",
      "4900\n",
      "4950\n",
      "5000\n",
      "5050\n",
      "5100\n",
      "5150\n",
      "5200\n",
      "5250\n",
      "5300\n",
      "5350\n",
      "5400\n",
      "5450\n",
      "5500\n",
      "5550\n",
      "5600\n",
      "5650\n",
      "5700\n",
      "5750\n",
      "5800\n",
      "5850\n",
      "5900\n",
      "5950\n",
      "6000\n",
      "6050\n",
      "6100\n",
      "6150\n",
      "6200\n",
      "6250\n",
      "6300\n",
      "6350\n",
      "6400\n",
      "6450\n",
      "6500\n",
      "6550\n",
      "6600\n",
      "6650\n",
      "6700\n",
      "6750\n",
      "6800\n",
      "6850\n",
      "6900\n",
      "6950\n",
      "7000\n",
      "7050\n",
      "7100\n",
      "7150\n",
      "7200\n",
      "7250\n",
      "7300\n",
      "7350\n",
      "7400\n",
      "7450\n",
      "7500\n",
      "7550\n",
      "7600\n",
      "7650\n",
      "7700\n",
      "7750\n",
      "7800\n",
      "7850\n",
      "7900\n",
      "7950\n",
      "8000\n",
      "8050\n",
      "8100\n",
      "8150\n",
      "8200\n",
      "8250\n",
      "8300\n",
      "8350\n",
      "8400\n",
      "8450\n",
      "8500\n",
      "8550\n",
      "8600\n",
      "8650\n",
      "8700\n",
      "8750\n",
      "8800\n",
      "8850\n",
      "8900\n",
      "8950\n",
      "9000\n",
      "9050\n",
      "9100\n",
      "9150\n",
      "9200\n",
      "9250\n",
      "9300\n",
      "9350\n",
      "9400\n",
      "9450\n",
      "9500\n",
      "9550\n",
      "9600\n",
      "9650\n",
      "9700\n",
      "9750\n",
      "9800\n",
      "9850\n",
      "9900\n",
      "9950\n",
      "10000\n",
      "10050\n",
      "10100\n",
      "10150\n",
      "10200\n",
      "10250\n",
      "10300\n",
      "10350\n",
      "10400\n",
      "10450\n",
      "10500\n",
      "10550\n",
      "10600\n",
      "10650\n",
      "10700\n",
      "10750\n",
      "10800\n",
      "10850\n",
      "10900\n",
      "10950\n",
      "11000\n",
      "11050\n",
      "11100\n",
      "11150\n",
      "11200\n",
      "11250\n",
      "11300\n",
      "11350\n",
      "11400\n",
      "11450\n",
      "11500\n",
      "11550\n",
      "11600\n",
      "11650\n",
      "11700\n",
      "11750\n",
      "11800\n",
      "11850\n",
      "11900\n",
      "11950\n",
      "12000\n",
      "12050\n",
      "12100\n",
      "12150\n",
      "12200\n",
      "12250\n",
      "12300\n",
      "12350\n",
      "12400\n",
      "12450\n",
      "12500\n",
      "12550\n",
      "12600\n",
      "12650\n",
      "12700\n",
      "12750\n",
      "12800\n",
      "12850\n",
      "12900\n",
      "12950\n",
      "13000\n",
      "13050\n",
      "13100\n",
      "13150\n",
      "13200\n",
      "13250\n",
      "13300\n",
      "13350\n",
      "13400\n",
      "13450\n",
      "13500\n",
      "13550\n",
      "13600\n",
      "13650\n",
      "13700\n",
      "13750\n",
      "13800\n",
      "13850\n",
      "13900\n",
      "13950\n",
      "14000\n",
      "14050\n",
      "14100\n",
      "14150\n",
      "14200\n",
      "14250\n",
      "14300\n",
      "14350\n",
      "14400\n",
      "14450\n",
      "14500\n",
      "14550\n",
      "14600\n",
      "14650\n",
      "14700\n",
      "14750\n",
      "14800\n",
      "14850\n",
      "14900\n",
      "14950\n",
      "15000\n",
      "15050\n",
      "15100\n",
      "15150\n",
      "15200\n",
      "15250\n",
      "15300\n",
      "15350\n",
      "15400\n",
      "15450\n",
      "15500\n",
      "15550\n",
      "15600\n",
      "15650\n",
      "15700\n",
      "15750\n",
      "15800\n",
      "15850\n",
      "15900\n",
      "15950\n",
      "16000\n",
      "16050\n",
      "16100\n",
      "16150\n",
      "16200\n",
      "16250\n",
      "16300\n",
      "16350\n",
      "16400\n",
      "16450\n",
      "16500\n",
      "16550\n",
      "16600\n",
      "16650\n",
      "16700\n",
      "16750\n",
      "16800\n",
      "16850\n",
      "16900\n",
      "16950\n",
      "17000\n",
      "17050\n",
      "17100\n",
      "17150\n",
      "17200\n",
      "17250\n",
      "17300\n",
      "17350\n",
      "17400\n",
      "17450\n",
      "17500\n",
      "17550\n",
      "17600\n",
      "17650\n",
      "17700\n",
      "17750\n",
      "17800\n",
      "17850\n",
      "17900\n",
      "17950\n",
      "18000\n",
      "18050\n",
      "18100\n",
      "18150\n",
      "18200\n",
      "18250\n",
      "18300\n",
      "18350\n",
      "18400\n",
      "18450\n",
      "18500\n",
      "18550\n",
      "18600\n",
      "18650\n",
      "18700\n",
      "18750\n",
      "18800\n",
      "18850\n",
      "18900\n",
      "18950\n",
      "19000\n",
      "19050\n",
      "19100\n",
      "19150\n",
      "19200\n",
      "19250\n",
      "19300\n",
      "19350\n",
      "19400\n",
      "19450\n",
      "19500\n",
      "19550\n",
      "19600\n",
      "19650\n",
      "19700\n",
      "19750\n",
      "19800\n",
      "19850\n",
      "19900\n",
      "19950\n",
      "20000\n",
      "20050\n",
      "20100\n",
      "20150\n",
      "20200\n",
      "20250\n",
      "20300\n",
      "20350\n",
      "20400\n",
      "20450\n",
      "20500\n",
      "20550\n",
      "20600\n",
      "20650\n",
      "20700\n",
      "20750\n",
      "20800\n",
      "20850\n",
      "20900\n",
      "20950\n",
      "21000\n",
      "21050\n",
      "21100\n",
      "21150\n",
      "21200\n",
      "21250\n",
      "21300\n",
      "21350\n",
      "21400\n",
      "21450\n",
      "21500\n",
      "21550\n",
      "21600\n",
      "21650\n",
      "21700\n",
      "21750\n",
      "21800\n",
      "21850\n",
      "21900\n",
      "21950\n",
      "22000\n",
      "22050\n",
      "22100\n",
      "22150\n",
      "22200\n",
      "22250\n",
      "22300\n",
      "22350\n",
      "22400\n",
      "22450\n",
      "22500\n",
      "22550\n",
      "22600\n",
      "22650\n",
      "22700\n",
      "22750\n",
      "22800\n",
      "22850\n",
      "22900\n",
      "22950\n",
      "23000\n",
      "23050\n",
      "23100\n",
      "23150\n",
      "23200\n",
      "23250\n",
      "23300\n",
      "23350\n",
      "23400\n",
      "23450\n",
      "23500\n",
      "23550\n",
      "23600\n",
      "23650\n",
      "23700\n",
      "23750\n",
      "23800\n",
      "23850\n",
      "23900\n",
      "23950\n",
      "24000\n",
      "24050\n",
      "24100\n",
      "24150\n",
      "24200\n",
      "24250\n",
      "24300\n",
      "24350\n",
      "24400\n",
      "24450\n",
      "24500\n",
      "24550\n",
      "24600\n",
      "24650\n",
      "24700\n",
      "24750\n",
      "24800\n",
      "24850\n",
      "24900\n",
      "24950\n",
      "25000\n",
      "25050\n",
      "25100\n",
      "25150\n",
      "25200\n",
      "25250\n",
      "25300\n",
      "25350\n",
      "25400\n",
      "25450\n",
      "25500\n",
      "25550\n",
      "25600\n",
      "25650\n",
      "25700\n",
      "25750\n",
      "25800\n",
      "25850\n",
      "25900\n",
      "25950\n",
      "26000\n",
      "26050\n",
      "26100\n",
      "26150\n",
      "26200\n",
      "26250\n",
      "26300\n",
      "26350\n",
      "26400\n",
      "26450\n",
      "26500\n",
      "26550\n",
      "26600\n",
      "26650\n",
      "26700\n",
      "26750\n",
      "26800\n",
      "26850\n",
      "26900\n",
      "26950\n",
      "27000\n",
      "27050\n",
      "27100\n",
      "27150\n",
      "27200\n",
      "27250\n",
      "27300\n",
      "27350\n",
      "27400\n",
      "27450\n",
      "27500\n",
      "27550\n",
      "27600\n",
      "27650\n",
      "27700\n",
      "27750\n",
      "27800\n",
      "27850\n",
      "27900\n",
      "27950\n",
      "28000\n",
      "28050\n",
      "28100\n",
      "28150\n",
      "28200\n",
      "28250\n",
      "28300\n",
      "28350\n",
      "28400\n",
      "28450\n",
      "28500\n",
      "28550\n",
      "28600\n",
      "28650\n",
      "28700\n",
      "28750\n",
      "28800\n",
      "28850\n",
      "28900\n",
      "28950\n",
      "29000\n",
      "29050\n",
      "29100\n",
      "29150\n",
      "29200\n",
      "29250\n",
      "29300\n",
      "29350\n",
      "29400\n",
      "29450\n",
      "29500\n",
      "29550\n",
      "29600\n",
      "29650\n",
      "29700\n",
      "29750\n",
      "29800\n",
      "29850\n",
      "29900\n",
      "29950\n",
      "30000\n",
      "30050\n",
      "30100\n",
      "30150\n",
      "30200\n",
      "30250\n",
      "30300\n",
      "30350\n",
      "30400\n",
      "30450\n",
      "30500\n",
      "30550\n",
      "30600\n",
      "30650\n",
      "30700\n",
      "30750\n",
      "30800\n",
      "30850\n",
      "30900\n",
      "30950\n",
      "31000\n",
      "31050\n",
      "31100\n",
      "31150\n",
      "31200\n",
      "31250\n",
      "31300\n",
      "31350\n",
      "31400\n",
      "31450\n",
      "31500\n",
      "31550\n",
      "31600\n",
      "31650\n",
      "31700\n",
      "31750\n",
      "31800\n",
      "31850\n",
      "31900\n",
      "31950\n",
      "32000\n",
      "32050\n",
      "32100\n",
      "32150\n",
      "32200\n",
      "32250\n",
      "32300\n",
      "32350\n",
      "32400\n",
      "32450\n",
      "32500\n",
      "32550\n",
      "32600\n",
      "32650\n",
      "32700\n",
      "32750\n",
      "32800\n",
      "32850\n",
      "32900\n",
      "32950\n",
      "33000\n",
      "33050\n",
      "33100\n",
      "33150\n",
      "33200\n",
      "33250\n",
      "33300\n",
      "33350\n",
      "33400\n",
      "33450\n",
      "33500\n",
      "33550\n",
      "33600\n",
      "33650\n",
      "33700\n",
      "33750\n",
      "33800\n",
      "33850\n",
      "33900\n",
      "33950\n",
      "34000\n",
      "34050\n",
      "34100\n",
      "34150\n",
      "34200\n",
      "34250\n",
      "34300\n",
      "34350\n",
      "34400\n",
      "34450\n",
      "34500\n",
      "34550\n",
      "34600\n",
      "34650\n",
      "34700\n",
      "34750\n",
      "34800\n",
      "34850\n",
      "34900\n",
      "34950\n",
      "35000\n",
      "35050\n",
      "35100\n",
      "35150\n",
      "35200\n",
      "35250\n",
      "35300\n",
      "35350\n",
      "35400\n",
      "35450\n",
      "35500\n",
      "35550\n",
      "35600\n",
      "35650\n",
      "35700\n",
      "35750\n",
      "35800\n",
      "35850\n",
      "35900\n",
      "35950\n",
      "36000\n",
      "36050\n",
      "36100\n",
      "36150\n",
      "36200\n",
      "36250\n",
      "36300\n",
      "36350\n",
      "36400\n",
      "36450\n",
      "36500\n",
      "36550\n",
      "36600\n",
      "36650\n",
      "36700\n",
      "36750\n",
      "36800\n",
      "36850\n",
      "36900\n",
      "36950\n",
      "37000\n",
      "37050\n",
      "37100\n",
      "37150\n",
      "37200\n",
      "37250\n",
      "37300\n",
      "37350\n",
      "37400\n",
      "37450\n",
      "37500\n",
      "37550\n",
      "37600\n",
      "37650\n",
      "37700\n",
      "37750\n",
      "37800\n",
      "37850\n",
      "37900\n",
      "37950\n",
      "38000\n",
      "38050\n",
      "38100\n",
      "38150\n",
      "38200\n",
      "38250\n",
      "38300\n",
      "38350\n",
      "38400\n",
      "38450\n",
      "38500\n",
      "38550\n",
      "38600\n",
      "38650\n",
      "38700\n",
      "38750\n",
      "38800\n",
      "38850\n",
      "38900\n",
      "38950\n",
      "39000\n",
      "39050\n",
      "39100\n",
      "39150\n",
      "39200\n",
      "39250\n",
      "39300\n",
      "39350\n",
      "39400\n",
      "39450\n",
      "39500\n",
      "39550\n",
      "39600\n",
      "39650\n",
      "39700\n",
      "39750\n",
      "39800\n",
      "39850\n",
      "39900\n",
      "39950\n",
      "40000\n",
      "40050\n",
      "40100\n",
      "40150\n",
      "40200\n",
      "40250\n",
      "40300\n",
      "40350\n",
      "40400\n",
      "40450\n",
      "40500\n",
      "40550\n",
      "40600\n",
      "40650\n",
      "40700\n",
      "40750\n",
      "40800\n",
      "40850\n",
      "40900\n",
      "40950\n",
      "41000\n",
      "41050\n",
      "41100\n",
      "41150\n",
      "41200\n",
      "41250\n",
      "41300\n",
      "41350\n",
      "41400\n",
      "41450\n",
      "41500\n",
      "41550\n",
      "41600\n",
      "41650\n",
      "41700\n",
      "41750\n",
      "41800\n",
      "41850\n",
      "41900\n",
      "41950\n",
      "42000\n",
      "42050\n",
      "42100\n",
      "42150\n",
      "42200\n",
      "42250\n",
      "42300\n",
      "42350\n",
      "42400\n",
      "42450\n",
      "42500\n",
      "42550\n",
      "42600\n",
      "42650\n",
      "42700\n",
      "42750\n",
      "42800\n",
      "42850\n",
      "42900\n",
      "42950\n",
      "43000\n",
      "43050\n",
      "43100\n",
      "43150\n",
      "43200\n",
      "43250\n",
      "43300\n",
      "43350\n",
      "43400\n",
      "43450\n",
      "43500\n",
      "43550\n",
      "43600\n",
      "43650\n",
      "43700\n",
      "43750\n",
      "43800\n",
      "43850\n",
      "43900\n",
      "43950\n",
      "44000\n",
      "44050\n",
      "44100\n",
      "44150\n",
      "44200\n",
      "44250\n",
      "44300\n",
      "44350\n",
      "44400\n",
      "44450\n",
      "44500\n",
      "44550\n",
      "44600\n",
      "44650\n",
      "44700\n",
      "44750\n",
      "44800\n",
      "44850\n",
      "44900\n",
      "44950\n",
      "45000\n",
      "45050\n",
      "45100\n",
      "45150\n",
      "45200\n",
      "45250\n",
      "45300\n",
      "45350\n",
      "45400\n",
      "45450\n",
      "45500\n",
      "45550\n",
      "45600\n",
      "45650\n",
      "45700\n",
      "45750\n",
      "45800\n",
      "45850\n",
      "45900\n",
      "45950\n",
      "46000\n",
      "46050\n",
      "46100\n",
      "46150\n",
      "46200\n",
      "46250\n",
      "46300\n",
      "46350\n",
      "46400\n",
      "46450\n",
      "46500\n",
      "46550\n",
      "46600\n",
      "46650\n",
      "46700\n",
      "46750\n",
      "46800\n",
      "46850\n",
      "46900\n",
      "46950\n",
      "47000\n",
      "47050\n",
      "47100\n",
      "47150\n",
      "47200\n",
      "47250\n",
      "47300\n",
      "47350\n",
      "47400\n",
      "47450\n",
      "47500\n",
      "47550\n",
      "47600\n",
      "47650\n",
      "47700\n",
      "47750\n",
      "47800\n",
      "47850\n",
      "47900\n",
      "47950\n",
      "48000\n",
      "48050\n",
      "48100\n",
      "48150\n",
      "48200\n",
      "48250\n",
      "48300\n",
      "48350\n",
      "48400\n",
      "48450\n",
      "48500\n",
      "48550\n",
      "48600\n",
      "48650\n",
      "48700\n",
      "48750\n",
      "48800\n",
      "48850\n",
      "48900\n",
      "48950\n",
      "49000\n",
      "49050\n",
      "49100\n",
      "49150\n",
      "49200\n",
      "49250\n",
      "49300\n",
      "49350\n",
      "49400\n",
      "49450\n",
      "49500\n",
      "49550\n",
      "49600\n",
      "49650\n",
      "49700\n",
      "49750\n",
      "49800\n",
      "49850\n",
      "49900\n",
      "49950\n"
     ]
    }
   ],
   "source": [
    "# loop\n",
    "for epoch in range(1):\n",
    "\n",
    "    # train_batch,train_label = shuffle(train_batch,train_label)\n",
    "\n",
    "    for batch_size_index in range(0,len(train_images),batch_size):\n",
    "        current_batch       = train_images[batch_size_index:batch_size_index+batch_size]\n",
    "        current_batch_label = train_labels[batch_size_index:batch_size_index+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# import lib\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import nn as nn\n",
    "\n",
    "import torch\n",
    "from torchvision.datasets.cifar import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import statistics as stats\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-11T22:56:47.004757Z",
     "start_time": "2019-02-11T22:56:41.146819Z"
    },
    "code_folding": [
     0,
     20,
     21,
     37,
     48,
     59,
     75,
     112
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                                | 0/781 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 64, 1, 1])\n",
      "torch.Size([64, 64, 26, 26])\n",
      "torch.Size([64, 192, 26, 26])\n",
      "torch.Size([64, 192, 26, 26])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.c0 = nn.Conv2d(3, 64, kernel_size=4, stride=1)\n",
    "        self.c1 = nn.Conv2d(64, 128, kernel_size=4, stride=1)\n",
    "        self.c2 = nn.Conv2d(128, 256, kernel_size=4, stride=1)\n",
    "        self.c3 = nn.Conv2d(256, 512, kernel_size=4, stride=1)\n",
    "        self.l1 = nn.Linear(512*20*20, 64)\n",
    "\n",
    "        self.b1 = nn.BatchNorm2d(128)\n",
    "        self.b2 = nn.BatchNorm2d(256)\n",
    "        self.b3 = nn.BatchNorm2d(512)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.c0(x))                      # (64, 64, 29, 29)\n",
    "        features = F.relu(self.b1(self.c1(h)))      # (64, 128, 26, 26)\n",
    "        h = F.relu(self.b2(self.c2(features)))      # (64, 256, 23, 23)\n",
    "        h = F.relu(self.b3(self.c3(h)))             # (64, 512, 20, 20)\n",
    "        encoded = self.l1(h.view(x.shape[0], -1))   # (batch,64)\n",
    "        return encoded, features  \n",
    "class GlobalDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.c0 = nn.Conv2d(128, 64, kernel_size=3) # (64, 64, 24, 24)\n",
    "        self.c1 = nn.Conv2d(64, 32,  kernel_size=3)  # (64, 32, 22, 22)\n",
    "        self.l0 = nn.Linear(32 * 22 * 22 + 64, 512) # (64, 512)\n",
    "        self.l1 = nn.Linear(512, 512)               # (512, 512)\n",
    "        self.l2 = nn.Linear(512, 1)                 # (512, 1)\n",
    "\n",
    "    def forward(self, y, M):\n",
    "        h = F.relu(self.c0(M))\n",
    "        h = self.c1(h)\n",
    "        h = h.view(y.shape[0], -1)\n",
    "        h = torch.cat((y, h), dim=1)\n",
    "        h = F.relu(self.l0(h))\n",
    "        h = F.relu(self.l1(h))\n",
    "        return self.l2(h)  \n",
    "class LocalDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.c0 = nn.Conv2d(192, 512, kernel_size=1)\n",
    "        self.c1 = nn.Conv2d(512, 512, kernel_size=1)\n",
    "        self.c2 = nn.Conv2d(512, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.c0(x))\n",
    "        h = F.relu(self.c1(h))\n",
    "        return self.c2(h)  \n",
    "class PriorDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l0 = nn.Linear(64, 1000)\n",
    "        self.l1 = nn.Linear(1000, 200)\n",
    "        self.l2 = nn.Linear(200, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.l0(x))\n",
    "        h = F.relu(self.l1(h))\n",
    "        return torch.sigmoid(self.l2(h))\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(64, 15)\n",
    "        self.bn1 = nn.BatchNorm1d(15)\n",
    "        self.l2 = nn.Linear(15, 10)\n",
    "        self.bn2 = nn.BatchNorm1d(10)\n",
    "        self.l3 = nn.Linear(10, 10)\n",
    "        self.bn3 = nn.BatchNorm1d(10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded, _ = x[0], x[1]\n",
    "        clazz = F.relu(self.bn1(self.l1(encoded)))\n",
    "        clazz = F.relu(self.bn2(self.l2(clazz)))\n",
    "        clazz = F.softmax(self.bn3(self.l3(clazz)), dim=1)\n",
    "        return clazz\n",
    "class DeepInfoMaxLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.5, beta=1.0, gamma=0.1):\n",
    "        super().__init__()\n",
    "        self.global_d = GlobalDiscriminator()\n",
    "        self.local_d = LocalDiscriminator()\n",
    "        self.prior_d = PriorDiscriminator()\n",
    "        self.alpha   = alpha\n",
    "        self.beta    = beta\n",
    "        self.gamma   = gamma\n",
    "\n",
    "    def forward(self, y, M, M_prime):\n",
    "\n",
    "        # see appendix 1A of https://arxiv.org/pdf/1808.06670.pdf\n",
    "        \n",
    "        # CREATE\n",
    "        y_exp = y.unsqueeze(-1).unsqueeze(-1)\n",
    "        y_exp = y_exp.expand(-1, -1, 26, 26)\n",
    "        y_M = torch.cat((M, y_exp), dim=1)\n",
    "        y_M_prime = torch.cat((M_prime, y_exp), dim=1)\n",
    "\n",
    "        # Local          \n",
    "        Ej = -F.softplus(-self.local_d(y_M)).mean()\n",
    "        Em = F.softplus(self.local_d(y_M_prime)).mean()\n",
    "        LOCAL = (Em - Ej) * self.beta\n",
    "       \n",
    "        # Global         \n",
    "        Ej = -F.softplus(-self.global_d(y, M)).mean()\n",
    "        Em = F.softplus(self.global_d(y, M_prime)).mean()\n",
    "        GLOBAL = (Em - Ej) * self.alpha\n",
    "        \n",
    "        # Prior\n",
    "        prior = torch.rand_like(y)\n",
    "        term_a = torch.log(self.prior_d(prior)).mean()\n",
    "        term_b = torch.log(1.0 - self.prior_d(y)).mean()\n",
    "        PRIOR = - (term_a + term_b) * self.gamma\n",
    "\n",
    "        return LOCAL + GLOBAL + PRIOR\n",
    "class DeepInfoAsLatent(nn.Module):\n",
    "    def __init__(self, run, epoch):\n",
    "        super().__init__()\n",
    "        model_path = Path(r'c:/data/deepinfomax/models') / Path(str(run)) / Path('encoder' + str(epoch) + '.wgt')\n",
    "        self.encoder = Encoder()\n",
    "        self.encoder.load_state_dict(torch.load(str(model_path)))\n",
    "        self.classifier = Classifier()\n",
    "\n",
    "    def forward(self, x):\n",
    "        z, features = self.encoder(x)\n",
    "        z = z.detach()\n",
    "        return self.classifier((z, features))\n",
    "# device = torch.device('cpu')\n",
    "batch_size = 64\n",
    "\n",
    "# image size 3, 32, 32 batch size must be an even numbershuffle must be True\n",
    "cifar_10_train_dt = CIFAR10(r'c:\\data\\tv',  download=True, transform=ToTensor())\n",
    "cifar_10_train_l  = DataLoader(cifar_10_train_dt, batch_size=batch_size, shuffle=True, drop_last=True,pin_memory=torch.cuda.is_available())\n",
    "\n",
    "encoder    = Encoder().to(device)\n",
    "loss_fn    = DeepInfoMaxLoss().to(device)\n",
    "optim      = Adam(encoder.parameters(), lr=1e-4)\n",
    "loss_optim = Adam(loss_fn.parameters(), lr=1e-4)\n",
    "\n",
    "for epoch in range(100):\n",
    "    batch = tqdm(cifar_10_train_l, total=len(cifar_10_train_dt) // batch_size)\n",
    "    train_loss = []\n",
    "    \n",
    "    for x, target in batch:\n",
    "        x = x.to(device)\n",
    "\n",
    "        optim.zero_grad(); loss_optim.zero_grad()\n",
    "        y, M = encoder(x)\n",
    "        # y - > (64, 128, 26, 26)\n",
    "        # M - > (batch,64)\n",
    "        \n",
    "        # rotate images to create pairs for comparison (ROTATING)\n",
    "        M_prime = torch.cat((M[1:], M[0].unsqueeze(0)), dim=0)\n",
    "        loss = loss_fn(y, M, M_prime) # ()\n",
    "        sys.exit()\n",
    "        \n",
    "        train_loss.append(loss.item())\n",
    "        batch.set_description(str(epoch) + ' Loss: ' + str(stats.mean(train_loss[-20:])))\n",
    "        loss.backward()\n",
    "        optim.step(); loss_optim.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-07T03:12:26.562738Z",
     "start_time": "2019-02-07T03:12:19.110Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = tqdm(cifar_10_train_l, total=len(cifar_10_train_dt) // batch_size)\n",
    "for x, target in batch:\n",
    "    temp = np.swapaxes(np.swapaxes(x.numpy(),1,3),2,1)\n",
    "    plt.imshow(temp[0])\n",
    "    plt.show()\n",
    "    print(temp.shape)\n",
    "    print(target.numpy().shape)\n",
    "    sys.exit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
