{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-12T02:26:45.460Z"
    },
    "code_folding": [
     0,
     32,
     34,
     61,
     69,
     93,
     96
    ]
   },
   "outputs": [],
   "source": [
    "# import Library and some random image data set\n",
    "import tensorflow as tf\n",
    "import numpy      as np\n",
    "import seaborn    as sns \n",
    "import pandas     as pd\n",
    "import os,sys\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "np.random.seed(78); tf.set_random_seed(78)\n",
    "\n",
    "# get some of the STL data set\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from skimage import util \n",
    "from skimage.transform import resize\n",
    "from skimage.io import imread\n",
    "import warnings\n",
    "from numpy import inf\n",
    "\n",
    "from scipy.stats import kurtosis,skew\n",
    "\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import gc\n",
    "from IPython.display import display, clear_output\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from matplotlib import animation\n",
    "%load_ext jupyternotify\n",
    "\n",
    "# Def: Read STL 10 images\n",
    "def read_STL10_data():\n",
    "    # read all of the data (STL 10) https://github.com/mttk/STL10\n",
    "    def read_all_images(path_to_data):\n",
    "        \"\"\"\n",
    "        :param path_to_data: the file containing the binary images from the STL-10 dataset\n",
    "        :return: an array containing all the images\n",
    "        \"\"\"\n",
    "\n",
    "        with open(path_to_data, 'rb') as f:\n",
    "            # read whole file in uint8 chunks\n",
    "            everything = np.fromfile(f, dtype=np.uint8)\n",
    "\n",
    "            # We force the data into 3x96x96 chunks, since the\n",
    "            # images are stored in \"column-major order\", meaning\n",
    "            # that \"the first 96*96 values are the red channel,\n",
    "            # the next 96*96 are green, and the last are blue.\"\n",
    "            # The -1 is since the size of the pictures depends\n",
    "            # on the input file, and this way numpy determines\n",
    "            # the size on its own.\n",
    "\n",
    "            images = np.reshape(everything, (-1, 3, 96, 96))\n",
    "\n",
    "            # Now transpose the images into a standard image format\n",
    "            # readable by, for example, matplotlib.imshow\n",
    "            # You might want to comment this line or reverse the shuffle\n",
    "            # if you will use a learning algorithm like CNN, since they like\n",
    "            # their channels separated.\n",
    "            images = np.transpose(images, (0, 3, 2, 1))\n",
    "            return images\n",
    "    def read_labels(path_to_labels):\n",
    "        \"\"\"\n",
    "        :param path_to_labels: path to the binary file containing labels from the STL-10 dataset\n",
    "        :return: an array containing the labels\n",
    "        \"\"\"\n",
    "        with open(path_to_labels, 'rb') as f:\n",
    "            labels = np.fromfile(f, dtype=np.uint8)\n",
    "            return labels\n",
    "    def show_images(data,row=1,col=1):\n",
    "        fig=plt.figure(figsize=(10,10))\n",
    "        columns = col; rows = row\n",
    "        for i in range(1, columns*rows +1):\n",
    "            fig.add_subplot(rows, columns, i)\n",
    "            plt.imshow(data[i-1])\n",
    "        plt.show()\n",
    "\n",
    "    train_images = read_all_images(\"../../../DataSet/STL10/stl10_binary/train_X.bin\") / 255.0\n",
    "    train_labels = read_labels    (\"../../../DataSet/STL10/stl10_binary/train_Y.bin\")\n",
    "    test_images  = read_all_images(\"../../../DataSet/STL10/stl10_binary/test_X.bin\")  / 255.0\n",
    "    test_labels  = read_labels    (\"../../../DataSet/STL10/stl10_binary/test_y.bin\")\n",
    "\n",
    "    label_encoder= OneHotEncoder(sparse=False,categories='auto')\n",
    "    train_labels = label_encoder.fit_transform(train_labels.reshape((-1,1)))\n",
    "    test_labels  = label_encoder.fit_transform(test_labels.reshape((-1,1)))\n",
    "\n",
    "    print(train_images.shape,train_images.max(),train_images.min())\n",
    "    print(train_labels.shape,train_labels.max(),train_labels.min())\n",
    "    print(test_images.shape,test_images.max(),test_images.min())\n",
    "    print(test_labels.shape,test_labels.max(),test_labels.min())\n",
    "    return train_images,train_labels,test_images,test_labels\n",
    "\n",
    "# Def: Read CIFAR 10 images\n",
    "def read_CIFAR10_data():\n",
    "    # ====== miscellaneous =====\n",
    "    # code from: https://github.com/tensorflow/tensorflow/issues/8246\n",
    "    def tf_repeat(tensor, repeats):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "\n",
    "        input: A Tensor. 1-D or higher.\n",
    "        repeats: A list. Number of repeat for each dimension, length must be the same as the number of dimensions in input\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        A Tensor. Has the same type as input. Has the shape of tensor.shape * repeats\n",
    "        \"\"\"\n",
    "        expanded_tensor = tf.expand_dims(tensor, -1)\n",
    "        multiples = [1] + repeats\n",
    "        tiled_tensor = tf.tile(expanded_tensor, multiples = multiples)\n",
    "        repeated_tesnor = tf.reshape(tiled_tensor, tf.shape(tensor) * repeats)\n",
    "        return repeated_tesnor\n",
    "    def unpickle(file):\n",
    "        import pickle\n",
    "        with open(file, 'rb') as fo:\n",
    "            dict = pickle.load(fo, encoding='bytes')\n",
    "        return dict\n",
    "    # ====== miscellaneous =====\n",
    "\n",
    "    # data\n",
    "    PathDicom = \"../../../Dataset/cifar-10-batches-py/\"\n",
    "    lstFilesDCM = []  # create an empty list\n",
    "    for dirName, subdirList, fileList in os.walk(PathDicom):\n",
    "        for filename in fileList:\n",
    "            if not \".html\" in filename.lower() and not  \".meta\" in filename.lower():  # check whether the file's DICOM\n",
    "                lstFilesDCM.append(os.path.join(dirName,filename))\n",
    "\n",
    "    # Read the data traind and Test\n",
    "    batch0 = unpickle(lstFilesDCM[0])\n",
    "    batch1 = unpickle(lstFilesDCM[1])\n",
    "    batch2 = unpickle(lstFilesDCM[2])\n",
    "    batch3 = unpickle(lstFilesDCM[3])\n",
    "    batch4 = unpickle(lstFilesDCM[4])\n",
    "\n",
    "    onehot_encoder = OneHotEncoder(sparse=True)\n",
    "    train_batch = np.vstack((batch0[b'data'],batch1[b'data'],batch2[b'data'],batch3[b'data'],batch4[b'data']))\n",
    "    train_label = np.expand_dims(np.hstack((batch0[b'labels'],batch1[b'labels'],batch2[b'labels'],batch3[b'labels'],batch4[b'labels'])).T,axis=1).astype(np.float64)\n",
    "    train_label = onehot_encoder.fit_transform(train_label).toarray().astype(np.float64)\n",
    "\n",
    "    test_batch = unpickle(lstFilesDCM[5])[b'data']\n",
    "    test_label = np.expand_dims(np.array(unpickle(lstFilesDCM[5])[b'labels']),axis=0).T.astype(np.float64)\n",
    "    test_label = onehot_encoder.fit_transform(test_label).toarray().astype(np.float64)\n",
    "\n",
    "    # reshape data\n",
    "    train_batch = np.reshape(train_batch,(len(train_batch),3,32,32)); test_batch = np.reshape(test_batch,(len(test_batch),3,32,32))\n",
    "    # rotate data\n",
    "    train_batch = np.rot90(np.rot90(train_batch,1,axes=(1,3)),3,axes=(1,2)).astype(np.float64); test_batch = np.rot90(np.rot90(test_batch,1,axes=(1,3)),3,axes=(1,2)).astype(np.float64)\n",
    "    # normalize\n",
    "    train_batch= train_batch/255.0; test_batch = test_batch/255.0\n",
    "\n",
    "    # print out the data shape and the max and min value\n",
    "    print(train_batch.shape,train_batch.max(),train_batch.min())\n",
    "    print(train_label.shape,train_label.max(),train_label.min())\n",
    "    print(test_batch.shape,test_batch.max(),test_batch.min())\n",
    "    print(test_label.shape,test_label.max(),test_label.min())\n",
    "    return train_batch,train_label,test_batch,test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-12T02:26:46.775Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# read the data\n",
    "train_images,train_labels,test_images,test_labels = read_CIFAR10_data()\n",
    "rand_choice = np.random.choice(len(train_images))\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(train_images[rand_choice])\n",
    "plt.title(str(train_labels[rand_choice]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-12T02:26:48.045Z"
    },
    "code_folding": [
     11,
     63,
     68,
     75,
     91,
     108
    ]
   },
   "outputs": [],
   "source": [
    "# create the layers\n",
    "def tf_softmax(x):    return tf.nn.softmax(x)\n",
    "def tf_relu(x):       return tf.nn.relu(x)\n",
    "def d_tf_relu(x):     return tf.cast(tf.greater_equal(x,0),tf.float32)\n",
    "def tf_iden(x):       return x\n",
    "def d_tf_iden(x):     return tf.ones_like(x)\n",
    "def tf_sigmoid(x):    return tf.nn.sigmoid(x)\n",
    "def d_tf_sigmoid(x):  return tf_sigmoid(x) * (1.0-tf_sigmoid(x))\n",
    "def tf_softplus(x):   return tf.nn.softplus(x)\n",
    "def d_tf_softplus(x): return tf.nn.sigmoid(x)\n",
    "\n",
    "class FNN():\n",
    "\n",
    "    def __init__(self,inc,outc,act=tf_iden,d_act=d_tf_iden,special_init=False):\n",
    "        if special_init:\n",
    "            interval = np.sqrt(6.0 / (inc + outc + 1.0))\n",
    "            self.w = tf.Variable(tf.random_uniform(shape=(inc, outc),minval=-interval,maxval=interval,dtype=tf.float32,seed=2))\n",
    "        else:\n",
    "            self.w = tf.Variable(tf.random_normal([inc,outc], stddev=0.05,seed=2,dtype=tf.float32))\n",
    "\n",
    "        self.m,self.v = tf.Variable(tf.zeros_like(self.w)),tf.Variable(tf.zeros_like(self.w))\n",
    "        self.act,self.d_act = act,d_act\n",
    "\n",
    "    def getw(self): return self.w\n",
    "    def feedforward(self,input=None):\n",
    "        self.input = input\n",
    "        self.layer = tf.matmul(input,self.w) \n",
    "        self.layerA = self.act(self.layer)\n",
    "        return self.layer,self.layerA\n",
    "\n",
    "    def backprop(self,gradient=None,which_reg=0):\n",
    "        grad_part_1 = gradient\n",
    "        grad_part_2 = self.d_act(self.layer)\n",
    "        grad_part_3 = self.input\n",
    "\n",
    "        grad_middle = grad_part_1 * grad_part_2\n",
    "        grad  = tf.matmul(tf.transpose(grad_part_3),grad_middle)\n",
    "        grad_b= tf.reduce_mean(grad_middle,axis=0)\n",
    "        grad_pass = tf.matmul(grad_middle,tf.transpose(self.w))\n",
    "\n",
    "        update_w = []\n",
    "\n",
    "        # Update the Weight First\n",
    "        update_w.append(tf.assign( self.m,self.m*beta1 + (1-beta1) * (grad)   ))\n",
    "        update_w.append(tf.assign( self.v,self.v*beta2 + (1-beta2) * (grad ** 2)   ))\n",
    "        m_hat = self.m / (1-beta1)\n",
    "        v_hat = self.v / (1-beta2)\n",
    "        adam_middle = m_hat *  learning_rate/(tf.sqrt(v_hat) + adam_e)\n",
    "        update_w.append(tf.assign(self.w,tf.subtract(self.w,adam_middle )))\n",
    "\n",
    "        return grad_pass,update_w\n",
    "class CNN():\n",
    "\n",
    "    def __init__(self,k,inc,out, stddev=0.05,act=tf_relu,d_act=d_tf_relu):\n",
    "        self.w              = tf.Variable(tf.random_normal([k,k,inc,out],stddev=stddev,seed=2,dtype=tf.float32))\n",
    "        self.m1,self.v1       = tf.Variable(tf.zeros_like(self.w)),tf.Variable(tf.zeros_like(self.w))\n",
    "        self.m2,self.v2       = tf.Variable(tf.zeros_like(self.w)),tf.Variable(tf.zeros_like(self.w))\n",
    "\n",
    "        self.act,self.d_act = act,d_act\n",
    "\n",
    "    def getw(self): return self.w\n",
    "    \n",
    "    # Feed Forward for two variables\n",
    "    def feedforward1(self,input,stride=1,padding='VALID'):\n",
    "        self.input1  = input\n",
    "        self.layer1  = tf.nn.conv2d(self.input1,self.w,strides=[1,stride,stride,1],padding=padding) \n",
    "        self.layerA1 = self.act(self.layer1)\n",
    "        return self.layer1, self.layerA1\n",
    "    def feedforward2(self,input,stride=1,padding='VALID'):\n",
    "        self.input2  = input\n",
    "        self.layer2  = tf.nn.conv2d(self.input2,self.w,strides=[1,stride,stride,1],padding=padding) \n",
    "        self.layerA2 = self.act(self.layer2)\n",
    "        return self.layer2, self.layerA2\n",
    "    \n",
    "    # Back Prop for two variables\n",
    "    def backprop1(self,gradient,stride=1,padding='VALID'):\n",
    "        grad_part_1 = gradient\n",
    "        grad_part_2 = self.d_act(self.layer1)\n",
    "        grad_part_3 = self.input1\n",
    "\n",
    "        grad_middle = grad_part_1 * grad_part_2\n",
    "        grad        = tf.nn.conv2d_backprop_filter(input = grad_part_3,filter_sizes = tf.shape(self.w),  out_backprop = grad_middle,strides=[1,stride,stride,1],padding=padding) \n",
    "        grad_pass   = tf.nn.conv2d_backprop_input (input_sizes = tf.shape(self.input1),filter= self.w,   out_backprop = grad_middle,strides=[1,stride,stride,1],padding=padding)\n",
    "\n",
    "        update_w = []\n",
    "        update_w.append(tf.assign( self.m1,self.m1*beta1 + (1-beta1) * (grad)   ))\n",
    "        update_w.append(tf.assign( self.v1,self.v1*beta2 + (1-beta2) * (grad ** 2)   ))\n",
    "        m_hat = self.m1 / (1-beta1) ; v_hat = self.v1 / (1-beta2)\n",
    "        adam_middle = m_hat * learning_rate/(tf.sqrt(v_hat) + adam_e)\n",
    "        update_w.append(tf.assign(self.w,tf.subtract(self.w,adam_middle  )))\n",
    "        return grad_pass,grad,update_w\n",
    "    def backprop2(self,gradient,stride=1,padding='VALID'):\n",
    "        grad_part_1 = gradient\n",
    "        grad_part_2 = self.d_act(self.layer2)\n",
    "        grad_part_3 = self.input2\n",
    "\n",
    "        grad_middle = grad_part_1 * grad_part_2\n",
    "        grad        = tf.nn.conv2d_backprop_filter(input = grad_part_3,filter_sizes = tf.shape(self.w),  out_backprop = grad_middle,strides=[1,stride,stride,1],padding=padding) \n",
    "        grad_pass   = tf.nn.conv2d_backprop_input (input_sizes = tf.shape(self.input1),filter= self.w,   out_backprop = grad_middle,strides=[1,stride,stride,1],padding=padding)\n",
    "\n",
    "        update_w = []\n",
    "        update_w.append(tf.assign( self.m2,self.m2*beta1 + (1-beta1) * (grad)   ))\n",
    "        update_w.append(tf.assign( self.v2,self.v2*beta2 + (1-beta2) * (grad ** 2)   ))\n",
    "        m_hat = self.m2 / (1-beta1) ; v_hat = self.v2 / (1-beta2)\n",
    "        adam_middle = m_hat * learning_rate/(tf.sqrt(v_hat) + adam_e)\n",
    "        update_w.append(tf.assign(self.w,tf.subtract(self.w,adam_middle  )))\n",
    "        return grad_pass,grad,update_w\n",
    "    \n",
    "class tf_batch_norm_layer():\n",
    "    \n",
    "    def __init__(self,vector_shape,axis):\n",
    "        self.moving_mean = tf.Variable(tf.zeros(shape=[1,1,1,vector_shape],dtype=tf.float32))\n",
    "        self.moving_vari = tf.Variable(tf.zeros(shape=[1,1,1,vector_shape],dtype=tf.float32))\n",
    "        self.axis        = axis\n",
    "        \n",
    "    def feedforward(self,input,training_phase=True,eps = 1e-8):\n",
    "        self.input = input\n",
    "        self.input_size          = self.input.shape\n",
    "        self.batch,self.h,self.w,self.c = self.input_size[0].value,self.input_size[1].value,self.input_size[2].value,self.input_size[3].value\n",
    "\n",
    "        # Training Moving Average Mean         \n",
    "        def training_fn():\n",
    "            self.mean    = tf.reduce_mean(self.input,axis=self.axis ,keepdims=True)\n",
    "            self.var     = tf.reduce_mean(tf.square(self.input-self.mean),axis=self.axis,keepdims=True)\n",
    "            centered_data= (self.input - self.mean)/tf.sqrt(self.var + eps)\n",
    "            \n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean*0.9 + 0.1 * self.mean ))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari*0.9 + 0.1 * self.var  ))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        # Testing Moving Average Mean        \n",
    "        def testing_fn():\n",
    "            centered_data   = (self.input - self.moving_mean)/tf.sqrt(self.moving_vari + eps)\n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        self.output,update_variable = tf.cond(training_phase,true_fn=training_fn,false_fn=testing_fn)\n",
    "        return self.output,update_variable\n",
    "    def backprop(self,grad,eps = 1e-8):\n",
    "        change_parts = 1.0 /(self.batch * self.h * self.w)\n",
    "        grad_sigma   = tf.reduce_sum( grad *  (self.input-self.mean)     ,axis=self.axis,keepdims=True) * -0.5 * (self.var+eps) ** -1.5\n",
    "        grad_mean    = tf.reduce_sum( grad *  (-1./tf.sqrt(self.var+eps)),axis=self.axis,keepdims=True) + grad_sigma * change_parts * 2.0 * tf.reduce_sum((self.input-self.mean),axis=self.axis,keepdims=True) * -1\n",
    "        grad_x       = grad * 1/(tf.sqrt(self.var+eps)) + grad_sigma * change_parts * 2.0 * (self.input-self.mean) + grad_mean * change_parts\n",
    "        return grad_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-12T02:26:49.787Z"
    }
   },
   "outputs": [],
   "source": [
    "# create layers\n",
    "num_eps   = 10; num_epoch = 200; learning_rate = 1e-4 ; batch_size = 50;  \n",
    "beta1,beta2,adam_e = 0.9,0.999,1e-8; print_iter = 100\n",
    "\n",
    "l1 = CNN(3,3, 32)\n",
    "l2 = CNN(3,32,64)\n",
    "l3 = CNN(3,64,128)\n",
    "l4 = CNN(3,128,256)\n",
    "l5 = FNN(24*24*256,64,act=tf_iden,d_act=d_tf_iden)\n",
    "\n",
    "local_l1 = CNN(1,192,192)\n",
    "local_l2 = CNN(1,192,192)\n",
    "local_l3 = CNN(1,192, 1,act=tf_iden,d_act=d_tf_iden)\n",
    "\n",
    "# ======== PLACE HOLDERS ========\n",
    "x = tf.placeholder(tf.float32,(batch_size,32,32,3))\n",
    "\n",
    "# ======== ENCODE ========\n",
    "layer1,layer1a = l1.feedforward1(x)\n",
    "layer2,layer2a = l2.feedforward1(layer1a)\n",
    "layer3,layer3a = l3.feedforward1(layer2a) # here is the cut!\n",
    "layer4,layer4a = l4.feedforward1(layer3a)\n",
    "layer5_input   = tf.reshape(layer4a,[batch_size,-1]) \n",
    "layer5,layer5a = l5.feedforward(layer5_input)\n",
    "\n",
    "# ======== GENERATE NEW FEATURES ========\n",
    "layer3r = tf.transpose(layer3a,[0,2,1,3])\n",
    "_,width,_,_    = layer3a.get_shape()\n",
    "encoded_expand = tf.tile(layer5a[:,None,None,:],[1,width,width,1])\n",
    "encoded_M        = tf.concat([layer3a,encoded_expand],3)\n",
    "encoded_M_prime  = tf.concat([layer3r,encoded_expand],3)\n",
    "\n",
    "# ======== LOCAL NEW FEATURES ========\n",
    "real_layer1,real_layer1a = local_l1.feedforward1(encoded_M)\n",
    "real_layer2,real_layer2a = local_l2.feedforward1(real_layer1a)\n",
    "real_layer3,real_layer3a = local_l3.feedforward1(real_layer2a)\n",
    "\n",
    "fake_layer1,fake_layer1a = local_l1.feedforward2(encoded_M_prime)\n",
    "fake_layer2,fake_layer2a = local_l2.feedforward2(fake_layer1a)\n",
    "fake_layer3,fake_layer3a = local_l3.feedforward2(fake_layer2a)\n",
    "\n",
    "# ======== LOSS ========\n",
    "loss =  tf.reduce_mean(tf_softplus(fake_layer3a)) - tf.reduce_mean(-tf_softplus(-real_layer3a)) \n",
    "\n",
    "# ======== BACK PROP FOR Discriminator ========\n",
    "fake_grad = 1/batch_size * tf_sigmoid(fake_layer3a) \n",
    "fake_grad3_pass,fake_grad3,fake_grad3up = local_l3.backprop2(fake_grad)\n",
    "fake_grad2_pass,fake_grad2,fake_grad2up = local_l2.backprop2(fake_grad3_pass)\n",
    "fake_grad1_pass,fake_grad1,fake_grad1up = local_l1.backprop2(fake_grad2_pass)\n",
    "\n",
    "real_grad = - 1/batch_size * -1.0 * tf_sigmoid(-real_layer3a) * -1.0\n",
    "real_grad3_pass,real_grad3,real_grad3up = local_l3.backprop1(real_grad)\n",
    "real_grad2_pass,real_grad2,real_grad2up = local_l2.backprop1(real_grad3_pass)\n",
    "real_grad1_pass,real_grad1,real_grad1up = local_l1.backprop1(real_grad2_pass)\n",
    "\n",
    "# ======== UNDO THE CHANGE ========\n",
    "fake_features,fake_encode = fake_grad1_pass[:,:,:,:128],fake_grad1_pass[:,:,:,128:]\n",
    "fake_encode   = tf.reduce_mean(fake_encode,[1,2])\n",
    "fake_features = tf.transpose(fake_features,[0,2,1,3])\n",
    "\n",
    "real_features,real_encode = real_grad1_pass[:,:,:,:128],real_grad1_pass[:,:,:,128:]\n",
    "real_encode   = tf.reduce_mean(real_encode,[1,2])\n",
    "\n",
    "# ======== BACK PROP FOR Encoder ========\n",
    "grad_feature_all = real_features + fake_features\n",
    "grad_encoded_all = real_encode   + fake_encode\n",
    "\n",
    "grad5p,grad5_up = l5.backprop(grad_encoded_all)\n",
    "grad4_input    = tf.reshape(grad5p,[batch_size,24,24,256])\n",
    "grad4p,grad4,grad4_up = l4.backprop1(grad4_input)\n",
    "grad3_input    = grad4p + grad_feature_all\n",
    "grad3p,grad3,grad3_up = l3.backprop1(grad3_input)\n",
    "grad2p,grad2,grad2_up = l2.backprop1(grad3p)\n",
    "grad1p,grad1,grad1_up = l1.backprop1(grad2p)\n",
    "\n",
    "grad_update_all = fake_grad3up + real_grad3up + \\\n",
    "                  fake_grad2up + real_grad2up + \\\n",
    "                  fake_grad1up + real_grad1up + \\\n",
    "                  grad5_up + grad4_up + grad3_up + grad2_up + grad1_up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T02:10:53.143897Z",
     "start_time": "2019-02-12T02:10:38.883039Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Current Iter : 0/200 batch : 35950/50000 loss : 1.4238364\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-138-cdbbf05a7c62>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcurrent_batch_index\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mcurrent_batch\u001b[0m       \u001b[1;33m=\u001b[0m \u001b[0mtrain_images\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcurrent_batch_index\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mcurrent_batch_index\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0msess_results\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgrad_update_all\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mcurrent_batch\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         sys.stdout.write(' Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + \n\u001b[0;32m     13\u001b[0m                          \u001b[1;34m' batch : '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_batch_index\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/'\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1332\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# loop\n",
    "# sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for iter in range(num_epoch):\n",
    "\n",
    "    # train_batch,train_label = shuffle(train_batch,train_label)\n",
    "    \n",
    "    for current_batch_index in range(0,len(train_images),batch_size):\n",
    "        current_batch       = train_images[current_batch_index:current_batch_index+batch_size]\n",
    "        sess_results  = sess.run([loss,grad_update_all],feed_dict={x:current_batch})\n",
    "        sys.stdout.write(' Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + \n",
    "                         ' batch : ' + str(current_batch_index) + '/'+ str(len(train_images)) + \n",
    "                         ' loss : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); \n",
    "        \n",
    "    if iter%2 == 0 : print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T02:21:48.585366Z",
     "start_time": "2019-02-12T02:21:48.357806Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# import lib\n",
    "import torch\n",
    "from torchvision.datasets.cifar import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import statistics as stats\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T02:24:14.808467Z",
     "start_time": "2019-02-12T02:23:51.650502Z"
    },
    "code_folding": [
     1,
     22,
     40,
     52,
     64,
     81
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                               | 0/1000 [00:00<?, ?it/s]\n",
      "0 Loss: 1.3864175081253052:   0%|                                                                   | 0/1000 [00:00<?, ?it/s]\n",
      "0 Loss: 1.3833853006362915:   0%|                                                                   | 0/1000 [00:00<?, ?it/s]\n",
      "0 Loss: 1.3833853006362915:   0%|                                                           | 2/1000 [00:00<01:49,  9.14it/s]\n",
      "0 Loss: 1.3808801968892415:   0%|                                                           | 2/1000 [00:00<01:49,  9.14it/s]\n",
      "0 Loss: 1.3808801968892415:   0%|▏                                                          | 3/1000 [00:00<01:58,  8.42it/s]\n",
      "0 Loss: 1.3856110870838165:   0%|▏                                                          | 3/1000 [00:00<01:58,  8.42it/s]\n",
      "0 Loss: 1.3856110870838165:   0%|▏                                                          | 4/1000 [00:00<02:04,  8.02it/s]\n",
      "0 Loss: 1.3834497928619385:   0%|▏                                                          | 4/1000 [00:00<02:04,  8.02it/s]\n",
      "0 Loss: 1.3834497928619385:   0%|▎                                                          | 5/1000 [00:00<02:07,  7.79it/s]\n",
      "0 Loss: 1.3810073137283325:   0%|▎                                                          | 5/1000 [00:00<02:07,  7.79it/s]\n",
      "0 Loss: 1.3810073137283325:   1%|▎                                                          | 6/1000 [00:00<02:11,  7.59it/s]\n",
      "0 Loss: 1.3790357283183508:   1%|▎                                                          | 6/1000 [00:00<02:11,  7.59it/s]\n",
      "0 Loss: 1.3790357283183508:   1%|▍                                                          | 7/1000 [00:00<02:13,  7.43it/s]\n",
      "0 Loss: 1.377213791012764:   1%|▍                                                           | 7/1000 [00:01<02:13,  7.43it/s]\n",
      "0 Loss: 1.377213791012764:   1%|▍                                                           | 8/1000 [00:01<02:13,  7.41it/s]\n",
      "0 Loss: 1.3750247425503201:   1%|▍                                                          | 8/1000 [00:01<02:13,  7.41it/s]\n",
      "0 Loss: 1.3750247425503201:   1%|▌                                                          | 9/1000 [00:01<02:15,  7.33it/s]\n",
      "0 Loss: 1.3721094608306885:   1%|▌                                                          | 9/1000 [00:01<02:15,  7.33it/s]\n",
      "0 Loss: 1.3721094608306885:   1%|▌                                                         | 10/1000 [00:01<02:15,  7.28it/s]\n",
      "0 Loss: 1.369563016024503:   1%|▌                                                          | 10/1000 [00:01<02:15,  7.28it/s]\n",
      "0 Loss: 1.369563016024503:   1%|▋                                                          | 11/1000 [00:01<02:19,  7.10it/s]\n",
      "0 Loss: 1.3665007750193279:   1%|▋                                                         | 11/1000 [00:01<02:19,  7.10it/s]\n",
      "0 Loss: 1.3665007750193279:   1%|▋                                                         | 12/1000 [00:01<02:20,  7.05it/s]\n",
      "0 Loss: 1.3641380163339467:   1%|▋                                                         | 12/1000 [00:01<02:20,  7.05it/s]\n",
      "0 Loss: 1.3641380163339467:   1%|▊                                                         | 13/1000 [00:01<02:19,  7.10it/s]\n",
      "0 Loss: 1.3627039705004012:   1%|▊                                                         | 13/1000 [00:01<02:19,  7.10it/s]\n",
      "0 Loss: 1.3627039705004012:   1%|▊                                                         | 14/1000 [00:01<02:21,  6.95it/s]\n",
      "0 Loss: 1.3599044879277546:   1%|▊                                                         | 14/1000 [00:02<02:21,  6.95it/s]\n",
      "0 Loss: 1.3599044879277546:   2%|▊                                                         | 15/1000 [00:02<02:22,  6.91it/s]\n",
      "0 Loss: 1.3580371364951134:   2%|▊                                                         | 15/1000 [00:02<02:22,  6.91it/s]\n",
      "0 Loss: 1.3580371364951134:   2%|▉                                                         | 16/1000 [00:02<02:21,  6.93it/s]\n",
      "0 Loss: 1.3561251093359554:   2%|▉                                                         | 16/1000 [00:02<02:21,  6.93it/s]\n",
      "0 Loss: 1.3561251093359554:   2%|▉                                                         | 17/1000 [00:02<02:24,  6.80it/s]\n",
      "0 Loss: 1.3542113900184631:   2%|▉                                                         | 17/1000 [00:02<02:24,  6.80it/s]\n",
      "0 Loss: 1.3542113900184631:   2%|█                                                         | 18/1000 [00:02<02:22,  6.91it/s]\n",
      "0 Loss: 1.3528664551283185:   2%|█                                                         | 18/1000 [00:02<02:22,  6.91it/s]\n",
      "0 Loss: 1.3528664551283185:   2%|█                                                         | 19/1000 [00:02<02:19,  7.02it/s]\n",
      "0 Loss: 1.352143543958664:   2%|█                                                          | 19/1000 [00:02<02:19,  7.02it/s]\n",
      "0 Loss: 1.352143543958664:   2%|█▏                                                         | 20/1000 [00:02<02:18,  7.09it/s]\n",
      "0 Loss: 1.3488995432853699:   2%|█▏                                                        | 20/1000 [00:02<02:18,  7.09it/s]\n",
      "0 Loss: 1.3488995432853699:   2%|█▏                                                        | 21/1000 [00:02<02:17,  7.14it/s]\n",
      "0 Loss: 1.3454519271850587:   2%|█▏                                                        | 21/1000 [00:03<02:17,  7.14it/s]\n",
      "0 Loss: 1.3454519271850587:   2%|█▎                                                        | 22/1000 [00:03<02:16,  7.14it/s]\n",
      "0 Loss: 1.3439947426319123:   2%|█▎                                                        | 22/1000 [00:03<02:16,  7.14it/s]\n",
      "0 Loss: 1.3439947426319123:   2%|█▎                                                        | 23/1000 [00:03<02:17,  7.13it/s]\n",
      "0 Loss: 1.3376925945281983:   2%|█▎                                                        | 23/1000 [00:03<02:17,  7.13it/s]\n",
      "0 Loss: 1.3376925945281983:   2%|█▍                                                        | 24/1000 [00:03<02:18,  7.06it/s]\n",
      "0 Loss: 1.335189312696457:   2%|█▍                                                         | 24/1000 [00:03<02:18,  7.06it/s]\n",
      "0 Loss: 1.335189312696457:   2%|█▍                                                         | 25/1000 [00:03<02:16,  7.16it/s]\n",
      "0 Loss: 1.3313141942024231:   2%|█▍                                                        | 25/1000 [00:03<02:16,  7.16it/s]\n",
      "0 Loss: 1.3313141942024231:   3%|█▌                                                        | 26/1000 [00:03<02:18,  7.02it/s]\n",
      "0 Loss: 1.3277504861354827:   3%|█▌                                                        | 26/1000 [00:03<02:18,  7.02it/s]\n",
      "0 Loss: 1.3277504861354827:   3%|█▌                                                        | 27/1000 [00:03<02:20,  6.92it/s]\n",
      "0 Loss: 1.324735313653946:   3%|█▌                                                         | 27/1000 [00:03<02:20,  6.92it/s]\n",
      "0 Loss: 1.324735313653946:   3%|█▋                                                         | 28/1000 [00:03<02:20,  6.90it/s]\n",
      "0 Loss: 1.320241928100586:   3%|█▋                                                         | 28/1000 [00:04<02:20,  6.90it/s]\n",
      "0 Loss: 1.320241928100586:   3%|█▋                                                         | 29/1000 [00:04<02:18,  7.02it/s]\n",
      "0 Loss: 1.316326951980591:   3%|█▋                                                         | 29/1000 [00:04<02:18,  7.02it/s]\n",
      "0 Loss: 1.316326951980591:   3%|█▊                                                         | 30/1000 [00:04<02:16,  7.12it/s]\n",
      "0 Loss: 1.3126783132553101:   3%|█▋                                                        | 30/1000 [00:04<02:16,  7.12it/s]\n",
      "0 Loss: 1.3126783132553101:   3%|█▊                                                        | 31/1000 [00:04<02:15,  7.16it/s]\n",
      "0 Loss: 1.310165709257126:   3%|█▊                                                         | 31/1000 [00:04<02:15,  7.16it/s]\n",
      "0 Loss: 1.310165709257126:   3%|█▉                                                         | 32/1000 [00:04<02:14,  7.18it/s]\n",
      "0 Loss: 1.3096785485744475:   3%|█▊                                                        | 32/1000 [00:04<02:14,  7.18it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0 Loss: 1.3096785485744475:   3%|█▉                                                        | 33/1000 [00:04<02:14,  7.21it/s]\n",
      "0 Loss: 1.3064698815345763:   3%|█▉                                                        | 33/1000 [00:04<02:14,  7.21it/s]\n",
      "0 Loss: 1.3064698815345763:   3%|█▉                                                        | 34/1000 [00:04<02:16,  7.09it/s]\n",
      "0 Loss: 1.306106823682785:   3%|██                                                         | 34/1000 [00:04<02:16,  7.09it/s]\n",
      "0 Loss: 1.306106823682785:   4%|██                                                         | 35/1000 [00:04<02:17,  7.01it/s]\n",
      "0 Loss: 1.3047845304012298:   4%|██                                                        | 35/1000 [00:05<02:17,  7.01it/s]\n",
      "0 Loss: 1.3047845304012298:   4%|██                                                        | 36/1000 [00:05<02:16,  7.06it/s]\n",
      "0 Loss: 1.3013425409793853:   4%|██                                                        | 36/1000 [00:05<02:16,  7.06it/s]\n",
      "0 Loss: 1.3013425409793853:   4%|██▏                                                       | 37/1000 [00:05<02:17,  7.03it/s]\n",
      "0 Loss: 1.3000861704349518:   4%|██▏                                                       | 37/1000 [00:05<02:17,  7.03it/s]\n",
      "0 Loss: 1.3000861704349518:   4%|██▏                                                       | 38/1000 [00:05<02:19,  6.90it/s]\n",
      "0 Loss: 1.2975715935230254:   4%|██▏                                                       | 38/1000 [00:05<02:19,  6.90it/s]\n",
      "0 Loss: 1.2975715935230254:   4%|██▎                                                       | 39/1000 [00:05<02:18,  6.92it/s]\n",
      "0 Loss: 1.293763816356659:   4%|██▎                                                        | 39/1000 [00:05<02:18,  6.92it/s]\n",
      "0 Loss: 1.293763816356659:   4%|██▎                                                        | 40/1000 [00:05<02:16,  7.01it/s]\n",
      "0 Loss: 1.2897583782672881:   4%|██▎                                                       | 40/1000 [00:05<02:16,  7.01it/s]\n",
      "0 Loss: 1.2897583782672881:   4%|██▍                                                       | 41/1000 [00:05<02:16,  7.02it/s]\n",
      "0 Loss: 1.289273303747177:   4%|██▍                                                        | 41/1000 [00:05<02:16,  7.02it/s]\n",
      "0 Loss: 1.289273303747177:   4%|██▍                                                        | 42/1000 [00:05<02:18,  6.91it/s]\n",
      "0 Loss: 1.2825584590435029:   4%|██▍                                                       | 42/1000 [00:06<02:18,  6.91it/s]\n",
      "0 Loss: 1.2825584590435029:   4%|██▍                                                       | 43/1000 [00:06<02:19,  6.88it/s]\n",
      "0 Loss: 1.2828510463237763:   4%|██▍                                                       | 43/1000 [00:06<02:19,  6.88it/s]\n",
      "0 Loss: 1.2828510463237763:   4%|██▌                                                       | 44/1000 [00:06<02:17,  6.93it/s]\n",
      "0 Loss: 1.2796334087848664:   4%|██▌                                                       | 44/1000 [00:06<02:17,  6.93it/s]\n",
      "0 Loss: 1.2796334087848664:   4%|██▌                                                       | 45/1000 [00:06<02:16,  6.99it/s]\n",
      "0 Loss: 1.2768310070037843:   4%|██▌                                                       | 45/1000 [00:06<02:16,  6.99it/s]\n",
      "0 Loss: 1.2768310070037843:   5%|██▋                                                       | 46/1000 [00:06<02:14,  7.09it/s]\n",
      "0 Loss: 1.2762353539466857:   5%|██▋                                                       | 46/1000 [00:06<02:14,  7.09it/s]\n",
      "0 Loss: 1.2762353539466857:   5%|██▋                                                       | 47/1000 [00:06<02:13,  7.12it/s]\n",
      "0 Loss: 1.2738895654678344:   5%|██▋                                                       | 47/1000 [00:06<02:13,  7.12it/s]\n",
      "0 Loss: 1.2738895654678344:   5%|██▊                                                       | 48/1000 [00:06<02:12,  7.20it/s]\n",
      "0 Loss: 1.2755543887615204:   5%|██▊                                                       | 48/1000 [00:06<02:12,  7.20it/s]\n",
      "0 Loss: 1.2755543887615204:   5%|██▊                                                       | 49/1000 [00:06<02:12,  7.18it/s]\n",
      "0 Loss: 1.272878462076187:   5%|██▉                                                        | 49/1000 [00:07<02:12,  7.18it/s]\n",
      "0 Loss: 1.272878462076187:   5%|██▉                                                        | 50/1000 [00:07<02:13,  7.13it/s]\n",
      "0 Loss: 1.2702899396419525:   5%|██▉                                                       | 50/1000 [00:07<02:13,  7.13it/s]\n",
      "0 Loss: 1.2702899396419525:   5%|██▉                                                       | 51/1000 [00:07<02:13,  7.11it/s]\n",
      "0 Loss: 1.2677671015262604:   5%|██▉                                                       | 51/1000 [00:07<02:13,  7.11it/s]\n",
      "0 Loss: 1.2677671015262604:   5%|███                                                       | 52/1000 [00:07<02:12,  7.13it/s]\n",
      "0 Loss: 1.263359785079956:   5%|███                                                        | 52/1000 [00:07<02:12,  7.13it/s]\n",
      "0 Loss: 1.263359785079956:   5%|███▏                                                       | 53/1000 [00:07<02:14,  7.04it/s]\n",
      "0 Loss: 1.2610492408275604:   5%|███                                                       | 53/1000 [00:07<02:14,  7.04it/s]\n",
      "0 Loss: 1.2610492408275604:   5%|███▏                                                      | 54/1000 [00:07<02:16,  6.91it/s]\n",
      "0 Loss: 1.2538324415683746:   5%|███▏                                                      | 54/1000 [00:07<02:16,  6.91it/s]\n",
      "0 Loss: 1.2538324415683746:   6%|███▏                                                      | 55/1000 [00:07<02:17,  6.89it/s]\n",
      "0 Loss: 1.2506908118724822:   6%|███▏                                                      | 55/1000 [00:07<02:17,  6.89it/s]\n",
      "0 Loss: 1.2506908118724822:   6%|███▏                                                      | 56/1000 [00:07<02:17,  6.88it/s]\n",
      "0 Loss: 1.248494154214859:   6%|███▎                                                       | 56/1000 [00:08<02:17,  6.88it/s]\n",
      "0 Loss: 1.248494154214859:   6%|███▎                                                       | 57/1000 [00:08<02:19,  6.75it/s]\n",
      "0 Loss: 1.2449704706668854:   6%|███▎                                                      | 57/1000 [00:08<02:19,  6.75it/s]\n",
      "0 Loss: 1.2449704706668854:   6%|███▎                                                      | 58/1000 [00:08<02:17,  6.87it/s]\n",
      "0 Loss: 1.2431133329868316:   6%|███▎                                                      | 58/1000 [00:08<02:17,  6.87it/s]\n",
      "0 Loss: 1.2431133329868316:   6%|███▍                                                      | 59/1000 [00:08<02:15,  6.96it/s]\n",
      "0 Loss: 1.2419945538043975:   6%|███▍                                                      | 59/1000 [00:08<02:15,  6.96it/s]\n",
      "0 Loss: 1.2419945538043975:   6%|███▍                                                      | 60/1000 [00:08<02:13,  7.05it/s]\n",
      "0 Loss: 1.2417484045028686:   6%|███▍                                                      | 60/1000 [00:08<02:13,  7.05it/s]\n",
      "0 Loss: 1.2417484045028686:   6%|███▌                                                      | 61/1000 [00:08<02:16,  6.88it/s]\n",
      "0 Loss: 1.234941691160202:   6%|███▌                                                       | 61/1000 [00:08<02:16,  6.88it/s]\n",
      "0 Loss: 1.234941691160202:   6%|███▋                                                       | 62/1000 [00:08<02:17,  6.84it/s]\n",
      "0 Loss: 1.2334358751773835:   6%|███▌                                                      | 62/1000 [00:08<02:17,  6.84it/s]\n",
      "0 Loss: 1.2334358751773835:   6%|███▋                                                      | 63/1000 [00:08<02:17,  6.84it/s]\n",
      "0 Loss: 1.22924502491951:   6%|███▊                                                        | 63/1000 [00:09<02:17,  6.84it/s]\n",
      "0 Loss: 1.22924502491951:   6%|███▊                                                        | 64/1000 [00:09<02:14,  6.94it/s]\n",
      "0 Loss: 1.22279412150383:   6%|███▊                                                        | 64/1000 [00:09<02:14,  6.94it/s]\n",
      "0 Loss: 1.22279412150383:   6%|███▉                                                        | 65/1000 [00:09<02:15,  6.89it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0 Loss: 1.2214983761310578:   6%|███▊                                                      | 65/1000 [00:09<02:15,  6.89it/s]\n",
      "0 Loss: 1.2214983761310578:   7%|███▊                                                      | 66/1000 [00:09<02:14,  6.92it/s]\n",
      "0 Loss: 1.2155211985111236:   7%|███▊                                                      | 66/1000 [00:09<02:14,  6.92it/s]\n",
      "0 Loss: 1.2155211985111236:   7%|███▉                                                      | 67/1000 [00:09<02:14,  6.91it/s]\n",
      "0 Loss: 1.2127982318401336:   7%|███▉                                                      | 67/1000 [00:09<02:14,  6.91it/s]\n",
      "0 Loss: 1.2127982318401336:   7%|███▉                                                      | 68/1000 [00:09<02:15,  6.88it/s]\n",
      "0 Loss: 1.2102938711643219:   7%|███▉                                                      | 68/1000 [00:09<02:15,  6.88it/s]\n",
      "0 Loss: 1.2102938711643219:   7%|████                                                      | 69/1000 [00:09<02:14,  6.94it/s]\n",
      "0 Loss: 1.21228466629982:   7%|████▏                                                       | 69/1000 [00:09<02:14,  6.94it/s]\n",
      "0 Loss: 1.21228466629982:   7%|████▏                                                       | 70/1000 [00:09<02:13,  6.98it/s]\n",
      "0 Loss: 1.210679942369461:   7%|████▏                                                      | 70/1000 [00:10<02:13,  6.98it/s]\n",
      "0 Loss: 1.210679942369461:   7%|████▏                                                      | 71/1000 [00:10<02:14,  6.89it/s]\n",
      "0 Loss: 1.2061226129531861:   7%|████                                                      | 71/1000 [00:10<02:14,  6.89it/s]\n",
      "0 Loss: 1.2061226129531861:   7%|████▏                                                     | 72/1000 [00:10<02:14,  6.91it/s]\n",
      "0 Loss: 1.2043278872966767:   7%|████▏                                                     | 72/1000 [00:10<02:14,  6.91it/s]\n",
      "0 Loss: 1.2043278872966767:   7%|████▏                                                     | 73/1000 [00:10<02:13,  6.94it/s]\n",
      "0 Loss: 1.2015096843242645:   7%|████▏                                                     | 73/1000 [00:10<02:13,  6.94it/s]\n",
      "0 Loss: 1.2015096843242645:   7%|████▎                                                     | 74/1000 [00:10<02:15,  6.83it/s]\n",
      "0 Loss: 1.1982692062854767:   7%|████▎                                                     | 74/1000 [00:10<02:15,  6.83it/s]\n",
      "0 Loss: 1.1982692062854767:   8%|████▎                                                     | 75/1000 [00:10<02:13,  6.91it/s]\n",
      "0 Loss: 1.1951570689678193:   8%|████▎                                                     | 75/1000 [00:10<02:13,  6.91it/s]\n",
      "0 Loss: 1.1951570689678193:   8%|████▍                                                     | 76/1000 [00:10<02:12,  6.99it/s]\n",
      "0 Loss: 1.1921784222126006:   8%|████▍                                                     | 76/1000 [00:10<02:12,  6.99it/s]\n",
      "0 Loss: 1.1921784222126006:   8%|████▍                                                     | 77/1000 [00:10<02:15,  6.83it/s]\n",
      "0 Loss: 1.189512836933136:   8%|████▌                                                      | 77/1000 [00:11<02:15,  6.83it/s]\n",
      "0 Loss: 1.189512836933136:   8%|████▌                                                      | 78/1000 [00:11<02:13,  6.90it/s]\n",
      "0 Loss: 1.183910882472992:   8%|████▌                                                      | 78/1000 [00:11<02:13,  6.90it/s]\n",
      "0 Loss: 1.183910882472992:   8%|████▋                                                      | 79/1000 [00:11<02:12,  6.98it/s]\n",
      "0 Loss: 1.1799619495868683:   8%|████▌                                                     | 79/1000 [00:11<02:12,  6.98it/s]\n",
      "0 Loss: 1.1799619495868683:   8%|████▋                                                     | 80/1000 [00:11<02:09,  7.11it/s]\n",
      "0 Loss: 1.1753375113010407:   8%|████▋                                                     | 80/1000 [00:11<02:09,  7.11it/s]\n",
      "0 Loss: 1.1753375113010407:   8%|████▋                                                     | 81/1000 [00:11<02:12,  6.93it/s]\n",
      "0 Loss: 1.1730014503002166:   8%|████▋                                                     | 81/1000 [00:11<02:12,  6.93it/s]\n",
      "0 Loss: 1.1730014503002166:   8%|████▊                                                     | 82/1000 [00:11<02:13,  6.87it/s]\n",
      "0 Loss: 1.175123918056488:   8%|████▊                                                      | 82/1000 [00:11<02:13,  6.87it/s]\n",
      "0 Loss: 1.175123918056488:   8%|████▉                                                      | 83/1000 [00:11<02:11,  7.00it/s]\n",
      "0 Loss: 1.175259953737259:   8%|████▉                                                      | 83/1000 [00:11<02:11,  7.00it/s]\n",
      "0 Loss: 1.175259953737259:   8%|████▉                                                      | 84/1000 [00:11<02:13,  6.87it/s]\n",
      "0 Loss: 1.179489368200302:   8%|████▉                                                      | 84/1000 [00:12<02:13,  6.87it/s]\n",
      "0 Loss: 1.179489368200302:   8%|█████                                                      | 85/1000 [00:12<02:13,  6.87it/s]\n",
      "0 Loss: 1.1796750128269196:   8%|████▉                                                     | 85/1000 [00:12<02:13,  6.87it/s]\n",
      "0 Loss: 1.1796750128269196:   9%|████▉                                                     | 86/1000 [00:12<02:11,  6.96it/s]\n",
      "0 Loss: 1.1818258464336395:   9%|████▉                                                     | 86/1000 [00:12<02:11,  6.96it/s]\n",
      "0 Loss: 1.1818258464336395:   9%|█████                                                     | 87/1000 [00:12<02:09,  7.07it/s]\n",
      "0 Loss: 1.1808899819850922:   9%|█████                                                     | 87/1000 [00:12<02:09,  7.07it/s]\n",
      "0 Loss: 1.1808899819850922:   9%|█████                                                     | 88/1000 [00:12<02:11,  6.95it/s]\n",
      "0 Loss: 1.1772873044013976:   9%|█████                                                     | 88/1000 [00:12<02:11,  6.95it/s]\n",
      "0 Loss: 1.1772873044013976:   9%|█████▏                                                    | 89/1000 [00:12<02:12,  6.87it/s]\n",
      "0 Loss: 1.1710909068584443:   9%|█████▏                                                    | 89/1000 [00:12<02:12,  6.87it/s]\n",
      "0 Loss: 1.1710909068584443:   9%|█████▏                                                    | 90/1000 [00:12<02:09,  7.01it/s]\n",
      "0 Loss: 1.1667467951774597:   9%|█████▏                                                    | 90/1000 [00:12<02:09,  7.01it/s]\n",
      "0 Loss: 1.1667467951774597:   9%|█████▎                                                    | 91/1000 [00:12<02:12,  6.87it/s]\n",
      "0 Loss: 1.1666891932487489:   9%|█████▎                                                    | 91/1000 [00:13<02:12,  6.87it/s]\n",
      "0 Loss: 1.1666891932487489:   9%|█████▎                                                    | 92/1000 [00:13<02:13,  6.80it/s]\n",
      "0 Loss: 1.1638575315475463:   9%|█████▎                                                    | 92/1000 [00:13<02:13,  6.80it/s]\n",
      "0 Loss: 1.1638575315475463:   9%|█████▍                                                    | 93/1000 [00:13<02:11,  6.91it/s]\n",
      "0 Loss: 1.1636317133903504:   9%|█████▍                                                    | 93/1000 [00:13<02:11,  6.91it/s]\n",
      "0 Loss: 1.1636317133903504:   9%|█████▍                                                    | 94/1000 [00:13<02:10,  6.94it/s]\n",
      "0 Loss: 1.1655855298042297:   9%|█████▍                                                    | 94/1000 [00:13<02:10,  6.94it/s]\n",
      "0 Loss: 1.1655855298042297:  10%|█████▌                                                    | 95/1000 [00:13<02:11,  6.90it/s]\n",
      "0 Loss: 1.1627025842666625:  10%|█████▌                                                    | 95/1000 [00:13<02:11,  6.90it/s]\n",
      "0 Loss: 1.1627025842666625:  10%|█████▌                                                    | 96/1000 [00:13<02:11,  6.88it/s]\n",
      "0 Loss: 1.1627494096755981:  10%|█████▌                                                    | 96/1000 [00:13<02:11,  6.88it/s]\n",
      "0 Loss: 1.1627494096755981:  10%|█████▋                                                    | 97/1000 [00:13<02:09,  6.96it/s]\n",
      "0 Loss: 1.1586212515830994:  10%|█████▋                                                    | 97/1000 [00:13<02:09,  6.96it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0 Loss: 1.1586212515830994:  10%|█████▋                                                    | 98/1000 [00:13<02:10,  6.92it/s]\n",
      "0 Loss: 1.1575596630573273:  10%|█████▋                                                    | 98/1000 [00:14<02:10,  6.92it/s]\n",
      "0 Loss: 1.1575596630573273:  10%|█████▋                                                    | 99/1000 [00:14<02:12,  6.81it/s]\n",
      "0 Loss: 1.15521097779274:  10%|█████▉                                                      | 99/1000 [00:14<02:12,  6.81it/s]\n",
      "0 Loss: 1.15521097779274:  10%|█████▉                                                     | 100/1000 [00:14<02:11,  6.87it/s]\n",
      "0 Loss: 1.154428344964981:  10%|█████▊                                                    | 100/1000 [00:14<02:11,  6.87it/s]\n",
      "0 Loss: 1.154428344964981:  10%|█████▊                                                    | 101/1000 [00:14<02:06,  7.08it/s]\n",
      "0 Loss: 1.1588531136512756:  10%|█████▊                                                   | 101/1000 [00:14<02:06,  7.08it/s]\n",
      "0 Loss: 1.1588531136512756:  10%|█████▊                                                   | 102/1000 [00:14<02:08,  6.98it/s]\n",
      "0 Loss: 1.1580644071102142:  10%|█████▊                                                   | 102/1000 [00:14<02:08,  6.98it/s]\n",
      "0 Loss: 1.1580644071102142:  10%|█████▊                                                   | 103/1000 [00:14<02:09,  6.90it/s]\n",
      "0 Loss: 1.1557278037071228:  10%|█████▊                                                   | 103/1000 [00:14<02:09,  6.90it/s]\n",
      "0 Loss: 1.1557278037071228:  10%|█████▉                                                   | 104/1000 [00:14<02:08,  6.95it/s]\n",
      "0 Loss: 1.1512540817260741:  10%|█████▉                                                   | 104/1000 [00:14<02:08,  6.95it/s]\n",
      "0 Loss: 1.1512540817260741:  10%|█████▉                                                   | 105/1000 [00:14<02:08,  6.97it/s]\n",
      "0 Loss: 1.149971866607666:  10%|██████                                                    | 105/1000 [00:15<02:08,  6.97it/s]\n",
      "0 Loss: 1.149971866607666:  11%|██████▏                                                   | 106/1000 [00:15<02:10,  6.86it/s]\n",
      "0 Loss: 1.1484925866127014:  11%|██████                                                   | 106/1000 [00:15<02:10,  6.86it/s]\n",
      "0 Loss: 1.1484925866127014:  11%|██████                                                   | 107/1000 [00:15<02:11,  6.80it/s]\n",
      "0 Loss: 1.1460572838783265:  11%|██████                                                   | 107/1000 [00:15<02:11,  6.80it/s]\n",
      "0 Loss: 1.1460572838783265:  11%|██████▏                                                  | 108/1000 [00:15<02:09,  6.87it/s]\n",
      "0 Loss: 1.1435686469078064:  11%|██████▏                                                  | 108/1000 [00:15<02:09,  6.87it/s]\n",
      "0 Loss: 1.1435686469078064:  11%|██████▏                                                  | 109/1000 [00:15<02:09,  6.86it/s]\n",
      "0 Loss: 1.1420230209827422:  11%|██████▏                                                  | 109/1000 [00:15<02:09,  6.86it/s]\n",
      "0 Loss: 1.1420230209827422:  11%|██████▎                                                  | 110/1000 [00:15<02:10,  6.80it/s]\n",
      "0 Loss: 1.1421903312206267:  11%|██████▎                                                  | 110/1000 [00:15<02:10,  6.80it/s]\n",
      "0 Loss: 1.1421903312206267:  11%|██████▎                                                  | 111/1000 [00:15<02:10,  6.81it/s]\n",
      "0 Loss: 1.1414058744907378:  11%|██████▎                                                  | 111/1000 [00:15<02:10,  6.81it/s]\n",
      "0 Loss: 1.1414058744907378:  11%|██████▍                                                  | 112/1000 [00:16<02:10,  6.83it/s]\n",
      "0 Loss: 1.1421998023986817:  11%|██████▍                                                  | 112/1000 [00:16<02:10,  6.83it/s]\n",
      "0 Loss: 1.1421998023986817:  11%|██████▍                                                  | 113/1000 [00:16<02:11,  6.76it/s]\n",
      "0 Loss: 1.1400900304317474:  11%|██████▍                                                  | 113/1000 [00:16<02:11,  6.76it/s]\n",
      "0 Loss: 1.1400900304317474:  11%|██████▍                                                  | 114/1000 [00:16<02:12,  6.67it/s]\n",
      "0 Loss: 1.1366523385047913:  11%|██████▍                                                  | 114/1000 [00:16<02:12,  6.67it/s]\n",
      "0 Loss: 1.1366523385047913:  12%|██████▌                                                  | 115/1000 [00:16<02:11,  6.75it/s]\n",
      "0 Loss: 1.140951430797577:  12%|██████▋                                                   | 115/1000 [00:16<02:11,  6.75it/s]\n",
      "0 Loss: 1.140951430797577:  12%|██████▋                                                   | 116/1000 [00:16<02:09,  6.82it/s]\n",
      "0 Loss: 1.1405393958091736:  12%|██████▌                                                  | 116/1000 [00:16<02:09,  6.82it/s]\n",
      "0 Loss: 1.1405393958091736:  12%|██████▋                                                  | 117/1000 [00:16<02:09,  6.82it/s]\n",
      "0 Loss: 1.1401985347270966:  12%|██████▋                                                  | 117/1000 [00:16<02:09,  6.82it/s]\n",
      "0 Loss: 1.1401985347270966:  12%|██████▋                                                  | 118/1000 [00:16<02:09,  6.81it/s]\n",
      "0 Loss: 1.137917959690094:  12%|██████▊                                                   | 118/1000 [00:17<02:09,  6.81it/s]\n",
      "0 Loss: 1.137917959690094:  12%|██████▉                                                   | 119/1000 [00:17<02:09,  6.78it/s]\n",
      "0 Loss: 1.1371991634368896:  12%|██████▊                                                  | 119/1000 [00:17<02:09,  6.78it/s]\n",
      "0 Loss: 1.1371991634368896:  12%|██████▊                                                  | 120/1000 [00:17<02:07,  6.89it/s]\n",
      "0 Loss: 1.1344965815544128:  12%|██████▊                                                  | 120/1000 [00:17<02:07,  6.89it/s]\n",
      "0 Loss: 1.1344965815544128:  12%|██████▉                                                  | 121/1000 [00:17<02:08,  6.82it/s]\n",
      "0 Loss: 1.1279480814933778:  12%|██████▉                                                  | 121/1000 [00:17<02:08,  6.82it/s]\n",
      "0 Loss: 1.1279480814933778:  12%|██████▉                                                  | 122/1000 [00:17<02:08,  6.81it/s]\n",
      "0 Loss: 1.1201990723609925:  12%|██████▉                                                  | 122/1000 [00:17<02:08,  6.81it/s]\n",
      "0 Loss: 1.1201990723609925:  12%|███████                                                  | 123/1000 [00:17<02:07,  6.89it/s]\n",
      "0 Loss: 1.1198936700820923:  12%|███████                                                  | 123/1000 [00:17<02:07,  6.89it/s]\n",
      "0 Loss: 1.1198936700820923:  12%|███████                                                  | 124/1000 [00:17<02:07,  6.85it/s]\n",
      "0 Loss: 1.1198229551315309:  12%|███████                                                  | 124/1000 [00:17<02:07,  6.85it/s]\n",
      "0 Loss: 1.1198229551315309:  12%|███████▏                                                 | 125/1000 [00:17<02:07,  6.87it/s]\n",
      "0 Loss: 1.1136247575283051:  12%|███████▏                                                 | 125/1000 [00:18<02:07,  6.87it/s]\n",
      "0 Loss: 1.1136247575283051:  13%|███████▏                                                 | 126/1000 [00:18<02:07,  6.87it/s]\n",
      "0 Loss: 1.1088928759098053:  13%|███████▏                                                 | 126/1000 [00:18<02:07,  6.87it/s]\n",
      "0 Loss: 1.1088928759098053:  13%|███████▏                                                 | 127/1000 [00:18<02:07,  6.83it/s]\n",
      "0 Loss: 1.1060525119304656:  13%|███████▏                                                 | 127/1000 [00:18<02:07,  6.83it/s]\n",
      "0 Loss: 1.1060525119304656:  13%|███████▎                                                 | 128/1000 [00:18<02:06,  6.92it/s]\n",
      "0 Loss: 1.0991854786872863:  13%|███████▎                                                 | 128/1000 [00:18<02:06,  6.92it/s]\n",
      "0 Loss: 1.0991854786872863:  13%|███████▎                                                 | 129/1000 [00:18<02:06,  6.86it/s]\n",
      "0 Loss: 1.0938054203987122:  13%|███████▎                                                 | 129/1000 [00:18<02:06,  6.86it/s]\n",
      "0 Loss: 1.0938054203987122:  13%|███████▍                                                 | 130/1000 [00:18<02:06,  6.88it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0 Loss: 1.0963169276714324:  13%|███████▍                                                 | 130/1000 [00:18<02:06,  6.88it/s]\n",
      "0 Loss: 1.0963169276714324:  13%|███████▍                                                 | 131/1000 [00:18<02:04,  7.01it/s]\n",
      "0 Loss: 1.0925605475902558:  13%|███████▍                                                 | 131/1000 [00:18<02:04,  7.01it/s]\n",
      "0 Loss: 1.0925605475902558:  13%|███████▌                                                 | 132/1000 [00:18<02:05,  6.94it/s]\n",
      "0 Loss: 1.0945217549800872:  13%|███████▌                                                 | 132/1000 [00:19<02:05,  6.94it/s]\n",
      "0 Loss: 1.0945217549800872:  13%|███████▌                                                 | 133/1000 [00:19<02:05,  6.92it/s]\n",
      "0 Loss: 1.0947747826576233:  13%|███████▌                                                 | 133/1000 [00:19<02:05,  6.92it/s]\n",
      "0 Loss: 1.0947747826576233:  13%|███████▋                                                 | 134/1000 [00:19<02:03,  7.01it/s]\n",
      "0 Loss: 1.098210746049881:  13%|███████▊                                                  | 134/1000 [00:19<02:03,  7.01it/s]\n",
      "0 Loss: 1.098210746049881:  14%|███████▊                                                  | 135/1000 [00:19<02:06,  6.84it/s]\n",
      "0 Loss: 1.097577655315399:  14%|███████▊                                                  | 135/1000 [00:19<02:06,  6.84it/s]\n",
      "0 Loss: 1.097577655315399:  14%|███████▉                                                  | 136/1000 [00:19<02:07,  6.77it/s]\n",
      "0 Loss: 1.0921282947063446:  14%|███████▊                                                 | 136/1000 [00:19<02:07,  6.77it/s]\n",
      "0 Loss: 1.0921282947063446:  14%|███████▊                                                 | 137/1000 [00:19<02:05,  6.89it/s]\n",
      "0 Loss: 1.0981387555599214:  14%|███████▊                                                 | 137/1000 [00:19<02:05,  6.89it/s]\n",
      "0 Loss: 1.0981387555599214:  14%|███████▊                                                 | 138/1000 [00:19<02:04,  6.90it/s]\n",
      "0 Loss: 1.1053566694259644:  14%|███████▊                                                 | 138/1000 [00:19<02:04,  6.90it/s]\n",
      "0 Loss: 1.1053566694259644:  14%|███████▉                                                 | 139/1000 [00:19<02:03,  6.99it/s]\n",
      "0 Loss: 1.106182074546814:  14%|████████                                                  | 139/1000 [00:20<02:03,  6.99it/s]\n",
      "0 Loss: 1.106182074546814:  14%|████████                                                  | 140/1000 [00:20<02:04,  6.90it/s]\n",
      "0 Loss: 1.1040325820446015:  14%|███████▉                                                 | 140/1000 [00:20<02:04,  6.90it/s]\n",
      "0 Loss: 1.1040325820446015:  14%|████████                                                 | 141/1000 [00:20<02:05,  6.82it/s]\n",
      "0 Loss: 1.1034506142139435:  14%|████████                                                 | 141/1000 [00:20<02:05,  6.82it/s]\n",
      "0 Loss: 1.1034506142139435:  14%|████████                                                 | 142/1000 [00:20<02:04,  6.88it/s]\n",
      "0 Loss: 1.1067500352859496:  14%|████████                                                 | 142/1000 [00:20<02:04,  6.88it/s]\n",
      "0 Loss: 1.1067500352859496:  14%|████████▏                                                | 143/1000 [00:20<02:06,  6.79it/s]\n",
      "0 Loss: 1.1013474702835082:  14%|████████▏                                                | 143/1000 [00:20<02:06,  6.79it/s]\n",
      "0 Loss: 1.1013474702835082:  14%|████████▏                                                | 144/1000 [00:20<02:05,  6.80it/s]\n",
      "0 Loss: 1.0990672945976256:  14%|████████▏                                                | 144/1000 [00:20<02:05,  6.80it/s]\n",
      "0 Loss: 1.0990672945976256:  14%|████████▎                                                | 145/1000 [00:20<02:03,  6.92it/s]\n",
      "0 Loss: 1.1015280425548553:  14%|████████▎                                                | 145/1000 [00:20<02:03,  6.92it/s]\n",
      "0 Loss: 1.1015280425548553:  15%|████████▎                                                | 146/1000 [00:20<02:02,  6.97it/s]\n",
      "0 Loss: 1.100703126192093:  15%|████████▍                                                 | 146/1000 [00:21<02:02,  6.97it/s]\n",
      "0 Loss: 1.100703126192093:  15%|████████▌                                                 | 147/1000 [00:21<02:01,  7.00it/s]\n",
      "0 Loss: 1.0975608825683594:  15%|████████▍                                                | 147/1000 [00:21<02:01,  7.00it/s]\n",
      "0 Loss: 1.0975608825683594:  15%|████████▍                                                | 148/1000 [00:21<02:00,  7.08it/s]\n",
      "0 Loss: 1.0978954792022706:  15%|████████▍                                                | 148/1000 [00:21<02:00,  7.08it/s]\n",
      "0 Loss: 1.0978954792022706:  15%|████████▍                                                | 149/1000 [00:21<01:59,  7.09it/s]\n",
      "0 Loss: 1.1046842575073241:  15%|████████▍                                                | 149/1000 [00:21<01:59,  7.09it/s]\n",
      "0 Loss: 1.1046842575073241:  15%|████████▌                                                | 150/1000 [00:21<01:59,  7.12it/s]\n",
      "0 Loss: 1.1012806653976441:  15%|████████▌                                                | 150/1000 [00:21<01:59,  7.12it/s]\n",
      "0 Loss: 1.1012806653976441:  15%|████████▌                                                | 151/1000 [00:21<01:58,  7.18it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-150-b1cf88408e7e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    132\u001b[0m     \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m         \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# pytorch\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.c0 = nn.Conv2d(3, 64, kernel_size=4, stride=1)\n",
    "        self.c1 = nn.Conv2d(64, 128, kernel_size=4, stride=1)\n",
    "        self.c2 = nn.Conv2d(128, 256, kernel_size=4, stride=1)\n",
    "        self.c3 = nn.Conv2d(256, 512, kernel_size=4, stride=1)\n",
    "        self.l1 = nn.Linear(512*20*20, 64)\n",
    "\n",
    "        self.b1 = nn.BatchNorm2d(128)\n",
    "        self.b2 = nn.BatchNorm2d(256)\n",
    "        self.b3 = nn.BatchNorm2d(512)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.c0(x))\n",
    "        features = F.relu(self.b1(self.c1(h)))\n",
    "        h = F.relu(self.b2(self.c2(features)))\n",
    "        h = F.relu(self.b3(self.c3(h)))\n",
    "        encoded = self.l1(h.view(x.shape[0], -1))\n",
    "        return encoded, features\n",
    "\n",
    "class GlobalDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.c0 = nn.Conv2d(128, 64, kernel_size=3)\n",
    "        self.c1 = nn.Conv2d(64, 32, kernel_size=3)\n",
    "        self.l0 = nn.Linear(32 * 22 * 22 + 64, 512)\n",
    "        self.l1 = nn.Linear(512, 512)\n",
    "        self.l2 = nn.Linear(512, 1)\n",
    "\n",
    "    def forward(self, y, M):\n",
    "        h = F.relu(self.c0(M))\n",
    "        h = self.c1(h)\n",
    "        h = h.view(y.shape[0], -1)\n",
    "        h = torch.cat((y, h), dim=1)\n",
    "        h = F.relu(self.l0(h))\n",
    "        h = F.relu(self.l1(h))\n",
    "        return self.l2(h)\n",
    "\n",
    "class LocalDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.c0 = nn.Conv2d(192, 512, kernel_size=1)\n",
    "        self.c1 = nn.Conv2d(512, 512, kernel_size=1)\n",
    "        self.c2 = nn.Conv2d(512, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.c0(x))\n",
    "        h = F.relu(self.c1(h))\n",
    "        return self.c2(h)\n",
    "\n",
    "class PriorDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l0 = nn.Linear(64, 1000)\n",
    "        self.l1 = nn.Linear(1000, 200)\n",
    "        self.l2 = nn.Linear(200, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.l0(x))\n",
    "        h = F.relu(self.l1(h))\n",
    "        return torch.sigmoid(self.l2(h))\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(64, 15)\n",
    "        self.bn1 = nn.BatchNorm1d(15)\n",
    "        self.l2 = nn.Linear(15, 10)\n",
    "        self.bn2 = nn.BatchNorm1d(10)\n",
    "        self.l3 = nn.Linear(10, 10)\n",
    "        self.bn3 = nn.BatchNorm1d(10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded, _ = x[0], x[1]\n",
    "        clazz = F.relu(self.bn1(self.l1(encoded)))\n",
    "        clazz = F.relu(self.bn2(self.l2(clazz)))\n",
    "        clazz = F.softmax(self.bn3(self.l3(clazz)), dim=1)\n",
    "        return clazz\n",
    "\n",
    "class DeepInfoMaxLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.5, beta=1.0, gamma=0.1):\n",
    "        super().__init__()\n",
    "        self.global_d = GlobalDiscriminator()\n",
    "        self.local_d = LocalDiscriminator()\n",
    "        self.prior_d = PriorDiscriminator()\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, y, M, M_prime):\n",
    "\n",
    "        # see appendix 1A of https://arxiv.org/pdf/1808.06670.pdf\n",
    "\n",
    "        y_exp = y.unsqueeze(-1).unsqueeze(-1)\n",
    "        y_exp = y_exp.expand(-1, -1, 26, 26)\n",
    "\n",
    "        y_M = torch.cat((M, y_exp), dim=1)\n",
    "        y_M_prime = torch.cat((M_prime, y_exp), dim=1)\n",
    "\n",
    "        Ej = -F.softplus(-self.local_d(y_M)).mean()\n",
    "        Em = F.softplus(self.local_d(y_M_prime)).mean()\n",
    "        LOCAL = (Em - Ej) \n",
    "\n",
    "        Ej = -F.softplus(-self.global_d(y, M)).mean()\n",
    "        Em = F.softplus(self.global_d(y, M_prime)).mean()\n",
    "        GLOBAL = (Em - Ej) * self.alpha\n",
    "\n",
    "        prior = torch.rand_like(y)\n",
    "\n",
    "        term_a = torch.log(self.prior_d(prior)).mean()\n",
    "        term_b = torch.log(1.0 - self.prior_d(y)).mean()\n",
    "        PRIOR = - (term_a + term_b) * self.gamma\n",
    "\n",
    "        return LOCAL \n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "batch_size = 50\n",
    "\n",
    "# image size 3, 32, 32 batch size must be an even number shuffle must be True\n",
    "cifar_10_train_dt = CIFAR10(r'c:\\data\\tv',  download=True, transform=ToTensor())\n",
    "cifar_10_train_l = DataLoader(cifar_10_train_dt, batch_size=batch_size, shuffle=True, drop_last=True,pin_memory=torch.cuda.is_available())\n",
    "\n",
    "encoder = Encoder().to(device)\n",
    "loss_fn = DeepInfoMaxLoss().to(device)\n",
    "optim = Adam(encoder.parameters(), lr=1e-4)\n",
    "loss_optim = Adam(loss_fn.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "for epoch in range(100):\n",
    "    batch = tqdm(cifar_10_train_l, total=len(cifar_10_train_dt) // batch_size)\n",
    "    train_loss = []\n",
    "    for x, target in batch:\n",
    "        x = x.to(device)\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss_optim.zero_grad()\n",
    "        y, M = encoder(x)\n",
    "        # rotate images to create pairs for comparison\n",
    "        M_prime = torch.cat((M[1:], M[0].unsqueeze(0)), dim=0)\n",
    "        loss = loss_fn(y, M, M_prime)\n",
    "        train_loss.append(loss.item())\n",
    "        batch.set_description(str(epoch) + ' Loss: ' + str(stats.mean(train_loss[-20:])))\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        loss_optim.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T00:15:38.469256Z",
     "start_time": "2019-02-12T00:15:38.459282Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T00:15:40.521984Z",
     "start_time": "2019-02-12T00:15:38.470253Z"
    },
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T00:16:11.113749Z",
     "start_time": "2019-02-12T00:15:40.522762Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T00:16:11.121727Z",
     "start_time": "2019-02-12T00:15:17.972Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# import lib\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import nn as nn\n",
    "\n",
    "import torch\n",
    "from torchvision.datasets.cifar import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import statistics as stats\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T00:16:11.123724Z",
     "start_time": "2019-02-12T00:15:17.975Z"
    },
    "code_folding": [
     0,
     20,
     21,
     37,
     48,
     59,
     75,
     112
    ]
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.c0 = nn.Conv2d(3, 64, kernel_size=4, stride=1)\n",
    "        self.c1 = nn.Conv2d(64, 128, kernel_size=4, stride=1)\n",
    "        self.c2 = nn.Conv2d(128, 256, kernel_size=4, stride=1)\n",
    "        self.c3 = nn.Conv2d(256, 512, kernel_size=4, stride=1)\n",
    "        self.l1 = nn.Linear(512*20*20, 64)\n",
    "\n",
    "        self.b1 = nn.BatchNorm2d(128)\n",
    "        self.b2 = nn.BatchNorm2d(256)\n",
    "        self.b3 = nn.BatchNorm2d(512)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.c0(x))                      # (64, 64, 29, 29)\n",
    "        features = F.relu(self.b1(self.c1(h)))      # (64, 128, 26, 26)\n",
    "        h = F.relu(self.b2(self.c2(features)))      # (64, 256, 23, 23)\n",
    "        h = F.relu(self.b3(self.c3(h)))             # (64, 512, 20, 20)\n",
    "        encoded = self.l1(h.view(x.shape[0], -1))   # (batch,64)\n",
    "        return encoded, features  \n",
    "class GlobalDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.c0 = nn.Conv2d(128, 64, kernel_size=3) # (64, 64, 24, 24)\n",
    "        self.c1 = nn.Conv2d(64, 32,  kernel_size=3)  # (64, 32, 22, 22)\n",
    "        self.l0 = nn.Linear(32 * 22 * 22 + 64, 512) # (64, 512)\n",
    "        self.l1 = nn.Linear(512, 512)               # (512, 512)\n",
    "        self.l2 = nn.Linear(512, 1)                 # (512, 1)\n",
    "\n",
    "    def forward(self, y, M):\n",
    "        h = F.relu(self.c0(M))\n",
    "        h = self.c1(h)\n",
    "        h = h.view(y.shape[0], -1)\n",
    "        h = torch.cat((y, h), dim=1)\n",
    "        h = F.relu(self.l0(h))\n",
    "        h = F.relu(self.l1(h))\n",
    "        return self.l2(h)  \n",
    "class LocalDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.c0 = nn.Conv2d(192, 512, kernel_size=1)\n",
    "        self.c1 = nn.Conv2d(512, 512, kernel_size=1)\n",
    "        self.c2 = nn.Conv2d(512, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.c0(x))\n",
    "        h = F.relu(self.c1(h))\n",
    "        return self.c2(h)  \n",
    "class PriorDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l0 = nn.Linear(64, 1000)\n",
    "        self.l1 = nn.Linear(1000, 200)\n",
    "        self.l2 = nn.Linear(200, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.l0(x))\n",
    "        h = F.relu(self.l1(h))\n",
    "        return torch.sigmoid(self.l2(h))\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(64, 15)\n",
    "        self.bn1 = nn.BatchNorm1d(15)\n",
    "        self.l2 = nn.Linear(15, 10)\n",
    "        self.bn2 = nn.BatchNorm1d(10)\n",
    "        self.l3 = nn.Linear(10, 10)\n",
    "        self.bn3 = nn.BatchNorm1d(10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded, _ = x[0], x[1]\n",
    "        clazz = F.relu(self.bn1(self.l1(encoded)))\n",
    "        clazz = F.relu(self.bn2(self.l2(clazz)))\n",
    "        clazz = F.softmax(self.bn3(self.l3(clazz)), dim=1)\n",
    "        return clazz\n",
    "class DeepInfoMaxLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.5, beta=1.0, gamma=0.1):\n",
    "        super().__init__()\n",
    "        self.global_d = GlobalDiscriminator()\n",
    "        self.local_d = LocalDiscriminator()\n",
    "        self.prior_d = PriorDiscriminator()\n",
    "        self.alpha   = alpha\n",
    "        self.beta    = beta\n",
    "        self.gamma   = gamma\n",
    "\n",
    "    def forward(self, y, M, M_prime):\n",
    "\n",
    "        # see appendix 1A of https://arxiv.org/pdf/1808.06670.pdf\n",
    "        \n",
    "        # CREATE\n",
    "        y_exp = y.unsqueeze(-1).unsqueeze(-1)\n",
    "        y_exp = y_exp.expand(-1, -1, 26, 26)\n",
    "        y_M = torch.cat((M, y_exp), dim=1)\n",
    "        y_M_prime = torch.cat((M_prime, y_exp), dim=1)\n",
    "\n",
    "        # Local          \n",
    "        Ej = -F.softplus(-self.local_d(y_M)).mean()\n",
    "        Em = F.softplus(self.local_d(y_M_prime)).mean()\n",
    "        LOCAL = (Em - Ej) * self.beta\n",
    "       \n",
    "        # Global         \n",
    "        Ej = -F.softplus(-self.global_d(y, M)).mean()\n",
    "        Em = F.softplus(self.global_d(y, M_prime)).mean()\n",
    "        GLOBAL = (Em - Ej) * self.alpha\n",
    "        \n",
    "        # Prior\n",
    "        prior = torch.rand_like(y)\n",
    "        term_a = torch.log(self.prior_d(prior)).mean()\n",
    "        term_b = torch.log(1.0 - self.prior_d(y)).mean()\n",
    "        PRIOR = - (term_a + term_b) * self.gamma\n",
    "\n",
    "        return LOCAL + GLOBAL + PRIOR\n",
    "class DeepInfoAsLatent(nn.Module):\n",
    "    def __init__(self, run, epoch):\n",
    "        super().__init__()\n",
    "        model_path = Path(r'c:/data/deepinfomax/models') / Path(str(run)) / Path('encoder' + str(epoch) + '.wgt')\n",
    "        self.encoder = Encoder()\n",
    "        self.encoder.load_state_dict(torch.load(str(model_path)))\n",
    "        self.classifier = Classifier()\n",
    "\n",
    "    def forward(self, x):\n",
    "        z, features = self.encoder(x)\n",
    "        z = z.detach()\n",
    "        return self.classifier((z, features))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "batch_size = 10\n",
    "\n",
    "# image size 3, 32, 32 batch size must be an even numbershuffle must be True\n",
    "cifar_10_train_dt = CIFAR10(r'c:\\data\\tv',  download=True, transform=ToTensor())\n",
    "cifar_10_train_l  = DataLoader(cifar_10_train_dt, batch_size=batch_size, shuffle=True, drop_last=True,pin_memory=torch.cuda.is_available())\n",
    "\n",
    "encoder    = Encoder().to(device)\n",
    "loss_fn    = DeepInfoMaxLoss().to(device)\n",
    "optim      = Adam(encoder.parameters(), lr=1e-4)\n",
    "loss_optim = Adam(loss_fn.parameters(), lr=1e-4)\n",
    "\n",
    "for epoch in range(100):\n",
    "    batch = tqdm(cifar_10_train_l, total=len(cifar_10_train_dt) // batch_size)\n",
    "    train_loss = []\n",
    "    \n",
    "    for x, target in batch:\n",
    "        x = x.to(device)\n",
    "\n",
    "        optim.zero_grad(); loss_optim.zero_grad()\n",
    "        y, M = encoder(x)\n",
    "        # y - > (64, 128, 26, 26)\n",
    "        # M - > (batch,64)\n",
    "        \n",
    "        # rotate images to create pairs for comparison (ROTATING)\n",
    "        M_prime = torch.cat((M[1:], M[0].unsqueeze(0)), dim=0)\n",
    "        loss = loss_fn(y, M, M_prime) # ()\n",
    "        # sys.exit()\n",
    "        \n",
    "        train_loss.append(loss.item())\n",
    "        batch.set_description(str(epoch) + ' Loss: ' + str(stats.mean(train_loss[-20:])))\n",
    "        loss.backward()\n",
    "        optim.step(); loss_optim.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-07T03:12:26.562738Z",
     "start_time": "2019-02-07T03:12:19.110Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T00:16:11.124721Z",
     "start_time": "2019-02-12T00:15:17.984Z"
    }
   },
   "outputs": [],
   "source": [
    "# batch = tqdm(cifar_10_train_l, total=len(cifar_10_train_dt) // batch_size)\n",
    "for x, target in batch:\n",
    "    temp = np.swapaxes(np.swapaxes(x.numpy(),1,3),2,1)\n",
    "    plt.imshow(temp[0])\n",
    "    plt.show()\n",
    "    print(temp.shape)\n",
    "    print(target.numpy().shape)\n",
    "    sys.exit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
