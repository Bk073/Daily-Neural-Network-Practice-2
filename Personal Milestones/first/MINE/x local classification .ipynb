{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T04:32:25.024103Z",
     "start_time": "2019-02-12T04:32:21.716319Z"
    },
    "code_folding": [
     0,
     33,
     35,
     62,
     70,
     94,
     97
    ]
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "if (!(\"Notification\" in window)) {\n",
       "    alert(\"This browser does not support desktop notifications, so the %%notify magic will not work.\");\n",
       "} else if (Notification.permission !== 'granted' && Notification.permission !== 'denied') {\n",
       "    Notification.requestPermission(function (permission) {\n",
       "        if(!('permission' in Notification)) {\n",
       "            Notification.permission = permission;\n",
       "        }\n",
       "    })\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import Library and some random image data set\n",
    "import tensorflow as tf\n",
    "import numpy      as np\n",
    "import seaborn    as sns \n",
    "import pandas     as pd\n",
    "import os,sys\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "np.random.seed(78); tf.set_random_seed(78)\n",
    "\n",
    "# get some of the STL data set\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils import shuffle\n",
    "from skimage import util \n",
    "from skimage.transform import resize\n",
    "from skimage.io import imread\n",
    "import warnings\n",
    "from numpy import inf\n",
    "\n",
    "from scipy.stats import kurtosis,skew\n",
    "\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import gc\n",
    "from IPython.display import display, clear_output\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from matplotlib import animation\n",
    "%load_ext jupyternotify\n",
    "\n",
    "# Def: Read STL 10 images\n",
    "def read_STL10_data():\n",
    "    # read all of the data (STL 10) https://github.com/mttk/STL10\n",
    "    def read_all_images(path_to_data):\n",
    "        \"\"\"\n",
    "        :param path_to_data: the file containing the binary images from the STL-10 dataset\n",
    "        :return: an array containing all the images\n",
    "        \"\"\"\n",
    "\n",
    "        with open(path_to_data, 'rb') as f:\n",
    "            # read whole file in uint8 chunks\n",
    "            everything = np.fromfile(f, dtype=np.uint8)\n",
    "\n",
    "            # We force the data into 3x96x96 chunks, since the\n",
    "            # images are stored in \"column-major order\", meaning\n",
    "            # that \"the first 96*96 values are the red channel,\n",
    "            # the next 96*96 are green, and the last are blue.\"\n",
    "            # The -1 is since the size of the pictures depends\n",
    "            # on the input file, and this way numpy determines\n",
    "            # the size on its own.\n",
    "\n",
    "            images = np.reshape(everything, (-1, 3, 96, 96))\n",
    "\n",
    "            # Now transpose the images into a standard image format\n",
    "            # readable by, for example, matplotlib.imshow\n",
    "            # You might want to comment this line or reverse the shuffle\n",
    "            # if you will use a learning algorithm like CNN, since they like\n",
    "            # their channels separated.\n",
    "            images = np.transpose(images, (0, 3, 2, 1))\n",
    "            return images\n",
    "    def read_labels(path_to_labels):\n",
    "        \"\"\"\n",
    "        :param path_to_labels: path to the binary file containing labels from the STL-10 dataset\n",
    "        :return: an array containing the labels\n",
    "        \"\"\"\n",
    "        with open(path_to_labels, 'rb') as f:\n",
    "            labels = np.fromfile(f, dtype=np.uint8)\n",
    "            return labels\n",
    "    def show_images(data,row=1,col=1):\n",
    "        fig=plt.figure(figsize=(10,10))\n",
    "        columns = col; rows = row\n",
    "        for i in range(1, columns*rows +1):\n",
    "            fig.add_subplot(rows, columns, i)\n",
    "            plt.imshow(data[i-1])\n",
    "        plt.show()\n",
    "\n",
    "    train_images = read_all_images(\"../../../DataSet/STL10/stl10_binary/train_X.bin\") / 255.0\n",
    "    train_labels = read_labels    (\"../../../DataSet/STL10/stl10_binary/train_Y.bin\")\n",
    "    test_images  = read_all_images(\"../../../DataSet/STL10/stl10_binary/test_X.bin\")  / 255.0\n",
    "    test_labels  = read_labels    (\"../../../DataSet/STL10/stl10_binary/test_y.bin\")\n",
    "\n",
    "    label_encoder= OneHotEncoder(sparse=False,categories='auto')\n",
    "    train_labels = label_encoder.fit_transform(train_labels.reshape((-1,1)))\n",
    "    test_labels  = label_encoder.fit_transform(test_labels.reshape((-1,1)))\n",
    "\n",
    "    print(train_images.shape,train_images.max(),train_images.min())\n",
    "    print(train_labels.shape,train_labels.max(),train_labels.min())\n",
    "    print(test_images.shape,test_images.max(),test_images.min())\n",
    "    print(test_labels.shape,test_labels.max(),test_labels.min())\n",
    "    return train_images,train_labels,test_images,test_labels\n",
    "\n",
    "# Def: Read CIFAR 10 images\n",
    "def read_CIFAR10_data():\n",
    "    # ====== miscellaneous =====\n",
    "    # code from: https://github.com/tensorflow/tensorflow/issues/8246\n",
    "    def tf_repeat(tensor, repeats):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "\n",
    "        input: A Tensor. 1-D or higher.\n",
    "        repeats: A list. Number of repeat for each dimension, length must be the same as the number of dimensions in input\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        A Tensor. Has the same type as input. Has the shape of tensor.shape * repeats\n",
    "        \"\"\"\n",
    "        expanded_tensor = tf.expand_dims(tensor, -1)\n",
    "        multiples = [1] + repeats\n",
    "        tiled_tensor = tf.tile(expanded_tensor, multiples = multiples)\n",
    "        repeated_tesnor = tf.reshape(tiled_tensor, tf.shape(tensor) * repeats)\n",
    "        return repeated_tesnor\n",
    "    def unpickle(file):\n",
    "        import pickle\n",
    "        with open(file, 'rb') as fo:\n",
    "            dict = pickle.load(fo, encoding='bytes')\n",
    "        return dict\n",
    "    # ====== miscellaneous =====\n",
    "\n",
    "    # data\n",
    "    PathDicom = \"../../../Dataset/cifar-10-batches-py/\"\n",
    "    lstFilesDCM = []  # create an empty list\n",
    "    for dirName, subdirList, fileList in os.walk(PathDicom):\n",
    "        for filename in fileList:\n",
    "            if not \".html\" in filename.lower() and not  \".meta\" in filename.lower():  # check whether the file's DICOM\n",
    "                lstFilesDCM.append(os.path.join(dirName,filename))\n",
    "\n",
    "    # Read the data traind and Test\n",
    "    batch0 = unpickle(lstFilesDCM[0])\n",
    "    batch1 = unpickle(lstFilesDCM[1])\n",
    "    batch2 = unpickle(lstFilesDCM[2])\n",
    "    batch3 = unpickle(lstFilesDCM[3])\n",
    "    batch4 = unpickle(lstFilesDCM[4])\n",
    "\n",
    "    onehot_encoder = OneHotEncoder(sparse=True)\n",
    "    train_batch = np.vstack((batch0[b'data'],batch1[b'data'],batch2[b'data'],batch3[b'data'],batch4[b'data']))\n",
    "    train_label = np.expand_dims(np.hstack((batch0[b'labels'],batch1[b'labels'],batch2[b'labels'],batch3[b'labels'],batch4[b'labels'])).T,axis=1).astype(np.float64)\n",
    "    train_label = onehot_encoder.fit_transform(train_label).toarray().astype(np.float64)\n",
    "\n",
    "    test_batch = unpickle(lstFilesDCM[5])[b'data']\n",
    "    test_label = np.expand_dims(np.array(unpickle(lstFilesDCM[5])[b'labels']),axis=0).T.astype(np.float64)\n",
    "    test_label = onehot_encoder.fit_transform(test_label).toarray().astype(np.float64)\n",
    "\n",
    "    # reshape data\n",
    "    train_batch = np.reshape(train_batch,(len(train_batch),3,32,32)); test_batch = np.reshape(test_batch,(len(test_batch),3,32,32))\n",
    "    # rotate data\n",
    "    train_batch = np.rot90(np.rot90(train_batch,1,axes=(1,3)),3,axes=(1,2)).astype(np.float64); test_batch = np.rot90(np.rot90(test_batch,1,axes=(1,3)),3,axes=(1,2)).astype(np.float64)\n",
    "    # normalize\n",
    "    train_batch= train_batch/255.0; test_batch = test_batch/255.0\n",
    "\n",
    "    # print out the data shape and the max and min value\n",
    "    print(train_batch.shape,train_batch.max(),train_batch.min())\n",
    "    print(train_label.shape,train_label.max(),train_label.min())\n",
    "    print(test_batch.shape,test_batch.max(),test_batch.min())\n",
    "    print(test_label.shape,test_label.max(),test_label.min())\n",
    "    return train_batch,train_label,test_batch,test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T04:32:27.161364Z",
     "start_time": "2019-02-12T04:32:25.027096Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3) 1.0 0.0\n",
      "(50000, 10) 1.0 0.0\n",
      "(10000, 32, 32, 3) 1.0 0.0\n",
      "(10000, 10) 1.0 0.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# read the data\n",
    "train_images,train_labels,test_images,test_labels = read_CIFAR10_data()\n",
    "rand_choice = np.random.choice(len(train_images))\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(train_images[rand_choice])\n",
    "plt.title(str(train_labels[rand_choice]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T04:32:27.197281Z",
     "start_time": "2019-02-12T04:32:27.162361Z"
    },
    "code_folding": [
     11,
     49
    ]
   },
   "outputs": [],
   "source": [
    "# create the layers\n",
    "def tf_softmax(x):    return tf.nn.softmax(x)\n",
    "def tf_relu(x):       return tf.nn.relu(x)\n",
    "def d_tf_relu(x):     return tf.cast(tf.greater_equal(x,0),tf.float32) * 1.0\n",
    "def tf_iden(x):       return x\n",
    "def d_tf_iden(x):     return tf.ones_like(x)\n",
    "def tf_sigmoid(x):    return tf.nn.sigmoid(x)\n",
    "def d_tf_sigmoid(x):  return tf_sigmoid(x) * (1.0-tf_sigmoid(x))\n",
    "def tf_softplus(x):   return tf.nn.softplus(x)\n",
    "def d_tf_softplus(x): return tf.nn.sigmoid(x)\n",
    "\n",
    "class FNN():\n",
    "\n",
    "    def __init__(self,inc,outc,act=tf_iden,d_act=d_tf_iden,special_init=False):\n",
    "        if special_init:\n",
    "            interval = np.sqrt(6.0 / (inc + outc + 1.0))\n",
    "            self.w = tf.Variable(tf.random_uniform(shape=(inc, outc),minval=-interval,maxval=interval,dtype=tf.float32,seed=2))\n",
    "        else:\n",
    "            self.w = tf.Variable(tf.random_normal([inc,outc], stddev=0.05,seed=2,dtype=tf.float32))\n",
    "\n",
    "        self.m,self.v = tf.Variable(tf.zeros_like(self.w)),tf.Variable(tf.zeros_like(self.w))\n",
    "        self.act,self.d_act = act,d_act\n",
    "\n",
    "    def getw(self): return self.w\n",
    "    def feedforward(self,input=None):\n",
    "        self.input = input\n",
    "        self.layer = tf.matmul(input,self.w) \n",
    "        self.layerA = self.act(self.layer)\n",
    "        return self.layer,self.layerA\n",
    "\n",
    "    def backprop(self,gradient=None,which_reg=0):\n",
    "        grad_part_1 = gradient\n",
    "        grad_part_2 = self.d_act(self.layer)\n",
    "        grad_part_3 = self.input\n",
    "\n",
    "        grad_middle = grad_part_1 * grad_part_2\n",
    "        grad  = tf.matmul(tf.transpose(grad_part_3),grad_middle) /batch_size\n",
    "        grad_pass = tf.matmul(grad_middle,tf.transpose(self.w))\n",
    "\n",
    "        update_w = []\n",
    "        update_w.append(tf.assign( self.m,self.m*beta1 + (1-beta1) * (grad)   ))\n",
    "        update_w.append(tf.assign( self.v,self.v*beta2 + (1-beta2) * (grad ** 2)   ))\n",
    "        m_hat = self.m / (1-beta1)\n",
    "        v_hat = self.v / (1-beta2)\n",
    "        adam_middle = m_hat *  learning_rate/(tf.sqrt(v_hat) + adam_e)\n",
    "        update_w.append(tf.assign(self.w,tf.subtract(self.w,adam_middle )))\n",
    "\n",
    "        return grad_pass,update_w\n",
    "    \n",
    "class CNN():\n",
    "\n",
    "    def __init__(self,k,inc,out, stddev=0.05,act=tf_relu,d_act=d_tf_relu):\n",
    "        self.w              = tf.Variable(tf.random_normal([k,k,inc,out],stddev=stddev,seed=2,dtype=tf.float32))\n",
    "        self.m1,self.v1       = tf.Variable(tf.zeros_like(self.w)),tf.Variable(tf.zeros_like(self.w))\n",
    "        self.m2,self.v2       = tf.Variable(tf.zeros_like(self.w)),tf.Variable(tf.zeros_like(self.w))\n",
    "\n",
    "        self.act,self.d_act = act,d_act\n",
    "\n",
    "    def getw(self): return self.w\n",
    "    \n",
    "    # Feed Forward for two variables\n",
    "    def feedforward1(self,input,stride=1,padding='VALID'):\n",
    "        self.input1  = input\n",
    "        self.layer1  = tf.nn.conv2d(self.input1,self.w,strides=[1,stride,stride,1],padding=padding) \n",
    "        self.layerA1 = self.act(self.layer1)\n",
    "        return self.layer1, self.layerA1\n",
    "    def feedforward2(self,input,stride=1,padding='VALID'):\n",
    "        self.input2  = input\n",
    "        self.layer2  = tf.nn.conv2d(self.input2,self.w,strides=[1,stride,stride,1],padding=padding) \n",
    "        self.layerA2 = self.act(self.layer2)\n",
    "        return self.layer2, self.layerA2\n",
    "    \n",
    "    # Back Prop for two variables\n",
    "    def backprop1(self,gradient,stride=1,padding='VALID'):\n",
    "        grad_part_1 = gradient\n",
    "        grad_part_2 = self.d_act(self.layer1)\n",
    "        grad_part_3 = self.input1\n",
    "\n",
    "        grad_middle = grad_part_1 * grad_part_2\n",
    "        grad        = tf.nn.conv2d_backprop_filter(input = grad_part_3,filter_sizes = tf.shape(self.w),  out_backprop = grad_middle,strides=[1,stride,stride,1],padding=padding) \n",
    "        grad_pass   = tf.nn.conv2d_backprop_input (input_sizes = tf.shape(self.input1),filter= self.w,   out_backprop = grad_middle,strides=[1,stride,stride,1],padding=padding)\n",
    "\n",
    "        update_w = []\n",
    "        update_w.append(tf.assign( self.m1,self.m1*beta1 + (1-beta1) * (grad)   ))\n",
    "        update_w.append(tf.assign( self.v1,self.v1*beta2 + (1-beta2) * (grad ** 2)   ))\n",
    "        m_hat = self.m1 / (1-beta1) ; v_hat = self.v1 / (1-beta2)\n",
    "        adam_middle = m_hat * learning_rate/(tf.sqrt(v_hat) + adam_e)\n",
    "        update_w.append(tf.assign(self.w,tf.subtract(self.w,adam_middle  )))\n",
    "        return grad_pass,grad,update_w\n",
    "    \n",
    "    def backprop2(self,gradient,stride=1,padding='VALID'):\n",
    "        grad_part_1 = gradient\n",
    "        grad_part_2 = self.d_act(self.layer2)\n",
    "        grad_part_3 = self.input2\n",
    "\n",
    "        grad_middle = grad_part_1 * grad_part_2\n",
    "        grad        = tf.nn.conv2d_backprop_filter(input = grad_part_3,filter_sizes = tf.shape(self.w),  out_backprop = grad_middle,strides=[1,stride,stride,1],padding=padding) \n",
    "        grad_pass   = tf.nn.conv2d_backprop_input (input_sizes = tf.shape(self.input1),filter= self.w,   out_backprop = grad_middle,strides=[1,stride,stride,1],padding=padding)\n",
    "\n",
    "        update_w = []\n",
    "        update_w.append(tf.assign( self.m2,self.m2*beta1 + (1-beta1) * (grad)   ))\n",
    "        update_w.append(tf.assign( self.v2,self.v2*beta2 + (1-beta2) * (grad ** 2)   ))\n",
    "        m_hat = self.m2 / (1-beta1) ; v_hat = self.v2 / (1-beta2)\n",
    "        adam_middle = m_hat * learning_rate/(tf.sqrt(v_hat) + adam_e)\n",
    "        update_w.append(tf.assign(self.w,tf.subtract(self.w,adam_middle  )))\n",
    "        return grad_pass,grad,update_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T04:32:27.898872Z",
     "start_time": "2019-02-12T04:32:27.200773Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Relu_1:0\", shape=(25, 26, 26, 18), dtype=float32)\n",
      "Tensor(\"concat:0\", shape=(25, 26, 26, 82), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# create layers\n",
    "num_eps   = 10; num_epoch = 200; learning_rate = 0.0001 ; batch_size = 25;  \n",
    "beta1,beta2,adam_e = 0.9,0.999,1e-8; print_iter = 100\n",
    "\n",
    "l1 = CNN(4,3,9)\n",
    "l2 = CNN(4,9,18)\n",
    "l3 = CNN(4,18,27)\n",
    "l4 = CNN(4,27,30)\n",
    "l5 = FNN(20*20*30,64,act=tf_iden,d_act=d_tf_iden)\n",
    "\n",
    "local_l1 = CNN(3,82,96)\n",
    "local_l2 = CNN(3,96,96)\n",
    "local_l3 = CNN(3,96, 1,act=tf_iden,d_act=d_tf_iden)\n",
    "\n",
    "# ======== PLACE HOLDERS ========\n",
    "x = tf.placeholder(tf.float32,(batch_size,32,32,3))\n",
    "\n",
    "# ======== ENCODE ========\n",
    "layer1,layer1a = l1.feedforward1(x)\n",
    "layer2,layer2a = l2.feedforward1(layer1a)\n",
    "print(layer2a)\n",
    "layer3,layer3a = l3.feedforward1(layer2a) # here is the cut!\n",
    "layer4,layer4a = l4.feedforward1(layer3a)\n",
    "layer5_input   = tf.reshape(layer4a,[batch_size,-1]) \n",
    "layer5,layer5a = l5.feedforward(layer5_input)\n",
    "\n",
    "# ======== GENERATE NEW FEATURES ========\n",
    "layer2r = tf.transpose(layer2a,[0,2,1,3])\n",
    "_,width,_,channel    = layer2a.get_shape()\n",
    "encoded_expand   = tf.tile(layer5a[:,None,None,:],[1,width,width,1])\n",
    "encoded_M        = tf.concat([layer2a,encoded_expand],3)\n",
    "print(encoded_M)\n",
    "encoded_M_prime  = tf.concat([layer2r,encoded_expand],3)\n",
    "\n",
    "# ======== LOCAL NEW FEATURES ========\n",
    "real_layer1,real_layer1a = local_l1.feedforward1(encoded_M)\n",
    "real_layer2,real_layer2a = local_l2.feedforward1(real_layer1a)\n",
    "real_layer3,real_layer3a = local_l3.feedforward1(real_layer2a)\n",
    "\n",
    "fake_layer1,fake_layer1a = local_l1.feedforward1(encoded_M_prime)\n",
    "fake_layer2,fake_layer2a = local_l2.feedforward1(fake_layer1a)\n",
    "fake_layer3,fake_layer3a = local_l3.feedforward1(fake_layer2a)\n",
    "\n",
    "# ======== LOSS ========\n",
    "loss =  tf.reduce_mean(tf_relu(fake_layer3a)) - tf.reduce_mean(-tf_relu(-real_layer3a)) \n",
    "# auto_train = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "\n",
    "# ======== BACK PROP FOR Discriminator ========\n",
    "real_grad = (d_tf_relu(-real_layer3a) * -1)/batch_size\n",
    "real_grad3_pass,real_grad3,real_grad3up = local_l3.backprop1(real_grad)\n",
    "real_grad2_pass,real_grad2,real_grad2up = local_l2.backprop1(real_grad3_pass)\n",
    "real_grad1_pass,real_grad1,real_grad1up = local_l1.backprop1(real_grad2_pass)\n",
    "\n",
    "fake_grad = (d_tf_relu(fake_layer3a))/batch_size\n",
    "fake_grad3_pass,fake_grad3,fake_grad3up = local_l3.backprop1(fake_grad)\n",
    "fake_grad2_pass,fake_grad2,fake_grad2up = local_l2.backprop1(fake_grad3_pass)\n",
    "fake_grad1_pass,fake_grad1,fake_grad1up = local_l1.backprop1(fake_grad2_pass)\n",
    "\n",
    "# ======== UNDO THE CHANGE ========\n",
    "fake_features,fake_encode = fake_grad1_pass[:,:,:,:channel],fake_grad1_pass[:,:,:,channel:]\n",
    "fake_encode   = tf.reduce_mean(fake_encode,[1,2])\n",
    "fake_features = tf.transpose(fake_features,[0,2,1,3])\n",
    "\n",
    "real_features,real_encode = real_grad1_pass[:,:,:,:channel],real_grad1_pass[:,:,:,channel:]\n",
    "real_encode   = tf.reduce_mean(real_encode,[1,2])\n",
    "\n",
    "# ======== BACK PROP FOR Encoder ========\n",
    "grad_feature_all = real_features + fake_features\n",
    "grad_encoded_all = real_encode   + fake_encode\n",
    "\n",
    "grad5p,grad5_up = l5.backprop(grad_encoded_all)\n",
    "grad4_input    = tf.reshape(grad5p,[batch_size,20,20,30])\n",
    "grad4p,grad4,grad4_up = l4.backprop1(grad4_input)\n",
    "grad3p,grad3,grad3_up = l3.backprop1(grad4p)\n",
    "grad2_input           = grad3p + grad_feature_all \n",
    "grad2p,grad2,grad2_up = l2.backprop1(grad2_input)\n",
    "grad1p,grad1,grad1_up = l1.backprop1(grad2p)\n",
    "\n",
    "grad_update_all = real_grad3up + real_grad2up + real_grad1up + fake_grad3up + fake_grad2up + fake_grad1up + grad5_up + grad4_up + grad3_up + grad2_up + grad1_up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T04:33:11.693920Z",
     "start_time": "2019-02-12T04:33:11.267144Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# create the classification network \n",
    "l1_c = CNN(3,18,64)\n",
    "l2_c = CNN(3,64,64)\n",
    "l3_c = CNN(3,64,128)\n",
    "\n",
    "l4_c = CNN(3,128,128)\n",
    "l5_c = CNN(1,128,128)\n",
    "l6_c = CNN(1,128,10)\n",
    "\n",
    "x_c = tf.placeholder(tf.float32,(batch_size,26,26,18))\n",
    "y_c = tf.placeholder(tf.float32,(batch_size,10))\n",
    "\n",
    "layer1_c,layer1_ca = l1_c.feedforward1(x_c)\n",
    "layer2_c,layer2_ca = l2_c.feedforward1(layer1_ca)\n",
    "layer3_c,layer3_ca = l3_c.feedforward1(layer2_ca,2)\n",
    "\n",
    "layer4_c,layer4_ca = l4_c.feedforward1(layer3_ca)\n",
    "layer5_c,layer5_ca = l5_c.feedforward1(layer4_ca)\n",
    "layer6_c,layer6_ca = l6_c.feedforward1(layer5_ca)\n",
    "\n",
    "final_layer   = tf.reduce_mean(layer6_ca,(1,2))\n",
    "final_softmax = tf_softmax(final_layer)\n",
    "cost          = -tf.reduce_mean(y_c * tf.log(final_softmax + 1e-8))\n",
    "correct_prediction = tf.equal(tf.argmax(final_softmax, 1), tf.argmax(y_c, 1))\n",
    "accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "gradient_c = tf.tile((final_softmax-y_c)[:,None,None,:],[1,8,8,1])/batch_size\n",
    "grad6p_c,grad6w_c,grad6_up_c = l6_c.backprop1(gradient_c)\n",
    "grad5p_c,grad5w_c,grad5_up_c = l5_c.backprop1(grad6p_c)\n",
    "grad4p_c,grad4w_c,grad4_up_c = l4_c.backprop1(grad5p_c)\n",
    "grad3p_c,grad3w_c,grad3_up_c = l3_c.backprop1(grad4p_c,stride=2)\n",
    "grad2p_c,grad2w_c,grad2_up_c = l2_c.backprop1(grad3p_c)\n",
    "grad1p_c,grad1w_c,grad1_up_c = l1_c.backprop1(grad2p_c)\n",
    "\n",
    "gradient_update_c = grad6_up_c + grad5_up_c + grad4_up_c + grad3_up_c + grad2_up_c + grad1_up_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T04:39:53.445357Z",
     "start_time": "2019-02-12T04:33:24.254779Z"
    },
    "code_folding": [
     12,
     24
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Current Iter : 0/200 batch : 49975/50000 loss : 0.00089617515\n",
      "\n",
      "Current : 0\t Train Acc : 0.243\t Test Acc : 0.0\t0.04\n",
      " Current Iter : 1/200 batch : 49975/50000 loss : 4.7301605e-065\n",
      "\n",
      " Current Iter : 2/200 batch : 49975/50000 loss : 0.0159305e-055\n",
      "\n",
      "Current : 2\t Train Acc : 0.208\t Test Acc : 0.0\t0.08\n",
      " Current Iter : 3/200 batch : 49975/50000 loss : 0.05840885e-05\n",
      "\n",
      " Current Iter : 4/200 batch : 49975/50000 loss : 0.0688696e-067\n",
      "\n",
      "Current : 4\t Train Acc : 0.2\t Test Acc : 0.0\t: 0.08\n",
      " Current Iter : 5/200 batch : 49975/50000 loss : 0.0810009e-065\n",
      "\n",
      " Current Iter : 6/200 batch : 49975/50000 loss : 0.04759e-06075\n",
      "\n",
      "Current Iter : 6/200 batch : 20050/50000 acc : 0.04\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-31b0ca418a8d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[0mencoded_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer2a\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mcurrent_batch\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m             \u001b[0msess_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgradient_update_c\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx_c\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mencoded_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_c\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mcurrent_batch_label\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m             sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + \n\u001b[0;32m     28\u001b[0m                              \u001b[1;34m' batch : '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_batch_index\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/'\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1332\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# loop\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "avg_acc_train = 0; avg_acc_test  = 0; train_acc = [];test_acc = []\n",
    "for iter in range(num_epoch):\n",
    "\n",
    "        \n",
    "    # 2. Use the encoding network for data generation and classification\n",
    "    for current_batch_index in range(0,len(train_images),batch_size):\n",
    "        current_batch       = train_images[current_batch_index:current_batch_index+batch_size] \n",
    "        current_batch_label = train_labels[current_batch_index:current_batch_index+batch_size] \n",
    "\n",
    "        encoded_data = sess.run(layer2a,feed_dict={x:current_batch})\n",
    "        sess_results = sess.run([accuracy,gradient_update_c],feed_dict={x_c:encoded_data,y_c:current_batch_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + \n",
    "                         ' batch : ' + str(current_batch_index) + '/'+ str(len(train_images)) + \n",
    "                         ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_train = avg_acc_train + sess_results[0]\n",
    "\n",
    "    # Test Accuracy   test_images,test_label  \n",
    "    for current_batch_index in range(0,len(test_images), batch_size):\n",
    "        current_data  = test_images[current_batch_index:current_batch_index+batch_size]\n",
    "        current_label = test_labels[current_batch_index:current_batch_index+batch_size]\n",
    "        sess_results  = sess.run([accuracy],feed_dict={x_c:current_data,y_c:current_label})\n",
    "        sys.stdout.write(' Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + \n",
    "                         ' batch : ' + str(current_batch_index) + '/'+ str(len(test_images)) + \n",
    "                         ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_test = avg_acc_test + sess_results[0]   \n",
    "\n",
    "    if avg_acc_train/(len(train_images/batch_size)>avg_acc_test/(len(test_images/batch_size):\n",
    "        # 1. Train the Encoding Network\n",
    "        for current_batch_index in range(0,len(train_images),batch_size):\n",
    "            current_batch       = train_images[current_batch_index:current_batch_index+batch_size]\n",
    "            sess_results  = sess.run([loss,grad_update_all],feed_dict={x:current_batch})\n",
    "            sys.stdout.write(' Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + \n",
    "                             ' batch : ' + str(current_batch_index) + '/'+ str(len(train_images)) + \n",
    "                             ' loss : ' + str(sess_results[0]) + '\\r')\n",
    "            sys.stdout.flush();     \n",
    "    # ======================== print reset ========================\n",
    "    if iter%2 == 0 :\n",
    "        sys.stdout.write(\"Current : \"+ str(iter) + \"\\t\" +\n",
    "              \" Train Acc : \" + str(np.around(avg_acc_train/(len(train_images)/batch_size),3)) + \"\\t\" +\n",
    "              \" Test Acc : \"  + str(np.around(avg_acc_test/(len(test_images)/batch_size),3)) + \"\\t\\n\")\n",
    "        sys.stdout.flush();\n",
    "    avg_acc_train = 0 ; avg_acc_test  = 0\n",
    "    # ======================== print reset ========================\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T04:32:28.359844Z",
     "start_time": "2019-02-12T04:32:21.738Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# import lib\n",
    "import torch\n",
    "from torchvision.datasets.cifar import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import statistics as stats\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T04:32:28.360840Z",
     "start_time": "2019-02-12T04:32:21.741Z"
    },
    "code_folding": [
     30,
     60,
     72
    ]
   },
   "outputs": [],
   "source": [
    "# pytorch\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.c0 = nn.Conv2d(3, 64, kernel_size=4, stride=1)\n",
    "        self.c1 = nn.Conv2d(64, 128, kernel_size=4, stride=1)\n",
    "        self.c2 = nn.Conv2d(128, 256, kernel_size=4, stride=1)\n",
    "        self.c3 = nn.Conv2d(256, 512, kernel_size=4, stride=1)\n",
    "        self.l1 = nn.Linear(512*20*20, 64)\n",
    "\n",
    "#         self.b1 = nn.BatchNorm2d(128)\n",
    "#         self.b2 = nn.BatchNorm2d(256)\n",
    "#         self.b3 = nn.BatchNorm2d(512)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.c0(x))\n",
    "        features = F.relu(self.c1(h))\n",
    "        h = F.relu(self.c2(features))\n",
    "        h = F.relu(self.c3(h))\n",
    "        encoded = self.l1(h.view(x.shape[0], -1))\n",
    "        return encoded, features\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         h = F.relu(self.c0(x))\n",
    "#         features = F.relu(self.b1(self.c1(h)))\n",
    "#         h = F.relu(self.b2(self.c2(features)))\n",
    "#         h = F.relu(self.b3(self.c3(h)))\n",
    "#         encoded = self.l1(h.view(x.shape[0], -1))\n",
    "#         return encoded, features\n",
    "\n",
    "class GlobalDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.c0 = nn.Conv2d(128, 64, kernel_size=3)\n",
    "        self.c1 = nn.Conv2d(64, 32, kernel_size=3)\n",
    "        self.l0 = nn.Linear(32 * 22 * 22 + 64, 512)\n",
    "        self.l1 = nn.Linear(512, 512)\n",
    "        self.l2 = nn.Linear(512, 1)\n",
    "\n",
    "    def forward(self, y, M):\n",
    "        h = F.relu(self.c0(M))\n",
    "        h = self.c1(h)\n",
    "        h = h.view(y.shape[0], -1)\n",
    "        h = torch.cat((y, h), dim=1)\n",
    "        h = F.relu(self.l0(h))\n",
    "        h = F.relu(self.l1(h))\n",
    "        return self.l2(h)\n",
    "\n",
    "class LocalDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.c0 = nn.Conv2d(192, 512, kernel_size=1)\n",
    "        self.c1 = nn.Conv2d(512, 512, kernel_size=1)\n",
    "        self.c2 = nn.Conv2d(512, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.c0(x))\n",
    "        h = F.relu(self.c1(h))\n",
    "        return self.c2(h)\n",
    "\n",
    "class PriorDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l0 = nn.Linear(64, 1000)\n",
    "        self.l1 = nn.Linear(1000, 200)\n",
    "        self.l2 = nn.Linear(200, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.l0(x))\n",
    "        h = F.relu(self.l1(h))\n",
    "        return torch.sigmoid(self.l2(h))\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(64, 15)\n",
    "        self.bn1 = nn.BatchNorm1d(15)\n",
    "        self.l2 = nn.Linear(15, 10)\n",
    "        self.bn2 = nn.BatchNorm1d(10)\n",
    "        self.l3 = nn.Linear(10, 10)\n",
    "        self.bn3 = nn.BatchNorm1d(10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded, _ = x[0], x[1]\n",
    "        clazz = F.relu(self.bn1(self.l1(encoded)))\n",
    "        clazz = F.relu(self.bn2(self.l2(clazz)))\n",
    "        clazz = F.softmax(self.bn3(self.l3(clazz)), dim=1)\n",
    "        return clazz\n",
    "\n",
    "class DeepInfoMaxLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.5, beta=1.0, gamma=0.1):\n",
    "        super().__init__()\n",
    "        self.global_d = GlobalDiscriminator()\n",
    "        self.local_d = LocalDiscriminator()\n",
    "        self.prior_d = PriorDiscriminator()\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, y, M, M_prime):\n",
    "\n",
    "        # see appendix 1A of https://arxiv.org/pdf/1808.06670.pdf\n",
    "\n",
    "        y_exp = y.unsqueeze(-1).unsqueeze(-1)\n",
    "        y_exp = y_exp.expand(-1, -1, 26, 26)\n",
    "\n",
    "        y_M = torch.cat((M, y_exp), dim=1)\n",
    "        y_M_prime = torch.cat((M_prime, y_exp), dim=1)\n",
    "\n",
    "        Ej = -F.relu(-self.local_d(y_M)).mean()\n",
    "        Em = F.relu(self.local_d(y_M_prime)).mean()\n",
    "        LOCAL = (Em - Ej) \n",
    "\n",
    "        Ej = -F.softplus(-self.global_d(y, M)).mean()\n",
    "        Em = F.softplus(self.global_d(y, M_prime)).mean()\n",
    "        GLOBAL = (Em - Ej) * self.alpha\n",
    "\n",
    "        prior = torch.rand_like(y)\n",
    "\n",
    "        term_a = torch.log(self.prior_d(prior)).mean()\n",
    "        term_b = torch.log(1.0 - self.prior_d(y)).mean()\n",
    "        PRIOR = - (term_a + term_b) * self.gamma\n",
    "\n",
    "        return LOCAL \n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "batch_size = 50\n",
    "\n",
    "# image size 3, 32, 32 batch size must be an even number shuffle must be True\n",
    "cifar_10_train_dt = CIFAR10(r'c:\\data\\tv',  download=True, transform=ToTensor())\n",
    "cifar_10_train_l = DataLoader(cifar_10_train_dt, batch_size=batch_size, shuffle=True, drop_last=True,pin_memory=torch.cuda.is_available())\n",
    "\n",
    "encoder = Encoder().to(device)\n",
    "loss_fn = DeepInfoMaxLoss().to(device)\n",
    "optim = Adam(encoder.parameters(), lr=1e-4)\n",
    "loss_optim = Adam(loss_fn.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "for epoch in range(100):\n",
    "    batch = tqdm(cifar_10_train_l, total=len(cifar_10_train_dt) // batch_size)\n",
    "    train_loss = []\n",
    "    for x, target in batch:\n",
    "        x = x.to(device)\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss_optim.zero_grad()\n",
    "        y, M = encoder(x)\n",
    "        # rotate images to create pairs for comparison\n",
    "        M_prime = torch.cat((M[1:], M[0].unsqueeze(0)), dim=0)\n",
    "        loss = loss_fn(y, M, M_prime)\n",
    "        train_loss.append(loss.item())\n",
    "        batch.set_description(str(epoch) + ' Loss: ' + str(stats.mean(train_loss[-20:])))\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        loss_optim.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T00:15:38.469256Z",
     "start_time": "2019-02-12T00:15:38.459282Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T00:15:40.521984Z",
     "start_time": "2019-02-12T00:15:38.470253Z"
    },
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T00:16:11.113749Z",
     "start_time": "2019-02-12T00:15:40.522762Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T04:32:28.360840Z",
     "start_time": "2019-02-12T04:32:21.760Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# import lib\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import nn as nn\n",
    "\n",
    "import torch\n",
    "from torchvision.datasets.cifar import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import statistics as stats\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T04:32:28.362839Z",
     "start_time": "2019-02-12T04:32:21.762Z"
    },
    "code_folding": [
     20,
     21,
     37,
     48,
     59,
     75,
     112
    ]
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.c0 = nn.Conv2d(3, 64, kernel_size=4, stride=1)\n",
    "        self.c1 = nn.Conv2d(64, 128, kernel_size=4, stride=1)\n",
    "        self.c2 = nn.Conv2d(128, 256, kernel_size=4, stride=1)\n",
    "        self.c3 = nn.Conv2d(256, 512, kernel_size=4, stride=1)\n",
    "        self.l1 = nn.Linear(512*20*20, 64)\n",
    "\n",
    "        self.b1 = nn.BatchNorm2d(128)\n",
    "        self.b2 = nn.BatchNorm2d(256)\n",
    "        self.b3 = nn.BatchNorm2d(512)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.c0(x))                      # (64, 64, 29, 29)\n",
    "        features = F.relu(self.b1(self.c1(h)))      # (64, 128, 26, 26)\n",
    "        h = F.relu(self.b2(self.c2(features)))      # (64, 256, 23, 23)\n",
    "        h = F.relu(self.b3(self.c3(h)))             # (64, 512, 20, 20)\n",
    "        encoded = self.l1(h.view(x.shape[0], -1))   # (batch,64)\n",
    "        return encoded, features  \n",
    "class GlobalDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.c0 = nn.Conv2d(128, 64, kernel_size=3) # (64, 64, 24, 24)\n",
    "        self.c1 = nn.Conv2d(64, 32,  kernel_size=3)  # (64, 32, 22, 22)\n",
    "        self.l0 = nn.Linear(32 * 22 * 22 + 64, 512) # (64, 512)\n",
    "        self.l1 = nn.Linear(512, 512)               # (512, 512)\n",
    "        self.l2 = nn.Linear(512, 1)                 # (512, 1)\n",
    "\n",
    "    def forward(self, y, M):\n",
    "        h = F.relu(self.c0(M))\n",
    "        h = self.c1(h)\n",
    "        h = h.view(y.shape[0], -1)\n",
    "        h = torch.cat((y, h), dim=1)\n",
    "        h = F.relu(self.l0(h))\n",
    "        h = F.relu(self.l1(h))\n",
    "        return self.l2(h)  \n",
    "class LocalDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.c0 = nn.Conv2d(192, 512, kernel_size=1)\n",
    "        self.c1 = nn.Conv2d(512, 512, kernel_size=1)\n",
    "        self.c2 = nn.Conv2d(512, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.c0(x))\n",
    "        h = F.relu(self.c1(h))\n",
    "        return self.c2(h)  \n",
    "class PriorDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l0 = nn.Linear(64, 1000)\n",
    "        self.l1 = nn.Linear(1000, 200)\n",
    "        self.l2 = nn.Linear(200, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.l0(x))\n",
    "        h = F.relu(self.l1(h))\n",
    "        return torch.sigmoid(self.l2(h))\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(64, 15)\n",
    "        self.bn1 = nn.BatchNorm1d(15)\n",
    "        self.l2 = nn.Linear(15, 10)\n",
    "        self.bn2 = nn.BatchNorm1d(10)\n",
    "        self.l3 = nn.Linear(10, 10)\n",
    "        self.bn3 = nn.BatchNorm1d(10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded, _ = x[0], x[1]\n",
    "        clazz = F.relu(self.bn1(self.l1(encoded)))\n",
    "        clazz = F.relu(self.bn2(self.l2(clazz)))\n",
    "        clazz = F.softmax(self.bn3(self.l3(clazz)), dim=1)\n",
    "        return clazz\n",
    "class DeepInfoMaxLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.5, beta=1.0, gamma=0.1):\n",
    "        super().__init__()\n",
    "        self.global_d = GlobalDiscriminator()\n",
    "        self.local_d = LocalDiscriminator()\n",
    "        self.prior_d = PriorDiscriminator()\n",
    "        self.alpha   = alpha\n",
    "        self.beta    = beta\n",
    "        self.gamma   = gamma\n",
    "\n",
    "    def forward(self, y, M, M_prime):\n",
    "\n",
    "        # see appendix 1A of https://arxiv.org/pdf/1808.06670.pdf\n",
    "        \n",
    "        # CREATE\n",
    "        y_exp = y.unsqueeze(-1).unsqueeze(-1)\n",
    "        y_exp = y_exp.expand(-1, -1, 26, 26)\n",
    "        y_M = torch.cat((M, y_exp), dim=1)\n",
    "        y_M_prime = torch.cat((M_prime, y_exp), dim=1)\n",
    "\n",
    "        # Local          \n",
    "        Ej = -F.softplus(-self.local_d(y_M)).mean()\n",
    "        Em = F.softplus(self.local_d(y_M_prime)).mean()\n",
    "        LOCAL = (Em - Ej) * self.beta\n",
    "       \n",
    "        # Global         \n",
    "        Ej = -F.softplus(-self.global_d(y, M)).mean()\n",
    "        Em = F.softplus(self.global_d(y, M_prime)).mean()\n",
    "        GLOBAL = (Em - Ej) * self.alpha\n",
    "        \n",
    "        # Prior\n",
    "        prior = torch.rand_like(y)\n",
    "        term_a = torch.log(self.prior_d(prior)).mean()\n",
    "        term_b = torch.log(1.0 - self.prior_d(y)).mean()\n",
    "        PRIOR = - (term_a + term_b) * self.gamma\n",
    "\n",
    "        return LOCAL + GLOBAL + PRIOR\n",
    "class DeepInfoAsLatent(nn.Module):\n",
    "    def __init__(self, run, epoch):\n",
    "        super().__init__()\n",
    "        model_path = Path(r'c:/data/deepinfomax/models') / Path(str(run)) / Path('encoder' + str(epoch) + '.wgt')\n",
    "        self.encoder = Encoder()\n",
    "        self.encoder.load_state_dict(torch.load(str(model_path)))\n",
    "        self.classifier = Classifier()\n",
    "\n",
    "    def forward(self, x):\n",
    "        z, features = self.encoder(x)\n",
    "        z = z.detach()\n",
    "        return self.classifier((z, features))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "batch_size = 10\n",
    "\n",
    "# image size 3, 32, 32 batch size must be an even numbershuffle must be True\n",
    "cifar_10_train_dt = CIFAR10(r'c:\\data\\tv',  download=True, transform=ToTensor())\n",
    "cifar_10_train_l  = DataLoader(cifar_10_train_dt, batch_size=batch_size, shuffle=True, drop_last=True,pin_memory=torch.cuda.is_available())\n",
    "\n",
    "encoder    = Encoder().to(device)\n",
    "loss_fn    = DeepInfoMaxLoss().to(device)\n",
    "optim      = Adam(encoder.parameters(), lr=1e-4)\n",
    "loss_optim = Adam(loss_fn.parameters(), lr=1e-4)\n",
    "\n",
    "for epoch in range(100):\n",
    "    batch = tqdm(cifar_10_train_l, total=len(cifar_10_train_dt) // batch_size)\n",
    "    train_loss = []\n",
    "    \n",
    "    for x, target in batch:\n",
    "        x = x.to(device)\n",
    "\n",
    "        optim.zero_grad(); loss_optim.zero_grad()\n",
    "        y, M = encoder(x)\n",
    "        # y - > (64, 128, 26, 26)\n",
    "        # M - > (batch,64)\n",
    "        \n",
    "        # rotate images to create pairs for comparison (ROTATING)\n",
    "        M_prime = torch.cat((M[1:], M[0].unsqueeze(0)), dim=0)\n",
    "        loss = loss_fn(y, M, M_prime) # ()\n",
    "        # sys.exit()\n",
    "        \n",
    "        train_loss.append(loss.item())\n",
    "        batch.set_description(str(epoch) + ' Loss: ' + str(stats.mean(train_loss[-20:])))\n",
    "        loss.backward()\n",
    "        optim.step(); loss_optim.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-07T03:12:26.562738Z",
     "start_time": "2019-02-07T03:12:19.110Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T04:32:28.364831Z",
     "start_time": "2019-02-12T04:32:21.769Z"
    }
   },
   "outputs": [],
   "source": [
    "# batch = tqdm(cifar_10_train_l, total=len(cifar_10_train_dt) // batch_size)\n",
    "for x, target in batch:\n",
    "    temp = np.swapaxes(np.swapaxes(x.numpy(),1,3),2,1)\n",
    "    plt.imshow(temp[0])\n",
    "    plt.show()\n",
    "    print(temp.shape)\n",
    "    print(target.numpy().shape)\n",
    "    sys.exit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
