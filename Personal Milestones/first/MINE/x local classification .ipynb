{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T04:32:25.024103Z",
     "start_time": "2019-02-12T04:32:21.716319Z"
    },
    "code_folding": [
     0,
     33,
     35,
     62,
     70,
     94,
     97
    ]
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "if (!(\"Notification\" in window)) {\n",
       "    alert(\"This browser does not support desktop notifications, so the %%notify magic will not work.\");\n",
       "} else if (Notification.permission !== 'granted' && Notification.permission !== 'denied') {\n",
       "    Notification.requestPermission(function (permission) {\n",
       "        if(!('permission' in Notification)) {\n",
       "            Notification.permission = permission;\n",
       "        }\n",
       "    })\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import Library and some random image data set\n",
    "import tensorflow as tf\n",
    "import numpy      as np\n",
    "import seaborn    as sns \n",
    "import pandas     as pd\n",
    "import os,sys\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "np.random.seed(78); tf.set_random_seed(78)\n",
    "\n",
    "# get some of the STL data set\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils import shuffle\n",
    "from skimage import util \n",
    "from skimage.transform import resize\n",
    "from skimage.io import imread\n",
    "import warnings\n",
    "from numpy import inf\n",
    "\n",
    "from scipy.stats import kurtosis,skew\n",
    "\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import gc\n",
    "from IPython.display import display, clear_output\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from matplotlib import animation\n",
    "%load_ext jupyternotify\n",
    "\n",
    "# Def: Read STL 10 images\n",
    "def read_STL10_data():\n",
    "    # read all of the data (STL 10) https://github.com/mttk/STL10\n",
    "    def read_all_images(path_to_data):\n",
    "        \"\"\"\n",
    "        :param path_to_data: the file containing the binary images from the STL-10 dataset\n",
    "        :return: an array containing all the images\n",
    "        \"\"\"\n",
    "\n",
    "        with open(path_to_data, 'rb') as f:\n",
    "            # read whole file in uint8 chunks\n",
    "            everything = np.fromfile(f, dtype=np.uint8)\n",
    "\n",
    "            # We force the data into 3x96x96 chunks, since the\n",
    "            # images are stored in \"column-major order\", meaning\n",
    "            # that \"the first 96*96 values are the red channel,\n",
    "            # the next 96*96 are green, and the last are blue.\"\n",
    "            # The -1 is since the size of the pictures depends\n",
    "            # on the input file, and this way numpy determines\n",
    "            # the size on its own.\n",
    "\n",
    "            images = np.reshape(everything, (-1, 3, 96, 96))\n",
    "\n",
    "            # Now transpose the images into a standard image format\n",
    "            # readable by, for example, matplotlib.imshow\n",
    "            # You might want to comment this line or reverse the shuffle\n",
    "            # if you will use a learning algorithm like CNN, since they like\n",
    "            # their channels separated.\n",
    "            images = np.transpose(images, (0, 3, 2, 1))\n",
    "            return images\n",
    "    def read_labels(path_to_labels):\n",
    "        \"\"\"\n",
    "        :param path_to_labels: path to the binary file containing labels from the STL-10 dataset\n",
    "        :return: an array containing the labels\n",
    "        \"\"\"\n",
    "        with open(path_to_labels, 'rb') as f:\n",
    "            labels = np.fromfile(f, dtype=np.uint8)\n",
    "            return labels\n",
    "    def show_images(data,row=1,col=1):\n",
    "        fig=plt.figure(figsize=(10,10))\n",
    "        columns = col; rows = row\n",
    "        for i in range(1, columns*rows +1):\n",
    "            fig.add_subplot(rows, columns, i)\n",
    "            plt.imshow(data[i-1])\n",
    "        plt.show()\n",
    "\n",
    "    train_images = read_all_images(\"../../../DataSet/STL10/stl10_binary/train_X.bin\") / 255.0\n",
    "    train_labels = read_labels    (\"../../../DataSet/STL10/stl10_binary/train_Y.bin\")\n",
    "    test_images  = read_all_images(\"../../../DataSet/STL10/stl10_binary/test_X.bin\")  / 255.0\n",
    "    test_labels  = read_labels    (\"../../../DataSet/STL10/stl10_binary/test_y.bin\")\n",
    "\n",
    "    label_encoder= OneHotEncoder(sparse=False,categories='auto')\n",
    "    train_labels = label_encoder.fit_transform(train_labels.reshape((-1,1)))\n",
    "    test_labels  = label_encoder.fit_transform(test_labels.reshape((-1,1)))\n",
    "\n",
    "    print(train_images.shape,train_images.max(),train_images.min())\n",
    "    print(train_labels.shape,train_labels.max(),train_labels.min())\n",
    "    print(test_images.shape,test_images.max(),test_images.min())\n",
    "    print(test_labels.shape,test_labels.max(),test_labels.min())\n",
    "    return train_images,train_labels,test_images,test_labels\n",
    "\n",
    "# Def: Read CIFAR 10 images\n",
    "def read_CIFAR10_data():\n",
    "    # ====== miscellaneous =====\n",
    "    # code from: https://github.com/tensorflow/tensorflow/issues/8246\n",
    "    def tf_repeat(tensor, repeats):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "\n",
    "        input: A Tensor. 1-D or higher.\n",
    "        repeats: A list. Number of repeat for each dimension, length must be the same as the number of dimensions in input\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        A Tensor. Has the same type as input. Has the shape of tensor.shape * repeats\n",
    "        \"\"\"\n",
    "        expanded_tensor = tf.expand_dims(tensor, -1)\n",
    "        multiples = [1] + repeats\n",
    "        tiled_tensor = tf.tile(expanded_tensor, multiples = multiples)\n",
    "        repeated_tesnor = tf.reshape(tiled_tensor, tf.shape(tensor) * repeats)\n",
    "        return repeated_tesnor\n",
    "    def unpickle(file):\n",
    "        import pickle\n",
    "        with open(file, 'rb') as fo:\n",
    "            dict = pickle.load(fo, encoding='bytes')\n",
    "        return dict\n",
    "    # ====== miscellaneous =====\n",
    "\n",
    "    # data\n",
    "    PathDicom = \"../../../Dataset/cifar-10-batches-py/\"\n",
    "    lstFilesDCM = []  # create an empty list\n",
    "    for dirName, subdirList, fileList in os.walk(PathDicom):\n",
    "        for filename in fileList:\n",
    "            if not \".html\" in filename.lower() and not  \".meta\" in filename.lower():  # check whether the file's DICOM\n",
    "                lstFilesDCM.append(os.path.join(dirName,filename))\n",
    "\n",
    "    # Read the data traind and Test\n",
    "    batch0 = unpickle(lstFilesDCM[0])\n",
    "    batch1 = unpickle(lstFilesDCM[1])\n",
    "    batch2 = unpickle(lstFilesDCM[2])\n",
    "    batch3 = unpickle(lstFilesDCM[3])\n",
    "    batch4 = unpickle(lstFilesDCM[4])\n",
    "\n",
    "    onehot_encoder = OneHotEncoder(sparse=True)\n",
    "    train_batch = np.vstack((batch0[b'data'],batch1[b'data'],batch2[b'data'],batch3[b'data'],batch4[b'data']))\n",
    "    train_label = np.expand_dims(np.hstack((batch0[b'labels'],batch1[b'labels'],batch2[b'labels'],batch3[b'labels'],batch4[b'labels'])).T,axis=1).astype(np.float64)\n",
    "    train_label = onehot_encoder.fit_transform(train_label).toarray().astype(np.float64)\n",
    "\n",
    "    test_batch = unpickle(lstFilesDCM[5])[b'data']\n",
    "    test_label = np.expand_dims(np.array(unpickle(lstFilesDCM[5])[b'labels']),axis=0).T.astype(np.float64)\n",
    "    test_label = onehot_encoder.fit_transform(test_label).toarray().astype(np.float64)\n",
    "\n",
    "    # reshape data\n",
    "    train_batch = np.reshape(train_batch,(len(train_batch),3,32,32)); test_batch = np.reshape(test_batch,(len(test_batch),3,32,32))\n",
    "    # rotate data\n",
    "    train_batch = np.rot90(np.rot90(train_batch,1,axes=(1,3)),3,axes=(1,2)).astype(np.float64); test_batch = np.rot90(np.rot90(test_batch,1,axes=(1,3)),3,axes=(1,2)).astype(np.float64)\n",
    "    # normalize\n",
    "    train_batch= train_batch/255.0; test_batch = test_batch/255.0\n",
    "\n",
    "    # print out the data shape and the max and min value\n",
    "    print(train_batch.shape,train_batch.max(),train_batch.min())\n",
    "    print(train_label.shape,train_label.max(),train_label.min())\n",
    "    print(test_batch.shape,test_batch.max(),test_batch.min())\n",
    "    print(test_label.shape,test_label.max(),test_label.min())\n",
    "    return train_batch,train_label,test_batch,test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T05:00:05.110366Z",
     "start_time": "2019-02-12T05:00:04.928428Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# read the data\n",
    "train_images,train_labels,test_images,test_labels = read_CIFAR10_data()\n",
    "rand_choice = np.random.choice(len(train_images))\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(train_images[rand_choice])\n",
    "plt.title(str(train_labels[rand_choice]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T04:32:27.197281Z",
     "start_time": "2019-02-12T04:32:27.162361Z"
    },
    "code_folding": [
     11,
     49
    ]
   },
   "outputs": [],
   "source": [
    "# create the layers\n",
    "def tf_softmax(x):    return tf.nn.softmax(x)\n",
    "def tf_relu(x):       return tf.nn.relu(x)\n",
    "def d_tf_relu(x):     return tf.cast(tf.greater_equal(x,0),tf.float32) * 1.0\n",
    "def tf_iden(x):       return x\n",
    "def d_tf_iden(x):     return tf.ones_like(x)\n",
    "def tf_sigmoid(x):    return tf.nn.sigmoid(x)\n",
    "def d_tf_sigmoid(x):  return tf_sigmoid(x) * (1.0-tf_sigmoid(x))\n",
    "def tf_softplus(x):   return tf.nn.softplus(x)\n",
    "def d_tf_softplus(x): return tf.nn.sigmoid(x)\n",
    "\n",
    "class FNN():\n",
    "\n",
    "    def __init__(self,inc,outc,act=tf_iden,d_act=d_tf_iden,special_init=False):\n",
    "        if special_init:\n",
    "            interval = np.sqrt(6.0 / (inc + outc + 1.0))\n",
    "            self.w = tf.Variable(tf.random_uniform(shape=(inc, outc),minval=-interval,maxval=interval,dtype=tf.float32,seed=2))\n",
    "        else:\n",
    "            self.w = tf.Variable(tf.random_normal([inc,outc], stddev=0.05,seed=2,dtype=tf.float32))\n",
    "\n",
    "        self.m,self.v = tf.Variable(tf.zeros_like(self.w)),tf.Variable(tf.zeros_like(self.w))\n",
    "        self.act,self.d_act = act,d_act\n",
    "\n",
    "    def getw(self): return self.w\n",
    "    def feedforward(self,input=None):\n",
    "        self.input = input\n",
    "        self.layer = tf.matmul(input,self.w) \n",
    "        self.layerA = self.act(self.layer)\n",
    "        return self.layer,self.layerA\n",
    "\n",
    "    def backprop(self,gradient=None,which_reg=0):\n",
    "        grad_part_1 = gradient\n",
    "        grad_part_2 = self.d_act(self.layer)\n",
    "        grad_part_3 = self.input\n",
    "\n",
    "        grad_middle = grad_part_1 * grad_part_2\n",
    "        grad  = tf.matmul(tf.transpose(grad_part_3),grad_middle) /batch_size\n",
    "        grad_pass = tf.matmul(grad_middle,tf.transpose(self.w))\n",
    "\n",
    "        update_w = []\n",
    "        update_w.append(tf.assign( self.m,self.m*beta1 + (1-beta1) * (grad)   ))\n",
    "        update_w.append(tf.assign( self.v,self.v*beta2 + (1-beta2) * (grad ** 2)   ))\n",
    "        m_hat = self.m / (1-beta1)\n",
    "        v_hat = self.v / (1-beta2)\n",
    "        adam_middle = m_hat *  learning_rate/(tf.sqrt(v_hat) + adam_e)\n",
    "        update_w.append(tf.assign(self.w,tf.subtract(self.w,adam_middle )))\n",
    "\n",
    "        return grad_pass,update_w\n",
    "    \n",
    "class CNN():\n",
    "\n",
    "    def __init__(self,k,inc,out, stddev=0.05,act=tf_relu,d_act=d_tf_relu):\n",
    "        self.w              = tf.Variable(tf.random_normal([k,k,inc,out],stddev=stddev,seed=2,dtype=tf.float32))\n",
    "        self.m1,self.v1       = tf.Variable(tf.zeros_like(self.w)),tf.Variable(tf.zeros_like(self.w))\n",
    "        self.m2,self.v2       = tf.Variable(tf.zeros_like(self.w)),tf.Variable(tf.zeros_like(self.w))\n",
    "\n",
    "        self.act,self.d_act = act,d_act\n",
    "\n",
    "    def getw(self): return self.w\n",
    "    \n",
    "    # Feed Forward for two variables\n",
    "    def feedforward1(self,input,stride=1,padding='VALID'):\n",
    "        self.input1  = input\n",
    "        self.layer1  = tf.nn.conv2d(self.input1,self.w,strides=[1,stride,stride,1],padding=padding) \n",
    "        self.layerA1 = self.act(self.layer1)\n",
    "        return self.layer1, self.layerA1\n",
    "    def feedforward2(self,input,stride=1,padding='VALID'):\n",
    "        self.input2  = input\n",
    "        self.layer2  = tf.nn.conv2d(self.input2,self.w,strides=[1,stride,stride,1],padding=padding) \n",
    "        self.layerA2 = self.act(self.layer2)\n",
    "        return self.layer2, self.layerA2\n",
    "    \n",
    "    # Back Prop for two variables\n",
    "    def backprop1(self,gradient,stride=1,padding='VALID'):\n",
    "        grad_part_1 = gradient\n",
    "        grad_part_2 = self.d_act(self.layer1)\n",
    "        grad_part_3 = self.input1\n",
    "\n",
    "        grad_middle = grad_part_1 * grad_part_2\n",
    "        grad        = tf.nn.conv2d_backprop_filter(input = grad_part_3,filter_sizes = tf.shape(self.w),  out_backprop = grad_middle,strides=[1,stride,stride,1],padding=padding) \n",
    "        grad_pass   = tf.nn.conv2d_backprop_input (input_sizes = tf.shape(self.input1),filter= self.w,   out_backprop = grad_middle,strides=[1,stride,stride,1],padding=padding)\n",
    "\n",
    "        update_w = []\n",
    "        update_w.append(tf.assign( self.m1,self.m1*beta1 + (1-beta1) * (grad)   ))\n",
    "        update_w.append(tf.assign( self.v1,self.v1*beta2 + (1-beta2) * (grad ** 2)   ))\n",
    "        m_hat = self.m1 / (1-beta1) ; v_hat = self.v1 / (1-beta2)\n",
    "        adam_middle = m_hat * learning_rate/(tf.sqrt(v_hat) + adam_e)\n",
    "        update_w.append(tf.assign(self.w,tf.subtract(self.w,adam_middle  )))\n",
    "        return grad_pass,grad,update_w\n",
    "    \n",
    "    def backprop2(self,gradient,stride=1,padding='VALID'):\n",
    "        grad_part_1 = gradient\n",
    "        grad_part_2 = self.d_act(self.layer2)\n",
    "        grad_part_3 = self.input2\n",
    "\n",
    "        grad_middle = grad_part_1 * grad_part_2\n",
    "        grad        = tf.nn.conv2d_backprop_filter(input = grad_part_3,filter_sizes = tf.shape(self.w),  out_backprop = grad_middle,strides=[1,stride,stride,1],padding=padding) \n",
    "        grad_pass   = tf.nn.conv2d_backprop_input (input_sizes = tf.shape(self.input1),filter= self.w,   out_backprop = grad_middle,strides=[1,stride,stride,1],padding=padding)\n",
    "\n",
    "        update_w = []\n",
    "        update_w.append(tf.assign( self.m2,self.m2*beta1 + (1-beta1) * (grad)   ))\n",
    "        update_w.append(tf.assign( self.v2,self.v2*beta2 + (1-beta2) * (grad ** 2)   ))\n",
    "        m_hat = self.m2 / (1-beta1) ; v_hat = self.v2 / (1-beta2)\n",
    "        adam_middle = m_hat * learning_rate/(tf.sqrt(v_hat) + adam_e)\n",
    "        update_w.append(tf.assign(self.w,tf.subtract(self.w,adam_middle  )))\n",
    "        return grad_pass,grad,update_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T04:32:27.898872Z",
     "start_time": "2019-02-12T04:32:27.200773Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Relu_1:0\", shape=(25, 26, 26, 18), dtype=float32)\n",
      "Tensor(\"concat:0\", shape=(25, 26, 26, 82), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# create layers\n",
    "num_eps   = 10; num_epoch = 200; learning_rate = 0.0001 ; batch_size = 25;  \n",
    "beta1,beta2,adam_e = 0.9,0.999,1e-8; print_iter = 100\n",
    "\n",
    "l1 = CNN(4,3,9)\n",
    "l2 = CNN(4,9,18)\n",
    "l3 = CNN(4,18,27)\n",
    "l4 = CNN(4,27,30)\n",
    "l5 = FNN(20*20*30,64,act=tf_iden,d_act=d_tf_iden)\n",
    "\n",
    "local_l1 = CNN(3,82,96)\n",
    "local_l2 = CNN(3,96,96)\n",
    "local_l3 = CNN(3,96, 1,act=tf_iden,d_act=d_tf_iden)\n",
    "\n",
    "# ======== PLACE HOLDERS ========\n",
    "x = tf.placeholder(tf.float32,(batch_size,32,32,3))\n",
    "\n",
    "# ======== ENCODE ========\n",
    "layer1,layer1a = l1.feedforward1(x)\n",
    "layer2,layer2a = l2.feedforward1(layer1a)\n",
    "print(layer2a)\n",
    "layer3,layer3a = l3.feedforward1(layer2a) # here is the cut!\n",
    "layer4,layer4a = l4.feedforward1(layer3a)\n",
    "layer5_input   = tf.reshape(layer4a,[batch_size,-1]) \n",
    "layer5,layer5a = l5.feedforward(layer5_input)\n",
    "\n",
    "# ======== GENERATE NEW FEATURES ========\n",
    "layer2r = tf.transpose(layer2a,[0,2,1,3])\n",
    "_,width,_,channel    = layer2a.get_shape()\n",
    "encoded_expand   = tf.tile(layer5a[:,None,None,:],[1,width,width,1])\n",
    "encoded_M        = tf.concat([layer2a,encoded_expand],3)\n",
    "print(encoded_M)\n",
    "encoded_M_prime  = tf.concat([layer2r,encoded_expand],3)\n",
    "\n",
    "# ======== LOCAL NEW FEATURES ========\n",
    "real_layer1,real_layer1a = local_l1.feedforward1(encoded_M)\n",
    "real_layer2,real_layer2a = local_l2.feedforward1(real_layer1a)\n",
    "real_layer3,real_layer3a = local_l3.feedforward1(real_layer2a)\n",
    "\n",
    "fake_layer1,fake_layer1a = local_l1.feedforward1(encoded_M_prime)\n",
    "fake_layer2,fake_layer2a = local_l2.feedforward1(fake_layer1a)\n",
    "fake_layer3,fake_layer3a = local_l3.feedforward1(fake_layer2a)\n",
    "\n",
    "# ======== LOSS ========\n",
    "loss =  tf.reduce_mean(tf_relu(fake_layer3a)) - tf.reduce_mean(-tf_relu(-real_layer3a)) \n",
    "# auto_train = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "\n",
    "# ======== BACK PROP FOR Discriminator ========\n",
    "real_grad = (d_tf_relu(-real_layer3a) * -1)/batch_size\n",
    "real_grad3_pass,real_grad3,real_grad3up = local_l3.backprop1(real_grad)\n",
    "real_grad2_pass,real_grad2,real_grad2up = local_l2.backprop1(real_grad3_pass)\n",
    "real_grad1_pass,real_grad1,real_grad1up = local_l1.backprop1(real_grad2_pass)\n",
    "\n",
    "fake_grad = (d_tf_relu(fake_layer3a))/batch_size\n",
    "fake_grad3_pass,fake_grad3,fake_grad3up = local_l3.backprop1(fake_grad)\n",
    "fake_grad2_pass,fake_grad2,fake_grad2up = local_l2.backprop1(fake_grad3_pass)\n",
    "fake_grad1_pass,fake_grad1,fake_grad1up = local_l1.backprop1(fake_grad2_pass)\n",
    "\n",
    "# ======== UNDO THE CHANGE ========\n",
    "fake_features,fake_encode = fake_grad1_pass[:,:,:,:channel],fake_grad1_pass[:,:,:,channel:]\n",
    "fake_encode   = tf.reduce_mean(fake_encode,[1,2])\n",
    "fake_features = tf.transpose(fake_features,[0,2,1,3])\n",
    "\n",
    "real_features,real_encode = real_grad1_pass[:,:,:,:channel],real_grad1_pass[:,:,:,channel:]\n",
    "real_encode   = tf.reduce_mean(real_encode,[1,2])\n",
    "\n",
    "# ======== BACK PROP FOR Encoder ========\n",
    "grad_feature_all = real_features + fake_features\n",
    "grad_encoded_all = real_encode   + fake_encode\n",
    "\n",
    "grad5p,grad5_up = l5.backprop(grad_encoded_all)\n",
    "grad4_input    = tf.reshape(grad5p,[batch_size,20,20,30])\n",
    "grad4p,grad4,grad4_up = l4.backprop1(grad4_input)\n",
    "grad3p,grad3,grad3_up = l3.backprop1(grad4p)\n",
    "grad2_input           = grad3p + grad_feature_all \n",
    "grad2p,grad2,grad2_up = l2.backprop1(grad2_input)\n",
    "grad1p,grad1,grad1_up = l1.backprop1(grad2p)\n",
    "\n",
    "grad_update_all = real_grad3up + real_grad2up + real_grad1up + fake_grad3up + fake_grad2up + fake_grad1up + grad5_up + grad4_up + grad3_up + grad2_up + grad1_up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T04:33:11.693920Z",
     "start_time": "2019-02-12T04:33:11.267144Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# create the classification network \n",
    "l1_c = CNN(3,18,64)\n",
    "l2_c = CNN(3,64,64)\n",
    "l3_c = CNN(3,64,128)\n",
    "\n",
    "l4_c = CNN(3,128,128)\n",
    "l5_c = CNN(1,128,128)\n",
    "l6_c = CNN(1,128,10)\n",
    "\n",
    "x_c = tf.placeholder(tf.float32,(batch_size,26,26,18))\n",
    "y_c = tf.placeholder(tf.float32,(batch_size,10))\n",
    "\n",
    "layer1_c,layer1_ca = l1_c.feedforward1(x_c)\n",
    "layer2_c,layer2_ca = l2_c.feedforward1(layer1_ca)\n",
    "layer3_c,layer3_ca = l3_c.feedforward1(layer2_ca,2)\n",
    "\n",
    "layer4_c,layer4_ca = l4_c.feedforward1(layer3_ca)\n",
    "layer5_c,layer5_ca = l5_c.feedforward1(layer4_ca)\n",
    "layer6_c,layer6_ca = l6_c.feedforward1(layer5_ca)\n",
    "\n",
    "final_layer   = tf.reduce_mean(layer6_ca,(1,2))\n",
    "final_softmax = tf_softmax(final_layer)\n",
    "cost          = -tf.reduce_mean(y_c * tf.log(final_softmax + 1e-8))\n",
    "correct_prediction = tf.equal(tf.argmax(final_softmax, 1), tf.argmax(y_c, 1))\n",
    "accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "gradient_c = tf.tile((final_softmax-y_c)[:,None,None,:],[1,8,8,1])/batch_size\n",
    "grad6p_c,grad6w_c,grad6_up_c = l6_c.backprop1(gradient_c)\n",
    "grad5p_c,grad5w_c,grad5_up_c = l5_c.backprop1(grad6p_c)\n",
    "grad4p_c,grad4w_c,grad4_up_c = l4_c.backprop1(grad5p_c)\n",
    "grad3p_c,grad3w_c,grad3_up_c = l3_c.backprop1(grad4p_c,stride=2)\n",
    "grad2p_c,grad2w_c,grad2_up_c = l2_c.backprop1(grad3p_c)\n",
    "grad1p_c,grad1w_c,grad1_up_c = l1_c.backprop1(grad2p_c)\n",
    "\n",
    "gradient_update_c = grad6_up_c + grad5_up_c + grad4_up_c + grad3_up_c + grad2_up_c + grad1_up_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T04:59:38.918197Z",
     "start_time": "2019-02-12T04:43:39.782543Z"
    },
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current : 0\t Train Acc : 0.154\t Test Acc : 0.186\t22\n",
      "Current : 2\t Train Acc : 0.207\t Test Acc : 0.211\t16\n",
      "Current : 4\t Train Acc : 0.213\t Test Acc : 0.213\t26\n",
      "Current : 6\t Train Acc : 0.218\t Test Acc : 0.218\t0.005464e-0574\n",
      "Current : 8\t Train Acc : 0.107\t Test Acc : 0.107\t0.0430364e-055\n",
      "Current : 10\t Train Acc : 0.102\t Test Acc : 0.101\t0.031227e-0565\n",
      "Current : 12\t Train Acc : 0.101\t Test Acc : 0.101\t12\n",
      "Current : 14\t Train Acc : 0.101\t Test Acc : 0.101\t12\n",
      "Current : 16\t Train Acc : 0.101\t Test Acc : 0.101\t12\n",
      "Current : 18\t Train Acc : 0.101\t Test Acc : 0.101\t12\n",
      "Current : 20\t Train Acc : 0.101\t Test Acc : 0.101\t12\n",
      "Current : 22\t Train Acc : 0.101\t Test Acc : 0.101\t12\n",
      "Current : 24\t Train Acc : 0.101\t Test Acc : 0.101\t12\n",
      "Current : 26\t Train Acc : 0.101\t Test Acc : 0.101\t12\n",
      "Current : 28\t Train Acc : 0.101\t Test Acc : 0.101\t12\n",
      "Current : 30\t Train Acc : 0.101\t Test Acc : 0.101\t12\n",
      "Current : 32\t Train Acc : 0.101\t Test Acc : 0.101\t12\n",
      "Current : 34\t Train Acc : 0.101\t Test Acc : 0.101\t12\n",
      "Current : 36\t Train Acc : 0.101\t Test Acc : 0.101\t12\n",
      "Current : 38\t Train Acc : 0.101\t Test Acc : 0.101\t12\n",
      "Current : 40\t Train Acc : 0.101\t Test Acc : 0.101\t12\n",
      "Current : 42\t Train Acc : 0.101\t Test Acc : 0.101\t12\n",
      "Current : 44\t Train Acc : 0.101\t Test Acc : 0.101\t12\n",
      "Current : 46\t Train Acc : 0.101\t Test Acc : 0.101\t12\n",
      "Current : 48\t Train Acc : 0.101\t Test Acc : 0.101\t12\n",
      "Current : 50\t Train Acc : 0.101\t Test Acc : 0.101\t12\n",
      "Current : 52\t Train Acc : 0.101\t Test Acc : 0.101\t12\n",
      "Current Iter : 53/200 batch : 27975/50000 acc : 0.08\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-88001dc1783b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mencoded_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer2a\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mcurrent_batch\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0msess_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgradient_update_c\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx_c\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mencoded_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_c\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mcurrent_batch_label\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + \n\u001b[0;32m     17\u001b[0m                          \u001b[1;34m' batch : '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_batch_index\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/'\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1332\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# loop\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "avg_acc_train = 0; avg_acc_test  = 0; train_acc = [];test_acc = []\n",
    "for iter in range(num_epoch):\n",
    "\n",
    "        \n",
    "    # 2. Use the encoding network for data generation and classification\n",
    "    for current_batch_index in range(0,len(train_images),batch_size):\n",
    "        current_batch       = train_images[current_batch_index:current_batch_index+batch_size] \n",
    "        current_batch_label = train_labels[current_batch_index:current_batch_index+batch_size] \n",
    "\n",
    "        encoded_data = sess.run(layer2a,feed_dict={x:current_batch})\n",
    "        sess_results = sess.run([accuracy,gradient_update_c],feed_dict={x_c:encoded_data,y_c:current_batch_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + \n",
    "                         ' batch : ' + str(current_batch_index) + '/'+ str(len(train_images)) + \n",
    "                         ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_train = avg_acc_train + sess_results[0]\n",
    "\n",
    "    # Test Accuracy   test_images,test_label  \n",
    "    for current_batch_index in range(0,len(test_images), batch_size):\n",
    "        current_data  = test_images[current_batch_index:current_batch_index+batch_size]\n",
    "        current_label = test_labels[current_batch_index:current_batch_index+batch_size]\n",
    "        \n",
    "        encoded_data  = sess.run(layer2a,feed_dict={x:current_data})\n",
    "        sess_results  = sess.run([accuracy],feed_dict={x_c:encoded_data,y_c:current_label})\n",
    "        sys.stdout.write(' Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + \n",
    "                         ' batch : ' + str(current_batch_index) + '/'+ str(len(test_images)) + \n",
    "                         ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_test = avg_acc_test + sess_results[0]   \n",
    "\n",
    "    if avg_acc_train/(len(train_images)/batch_size) > avg_acc_test/(len(test_images)/batch_size) :\n",
    "        # 1. Train the Encoding Network\n",
    "        for current_batch_index in range(0,len(train_images),batch_size):\n",
    "            current_batch       = train_images[current_batch_index:current_batch_index+batch_size]\n",
    "            sess_results  = sess.run([loss,grad_update_all],feed_dict={x:current_batch})\n",
    "            sys.stdout.write(' Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + \n",
    "                             ' batch : ' + str(current_batch_index) + '/'+ str(len(train_images)) + \n",
    "                             ' loss : ' + str(sess_results[0]) + '\\r')\n",
    "            sys.stdout.flush();     \n",
    "    # ======================== print reset ========================\n",
    "    if iter%2 == 0 :\n",
    "        sys.stdout.write(\"Current : \"+ str(iter) + \"\\t\" +\n",
    "              \" Train Acc : \" + str(np.around(avg_acc_train/(len(train_images)/batch_size),3)) + \"\\t\" +\n",
    "              \" Test Acc : \"  + str(np.around(avg_acc_test/(len(test_images)/batch_size),3)) + \"\\t\\n\")\n",
    "        sys.stdout.flush();\n",
    "    avg_acc_train = 0 ; avg_acc_test  = 0\n",
    "    # ======================== print reset ========================\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T04:32:28.359844Z",
     "start_time": "2019-02-12T04:32:21.738Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# import lib\n",
    "import torch\n",
    "from torchvision.datasets.cifar import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import statistics as stats\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T04:32:28.360840Z",
     "start_time": "2019-02-12T04:32:21.741Z"
    },
    "code_folding": [
     30,
     60,
     72
    ]
   },
   "outputs": [],
   "source": [
    "# pytorch\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.c0 = nn.Conv2d(3, 64, kernel_size=4, stride=1)\n",
    "        self.c1 = nn.Conv2d(64, 128, kernel_size=4, stride=1)\n",
    "        self.c2 = nn.Conv2d(128, 256, kernel_size=4, stride=1)\n",
    "        self.c3 = nn.Conv2d(256, 512, kernel_size=4, stride=1)\n",
    "        self.l1 = nn.Linear(512*20*20, 64)\n",
    "\n",
    "#         self.b1 = nn.BatchNorm2d(128)\n",
    "#         self.b2 = nn.BatchNorm2d(256)\n",
    "#         self.b3 = nn.BatchNorm2d(512)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.c0(x))\n",
    "        features = F.relu(self.c1(h))\n",
    "        h = F.relu(self.c2(features))\n",
    "        h = F.relu(self.c3(h))\n",
    "        encoded = self.l1(h.view(x.shape[0], -1))\n",
    "        return encoded, features\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         h = F.relu(self.c0(x))\n",
    "#         features = F.relu(self.b1(self.c1(h)))\n",
    "#         h = F.relu(self.b2(self.c2(features)))\n",
    "#         h = F.relu(self.b3(self.c3(h)))\n",
    "#         encoded = self.l1(h.view(x.shape[0], -1))\n",
    "#         return encoded, features\n",
    "\n",
    "class GlobalDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.c0 = nn.Conv2d(128, 64, kernel_size=3)\n",
    "        self.c1 = nn.Conv2d(64, 32, kernel_size=3)\n",
    "        self.l0 = nn.Linear(32 * 22 * 22 + 64, 512)\n",
    "        self.l1 = nn.Linear(512, 512)\n",
    "        self.l2 = nn.Linear(512, 1)\n",
    "\n",
    "    def forward(self, y, M):\n",
    "        h = F.relu(self.c0(M))\n",
    "        h = self.c1(h)\n",
    "        h = h.view(y.shape[0], -1)\n",
    "        h = torch.cat((y, h), dim=1)\n",
    "        h = F.relu(self.l0(h))\n",
    "        h = F.relu(self.l1(h))\n",
    "        return self.l2(h)\n",
    "\n",
    "class LocalDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.c0 = nn.Conv2d(192, 512, kernel_size=1)\n",
    "        self.c1 = nn.Conv2d(512, 512, kernel_size=1)\n",
    "        self.c2 = nn.Conv2d(512, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.c0(x))\n",
    "        h = F.relu(self.c1(h))\n",
    "        return self.c2(h)\n",
    "\n",
    "class PriorDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l0 = nn.Linear(64, 1000)\n",
    "        self.l1 = nn.Linear(1000, 200)\n",
    "        self.l2 = nn.Linear(200, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.l0(x))\n",
    "        h = F.relu(self.l1(h))\n",
    "        return torch.sigmoid(self.l2(h))\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(64, 15)\n",
    "        self.bn1 = nn.BatchNorm1d(15)\n",
    "        self.l2 = nn.Linear(15, 10)\n",
    "        self.bn2 = nn.BatchNorm1d(10)\n",
    "        self.l3 = nn.Linear(10, 10)\n",
    "        self.bn3 = nn.BatchNorm1d(10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded, _ = x[0], x[1]\n",
    "        clazz = F.relu(self.bn1(self.l1(encoded)))\n",
    "        clazz = F.relu(self.bn2(self.l2(clazz)))\n",
    "        clazz = F.softmax(self.bn3(self.l3(clazz)), dim=1)\n",
    "        return clazz\n",
    "\n",
    "class DeepInfoMaxLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.5, beta=1.0, gamma=0.1):\n",
    "        super().__init__()\n",
    "        self.global_d = GlobalDiscriminator()\n",
    "        self.local_d = LocalDiscriminator()\n",
    "        self.prior_d = PriorDiscriminator()\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, y, M, M_prime):\n",
    "\n",
    "        # see appendix 1A of https://arxiv.org/pdf/1808.06670.pdf\n",
    "\n",
    "        y_exp = y.unsqueeze(-1).unsqueeze(-1)\n",
    "        y_exp = y_exp.expand(-1, -1, 26, 26)\n",
    "\n",
    "        y_M = torch.cat((M, y_exp), dim=1)\n",
    "        y_M_prime = torch.cat((M_prime, y_exp), dim=1)\n",
    "\n",
    "        Ej = -F.relu(-self.local_d(y_M)).mean()\n",
    "        Em = F.relu(self.local_d(y_M_prime)).mean()\n",
    "        LOCAL = (Em - Ej) \n",
    "\n",
    "        Ej = -F.softplus(-self.global_d(y, M)).mean()\n",
    "        Em = F.softplus(self.global_d(y, M_prime)).mean()\n",
    "        GLOBAL = (Em - Ej) * self.alpha\n",
    "\n",
    "        prior = torch.rand_like(y)\n",
    "\n",
    "        term_a = torch.log(self.prior_d(prior)).mean()\n",
    "        term_b = torch.log(1.0 - self.prior_d(y)).mean()\n",
    "        PRIOR = - (term_a + term_b) * self.gamma\n",
    "\n",
    "        return LOCAL \n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "batch_size = 50\n",
    "\n",
    "# image size 3, 32, 32 batch size must be an even number shuffle must be True\n",
    "cifar_10_train_dt = CIFAR10(r'c:\\data\\tv',  download=True, transform=ToTensor())\n",
    "cifar_10_train_l = DataLoader(cifar_10_train_dt, batch_size=batch_size, shuffle=True, drop_last=True,pin_memory=torch.cuda.is_available())\n",
    "\n",
    "encoder = Encoder().to(device)\n",
    "loss_fn = DeepInfoMaxLoss().to(device)\n",
    "optim = Adam(encoder.parameters(), lr=1e-4)\n",
    "loss_optim = Adam(loss_fn.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "for epoch in range(100):\n",
    "    batch = tqdm(cifar_10_train_l, total=len(cifar_10_train_dt) // batch_size)\n",
    "    train_loss = []\n",
    "    for x, target in batch:\n",
    "        x = x.to(device)\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss_optim.zero_grad()\n",
    "        y, M = encoder(x)\n",
    "        # rotate images to create pairs for comparison\n",
    "        M_prime = torch.cat((M[1:], M[0].unsqueeze(0)), dim=0)\n",
    "        loss = loss_fn(y, M, M_prime)\n",
    "        train_loss.append(loss.item())\n",
    "        batch.set_description(str(epoch) + ' Loss: ' + str(stats.mean(train_loss[-20:])))\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        loss_optim.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T00:15:38.469256Z",
     "start_time": "2019-02-12T00:15:38.459282Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T00:15:40.521984Z",
     "start_time": "2019-02-12T00:15:38.470253Z"
    },
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T00:16:11.113749Z",
     "start_time": "2019-02-12T00:15:40.522762Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T04:32:28.360840Z",
     "start_time": "2019-02-12T04:32:21.760Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# import lib\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import nn as nn\n",
    "\n",
    "import torch\n",
    "from torchvision.datasets.cifar import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import statistics as stats\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T04:32:28.362839Z",
     "start_time": "2019-02-12T04:32:21.762Z"
    },
    "code_folding": [
     20,
     21,
     37,
     48,
     59,
     75,
     112
    ]
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.c0 = nn.Conv2d(3, 64, kernel_size=4, stride=1)\n",
    "        self.c1 = nn.Conv2d(64, 128, kernel_size=4, stride=1)\n",
    "        self.c2 = nn.Conv2d(128, 256, kernel_size=4, stride=1)\n",
    "        self.c3 = nn.Conv2d(256, 512, kernel_size=4, stride=1)\n",
    "        self.l1 = nn.Linear(512*20*20, 64)\n",
    "\n",
    "        self.b1 = nn.BatchNorm2d(128)\n",
    "        self.b2 = nn.BatchNorm2d(256)\n",
    "        self.b3 = nn.BatchNorm2d(512)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.c0(x))                      # (64, 64, 29, 29)\n",
    "        features = F.relu(self.b1(self.c1(h)))      # (64, 128, 26, 26)\n",
    "        h = F.relu(self.b2(self.c2(features)))      # (64, 256, 23, 23)\n",
    "        h = F.relu(self.b3(self.c3(h)))             # (64, 512, 20, 20)\n",
    "        encoded = self.l1(h.view(x.shape[0], -1))   # (batch,64)\n",
    "        return encoded, features  \n",
    "class GlobalDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.c0 = nn.Conv2d(128, 64, kernel_size=3) # (64, 64, 24, 24)\n",
    "        self.c1 = nn.Conv2d(64, 32,  kernel_size=3)  # (64, 32, 22, 22)\n",
    "        self.l0 = nn.Linear(32 * 22 * 22 + 64, 512) # (64, 512)\n",
    "        self.l1 = nn.Linear(512, 512)               # (512, 512)\n",
    "        self.l2 = nn.Linear(512, 1)                 # (512, 1)\n",
    "\n",
    "    def forward(self, y, M):\n",
    "        h = F.relu(self.c0(M))\n",
    "        h = self.c1(h)\n",
    "        h = h.view(y.shape[0], -1)\n",
    "        h = torch.cat((y, h), dim=1)\n",
    "        h = F.relu(self.l0(h))\n",
    "        h = F.relu(self.l1(h))\n",
    "        return self.l2(h)  \n",
    "class LocalDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.c0 = nn.Conv2d(192, 512, kernel_size=1)\n",
    "        self.c1 = nn.Conv2d(512, 512, kernel_size=1)\n",
    "        self.c2 = nn.Conv2d(512, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.c0(x))\n",
    "        h = F.relu(self.c1(h))\n",
    "        return self.c2(h)  \n",
    "class PriorDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l0 = nn.Linear(64, 1000)\n",
    "        self.l1 = nn.Linear(1000, 200)\n",
    "        self.l2 = nn.Linear(200, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.l0(x))\n",
    "        h = F.relu(self.l1(h))\n",
    "        return torch.sigmoid(self.l2(h))\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(64, 15)\n",
    "        self.bn1 = nn.BatchNorm1d(15)\n",
    "        self.l2 = nn.Linear(15, 10)\n",
    "        self.bn2 = nn.BatchNorm1d(10)\n",
    "        self.l3 = nn.Linear(10, 10)\n",
    "        self.bn3 = nn.BatchNorm1d(10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded, _ = x[0], x[1]\n",
    "        clazz = F.relu(self.bn1(self.l1(encoded)))\n",
    "        clazz = F.relu(self.bn2(self.l2(clazz)))\n",
    "        clazz = F.softmax(self.bn3(self.l3(clazz)), dim=1)\n",
    "        return clazz\n",
    "class DeepInfoMaxLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.5, beta=1.0, gamma=0.1):\n",
    "        super().__init__()\n",
    "        self.global_d = GlobalDiscriminator()\n",
    "        self.local_d = LocalDiscriminator()\n",
    "        self.prior_d = PriorDiscriminator()\n",
    "        self.alpha   = alpha\n",
    "        self.beta    = beta\n",
    "        self.gamma   = gamma\n",
    "\n",
    "    def forward(self, y, M, M_prime):\n",
    "\n",
    "        # see appendix 1A of https://arxiv.org/pdf/1808.06670.pdf\n",
    "        \n",
    "        # CREATE\n",
    "        y_exp = y.unsqueeze(-1).unsqueeze(-1)\n",
    "        y_exp = y_exp.expand(-1, -1, 26, 26)\n",
    "        y_M = torch.cat((M, y_exp), dim=1)\n",
    "        y_M_prime = torch.cat((M_prime, y_exp), dim=1)\n",
    "\n",
    "        # Local          \n",
    "        Ej = -F.softplus(-self.local_d(y_M)).mean()\n",
    "        Em = F.softplus(self.local_d(y_M_prime)).mean()\n",
    "        LOCAL = (Em - Ej) * self.beta\n",
    "       \n",
    "        # Global         \n",
    "        Ej = -F.softplus(-self.global_d(y, M)).mean()\n",
    "        Em = F.softplus(self.global_d(y, M_prime)).mean()\n",
    "        GLOBAL = (Em - Ej) * self.alpha\n",
    "        \n",
    "        # Prior\n",
    "        prior = torch.rand_like(y)\n",
    "        term_a = torch.log(self.prior_d(prior)).mean()\n",
    "        term_b = torch.log(1.0 - self.prior_d(y)).mean()\n",
    "        PRIOR = - (term_a + term_b) * self.gamma\n",
    "\n",
    "        return LOCAL + GLOBAL + PRIOR\n",
    "class DeepInfoAsLatent(nn.Module):\n",
    "    def __init__(self, run, epoch):\n",
    "        super().__init__()\n",
    "        model_path = Path(r'c:/data/deepinfomax/models') / Path(str(run)) / Path('encoder' + str(epoch) + '.wgt')\n",
    "        self.encoder = Encoder()\n",
    "        self.encoder.load_state_dict(torch.load(str(model_path)))\n",
    "        self.classifier = Classifier()\n",
    "\n",
    "    def forward(self, x):\n",
    "        z, features = self.encoder(x)\n",
    "        z = z.detach()\n",
    "        return self.classifier((z, features))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "batch_size = 10\n",
    "\n",
    "# image size 3, 32, 32 batch size must be an even numbershuffle must be True\n",
    "cifar_10_train_dt = CIFAR10(r'c:\\data\\tv',  download=True, transform=ToTensor())\n",
    "cifar_10_train_l  = DataLoader(cifar_10_train_dt, batch_size=batch_size, shuffle=True, drop_last=True,pin_memory=torch.cuda.is_available())\n",
    "\n",
    "encoder    = Encoder().to(device)\n",
    "loss_fn    = DeepInfoMaxLoss().to(device)\n",
    "optim      = Adam(encoder.parameters(), lr=1e-4)\n",
    "loss_optim = Adam(loss_fn.parameters(), lr=1e-4)\n",
    "\n",
    "for epoch in range(100):\n",
    "    batch = tqdm(cifar_10_train_l, total=len(cifar_10_train_dt) // batch_size)\n",
    "    train_loss = []\n",
    "    \n",
    "    for x, target in batch:\n",
    "        x = x.to(device)\n",
    "\n",
    "        optim.zero_grad(); loss_optim.zero_grad()\n",
    "        y, M = encoder(x)\n",
    "        # y - > (64, 128, 26, 26)\n",
    "        # M - > (batch,64)\n",
    "        \n",
    "        # rotate images to create pairs for comparison (ROTATING)\n",
    "        M_prime = torch.cat((M[1:], M[0].unsqueeze(0)), dim=0)\n",
    "        loss = loss_fn(y, M, M_prime) # ()\n",
    "        # sys.exit()\n",
    "        \n",
    "        train_loss.append(loss.item())\n",
    "        batch.set_description(str(epoch) + ' Loss: ' + str(stats.mean(train_loss[-20:])))\n",
    "        loss.backward()\n",
    "        optim.step(); loss_optim.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-07T03:12:26.562738Z",
     "start_time": "2019-02-07T03:12:19.110Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T04:32:28.364831Z",
     "start_time": "2019-02-12T04:32:21.769Z"
    }
   },
   "outputs": [],
   "source": [
    "# batch = tqdm(cifar_10_train_l, total=len(cifar_10_train_dt) // batch_size)\n",
    "for x, target in batch:\n",
    "    temp = np.swapaxes(np.swapaxes(x.numpy(),1,3),2,1)\n",
    "    plt.imshow(temp[0])\n",
    "    plt.show()\n",
    "    print(temp.shape)\n",
    "    print(target.numpy().shape)\n",
    "    sys.exit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
