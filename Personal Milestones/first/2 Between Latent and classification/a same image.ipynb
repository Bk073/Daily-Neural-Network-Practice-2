{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T06:22:37.170867Z",
     "start_time": "2019-02-27T06:22:37.118995Z"
    },
    "code_folding": [
     0,
     34,
     94
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The jupyternotify extension is already loaded. To reload it, use:\n",
      "  %reload_ext jupyternotify\n"
     ]
    }
   ],
   "source": [
    "# import Library and some random image data set\n",
    "import tensorflow as tf\n",
    "import numpy      as np\n",
    "import seaborn    as sns \n",
    "import pandas     as pd\n",
    "import os,sys\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "np.random.seed(78); tf.set_random_seed(78)\n",
    "\n",
    "# get some of the STL data set\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils import shuffle\n",
    "from skimage import util \n",
    "from skimage.transform import resize\n",
    "from scipy.misc import imresize\n",
    "from skimage.io import imread\n",
    "import warnings\n",
    "from numpy import inf\n",
    "\n",
    "from scipy.stats import kurtosis,skew\n",
    "\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import gc\n",
    "from IPython.display import display, clear_output\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from matplotlib import animation\n",
    "%load_ext jupyternotify\n",
    "\n",
    "# Def: Read STL 10 images\n",
    "def read_STL10_data():\n",
    "    # read all of the data (STL 10) https://github.com/mttk/STL10\n",
    "    def read_all_images(path_to_data):\n",
    "        \"\"\"\n",
    "        :param path_to_data: the file containing the binary images from the STL-10 dataset\n",
    "        :return: an array containing all the images\n",
    "        \"\"\"\n",
    "\n",
    "        with open(path_to_data, 'rb') as f:\n",
    "            # read whole file in uint8 chunks\n",
    "            everything = np.fromfile(f, dtype=np.uint8)\n",
    "\n",
    "            # We force the data into 3x96x96 chunks, since the\n",
    "            # images are stored in \"column-major order\", meaning\n",
    "            # that \"the first 96*96 values are the red channel,\n",
    "            # the next 96*96 are green, and the last are blue.\"\n",
    "            # The -1 is since the size of the pictures depends\n",
    "            # on the input file, and this way numpy determines\n",
    "            # the size on its own.\n",
    "\n",
    "            images = np.reshape(everything, (-1, 3, 96, 96))\n",
    "\n",
    "            # Now transpose the images into a standard image format\n",
    "            # readable by, for example, matplotlib.imshow\n",
    "            # You might want to comment this line or reverse the shuffle\n",
    "            # if you will use a learning algorithm like CNN, since they like\n",
    "            # their channels separated.\n",
    "            images = np.transpose(images, (0, 3, 2, 1))\n",
    "            return images\n",
    "    def read_labels(path_to_labels):\n",
    "        \"\"\"\n",
    "        :param path_to_labels: path to the binary file containing labels from the STL-10 dataset\n",
    "        :return: an array containing the labels\n",
    "        \"\"\"\n",
    "        with open(path_to_labels, 'rb') as f:\n",
    "            labels = np.fromfile(f, dtype=np.uint8)\n",
    "            return labels\n",
    "    def show_images(data,row=1,col=1):\n",
    "        fig=plt.figure(figsize=(10,10))\n",
    "        columns = col; rows = row\n",
    "        for i in range(1, columns*rows +1):\n",
    "            fig.add_subplot(rows, columns, i)\n",
    "            plt.imshow(data[i-1])\n",
    "        plt.show()\n",
    "\n",
    "    train_images = read_all_images(\"../../../DataSet/STL10/stl10_binary/train_X.bin\") / 255.0\n",
    "    train_labels = read_labels    (\"../../../DataSet/STL10/stl10_binary/train_Y.bin\")\n",
    "    test_images  = read_all_images(\"../../../DataSet/STL10/stl10_binary/test_X.bin\")  / 255.0\n",
    "    test_labels  = read_labels    (\"../../../DataSet/STL10/stl10_binary/test_y.bin\")\n",
    "\n",
    "    label_encoder= OneHotEncoder(sparse=False,categories='auto')\n",
    "    train_labels = label_encoder.fit_transform(train_labels.reshape((-1,1)))\n",
    "    test_labels  = label_encoder.fit_transform(test_labels.reshape((-1,1)))\n",
    "\n",
    "    print(train_images.shape,train_images.max(),train_images.min())\n",
    "    print(train_labels.shape,train_labels.max(),train_labels.min())\n",
    "    print(test_images.shape,test_images.max(),test_images.min())\n",
    "    print(test_labels.shape,test_labels.max(),test_labels.min())\n",
    "    return train_images,train_labels,test_images,test_labels\n",
    "# Def: Read CIFAR 10 images\n",
    "def read_CIFAR10_data():\n",
    "    # ====== miscellaneous =====\n",
    "    # code from: https://github.com/tensorflow/tensorflow/issues/8246\n",
    "    def tf_repeat(tensor, repeats):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "\n",
    "        input: A Tensor. 1-D or higher.\n",
    "        repeats: A list. Number of repeat for each dimension, length must be the same as the number of dimensions in input\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        A Tensor. Has the same type as input. Has the shape of tensor.shape * repeats\n",
    "        \"\"\"\n",
    "        expanded_tensor = tf.expand_dims(tensor, -1)\n",
    "        multiples = [1] + repeats\n",
    "        tiled_tensor = tf.tile(expanded_tensor, multiples = multiples)\n",
    "        repeated_tesnor = tf.reshape(tiled_tensor, tf.shape(tensor) * repeats)\n",
    "        return repeated_tesnor\n",
    "    def unpickle(file):\n",
    "        import pickle\n",
    "        with open(file, 'rb') as fo:\n",
    "            dict = pickle.load(fo, encoding='bytes')\n",
    "        return dict\n",
    "    # ====== miscellaneous =====\n",
    "\n",
    "    # data\n",
    "    PathDicom = \"../../Dataset/cifar-10-batches-py/\"\n",
    "    lstFilesDCM = []  # create an empty list\n",
    "    for dirName, subdirList, fileList in os.walk(PathDicom):\n",
    "        for filename in fileList:\n",
    "            if not \".html\" in filename.lower() and not  \".meta\" in filename.lower():  # check whether the file's DICOM\n",
    "                lstFilesDCM.append(os.path.join(dirName,filename))\n",
    "\n",
    "    # Read the data traind and Test\n",
    "    batch0 = unpickle(lstFilesDCM[0])\n",
    "    batch1 = unpickle(lstFilesDCM[1])\n",
    "    batch2 = unpickle(lstFilesDCM[2])\n",
    "    batch3 = unpickle(lstFilesDCM[3])\n",
    "    batch4 = unpickle(lstFilesDCM[4])\n",
    "\n",
    "    onehot_encoder = OneHotEncoder(sparse=True)\n",
    "    train_batch = np.vstack((batch0[b'data'],batch1[b'data'],batch2[b'data'],batch3[b'data'],batch4[b'data']))\n",
    "    train_label = np.expand_dims(np.hstack((batch0[b'labels'],batch1[b'labels'],batch2[b'labels'],batch3[b'labels'],batch4[b'labels'])).T,axis=1).astype(np.float64)\n",
    "    train_label = onehot_encoder.fit_transform(train_label).toarray().astype(np.float64)\n",
    "\n",
    "    test_batch = unpickle(lstFilesDCM[5])[b'data']\n",
    "    test_label = np.expand_dims(np.array(unpickle(lstFilesDCM[5])[b'labels']),axis=0).T.astype(np.float64)\n",
    "    test_label = onehot_encoder.fit_transform(test_label).toarray().astype(np.float64)\n",
    "\n",
    "    # reshape data\n",
    "    train_batch = np.reshape(train_batch,(len(train_batch),3,32,32)); test_batch = np.reshape(test_batch,(len(test_batch),3,32,32))\n",
    "    # rotate data\n",
    "    train_batch = np.rot90(np.rot90(train_batch,1,axes=(1,3)),3,axes=(1,2)).astype(np.float64); test_batch = np.rot90(np.rot90(test_batch,1,axes=(1,3)),3,axes=(1,2)).astype(np.float64)\n",
    "    # normalize\n",
    "    train_batch= train_batch/255.0; test_batch = test_batch/255.0\n",
    "\n",
    "    # print out the data shape and the max and min value\n",
    "    print(train_batch.shape,train_batch.max(),train_batch.min())\n",
    "    print(train_label.shape,train_label.max(),train_label.min())\n",
    "    print(test_batch.shape,test_batch.max(),test_batch.min())\n",
    "    print(test_label.shape,test_label.max(),test_label.min())\n",
    "    return train_batch,train_label,test_batch,test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T06:22:39.841013Z",
     "start_time": "2019-02-27T06:22:39.814087Z"
    },
    "code_folding": [
     22,
     27,
     43,
     50,
     67
    ]
   },
   "outputs": [],
   "source": [
    "# create the layers\n",
    "def tf_softmax(x):    return tf.nn.softmax(x)\n",
    "def tf_elu(x):       return tf.nn.elu(x)\n",
    "\n",
    "def tf_relu(x):       return tf.nn.relu(x)\n",
    "def d_tf_relu(x):     return tf.cast(tf.greater_equal(x,0),tf.float32) * 1.0\n",
    "\n",
    "def tf_iden(x):       return x\n",
    "def d_tf_iden(x):     return tf.ones_like(x)\n",
    "\n",
    "def tf_sigmoid(x):    return tf.nn.sigmoid(x)\n",
    "def d_tf_sigmoid(x):  return tf_sigmoid(x) * (1.0-tf_sigmoid(x))\n",
    "\n",
    "def tf_tanh(x):    return tf.nn.tanh(x)\n",
    "\n",
    "def tf_softplus(x):   return tf.nn.softplus(x)\n",
    "def d_tf_softplus(x): return tf.nn.sigmoid(x)\n",
    "\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from skimage.filters import roberts, sobel, scharr, prewitt\n",
    "from skimage import feature\n",
    "def calc_MI(x, y, bins=(90*90)/4):\n",
    "    c_xy = np.histogram2d(x, y, bins)[0]\n",
    "    mi = mutual_info_score(None, None, contingency=c_xy)\n",
    "    return mi\n",
    "\n",
    "class CNN():\n",
    "\n",
    "    def __init__(self,k,inc,out, stddev=0.05,act=tf_elu,d_act=d_tf_relu):\n",
    "        self.w              = tf.Variable(tf.random_normal([k,k,inc,out],stddev=stddev,seed=2,dtype=tf.float32))\n",
    "        self.m,self.v       = tf.Variable(tf.zeros_like(self.w)),tf.Variable(tf.zeros_like(self.w))\n",
    "        self.m2,self.v2       = tf.Variable(tf.zeros_like(self.w)),tf.Variable(tf.zeros_like(self.w))\n",
    "        self.act,self.d_act = act,d_act\n",
    "\n",
    "    def getw(self): return self.w\n",
    "    \n",
    "    # Feed Forward for two variables\n",
    "    def feedforward(self,input,stride=1,padding='VALID'):\n",
    "        self.input1  = input\n",
    "        self.layer1  = tf.nn.conv2d(self.input1,self.w,strides=[1,stride,stride,1],padding=padding) \n",
    "        self.layerA1 = self.act(self.layer1)\n",
    "        return self.layer1, self.layerA1\n",
    "    def feedforward2(self,input,stride=1,padding='VALID'):\n",
    "        self.input2  = input\n",
    "        self.layer2  = tf.nn.conv2d(self.input2,self.w,strides=[1,stride,stride,1],padding=padding) \n",
    "        self.layerA2 = self.act(self.layer2)\n",
    "        return self.layer2, self.layerA2\n",
    "    \n",
    "    # Back Prop for two variables\n",
    "    def backprop(self,gradient,stride=1,padding='VALID'):\n",
    "        grad_part_1 = gradient\n",
    "        grad_part_2 = self.d_act(self.layer1)\n",
    "        grad_part_3 = self.input1\n",
    "\n",
    "        grad_middle = grad_part_1 * grad_part_2\n",
    "        grad        = tf.nn.conv2d_backprop_filter(input = grad_part_3,filter_sizes = tf.shape(self.w),  out_backprop = grad_middle,strides=[1,stride,stride,1],padding=padding) \n",
    "        grad_pass   = tf.nn.conv2d_backprop_input (input_sizes = tf.shape(self.input1),filter= self.w,   out_backprop = grad_middle,strides=[1,stride,stride,1],padding=padding)\n",
    "\n",
    "        update_w = []\n",
    "        update_w.append(tf.assign( self.m,self.m*beta1 + (1-beta1) * (grad)   ))\n",
    "        update_w.append(tf.assign( self.v,self.v*beta2 + (1-beta2) * (grad ** 2)   ))\n",
    "        m_hat = self.m / (1-beta1) ; v_hat = self.v / (1-beta2)\n",
    "        adam_middle = m_hat * learning_rate/(tf.sqrt(v_hat) + adam_e)\n",
    "        update_w.append(tf.assign(self.w,tf.subtract(self.w,adam_middle  )))\n",
    "        return grad_pass,grad,update_w\n",
    "    \n",
    "    def backprop2(self,gradient,stride=1,padding='VALID'):\n",
    "        grad_part_1 = gradient\n",
    "        grad_part_2 = self.d_act(self.layer2)\n",
    "        grad_part_3 = self.input2\n",
    "\n",
    "        grad_middle = grad_part_1 * grad_part_2\n",
    "        grad        = tf.nn.conv2d_backprop_filter(input = grad_part_3,filter_sizes = tf.shape(self.w),  out_backprop = grad_middle,strides=[1,stride,stride,1],padding=padding) \n",
    "        grad_pass   = tf.nn.conv2d_backprop_input (input_sizes = tf.shape(self.input1),filter= self.w,   out_backprop = grad_middle,strides=[1,stride,stride,1],padding=padding)\n",
    "\n",
    "        update_w = []\n",
    "        update_w.append(tf.assign( self.m2,self.m2*beta1 + (1-beta1) * (grad)   ))\n",
    "        update_w.append(tf.assign( self.v2,self.v2*beta2 + (1-beta2) * (grad ** 2)   ))\n",
    "        m_hat = self.m2 / (1-beta1) ; v_hat = self.v2 / (1-beta2)\n",
    "        adam_middle = m_hat * learning_rate/(tf.sqrt(v_hat) + adam_e)\n",
    "        update_w.append(tf.assign(self.w,tf.subtract(self.w,adam_middle  )))\n",
    "        return grad_pass,grad,update_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T06:22:45.717742Z",
     "start_time": "2019-02-27T06:22:42.551192Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 96, 96, 3) 1.0 0.0\n",
      "(5000, 10) 1.0 0.0\n",
      "(8000, 96, 96, 3) 1.0 0.0\n",
      "(8000, 10) 1.0 0.0\n"
     ]
    }
   ],
   "source": [
    "# read the data\n",
    "train_images,train_labels,test_images,test_labels = read_STL10_data()\n",
    "train_images = train_images.mean(3,keepdims=True)\n",
    "train_images = (train_images-train_images.min((0,1,2),keepdims=True))/(train_images.max((0,1,2),keepdims=True)-train_images.min((0,1,2),keepdims=True)+1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T06:22:50.314955Z",
     "start_time": "2019-02-27T06:22:50.309968Z"
    }
   },
   "outputs": [],
   "source": [
    "# hyeper\n",
    "num_eps   = 1; num_epoch = 5; learning_rate = 0.0001; batch_size = 10;  alpha = 0.5\n",
    "beta1,beta2,adam_e = 0.9,0.999,1e-8; print_iter = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T06:23:55.543226Z",
     "start_time": "2019-02-27T06:23:54.786222Z"
    },
    "code_folding": [
     13,
     18
    ]
   },
   "outputs": [],
   "source": [
    "# create layers\n",
    "l1_encoder = CNN(3,1,3,act=tf_elu)\n",
    "l2_encoder = CNN(3,3,6,act=tf_elu)\n",
    "l3_encoder = CNN(3,6,9,act=tf_sigmoid)\n",
    "\n",
    "l1_global  = CNN(3,10,60)\n",
    "l2_global  = CNN(3,60,60)\n",
    "l3_global  = CNN(3,60,1,act=tf_iden)\n",
    "\n",
    "l1_prior = CNN(3,9,9)\n",
    "l2_prior = CNN(3,9,9)\n",
    "l3_prior = CNN(3,9,1,act=tf_sigmoid,d_act=d_tf_sigmoid)\n",
    "\n",
    "def global_feed(input_data):\n",
    "    _,layer1_g = l1_global.feedforward(input_data)\n",
    "    _,layer2_g = l2_global.feedforward(layer1_g)\n",
    "    _,layer3_g = l3_global.feedforward(layer2_g)\n",
    "    return layer3_g\n",
    "def prior_feed(input_data):\n",
    "    _,layer1_p = l1_prior.feedforward(input_data)\n",
    "    _,layer2_p = l2_prior.feedforward(layer1_p)\n",
    "    _,layer3_p = l3_prior.feedforward(layer2_p)\n",
    "    return layer3_p\n",
    "\n",
    "x_encoding = tf.placeholder(tf.float32,(batch_size,96,96,1))\n",
    "x_reisze   = tf.image.resize_images(x_encoding,(90,90))\n",
    "\n",
    "_,layer1_e = l1_encoder.feedforward(x_encoding)\n",
    "_,layer2_e = l2_encoder.feedforward(layer1_e)\n",
    "_,layer3_e = l3_encoder.feedforward(layer2_e)\n",
    "\n",
    "layer3_s   = tf.transpose(layer3_e,(0,2,1,3))\n",
    "encoded_gt = tf.concat([layer3_e,x_reisze],3)\n",
    "encoded_rd = tf.concat([layer3_s,x_reisze],3)\n",
    "global_gt  = tf.reduce_mean(-tf_relu(-global_feed(encoded_gt)))\n",
    "global_rd  = tf.reduce_mean( tf_relu( global_feed(encoded_rd)))\n",
    "GLOBAL     = (global_rd - global_gt) \n",
    "\n",
    "prior  = tf.random.uniform(shape=layer3_e.shape)\n",
    "term_a = tf.reduce_mean(tf.log(prior_feed(prior)))\n",
    "term_b = tf.reduce_mean(tf.log(1.0 - prior_feed(layer3_e)))\n",
    "PRIOR  =  - (term_a + term_b) \n",
    "\n",
    "TOTAL_LOSS = GLOBAL + PRIOR\n",
    "auto_train = tf.train.AdamOptimizer(learning_rate).minimize(TOTAL_LOSS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T06:23:59.119478Z",
     "start_time": "2019-02-27T06:23:58.430749Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# creat classification network\n",
    "x = tf.placeholder(tf.float32,(batch_size*2,90,90,1))\n",
    "y = tf.placeholder(tf.float32,(batch_size*2,10))\n",
    "\n",
    "l1 = CNN(3,1, 16)\n",
    "l2 = CNN(3,16,16)\n",
    "l3 = CNN(3,16,16)\n",
    "\n",
    "l4 = CNN(3,16,32)\n",
    "l5 = CNN(3,32,32)\n",
    "l6 = CNN(3,32,32)\n",
    "\n",
    "l7 = CNN(3,32,64)\n",
    "l8 = CNN(1,64,64)\n",
    "l9 = CNN(1,64,10)\n",
    "\n",
    "_,layer1 = l1.feedforward(x)\n",
    "_,layer2 = l2.feedforward(layer1)\n",
    "_,layer3 = l3.feedforward(layer2)\n",
    "layer3   = tf.nn.avg_pool(layer3,ksize=[1,2,2,1],strides=[1,2,2,1],padding='VALID')\n",
    "\n",
    "_,layer4 = l4.feedforward(layer3)\n",
    "_,layer5 = l5.feedforward(layer4)\n",
    "_,layer6 = l6.feedforward(layer5)\n",
    "layer6   = tf.nn.avg_pool(layer6,ksize=[1,2,2,1],strides=[1,2,2,1],padding='VALID')\n",
    "\n",
    "_,layer7 = l7.feedforward(layer6)\n",
    "_,layer8 = l8.feedforward(layer7)\n",
    "_,layer9 = l9.feedforward(layer8)\n",
    "\n",
    "final_layer = tf.reduce_mean(layer9,(1,2))\n",
    "final_soft  = tf_softmax(final_layer)\n",
    "cost               = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=final_layer,labels=y))\n",
    "correct_prediction = tf.equal(tf.argmax(final_soft, 1), tf.argmax(y, 1))\n",
    "accuracy  = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "class_train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-27T06:23:56.681Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Current Iter : 0/5 batch : 2430/5000 loss : 0.58687167\r"
     ]
    }
   ],
   "source": [
    "# train the network \n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for iter in range(num_epoch):\n",
    "    for current_batch_index in range(0,len(train_images),batch_size):\n",
    "        current_batch = train_images[current_batch_index:current_batch_index+batch_size]\n",
    "        sess_results  = sess.run([TOTAL_LOSS,auto_train],feed_dict={x_encoding:current_batch})\n",
    "        sys.stdout.write(' Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + \n",
    "                         ' batch : ' + str(current_batch_index) + '/'+ str(len(train_images)) + \n",
    "                         ' loss : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush();    \n",
    "    if iter%1==0: print('\\n')\n",
    "    train_images = shuffle(train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-27T06:24:05.000Z"
    },
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# show images\n",
    "train_images = shuffle(train_images)\n",
    "for current_batch_index in range(0,len(train_images),batch_size):\n",
    "    current_batch = train_images[current_batch_index:current_batch_index+batch_size]\n",
    "    \n",
    "    current_image_resize = np.asarray([resize(np.squeeze(x),(90,90)) for x in current_batch])[:,:,:,None]\n",
    "    current_image_resize = (current_image_resize-current_image_resize.min((0,1,2),keepdims=True))/(current_image_resize.max((0,1,2),keepdims=True)-current_image_resize.min((0,1,2),keepdims=True)+1e-8)\n",
    "    latent_feature= sess.run(layer3_e,feed_dict={x_encoding:current_batch})\n",
    "    \n",
    "    for image_index in range(len(current_batch)):\n",
    "        plt.figure(figsize=(18,3))\n",
    "        plt.subplot(1,10,1)\n",
    "        plt.imshow(np.squeeze(current_batch[image_index]),cmap='gray')\n",
    "        plt.title(\n",
    "                 str(np.around(calc_MI(current_image_resize[image_index].ravel(),current_image_resize[image_index].ravel()),2))\n",
    "                 )\n",
    "        plt.axis('off')\n",
    "        \n",
    "        all_image_mi = []\n",
    "        for image_index2 in range(2,len(current_batch)+1): \n",
    "            all_image_mi.append(calc_MI(latent_feature[image_index,:,:,image_index2-2].ravel(),current_image_resize[image_index].ravel()))\n",
    "        all_image_mi_sort = sorted(range(len(all_image_mi)), key=lambda k: all_image_mi[k])[::-1]\n",
    "        \n",
    "        count = 2\n",
    "        for high_index in all_image_mi_sort:\n",
    "            plt.subplot(1,10,count)\n",
    "            plt.imshow(np.squeeze(latent_feature[image_index,:,:,high_index]),cmap='gray')\n",
    "            plt.axis('off')\n",
    "            plt.title(str(np.around(calc_MI(latent_feature[image_index,:,:,high_index].ravel(),current_image_resize[image_index].ravel()),2)))\n",
    "            count = count + 1\n",
    "        plt.show()\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T06:21:53.636540Z",
     "start_time": "2019-02-27T06:21:46.640072Z"
    },
    "code_folding": [
     15
    ]
   },
   "outputs": [
    {
     "ename": "FailedPreconditionError",
     "evalue": "Attempting to use uninitialized value Variable_900\n\t [[node Variable_900/read (defined at <ipython-input-2-571edeb18cf5>:31)  = Identity[T=DT_FLOAT, _class=[\"loc:@Variable_900\"], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](Variable_900)]]\n\t [[{{node Mean_42/_19}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_473_Mean_42\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'Variable_900/read', defined at:\n  File \"C:\\ProgramData\\Miniconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\ProgramData\\Miniconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\ProgramData\\Miniconda3\\lib\\asyncio\\base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"C:\\ProgramData\\Miniconda3\\lib\\asyncio\\base_events.py\", line 1434, in _run_once\n    handle._run()\n  File \"C:\\ProgramData\\Miniconda3\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tornado\\gen.py\", line 1233, in inner\n    self.run()\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tornado\\gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2819, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2845, in _run_cell\n    return runner(coro)\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3020, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3185, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3267, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-49-87167590150e>\", line 5, in <module>\n    l1 = CNN(3,1, 16)\n  File \"<ipython-input-2-571edeb18cf5>\", line 31, in __init__\n    self.w              = tf.Variable(tf.random_normal([k,k,inc,out],stddev=stddev,seed=2,dtype=tf.float32))\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 183, in __call__\n    return cls._variable_v1_call(*args, **kwargs)\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 146, in _variable_v1_call\n    aggregation=aggregation)\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 125, in <lambda>\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 2444, in default_variable_creator\n    expected_shape=expected_shape, import_scope=import_scope)\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 187, in __call__\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 1329, in __init__\n    constraint=constraint)\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 1491, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 81, in identity\n    return gen_array_ops.identity(input, name=name)\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 3454, in identity\n    \"Identity\", input=input, name=name)\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3274, in create_op\n    op_def=op_def)\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1770, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value Variable_900\n\t [[node Variable_900/read (defined at <ipython-input-2-571edeb18cf5>:31)  = Identity[T=DT_FLOAT, _class=[\"loc:@Variable_900\"], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](Variable_900)]]\n\t [[{{node Mean_42/_19}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_473_Mean_42\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value Variable_900\n\t [[{{node Variable_900/read}} = Identity[T=DT_FLOAT, _class=[\"loc:@Variable_900\"], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](Variable_900)]]\n\t [[{{node Mean_42/_19}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_473_Mean_42\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-81-0143bde565c4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mcurrent_aug_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcurrent_label\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0msess_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mauto_train\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mcurrent_aug_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mcurrent_aug_label\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         sys.stdout.write(' Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + \n\u001b[0;32m     28\u001b[0m                          \u001b[1;34m' batch : '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_batch_index\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/'\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1346\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1348\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1350\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value Variable_900\n\t [[node Variable_900/read (defined at <ipython-input-2-571edeb18cf5>:31)  = Identity[T=DT_FLOAT, _class=[\"loc:@Variable_900\"], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](Variable_900)]]\n\t [[{{node Mean_42/_19}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_473_Mean_42\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'Variable_900/read', defined at:\n  File \"C:\\ProgramData\\Miniconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\ProgramData\\Miniconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\ProgramData\\Miniconda3\\lib\\asyncio\\base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"C:\\ProgramData\\Miniconda3\\lib\\asyncio\\base_events.py\", line 1434, in _run_once\n    handle._run()\n  File \"C:\\ProgramData\\Miniconda3\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tornado\\gen.py\", line 1233, in inner\n    self.run()\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tornado\\gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2819, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2845, in _run_cell\n    return runner(coro)\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3020, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3185, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3267, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-49-87167590150e>\", line 5, in <module>\n    l1 = CNN(3,1, 16)\n  File \"<ipython-input-2-571edeb18cf5>\", line 31, in __init__\n    self.w              = tf.Variable(tf.random_normal([k,k,inc,out],stddev=stddev,seed=2,dtype=tf.float32))\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 183, in __call__\n    return cls._variable_v1_call(*args, **kwargs)\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 146, in _variable_v1_call\n    aggregation=aggregation)\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 125, in <lambda>\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 2444, in default_variable_creator\n    expected_shape=expected_shape, import_scope=import_scope)\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 187, in __call__\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 1329, in __init__\n    constraint=constraint)\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 1491, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 81, in identity\n    return gen_array_ops.identity(input, name=name)\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 3454, in identity\n    \"Identity\", input=input, name=name)\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3274, in create_op\n    op_def=op_def)\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1770, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value Variable_900\n\t [[node Variable_900/read (defined at <ipython-input-2-571edeb18cf5>:31)  = Identity[T=DT_FLOAT, _class=[\"loc:@Variable_900\"], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](Variable_900)]]\n\t [[{{node Mean_42/_19}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_473_Mean_42\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "# train classifier\n",
    "avg_acc_train = 0; avg_acc_test  = 0; train_acc = [];test_acc = []\n",
    "\n",
    "for iter in range(num_epoch):\n",
    "    \n",
    "    for current_batch_index in range(0,len(train_images),batch_size):\n",
    "        \n",
    "        current_batch = train_images[current_batch_index:current_batch_index+batch_size]\n",
    "        current_label = train_labels[current_batch_index:current_batch_index+batch_size]\n",
    "        \n",
    "        current_image_resize = np.asarray([resize(np.squeeze(x),(90,90)) for x in current_batch])[:,:,:,None]\n",
    "        current_image_resize = (current_image_resize-current_image_resize.min((0,1,2),keepdims=True))/(current_image_resize.max((0,1,2),keepdims=True)-current_image_resize.min((0,1,2),keepdims=True)+1e-8)\n",
    "        latent_feature = sess.run(layer3_e,feed_dict={x_encoding:current_batch}) \n",
    "        \n",
    "        current_aug_batch = []\n",
    "        for image_index in range(len(latent_feature)):\n",
    "            all_image_mi = []\n",
    "            for image_index2 in range(len(latent_feature.T)): \n",
    "                all_image_mi.append(calc_MI(latent_feature[image_index,:,:,image_index2].ravel(),current_image_resize[image_index].ravel()))\n",
    "            all_image_mi_sort = sorted(range(len(all_image_mi)), key=lambda k: all_image_mi[k])[::-1][:2]\n",
    "            current_aug_batch.append(latent_feature[image_index,:,:,all_image_mi_sort[0]])\n",
    "            current_aug_batch.append(latent_feature[image_index,:,:,all_image_mi_sort[1]])\n",
    "        current_aug_batch = np.asarray(current_aug_batch)[:,:,:,None]\n",
    "        current_aug_label = np.asarray([[x,x] for x in current_label ]).reshape((20,10))\n",
    "        \n",
    "        sess_results = sess.run([accuracy,auto_train],feed_dict={x:current_aug_batch,y:current_aug_label})\n",
    "        sys.stdout.write(' Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + \n",
    "                         ' batch : ' + str(current_batch_index) + '/'+ str(len(train_images)) + \n",
    "                         ' loss : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush();   \n",
    "        sys.exit()\n",
    "\n",
    "    if iter%1==0: print('\\n')\n",
    "    train_images = shuffle(train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T06:18:54.584529Z",
     "start_time": "2019-02-27T06:18:54.198939Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T06:17:12.615183Z",
     "start_time": "2019-02-27T06:17:12.610171Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-27T05:42:06.077Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
