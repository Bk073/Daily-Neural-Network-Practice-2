{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T02:41:11.514563Z",
     "start_time": "2019-02-06T02:41:11.475669Z"
    },
    "code_folding": [
     8,
     29,
     47,
     59,
     71,
     88
    ]
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import nn as nn\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.c0 = nn.Conv2d(3, 64, kernel_size=4, stride=1)\n",
    "        self.c1 = nn.Conv2d(64, 128, kernel_size=4, stride=1)\n",
    "        self.c2 = nn.Conv2d(128, 256, kernel_size=4, stride=1)\n",
    "        self.c3 = nn.Conv2d(256, 512, kernel_size=4, stride=1)\n",
    "        self.l1 = nn.Linear(512*20*20, 64)\n",
    "\n",
    "        self.b1 = nn.BatchNorm2d(128)\n",
    "        self.b2 = nn.BatchNorm2d(256)\n",
    "        self.b3 = nn.BatchNorm2d(512)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.c0(x))\n",
    "        features = F.relu(self.b1(self.c1(h)))\n",
    "        h = F.relu(self.b2(self.c2(features)))\n",
    "        h = F.relu(self.b3(self.c3(h)))\n",
    "        encoded = self.l1(h.view(x.shape[0], -1))\n",
    "        return encoded, features\n",
    "\n",
    "class GlobalDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.c0 = nn.Conv2d(128, 64, kernel_size=3)\n",
    "        self.c1 = nn.Conv2d(64, 32, kernel_size=3)\n",
    "        self.l0 = nn.Linear(32 * 22 * 22 + 64, 512)\n",
    "        self.l1 = nn.Linear(512, 512)\n",
    "        self.l2 = nn.Linear(512, 1)\n",
    "\n",
    "    def forward(self, y, M):\n",
    "        h = F.relu(self.c0(M))\n",
    "        h = self.c1(h)\n",
    "        h = h.view(y.shape[0], -1)\n",
    "        h = torch.cat((y, h), dim=1)\n",
    "        h = F.relu(self.l0(h))\n",
    "        h = F.relu(self.l1(h))\n",
    "        return self.l2(h)\n",
    "\n",
    "class LocalDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.c0 = nn.Conv2d(192, 512, kernel_size=1)\n",
    "        self.c1 = nn.Conv2d(512, 512, kernel_size=1)\n",
    "        self.c2 = nn.Conv2d(512, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.c0(x))\n",
    "        h = F.relu(self.c1(h))\n",
    "        return self.c2(h)\n",
    "\n",
    "class PriorDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l0 = nn.Linear(64, 1000)\n",
    "        self.l1 = nn.Linear(1000, 200)\n",
    "        self.l2 = nn.Linear(200, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.l0(x))\n",
    "        h = F.relu(self.l1(h))\n",
    "        return torch.sigmoid(self.l2(h))\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(64, 15)\n",
    "        self.bn1 = nn.BatchNorm1d(15)\n",
    "        self.l2 = nn.Linear(15, 10)\n",
    "        self.bn2 = nn.BatchNorm1d(10)\n",
    "        self.l3 = nn.Linear(10, 10)\n",
    "        self.bn3 = nn.BatchNorm1d(10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded, _ = x[0], x[1]\n",
    "        clazz = F.relu(self.bn1(self.l1(encoded)))\n",
    "        clazz = F.relu(self.bn2(self.l2(clazz)))\n",
    "        clazz = F.softmax(self.bn3(self.l3(clazz)), dim=1)\n",
    "        return clazz\n",
    "\n",
    "class DeepInfoAsLatent(nn.Module):\n",
    "    def __init__(self, run, epoch):\n",
    "        super().__init__()\n",
    "        model_path = Path(r'c:/data/deepinfomax/models') / Path(str(run)) / Path('encoder' + str(epoch) + '.wgt')\n",
    "        self.encoder = Encoder()\n",
    "        self.encoder.load_state_dict(torch.load(str(model_path)))\n",
    "        self.classifier = Classifier()\n",
    "\n",
    "    def forward(self, x):\n",
    "        z, features = self.encoder(x)\n",
    "        z = z.detach()\n",
    "        return self.classifier((z, features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-06T02:43:27.494Z"
    },
    "code_folding": [
     14
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "861 Loss: 1.079176139831543: 100%|█████████████████████████████████████████████████████████| 781/781 [02:30<00:00,  5.92it/s]\n",
      "862 Loss: 0.9797142416238784: 100%|████████████████████████████████████████████████████████| 781/781 [02:12<00:00,  5.83it/s]\n",
      "863 Loss: 0.9199020564556122: 100%|████████████████████████████████████████████████████████| 781/781 [02:11<00:00,  5.96it/s]\n",
      "864 Loss: 0.8502464711666107: 100%|████████████████████████████████████████████████████████| 781/781 [02:11<00:00,  5.88it/s]\n",
      "865 Loss: 0.7808394730091095: 100%|████████████████████████████████████████████████████████| 781/781 [02:11<00:00,  5.92it/s]\n",
      "866 Loss: 0.7401991724967957: 100%|████████████████████████████████████████████████████████| 781/781 [02:11<00:00,  5.92it/s]\n",
      "867 Loss: 0.7200962513685226: 100%|████████████████████████████████████████████████████████| 781/781 [02:12<00:00,  5.95it/s]\n",
      "868 Loss: 0.652477577328682: 100%|█████████████████████████████████████████████████████████| 781/781 [02:11<00:00,  5.93it/s]\n",
      "869 Loss: 0.6325627595186234: 100%|████████████████████████████████████████████████████████| 781/781 [02:11<00:00,  5.94it/s]\n",
      "870 Loss: 0.6382431089878082: 100%|████████████████████████████████████████████████████████| 781/781 [02:11<00:00,  5.83it/s]\n",
      "871 Loss: 0.6043556705117226: 100%|████████████████████████████████████████████████████████| 781/781 [02:11<00:00,  5.89it/s]\n",
      "872 Loss: 0.571001709997654: 100%|█████████████████████████████████████████████████████████| 781/781 [02:11<00:00,  5.86it/s]\n",
      "873 Loss: 0.5915541559457779: 100%|████████████████████████████████████████████████████████| 781/781 [02:11<00:00,  5.95it/s]\n",
      "874 Loss: 0.5429372429847718: 100%|████████████████████████████████████████████████████████| 781/781 [02:11<00:00,  5.95it/s]\n",
      "875 Loss: 0.5464610546827317: 100%|████████████████████████████████████████████████████████| 781/781 [02:11<00:00,  5.96it/s]\n",
      "876 Loss: 0.5258807688951492: 100%|████████████████████████████████████████████████████████| 781/781 [02:12<00:00,  5.95it/s]\n",
      "877 Loss: 0.5082048490643502: 100%|████████████████████████████████████████████████████████| 781/781 [02:11<00:00,  5.94it/s]\n",
      "878 Loss: 0.4979823470115662: 100%|████████████████████████████████████████████████████████| 781/781 [02:11<00:00,  5.92it/s]\n",
      "879 Loss: 0.47884813845157626: 100%|███████████████████████████████████████████████████████| 781/781 [02:11<00:00,  5.89it/s]\n",
      "880 Loss: 0.47982993721961975: 100%|███████████████████████████████████████████████████████| 781/781 [02:11<00:00,  5.96it/s]\n",
      "881 Loss: 0.46286064237356184: 100%|███████████████████████████████████████████████████████| 781/781 [02:11<00:00,  5.94it/s]\n",
      "882 Loss: 0.43170828819274903: 100%|███████████████████████████████████████████████████████| 781/781 [02:11<00:00,  5.75it/s]\n",
      "883 Loss: 0.4629965379834175: 100%|████████████████████████████████████████████████████████| 781/781 [02:11<00:00,  5.93it/s]\n",
      "884 Loss: 0.39330381751060484: 100%|███████████████████████████████████████████████████████| 781/781 [02:11<00:00,  5.89it/s]\n",
      "885 Loss: 0.39236889630556104: 100%|███████████████████████████████████████████████████████| 781/781 [02:11<00:00,  5.94it/s]\n",
      "886 Loss: 0.4085193261504173: 100%|████████████████████████████████████████████████████████| 781/781 [02:11<00:00,  5.93it/s]\n",
      "887 Loss: 0.4119227185845375: 100%|████████████████████████████████████████████████████████| 781/781 [02:11<00:00,  5.95it/s]\n",
      "888 Loss: 0.36778913885354997: 100%|███████████████████████████████████████████████████████| 781/781 [02:11<00:00,  5.93it/s]\n",
      "889 Loss: 0.3660557046532631: 100%|████████████████████████████████████████████████████████| 781/781 [02:11<00:00,  5.90it/s]\n",
      "890 Loss: 0.35371066629886627: 100%|███████████████████████████████████████████████████████| 781/781 [02:11<00:00,  5.96it/s]\n",
      "891 Loss: 0.3537528559565544: 100%|████████████████████████████████████████████████████████| 781/781 [02:11<00:00,  5.89it/s]\n",
      "892 Loss: 0.372491879761219: 100%|█████████████████████████████████████████████████████████| 781/781 [02:11<00:00,  5.97it/s]\n",
      "893 Loss: 0.34722319692373277: 100%|███████████████████████████████████████████████████████| 781/781 [02:11<00:00,  5.90it/s]\n",
      "894 Loss: 0.3460047274827957: 100%|████████████████████████████████████████████████████████| 781/781 [02:11<00:00,  5.91it/s]\n",
      "895 Loss: 0.3445689037442207: 100%|████████████████████████████████████████████████████████| 781/781 [02:11<00:00,  5.81it/s]\n",
      "896 Loss: 0.3294647835195065: 100%|████████████████████████████████████████████████████████| 781/781 [02:11<00:00,  5.90it/s]\n",
      "897 Loss: 0.32713085412979126: 100%|███████████████████████████████████████████████████████| 781/781 [02:11<00:00,  5.91it/s]\n",
      "898 Loss: 0.3032299667596817: 100%|████████████████████████████████████████████████████████| 781/781 [02:11<00:00,  5.92it/s]\n",
      "899 Loss: 0.3341494336724281: 100%|████████████████████████████████████████████████████████| 781/781 [02:11<00:00,  5.93it/s]\n",
      "900 Loss: 0.30697206780314445: 100%|███████████████████████████████████████████████████████| 781/781 [02:11<00:00,  5.89it/s]\n",
      "901 Loss: 0.2918452858924866: 100%|████████████████████████████████████████████████████████| 781/781 [02:11<00:00,  5.91it/s]\n",
      "902 Loss: 0.2922953382134438: 100%|████████████████████████████████████████████████████████| 781/781 [02:11<00:00,  5.88it/s]\n",
      "903 Loss: 0.2988049380481243: 100%|████████████████████████████████████████████████████████| 781/781 [02:11<00:00,  5.97it/s]\n",
      "904 Loss: 0.279029793292284: 100%|█████████████████████████████████████████████████████████| 781/781 [02:11<00:00,  5.84it/s]\n",
      "905 Loss: 0.27950904220342637: 100%|███████████████████████████████████████████████████████| 781/781 [02:11<00:00,  5.96it/s]\n",
      "906 Loss: 0.2857613660395145: 100%|████████████████████████████████████████████████████████| 781/781 [02:11<00:00,  5.94it/s]\n",
      "907 Loss: 0.27482631504535676: 100%|███████████████████████████████████████████████████████| 781/781 [02:11<00:00,  5.92it/s]\n",
      "908 Loss: 0.2660746298730373: 100%|████████████████████████████████████████████████████████| 781/781 [02:11<00:00,  5.80it/s]\n",
      "909 Loss: 0.2632285043597221: 100%|████████████████████████████████████████████████████████| 781/781 [02:11<00:00,  5.95it/s]\n",
      "910 Loss: 0.28050254955887793: 100%|███████████████████████████████████████████████████████| 781/781 [02:11<00:00,  5.95it/s]\n",
      "911 Loss: 0.25175138711929324: 100%|███████████████████████████████████████████████████████| 781/781 [02:11<00:00,  5.94it/s]\n",
      "912 Loss: 0.2597913347184658: 100%|████████████████████████████████████████████████████████| 781/781 [02:11<00:00,  5.92it/s]\n",
      "913 Loss: 0.24653537422418595: 100%|███████████████████████████████████████████████████████| 781/781 [02:11<00:00,  5.90it/s]\n",
      "914 Loss: 0.2614444449543953: 100%|████████████████████████████████████████████████████████| 781/781 [02:12<00:00,  5.96it/s]\n",
      "915 Loss: 0.24480979666113853: 100%|███████████████████████████████████████████████████████| 781/781 [02:11<00:00,  5.95it/s]\n",
      "916 Loss: 0.2656494349241257: 100%|████████████████████████████████████████████████████████| 781/781 [02:11<00:00,  5.92it/s]\n",
      "917 Loss: 0.2460056222975254: 100%|████████████████████████████████████████████████████████| 781/781 [02:11<00:00,  5.89it/s]\n",
      "918 Loss: 0.24088286161422728: 100%|███████████████████████████████████████████████████████| 781/781 [02:11<00:00,  5.88it/s]\n",
      "919 Loss: 0.2524561174213886: 100%|████████████████████████████████████████████████████████| 781/781 [02:11<00:00,  5.94it/s]\n",
      "920 Loss: 0.23550264909863472: 100%|███████████████████████████████████████████████████████| 781/781 [02:11<00:00,  5.95it/s]\n",
      "921 Loss: 0.23862103447318078: 100%|███████████████████████████████████████████████████████| 781/781 [02:11<00:00,  5.94it/s]\n",
      "922 Loss: 0.2295067846775055: 100%|████████████████████████████████████████████████████████| 781/781 [02:11<00:00,  5.91it/s]\n",
      "923 Loss: 0.2415560431778431: 100%|████████████████████████████████████████████████████████| 781/781 [02:11<00:00,  6.01it/s]\n",
      "924 Loss: 0.21300407946109773: 100%|███████████████████████████████████████████████████████| 781/781 [02:11<00:00,  5.92it/s]\n",
      "925 Loss: 0.22482107654213906: 100%|███████████████████████████████████████████████████████| 781/781 [02:11<00:00,  5.94it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "926 Loss: 0.21468398347496986: 100%|███████████████████████████████████████████████████████| 781/781 [02:11<00:00,  5.92it/s]\n",
      "927 Loss: 0.20089171305298806: 100%|███████████████████████████████████████████████████████| 781/781 [02:11<00:00,  5.95it/s]\n",
      "928 Loss: 0.22822452411055566: 100%|███████████████████████████████████████████████████████| 781/781 [02:11<00:00,  5.89it/s]\n",
      "929 Loss: 0.2273866802453995: 100%|████████████████████████████████████████████████████████| 781/781 [02:11<00:00,  5.93it/s]\n",
      "930 Loss: 0.23043190017342569:  64%|███████████████████████████████████▏                   | 500/781 [01:24<00:47,  5.88it/s]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from models import Encoder, GlobalDiscriminator, LocalDiscriminator, PriorDiscriminator\n",
    "from torchvision.datasets.cifar import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import statistics as stats\n",
    "import argparse\n",
    "\n",
    "\n",
    "class DeepInfoMaxLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.5, beta=1.0, gamma=0.1):\n",
    "        super().__init__()\n",
    "        self.global_d = GlobalDiscriminator()\n",
    "        self.local_d = LocalDiscriminator()\n",
    "        self.prior_d = PriorDiscriminator()\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, y, M, M_prime):\n",
    "\n",
    "        # see appendix 1A of https://arxiv.org/pdf/1808.06670.pdf\n",
    "\n",
    "        y_exp = y.unsqueeze(-1).unsqueeze(-1)\n",
    "        y_exp = y_exp.expand(-1, -1, 26, 26)\n",
    "\n",
    "        y_M = torch.cat((M, y_exp), dim=1)\n",
    "        y_M_prime = torch.cat((M_prime, y_exp), dim=1)\n",
    "\n",
    "        Ej = -F.softplus(-self.local_d(y_M)).mean()\n",
    "        Em = F.softplus(self.local_d(y_M_prime)).mean()\n",
    "        LOCAL = (Em - Ej) * self.beta\n",
    "\n",
    "        Ej = -F.softplus(-self.global_d(y, M)).mean()\n",
    "        Em = F.softplus(self.global_d(y, M_prime)).mean()\n",
    "        GLOBAL = (Em - Ej) * self.alpha\n",
    "\n",
    "        prior = torch.rand_like(y)\n",
    "\n",
    "        term_a = torch.log(self.prior_d(prior)).mean()\n",
    "        term_b = torch.log(1.0 - self.prior_d(y)).mean()\n",
    "        PRIOR = - (term_a + term_b) * self.gamma\n",
    "\n",
    "        return LOCAL + GLOBAL + PRIOR\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    #parser = argparse.ArgumentParser(description='DeepInfomax pytorch')\n",
    "    #parser.add_argument('--batch_size', default=64, type=int, help='batch_size')\n",
    "    #args = parser.parse_args()\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    batch_size = 64\n",
    "\n",
    "    # image size 3, 32, 32 batch size must be an even numbershuffle must be True\n",
    "    cifar_10_train_dt = CIFAR10(r'c:\\data\\tv',  download=True, transform=ToTensor())\n",
    "    cifar_10_train_l  = DataLoader(cifar_10_train_dt, batch_size=batch_size, shuffle=True, drop_last=True,pin_memory=torch.cuda.is_available())\n",
    "\n",
    "    encoder = Encoder().to(device)\n",
    "    loss_fn = DeepInfoMaxLoss().to(device)\n",
    "    optim = Adam(encoder.parameters(), lr=1e-4)\n",
    "    loss_optim = Adam(loss_fn.parameters(), lr=1e-4)\n",
    "\n",
    "    epoch_restart = 860\n",
    "    root = Path(r'c:\\data\\deepinfomax\\models\\run5')\n",
    "\n",
    "#     if epoch_restart is not None and root is not None:\n",
    "#         enc_file = root / Path('encoder' + str(epoch_restart) + '.wgt')\n",
    "#         loss_file = root / Path('loss' + str(epoch_restart) + '.wgt')\n",
    "#         encoder.load_state_dict(torch.load(str(enc_file)))\n",
    "#         loss_fn.load_state_dict(torch.load(str(loss_file)))\n",
    "\n",
    "    for epoch in range(epoch_restart + 1, 1000):\n",
    "        batch = tqdm(cifar_10_train_l, total=len(cifar_10_train_dt) // batch_size)\n",
    "        train_loss = []\n",
    "        for x, target in batch:\n",
    "            x = x.to(device)\n",
    "\n",
    "            optim.zero_grad()\n",
    "            loss_optim.zero_grad()\n",
    "            y, M = encoder(x)\n",
    "            # rotate images to create pairs for comparison\n",
    "            M_prime = torch.cat((M[1:], M[0].unsqueeze(0)), dim=0)\n",
    "            loss = loss_fn(y, M, M_prime)\n",
    "            train_loss.append(loss.item())\n",
    "            batch.set_description(str(epoch) + ' Loss: ' + str(stats.mean(train_loss[-20:])))\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            loss_optim.step()\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            root = Path(r'c:\\data\\deepinfomax\\models\\run5')\n",
    "            enc_file = root / Path('encoder' + str(epoch) + '.wgt')\n",
    "            loss_file = root / Path('loss' + str(epoch) + '.wgt')\n",
    "            enc_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "            torch.save(encoder.state_dict(), str(enc_file))\n",
    "            torch.save(loss_fn.state_dict(), str(loss_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T02:43:12.554874Z",
     "start_time": "2019-02-06T02:43:12.538916Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
