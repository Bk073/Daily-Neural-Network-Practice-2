{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-05T00:53:29.432658Z",
     "start_time": "2019-01-05T00:53:26.404244Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# import Library and some random image data set\n",
    "import tensorflow as tf\n",
    "import numpy      as np\n",
    "import seaborn    as sns \n",
    "import pandas     as pd\n",
    "import os,sys\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "np.random.seed(78); tf.set_random_seed(78)\n",
    "\n",
    "# get some of the STL data set\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from skimage import util \n",
    "from skimage.transform import resize\n",
    "from skimage.io import imread\n",
    "import warnings\n",
    "from numpy import inf\n",
    "\n",
    "from scipy.stats import kurtosis,skew\n",
    "\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Things to compare</h1>\n",
    "<div>\n",
    "<ul>\n",
    "  <li>Z: Baseline </li>\n",
    "  <li>A: Theta ^ 2 </li>\n",
    "  <li>B: abs(Theta) + sqrt(abs(Theta) ^ 2)</li>\n",
    "  <li>C: Theta </li>\n",
    "  <li>D: sqrt(Theta ^ 2)</li>\n",
    "  <li>E: abs(Theta) </li>\n",
    "  <li>F: sqrt(abs(Theta) ^ 2)</li>\n",
    "  <li>G: sqrt(Theta ^ 2)/Theta</li>\n",
    "  <li>H: - tanh(Theta)</li>\n",
    "  <li>I: - tanh(Theta ^ 2)</li>\n",
    "  <li>J: - log(1+Theta ^ 2)</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-05T00:53:31.333603Z",
     "start_time": "2019-01-05T00:53:29.434675Z"
    },
    "code_folding": [
     0,
     3,
     30,
     38
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 96, 96, 3) 1.0 0.0\n",
      "(5000, 10) 1.0 0.0\n",
      "(8000, 96, 96, 3) 1.0 0.0\n",
      "(8000, 10) 1.0 0.0\n"
     ]
    }
   ],
   "source": [
    "# read all of the data\n",
    "# https://github.com/mttk/STL10\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "def read_all_images(path_to_data):\n",
    "    \"\"\"\n",
    "    :param path_to_data: the file containing the binary images from the STL-10 dataset\n",
    "    :return: an array containing all the images\n",
    "    \"\"\"\n",
    "\n",
    "    with open(path_to_data, 'rb') as f:\n",
    "        # read whole file in uint8 chunks\n",
    "        everything = np.fromfile(f, dtype=np.uint8)\n",
    "\n",
    "        # We force the data into 3x96x96 chunks, since the\n",
    "        # images are stored in \"column-major order\", meaning\n",
    "        # that \"the first 96*96 values are the red channel,\n",
    "        # the next 96*96 are green, and the last are blue.\"\n",
    "        # The -1 is since the size of the pictures depends\n",
    "        # on the input file, and this way numpy determines\n",
    "        # the size on its own.\n",
    "\n",
    "        images = np.reshape(everything, (-1, 3, 96, 96))\n",
    "\n",
    "        # Now transpose the images into a standard image format\n",
    "        # readable by, for example, matplotlib.imshow\n",
    "        # You might want to comment this line or reverse the shuffle\n",
    "        # if you will use a learning algorithm like CNN, since they like\n",
    "        # their channels separated.\n",
    "        images = np.transpose(images, (0, 3, 2, 1))\n",
    "        return images\n",
    "def read_labels(path_to_labels):\n",
    "    \"\"\"\n",
    "    :param path_to_labels: path to the binary file containing labels from the STL-10 dataset\n",
    "    :return: an array containing the labels\n",
    "    \"\"\"\n",
    "    with open(path_to_labels, 'rb') as f:\n",
    "        labels = np.fromfile(f, dtype=np.uint8)\n",
    "        return labels\n",
    "def show_images(data,row=1,col=1):\n",
    "    fig=plt.figure(figsize=(10,10))\n",
    "    columns = col; rows = row\n",
    "    for i in range(1, columns*rows +1):\n",
    "        fig.add_subplot(rows, columns, i)\n",
    "        plt.imshow(data[i-1])\n",
    "    plt.show()\n",
    "\n",
    "train_images = read_all_images(\"../../../DataSet/STL10/stl10_binary/train_X.bin\") / 255.0\n",
    "train_labels = read_labels    (\"../../../DataSet/STL10/stl10_binary/train_Y.bin\")\n",
    "test_images  = read_all_images(\"../../../DataSet/STL10/stl10_binary/test_X.bin\")  / 255.0\n",
    "test_labels  = read_labels    (\"../../../DataSet/STL10/stl10_binary/test_y.bin\")\n",
    "\n",
    "label_encoder= OneHotEncoder(sparse=False,categories='auto')\n",
    "train_labels = label_encoder.fit_transform(train_labels.reshape((-1,1)))\n",
    "test_labels  = label_encoder.fit_transform(test_labels.reshape((-1,1)))\n",
    "\n",
    "print(train_images.shape,train_images.max(),train_images.min())\n",
    "print(train_labels.shape,train_labels.max(),train_labels.min())\n",
    "print(test_images.shape,test_images.max(),test_images.min())\n",
    "print(test_labels.shape,test_labels.max(),test_labels.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-05T00:53:31.406429Z",
     "start_time": "2019-01-05T00:53:31.335573Z"
    },
    "code_folding": [
     15,
     60,
     75,
     83,
     87
    ]
   },
   "outputs": [],
   "source": [
    "# create the layers\n",
    "def tf_softmax(x): return tf.nn.softmax(x)\n",
    "\n",
    "def tf_elu(x):   return tf.nn.elu(x)\n",
    "def d_tf_elu(x): return tf.cast(tf.greater(x,0),tf.float32)  + (tf_elu(tf.cast(tf.less_equal(x,0),tf.float32) * x) + 1.0)\n",
    "\n",
    "def tf_relu(x):   return tf.nn.relu(x)\n",
    "def d_tf_relu(x): return tf.cast(tf.greater(x,0),tf.float32)\n",
    "\n",
    "def tf_tanh(x):   return tf.nn.tanh(x)\n",
    "def d_tf_tanh(x): return 1 - tf_tanh(x) ** 2\n",
    "\n",
    "def tf_sigmoid(x):   return tf.nn.sigmoid(x)\n",
    "def d_tf_sigmoid(x): return tf_sigmoid(x) * (1.0-tf_sigmoid(x))\n",
    "\n",
    "class CNN():\n",
    "\n",
    "    def __init__(self,k,inc,out, stddev=0.05,which_reg=0,act=tf_relu,d_act=d_tf_relu):\n",
    "        self.w              = tf.Variable(tf.random_normal([k,k,inc,out],stddev=stddev,seed=2,dtype=tf.float32))\n",
    "        self.m,self.v       = tf.Variable(tf.zeros_like(self.w)),tf.Variable(tf.zeros_like(self.w))\n",
    "        self.act,self.d_act = act,d_act\n",
    "        self.regularizer    = which_reg\n",
    "        \n",
    "    def getw(self): return self.w\n",
    "\n",
    "    def feedforward(self,input,stride=1,padding='SAME'):\n",
    "        self.input  = input\n",
    "        self.layer  = tf.nn.conv2d(input,self.w,strides=[1,stride,stride,1],padding=padding) \n",
    "        self.layerA = self.act(self.layer)\n",
    "        return self.layer, self.layerA\n",
    "    \n",
    "    def backprop(self,gradient,stride=1,padding='SAME'):\n",
    "        grad_part_1 = gradient\n",
    "        grad_part_2 = self.d_act(self.layer)\n",
    "        grad_part_3 = self.input\n",
    "\n",
    "        grad_middle = grad_part_1 * grad_part_2\n",
    "        grad        = tf.nn.conv2d_backprop_filter(input = grad_part_3,filter_sizes = tf.shape(self.w),  out_backprop = grad_middle,strides=[1,stride,stride,1],padding=padding) / batch_size\n",
    "        grad_pass   = tf.nn.conv2d_backprop_input (input_sizes = tf.shape(self.input),filter= self.w,out_backprop = grad_middle,strides=[1,stride,stride,1],padding=padding)\n",
    "\n",
    "        if self.regularizer == 'A': grad = grad + lamda * 2.0 * self.w\n",
    "        if self.regularizer == 'B': grad = grad + lamda * 0.5 * (tf.sign(self.w) + (1.0/tf.sqrt(tf.square(tf.abs(self.w))+ 10e-8)) * tf.abs(self.w) * tf.sign(self.w))\n",
    "        if self.regularizer == 'C': grad = grad + lamda * tf.ones_like(self.w)\n",
    "        if self.regularizer == 'D': grad = grad + lamda * (1.0/tf.sqrt(tf.square(self.w)+ 10e-8)) * self.w\n",
    "        if self.regularizer == 'E': grad = grad + lamda * tf.sign(self.w)\n",
    "        if self.regularizer == 'F': grad = grad + lamda * (1.0/tf.sqrt(tf.square(tf.abs(self.w))+ 10e-8)) * tf.abs(self.w) * tf.sign(self.w)\n",
    "        if self.regularizer == 'G': grad = grad + lamda * ( self.w**2/(tf.sqrt(self.w)+10e-8) - tf.sqrt(self.w ** 2))/(self.w**2 + 10e-8)\n",
    "        if self.regularizer == 'H': grad = grad + lamda * -1.0 * (1.0-tf.tanh(self.w) ** 2)\n",
    "        if self.regularizer == 'I': grad = grad + lamda * -1.0 * (1.0-tf.tanh(self.w ** 2) ** 2) * 2.0 * self.w\n",
    "        if self.regularizer == 'J': grad = grad + lamda * -1.0 * (1/(1+self.w ** 2)) * 2.0 * self.w\n",
    "        \n",
    "        update_w = []\n",
    "        update_w.append(tf.assign( self.m,self.m*beta1 + (1-beta1) * (grad)   ))\n",
    "        update_w.append(tf.assign( self.v,self.v*beta2 + (1-beta2) * (grad ** 2)   ))\n",
    "        m_hat = self.m / (1-beta1) ; v_hat = self.v / (1-beta2)\n",
    "        adam_middle = m_hat * learning_rate/(tf.sqrt(v_hat) + adam_e)\n",
    "        update_w.append(tf.assign(self.w,tf.subtract(self.w,adam_middle  )))\n",
    "        \n",
    "        return grad_pass,grad,update_w\n",
    "    \n",
    "def save_to_image(data,name):\n",
    "    l1g,l2g,l3g,l4g,l5g,l6g = data\n",
    "    l1g,l2g,l3g,l4g,l5g,l6g = np.asarray(l1g),np.asarray(l2g),np.asarray(l3g),np.asarray(l4g),np.asarray(l5g),np.asarray(l6g)\n",
    "    plt.figure(figsize=(25,15))\n",
    "    plt.suptitle('Current Iter : ' + str(iter))\n",
    "    plt.subplot(231); plt.hist(l1g.ravel(),50); plt.title('layer 1')\n",
    "    plt.subplot(232); plt.hist(l2g.ravel(),50); plt.title('layer 2')\n",
    "    plt.subplot(233); plt.hist(l3g.ravel(),50); plt.title('layer 3')\n",
    "    plt.subplot(234); plt.hist(l4g.ravel(),50); plt.title('layer 4')\n",
    "    plt.subplot(235); plt.hist(l5g.ravel(),50); plt.title('layer 5')\n",
    "    plt.subplot(236); plt.hist(l6g.ravel(),50); plt.title('layer 6')\n",
    "    plt.savefig(name + str(iter)+'.png')\n",
    "    plt.tight_layout()\n",
    "    plt.close('all')     \n",
    "    \n",
    "def append_stat(current_list,data,number):\n",
    "    current_list[0].append(data[number].mean())\n",
    "    current_list[1].append(data[number].std())\n",
    "    current_list[2].append(skew    (data[number].ravel()))\n",
    "    current_list[3].append(kurtosis(data[number].ravel()))\n",
    "    current_list[4].append(np.count_nonzero(data[number]))\n",
    "    return current_list\n",
    "\n",
    "def transform_to_2d(data):\n",
    "    batch,width,height,chan = data.shape\n",
    "    return data.reshape((batch*width,height*chan))\n",
    "\n",
    "def save_to_image(main_data,one,two,three,four,five,six,experiment_name,tran_acc,test_acc,current_exp,iter):\n",
    "    plt.figure(figsize=(20,40))\n",
    "    G = gridspec.GridSpec(8, 6)\n",
    "\n",
    "    plt.figtext(0.5,1.0,\"Iter: \" + str(iter) + \" Histogram Per \" + experiment_name,ha=\"center\", va=\"top\", fontsize=35, color=\"black\")\n",
    "    plt.subplot(G[0, 0]).hist(main_data[0].ravel(),50,color='red');       plt.subplot(G[0, 0]).set_title(experiment_name+' 1')\n",
    "    plt.subplot(G[0, 1]).hist(main_data[1].ravel(),50,color='orange');    plt.subplot(G[0, 1]).set_title(experiment_name+' 2')\n",
    "    plt.subplot(G[0, 2]).hist(main_data[2].ravel(),50,color='yellow');  plt.subplot(G[0, 2]).set_title(experiment_name+' 3')\n",
    "    plt.subplot(G[0, 3]).hist(main_data[3].ravel(),50,color='green');    plt.subplot(G[0, 3]).set_title(experiment_name+' 4')\n",
    "    plt.subplot(G[0, 4]).hist(main_data[4].ravel(),50,color='blue');     plt.subplot(G[0, 4]).set_title(experiment_name+' 5')\n",
    "    plt.subplot(G[0, 5]).hist(main_data[5].ravel(),50,color='black');     plt.subplot(G[0, 5]).set_title(experiment_name+' 6')\n",
    "\n",
    "    plt.subplot(G[1, :]).set_title(\"Mean Per \"+ experiment_name)\n",
    "    plt.subplot(G[1, :]).plot(one[0]  ,c='red',alpha=0.9,label='1')\n",
    "    plt.subplot(G[1, :]).plot(two[0]  ,c='orange',alpha=0.9,label='2')\n",
    "    plt.subplot(G[1, :]).plot(three[0],c='yellow',alpha=0.9,label='3')\n",
    "    plt.subplot(G[1, :]).plot(four[0],c='green',alpha=0.9,label='4')\n",
    "    plt.subplot(G[1, :]).plot(five[0],c='blue',alpha=0.9,label='5')\n",
    "    plt.subplot(G[1, :]).plot(six[0],c='black',alpha=0.9,label='6')\n",
    "    plt.legend(bbox_to_anchor=(0., 0.95, 1., .05), loc=9,ncol=6, mode=\"expand\", borderaxespad=0.)\n",
    "\n",
    "    plt.subplot(G[2, :]).set_title(\"Standard Deviation Per \"+ experiment_name)\n",
    "    plt.subplot(G[2, :]).plot(one[1]  ,c='red',alpha=0.9,label='1')\n",
    "    plt.subplot(G[2, :]).plot(two[1]  ,c='orange',alpha=0.9,label='2')\n",
    "    plt.subplot(G[2, :]).plot(three[1],c='yellow',alpha=0.9,label='3')\n",
    "    plt.subplot(G[2, :]).plot(four[1],c='green',alpha=0.9,label='4')\n",
    "    plt.subplot(G[2, :]).plot(five[1],c='blue',alpha=0.9,label='5')\n",
    "    plt.subplot(G[2, :]).plot(six[1],c='black',alpha=0.9,label='6')\n",
    "    plt.legend(bbox_to_anchor=(0., 0.95, 1., .05), loc=9,ncol=6, mode=\"expand\", borderaxespad=0.)\n",
    "\n",
    "    plt.subplot(G[3, :]).set_title(\"Skewness Per \"+ experiment_name)\n",
    "    plt.subplot(G[3, :]).plot(one[2]  ,c='red',alpha=0.9,label='1')\n",
    "    plt.subplot(G[3, :]).plot(two[2]  ,c='orange',alpha=0.9,label='2')\n",
    "    plt.subplot(G[3, :]).plot(three[2],c='yellow',alpha=0.9,label='3')\n",
    "    plt.subplot(G[3, :]).plot(four[2],c='green',alpha=0.9,label='4')\n",
    "    plt.subplot(G[3, :]).plot(five[2],c='blue',alpha=0.9,label='5')\n",
    "    plt.subplot(G[3, :]).plot(six[2],c='black',alpha=0.9,label='6')\n",
    "    plt.legend(bbox_to_anchor=(0., 0.95, 1., .05), loc=9,ncol=6, mode=\"expand\", borderaxespad=0.)\n",
    "\n",
    "    plt.subplot(G[4, :]).set_title(\"Kurtosis Per \"+ experiment_name)\n",
    "    plt.subplot(G[4, :]).plot(one[3]  ,c='red',alpha=0.9,label='1')\n",
    "    plt.subplot(G[4, :]).plot(two[3]  ,c='orange',alpha=0.9,label='2')\n",
    "    plt.subplot(G[4, :]).plot(three[3],c='yellow',alpha=0.9,label='3')\n",
    "    plt.subplot(G[4, :]).plot(four[3],c='green',alpha=0.9,label='4')\n",
    "    plt.subplot(G[4, :]).plot(five[3],c='blue',alpha=0.9,label='5')\n",
    "    plt.subplot(G[4, :]).plot(six[3],c='black',alpha=0.9,label='6')\n",
    "    plt.legend(bbox_to_anchor=(0., 0.95, 1., .05), loc=9,ncol=6, mode=\"expand\", borderaxespad=0.)\n",
    "\n",
    "    plt.subplot(G[5, :]).set_title(\"# Non-Zero Per \"+ experiment_name)\n",
    "    plt.subplot(G[5, :]).plot(one[4]  ,c='red',alpha=0.9   ,label='1')\n",
    "    plt.subplot(G[5, :]).plot(two[4]  ,c='orange',alpha=0.9,label='2')\n",
    "    plt.subplot(G[5, :]).plot(three[4],c='yellow',alpha=0.9,label='3')\n",
    "    plt.subplot(G[5, :]).plot(four[4],c='green',alpha=0.9  ,label='4')\n",
    "    plt.subplot(G[5, :]).plot(five[4],c='blue',alpha=0.9   ,label='5')\n",
    "    plt.subplot(G[5, :]).plot(six[4],c='black',alpha=0.9   ,label='6')\n",
    "    plt.legend(bbox_to_anchor=(0., 0.95, 1., .05), loc=9,ncol=6, mode=\"expand\", borderaxespad=0.)\n",
    "\n",
    "    plt.subplot(G[6, :]).set_title(\"Train/Test accuracy\")\n",
    "    plt.subplot(G[6, :]).plot(train_acc  ,c='red',alpha=0.9, label='Train')\n",
    "    plt.subplot(G[6, :]).plot(test_acc   ,c='blue',alpha=0.9,label='Test')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.figtext(0.5,0,\"Correlation Matrix Per \"+ experiment_name,ha=\"center\", va=\"bottom\", fontsize=30, color=\"black\")\n",
    "    plt.subplot(G[7, 0]).imshow(np.corrcoef(transform_to_2d(main_data[0])),cmap='gray')\n",
    "    plt.subplot(G[7, 1]).imshow(np.corrcoef(transform_to_2d(main_data[1])),cmap='gray')\n",
    "    plt.subplot(G[7, 2]).imshow(np.corrcoef(transform_to_2d(main_data[2])),cmap='gray')\n",
    "    plt.subplot(G[7, 3]).imshow(np.corrcoef(transform_to_2d(main_data[3])),cmap='gray')\n",
    "    plt.subplot(G[7, 4]).imshow(np.corrcoef(transform_to_2d(main_data[4])),cmap='gray')\n",
    "    plt.subplot(G[7, 5]).imshow(np.corrcoef(transform_to_2d(main_data[5])),cmap='gray')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(current_exp + '/' + experiment_name + '/' + str(iter) + '.png')\n",
    "    plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-05T00:53:31.417355Z",
     "start_time": "2019-01-05T00:53:31.408378Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# set hyper parameter\n",
    "plt.rcParams.update({'font.size': 25})\n",
    "num_epoch = 3; learning_rate = 0.0008; batch_size = 20; beta1,beta2,adam_e = 0.9,0.999,1e-9; lamda = 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-05T01:05:02.755876Z",
     "start_time": "2019-01-05T00:53:31.421362Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current : 0 Train Acc : 0.12300000242888927 Test Acc : 0.18025000323541462\n",
      "\n",
      "Current : 1 Train Acc : 0.18160000319778918 Test Acc : 0.21162500325590372\n",
      "\n",
      "Current : 2 Train Acc : 0.23700000368058682 Test Acc : 0.28800000277347865\n",
      "\n",
      "Current : 3 Train Acc : 0.27660000309348104 Test Acc : 0.31500000204890966\n",
      "\n",
      "Current : 4 Train Acc : 0.30240000265836714 Test Acc : 0.3313750016409904\n",
      "\n",
      "Current : 5 Train Acc : 0.32480000162124634 Test Acc : 0.3413750018645078\n",
      "\n",
      "Current : 6 Train Acc : 0.337600002348423 Test Acc : 0.34637500187382103\n",
      "\n",
      "Current : 7 Train Acc : 0.34800000208616255 Test Acc : 0.35287500170059505\n",
      "\n",
      "Current : 8 Train Acc : 0.35420000123977663 Test Acc : 0.3563750016875565\n",
      "\n",
      "Current : 9 Train Acc : 0.35940000009536743 Test Acc : 0.36000000158324835\n",
      "\n",
      "Current : 10 Train Acc : 0.3636000009179115 Test Acc : 0.36537500182166693\n",
      "\n",
      "Current : 11 Train Acc : 0.37080000177025796 Test Acc : 0.371125001385808\n",
      "\n",
      "Current : 12 Train Acc : 0.3756000007390976 Test Acc : 0.37537500128149986\n",
      "\n",
      "Current : 13 Train Acc : 0.3784000009894371 Test Acc : 0.3805000012367964\n",
      "\n",
      "Current : 14 Train Acc : 0.3834000009000301 Test Acc : 0.38550000175833704\n",
      "\n",
      "Current : 15 Train Acc : 0.38720000088214873 Test Acc : 0.38862500147894025\n",
      "\n",
      "Current : 16 Train Acc : 0.39160000151395796 Test Acc : 0.3900000013783574\n",
      "\n",
      "Current : 17 Train Acc : 0.39600000143051145 Test Acc : 0.3917500015534461\n",
      "\n",
      "Current : 18 Train Acc : 0.3974000012278557 Test Acc : 0.39275000171735885\n",
      "\n",
      "Current : 19 Train Acc : 0.400000001013279 Test Acc : 0.39437500150874255\n",
      "\n",
      "Current : 20 Train Acc : 0.40160000014305114 Test Acc : 0.39825000148266554\n",
      "\n",
      "Current : 21 Train Acc : 0.4030000006556511 Test Acc : 0.400500001385808\n",
      "\n",
      "Current : 22 Train Acc : 0.40760000139474867 Test Acc : 0.40162500195205214\n",
      "\n",
      "Current : 23 Train Acc : 0.40840000081062316 Test Acc : 0.4041250015050173\n",
      "\n",
      "Current : 24 Train Acc : 0.41300000095367434 Test Acc : 0.40625000160187485\n",
      "\n",
      "Current : 25 Train Acc : 0.41600000160932543 Test Acc : 0.4090000019595027\n",
      "\n",
      "Current : 26 Train Acc : 0.420200001180172 Test Acc : 0.41025000154972074\n",
      "\n",
      "Current : 27 Train Acc : 0.4224000014066696 Test Acc : 0.4106250013411045\n",
      "\n",
      "Current : 28 Train Acc : 0.4248000017404556 Test Acc : 0.4110000013560057\n",
      "\n",
      "Current : 29 Train Acc : 0.42920000159740446 Test Acc : 0.4127500017359853\n",
      "\n",
      "Current : 30 Train Acc : 0.4312000014781952 Test Acc : 0.4155000014603138\n",
      "\n",
      "Current : 31 Train Acc : 0.4322000013589859 Test Acc : 0.41625000108033416\n",
      "\n",
      "Current : 32 Train Acc : 0.433800001680851 Test Acc : 0.4187500013411045\n",
      "\n",
      "Current : 33 Train Acc : 0.4382000016570091 Test Acc : 0.42050000112503766\n",
      "\n",
      "Current : 34 Train Acc : 0.4404000011086464 Test Acc : 0.422625001296401\n",
      "\n",
      "Current : 35 Train Acc : 0.4416000010967255 Test Acc : 0.42362500082701443\n",
      "\n",
      "Current : 36 Train Acc : 0.4418000011444092 Test Acc : 0.42600000094622376\n",
      "\n",
      "Current : 37 Train Acc : 0.44340000158548354 Test Acc : 0.4270000008866191\n",
      "\n",
      "Current : 38 Train Acc : 0.445400001168251 Test Acc : 0.4291250006854534\n",
      "\n",
      "Current : 39 Train Acc : 0.4454000012874603 Test Acc : 0.4276250005140901\n",
      "\n",
      "Current : 40 Train Acc : 0.4472000005841255 Test Acc : 0.4301250009611249\n",
      "\n",
      "Current Iter : 41/150 batch : 7980/8000 acc : 0.45\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-12c595adf0d7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    119\u001b[0m     \u001b[0msave_to_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmid_stat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;33m,\u001b[0m\u001b[0mllayer1a\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mllayer2a\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mllayer3a\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mllayer4a\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mllayer5a\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mllayer6a\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"layera\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_acc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcurrent_exp_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[0msave_to_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmid_stat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m18\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mweight1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mweight2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mweight3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mweight4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mweight5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mweight6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"weights\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_acc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcurrent_exp_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m     \u001b[0msave_to_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmid_stat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m18\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m24\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mgradw1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgradw2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgradw3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgradw4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgradw5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgradw6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"gradientw\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_acc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcurrent_exp_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m     \u001b[0msave_to_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmid_stat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m24\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mgradp1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgradp2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgradp3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgradp4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgradp5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgradp6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"gradientp\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_acc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcurrent_exp_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m     \u001b[0msave_to_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmid_stat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m36\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mgradup1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgradup2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgradup3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgradup4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgradup5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgradup6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"moment\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_acc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcurrent_exp_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-e74f113f20e0>\u001b[0m in \u001b[0;36msave_to_image\u001b[1;34m(main_data, one, two, three, four, five, six, experiment_name, tran_acc, test_acc, current_exp, iter)\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfive\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'blue'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.9\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'black'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.9\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'6'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbbox_to_anchor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.95\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m.05\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mncol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"expand\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mborderaxespad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Standard Deviation Per \"\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mexperiment_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mlegend\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   2707\u001b[0m \u001b[1;33m@\u001b[0m\u001b[0mdocstring\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy_dedent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2708\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mlegend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2709\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgca\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2710\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2711\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mlegend\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    413\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextra_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'legend only accepts two non-keyword arguments'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 415\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegend_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmlegend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLegend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandles\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    416\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegend_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_remove_method\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_remove_legend\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    417\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegend_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\matplotlib\\legend.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, parent, handles, labels, loc, numpoints, markerscale, markerfirst, scatterpoints, scatteryoffsets, prop, fontsize, borderpad, labelspacing, handlelength, handleheight, handletextpad, borderaxespad, columnspacing, ncol, mode, fancybox, shadow, title, title_fontsize, framealpha, edgecolor, facecolor, bbox_to_anchor, bbox_transform, frameon, handler_map)\u001b[0m\n\u001b[0;32m    554\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m         \u001b[1;31m# init with null renderer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 556\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_legend_box\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmarkerfirst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    557\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    558\u001b[0m         \u001b[1;31m# If shadow is activated use framealpha if not\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\matplotlib\\legend.py\u001b[0m in \u001b[0;36m_init_legend_box\u001b[1;34m(self, handles, labels, markerfirst)\u001b[0m\n\u001b[0;32m    804\u001b[0m                 textbox = TextArea(lab, textprops=label_prop,\n\u001b[0;32m    805\u001b[0m                                    \u001b[0mmultilinebaseline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 806\u001b[1;33m                                    minimumdescent=True)\n\u001b[0m\u001b[0;32m    807\u001b[0m                 handlebox = DrawingArea(width=self.handlelength * fontsize,\n\u001b[0;32m    808\u001b[0m                                         \u001b[0mheight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\matplotlib\\offsetbox.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, s, textprops, multilinebaseline, minimumdescent)\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moffset_transform\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmtransforms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAffine2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moffset_transform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 728\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moffset_transform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    729\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_baseline_transform\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmtransforms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAffine2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m         self._text.set_transform(self.offset_transform +\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\matplotlib\\transforms.py\u001b[0m in \u001b[0;36mtranslate\u001b[1;34m(self, tx, ty)\u001b[0m\n\u001b[0;32m   2021\u001b[0m         translate_mtx = np.array(\n\u001b[0;32m   2022\u001b[0m             [[1.0, 0.0, tx], [0.0, 1.0, ty], [0.0, 0.0, 1.0]], float)\n\u001b[1;32m-> 2023\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mtx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtranslate_mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mtx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2024\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minvalidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2025\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x2880 with 7 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Z\n",
    "current_exp_name = 'Z';\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# create layers\n",
    "l1 = CNN(3,3, 16,which_reg=current_exp_name); \n",
    "l2 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "l3 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "\n",
    "l4 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "l5 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "l6 = CNN(3,16,10,which_reg=current_exp_name); \n",
    "\n",
    "# 2. graph \n",
    "x = tf.placeholder(tf.float32,(batch_size,96,96,3))\n",
    "y = tf.placeholder(tf.float32,(batch_size,10))\n",
    "\n",
    "layer1, layer1a = l1. feedforward(x,stride=2)\n",
    "layer2, layer2a = l2. feedforward(layer1a,stride=2)\n",
    "layer3, layer3a = l3. feedforward(layer2a,stride=2)\n",
    "layer4, layer4a = l4. feedforward(layer3a,stride=2)\n",
    "layer5, layer5a = l5. feedforward(layer4a)\n",
    "layer6, layer6a = l6. feedforward(layer5a)\n",
    "\n",
    "final_layer   = tf.reduce_mean(layer6a,(1,2))\n",
    "final_softmax = tf_softmax(final_layer)\n",
    "cost          = -tf.reduce_mean(y * tf.log(final_softmax + 1e-8))\n",
    "correct_prediction = tf.equal(tf.argmax(final_softmax, 1), tf.argmax(y, 1))\n",
    "accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "gradient = tf.tile((final_softmax-y)[:,None,None,:],[1,6,6,1])/batch_size\n",
    "grad6p,grad6w,grad6_up = l6.backprop(gradient)\n",
    "grad5p,grad5w,grad5_up = l5.backprop(grad6p)\n",
    "grad4p,grad4w,grad4_up = l4.backprop(grad5p,stride=2)\n",
    "grad3p,grad3w,grad3_up = l3.backprop(grad4p,stride=2)\n",
    "grad2p,grad2w,grad2_up = l2.backprop(grad3p,stride=2)\n",
    "grad1p,grad1w,grad1_up = l1.backprop(grad2p,stride=2)\n",
    "\n",
    "gradient_update = grad6_up + grad5_up + grad4_up + grad3_up + grad2_up + grad1_up \n",
    "\n",
    "# train\n",
    "sess.run(tf.global_variables_initializer())\n",
    "avg_acc_train = 0; avg_acc_test  = 0; train_acc = [];test_acc = []\n",
    "\n",
    "# mean std skew kurt non-zero\n",
    "llayer1 = [[],[],[],[],[]]; llayer2 = [[],[],[],[],[]]; llayer3 = [[],[],[],[],[]]\n",
    "llayer4 = [[],[],[],[],[]]; llayer5 = [[],[],[],[],[]]; llayer6 = [[],[],[],[],[]]\n",
    "\n",
    "llayer1a = [[],[],[],[],[]]; llayer2a = [[],[],[],[],[]]; llayer3a = [[],[],[],[],[]]\n",
    "llayer4a = [[],[],[],[],[]]; llayer5a = [[],[],[],[],[]]; llayer6a = [[],[],[],[],[]]\n",
    "\n",
    "weight1 = [[],[],[],[],[]]; weight2 = [[],[],[],[],[]]; weight3 = [[],[],[],[],[]];\n",
    "weight4 = [[],[],[],[],[]]; weight5 = [[],[],[],[],[]]; weight6 = [[],[],[],[],[]];\n",
    "\n",
    "gradw1  = [[],[],[],[],[]]; gradw2  = [[],[],[],[],[]]; gradw3  = [[],[],[],[],[]];\n",
    "gradw4  = [[],[],[],[],[]]; gradw5  = [[],[],[],[],[]]; gradw6  = [[],[],[],[],[]];\n",
    "\n",
    "gradp1  = [[],[],[],[],[]]; gradp2  = [[],[],[],[],[]]; gradp3  = [[],[],[],[],[]];\n",
    "gradp4  = [[],[],[],[],[]]; gradp5  = [[],[],[],[],[]]; gradp6  = [[],[],[],[],[]];\n",
    "\n",
    "gradup1  = [[],[],[],[],[]]; gradup2  = [[],[],[],[],[]]; gradup3  = [[],[],[],[],[]];\n",
    "gradup4  = [[],[],[],[],[]]; gradup5  = [[],[],[],[],[]]; gradup6  = [[],[],[],[],[]];\n",
    "\n",
    "list_of_outputs = [\n",
    "    layer1,layer2,layer3,layer4,layer5,layer6,\n",
    "    layer1a,layer2a,layer3a,layer4a,layer5a,layer6a,\n",
    "    l1.getw(),l2.getw(),l3.getw(),l4.getw(),l5.getw(),l6.getw(),\n",
    "    grad1w,grad2w,grad3w,grad4w,grad5w,grad6w,\n",
    "    grad1p,grad2p,grad3p,grad4p,grad5p,grad6p,\n",
    "    grad1_up[0],grad2_up[0],grad3_up[0],grad4_up[0],grad5_up[0],grad6_up[0]\n",
    "]\n",
    "\n",
    "for iter in range(num_epoch):\n",
    "\n",
    "    # Training Accuracy    \n",
    "    for current_batch_index in range(0,len(train_images),batch_size):\n",
    "        current_data  = train_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = train_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy,gradient_update],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(train_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_train = avg_acc_train + sess_results[0]\n",
    "        \n",
    "    # get the results\n",
    "    mid_stat = sess.run(list_of_outputs,feed_dict={x:current_data,y:current_label})\n",
    "    \n",
    "    # Test Accuracy    \n",
    "    for current_batch_index in range(0,len(test_images), batch_size):\n",
    "        current_data  = test_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = test_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(test_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_test = avg_acc_test + sess_results[0]   \n",
    "        \n",
    "    # ======================== extract stats ========================\n",
    "    llayer1 = append_stat(llayer1,mid_stat,0);  llayer2 = append_stat(llayer2,mid_stat,1);  llayer3 = append_stat(llayer3,mid_stat,2);\n",
    "    llayer4 = append_stat(llayer4,mid_stat,3);  llayer5 = append_stat(llayer5,mid_stat,4);  llayer6 = append_stat(llayer6,mid_stat,5);\n",
    "\n",
    "    llayer1a = append_stat(llayer1a,mid_stat,6);  llayer2a = append_stat(llayer2a,mid_stat,7);  llayer3a = append_stat(llayer3a,mid_stat,8);\n",
    "    llayer4a = append_stat(llayer4a,mid_stat,9);  llayer5a = append_stat(llayer5a,mid_stat,10); llayer6a = append_stat(llayer6a,mid_stat,11);\n",
    "    \n",
    "    weight1 = append_stat(weight1,mid_stat,12);  weight2 = append_stat(weight2,mid_stat,13);  weight3 = append_stat(weight3,mid_stat,14);\n",
    "    weight4 = append_stat(weight4,mid_stat,15);  weight5 = append_stat(weight5,mid_stat,16);  weight6 = append_stat(weight6,mid_stat,17);\n",
    "    \n",
    "    gradw1 = append_stat(gradw1,mid_stat,18); gradw2 = append_stat(gradw2,mid_stat,19); gradw3 = append_stat(gradw3,mid_stat,20);\n",
    "    gradw4 = append_stat(gradw4,mid_stat,21); gradw5 = append_stat(gradw5,mid_stat,22); gradw6 = append_stat(gradw6,mid_stat,23);\n",
    "    \n",
    "    gradp1 = append_stat(gradp1,mid_stat,24); gradp2 = append_stat(gradp2,mid_stat,25); gradp3 = append_stat(gradp3,mid_stat,26);\n",
    "    gradp4 = append_stat(gradp4,mid_stat,27); gradp5 = append_stat(gradp5,mid_stat,28); gradp6 = append_stat(gradp6,mid_stat,29);\n",
    "\n",
    "    gradup1 = append_stat(gradup1,mid_stat,30); gradup2 = append_stat(gradup2,mid_stat,31); gradup3 = append_stat(gradup3,mid_stat,32);\n",
    "    gradup4 = append_stat(gradup4,mid_stat,33); gradup5 = append_stat(gradup5,mid_stat,34); gradup6 = append_stat(gradup6,mid_stat,35);\n",
    "\n",
    "    train_acc.append(avg_acc_train/(len(train_images)/batch_size))\n",
    "    test_acc .append(avg_acc_test / (len(test_images)/batch_size))\n",
    "    # ======================== extract stats ========================\n",
    "    \n",
    "    # ======================== save to image ========================\n",
    "    save_to_image(mid_stat[0:6]   ,llayer1,llayer2,llayer3,llayer4,llayer5,llayer6,\"layer\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[6:12]  ,llayer1a,llayer2a,llayer3a,llayer4a,llayer5a,llayer6a,\"layera\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[12:18] ,weight1,weight2,weight3,weight4,weight5,weight6,\"weights\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[18:24] ,gradw1,gradw2,gradw3,gradw4,gradw5,gradw6,\"gradientw\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[24:30] ,gradp1,gradp2,gradp3,gradp4,gradp5,gradp6,\"gradientp\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[30:36] ,gradup1,gradup2,gradup3,gradup4,gradup5,gradup6,\"moment\",train_acc,test_acc,current_exp_name,iter)\n",
    "    # ======================== save to image ========================\n",
    "        \n",
    "    # ======================== print reset ========================\n",
    "    print(\"Current : \"+ str(iter) + \" Train Acc : \" + str(avg_acc_train/(len(train_images)/batch_size)) + \" Test Acc : \" + str(avg_acc_test/(len(test_images)/batch_size)) + '\\n')\n",
    "    avg_acc_train = 0 ; avg_acc_test  = 0\n",
    "    # ======================== print reset ========================\n",
    "\n",
    "np.save(current_exp_name+'/train_acc.npy',train_acc); np.save(current_exp_name+'/test_acc.npy', test_acc)    \n",
    "np.save(current_exp_name+'/llayer1.npy', llayer1);  np.save(current_exp_name+'/llayer2.npy', llayer2);  np.save(current_exp_name+'/llayer3.npy', llayer3); \n",
    "np.save(current_exp_name+'/llayer4.npy', llayer4);  np.save(current_exp_name+'/llayer5.npy', llayer5);  np.save(current_exp_name+'/llayer6.npy', llayer6); \n",
    "\n",
    "np.save(current_exp_name+'/llayer1a.npy', llayer1a);  np.save(current_exp_name+'/llayer2a.npy', llayer2a);  np.save(current_exp_name+'/llayer3a.npy', llayer3a); \n",
    "np.save(current_exp_name+'/llayer4a.npy', llayer4a);  np.save(current_exp_name+'/llayer5a.npy', llayer5a);  np.save(current_exp_name+'/llayer6a.npy', llayer6a); \n",
    "\n",
    "np.save(current_exp_name+'/weight1.npy', weight1);  np.save(current_exp_name+'/weight2.npy', weight2);  np.save(current_exp_name+'/weight3.npy', weight3);  \n",
    "np.save(current_exp_name+'/weight4.npy', weight4);  np.save(current_exp_name+'/weight5.npy', weight5);  np.save(current_exp_name+'/weight6.npy', weight6);  \n",
    "\n",
    "np.save(current_exp_name+'/gradw1.npy', gradw1); np.save(current_exp_name+'/gradw2.npy', gradw2); np.save(current_exp_name+'/gradw3.npy', gradw3);\n",
    "np.save(current_exp_name+'/gradw4.npy', gradw4); np.save(current_exp_name+'/gradw5.npy', gradw5); np.save(current_exp_name+'/gradw6.npy', gradw6);\n",
    "\n",
    "np.save(current_exp_name+'/gradp1.npy', gradp1); np.save(current_exp_name+'/gradp2.npy', gradp2); np.save(current_exp_name+'/gradp3.npy', gradp3);\n",
    "np.save(current_exp_name+'/gradp4.npy', gradp4); np.save(current_exp_name+'/gradp5.npy', gradp5); np.save(current_exp_name+'/gradp6.npy', gradp6);\n",
    "\n",
    "np.save(current_exp_name+'/gradup1.npy', gradup1); np.save(current_exp_name+'/gradup2.npy', gradup2); np.save(current_exp_name+'/gradup3.npy', gradup3);\n",
    "np.save(current_exp_name+'/gradup4.npy', gradup4); np.save(current_exp_name+'/gradup5.npy', gradup5); np.save(current_exp_name+'/gradup6.npy', gradup6);\n",
    "\n",
    "sess.close(); tf.reset_default_graph();\n",
    "\n",
    "%reset_selective -f l1,l2,l3,l4,l5,l6\n",
    "%reset_selective -f layer1,layer2,layer3,layer4,layer5,layer6\n",
    "%reset_selective -f layer1a,layer2a,layer3a,layer4a,layer5a,layer6a\n",
    "%reset_selective -f train_acc,test_acc,llayer1,llayer2,llayer3,llayer4,llayer5,llayer6,llayer1a,llayer2a,llayer3a,llayer4a,llayer5a,llayer6a\n",
    "%reset_selective -f weight1,weight2,weight3,weight4,weight5,weight6\n",
    "%reset_selective -f gradw1,gradw2,gradw3,gradw4,gradw5,gradw6\n",
    "%reset_selective -f gradp1,gradp2,gradp3,gradp4,gradp5,gradp6\n",
    "%reset_selective -f gradup1,gradup2,gradup3,gradup4,gradup5,gradup6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-05T01:05:02.800754Z",
     "start_time": "2019-01-05T00:53:26.613Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# A\n",
    "current_exp_name = 'A';\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# create layers\n",
    "l1 = CNN(3,3, 16,which_reg=current_exp_name); \n",
    "l2 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "l3 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "\n",
    "l4 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "l5 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "l6 = CNN(3,16,10,which_reg=current_exp_name); \n",
    "\n",
    "# 2. graph \n",
    "x = tf.placeholder(tf.float32,(batch_size,96,96,3))\n",
    "y = tf.placeholder(tf.float32,(batch_size,10))\n",
    "\n",
    "layer1, layer1a = l1. feedforward(x,stride=2)\n",
    "layer2, layer2a = l2. feedforward(layer1a,stride=2)\n",
    "layer3, layer3a = l3. feedforward(layer2a,stride=2)\n",
    "layer4, layer4a = l4. feedforward(layer3a,stride=2)\n",
    "layer5, layer5a = l5. feedforward(layer4a)\n",
    "layer6, layer6a = l6. feedforward(layer5a)\n",
    "\n",
    "final_layer   = tf.reduce_mean(layer6a,(1,2))\n",
    "final_softmax = tf_softmax(final_layer)\n",
    "cost          = -tf.reduce_mean(y * tf.log(final_softmax + 1e-8))\n",
    "correct_prediction = tf.equal(tf.argmax(final_softmax, 1), tf.argmax(y, 1))\n",
    "accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "gradient = tf.tile((final_softmax-y)[:,None,None,:],[1,6,6,1])/batch_size\n",
    "grad6p,grad6w,grad6_up = l6.backprop(gradient)\n",
    "grad5p,grad5w,grad5_up = l5.backprop(grad6p)\n",
    "grad4p,grad4w,grad4_up = l4.backprop(grad5p,stride=2)\n",
    "grad3p,grad3w,grad3_up = l3.backprop(grad4p,stride=2)\n",
    "grad2p,grad2w,grad2_up = l2.backprop(grad3p,stride=2)\n",
    "grad1p,grad1w,grad1_up = l1.backprop(grad2p,stride=2)\n",
    "\n",
    "gradient_update = grad6_up + grad5_up + grad4_up + grad3_up + grad2_up + grad1_up \n",
    "\n",
    "# train\n",
    "sess.run(tf.global_variables_initializer())\n",
    "avg_acc_train = 0; avg_acc_test  = 0; train_acc = [];test_acc = []\n",
    "\n",
    "# mean std skew kurt non-zero\n",
    "llayer1 = [[],[],[],[],[]]; llayer2 = [[],[],[],[],[]]; llayer3 = [[],[],[],[],[]]\n",
    "llayer4 = [[],[],[],[],[]]; llayer5 = [[],[],[],[],[]]; llayer6 = [[],[],[],[],[]]\n",
    "\n",
    "llayer1a = [[],[],[],[],[]]; llayer2a = [[],[],[],[],[]]; llayer3a = [[],[],[],[],[]]\n",
    "llayer4a = [[],[],[],[],[]]; llayer5a = [[],[],[],[],[]]; llayer6a = [[],[],[],[],[]]\n",
    "\n",
    "weight1 = [[],[],[],[],[]]; weight2 = [[],[],[],[],[]]; weight3 = [[],[],[],[],[]];\n",
    "weight4 = [[],[],[],[],[]]; weight5 = [[],[],[],[],[]]; weight6 = [[],[],[],[],[]];\n",
    "\n",
    "gradw1  = [[],[],[],[],[]]; gradw2  = [[],[],[],[],[]]; gradw3  = [[],[],[],[],[]];\n",
    "gradw4  = [[],[],[],[],[]]; gradw5  = [[],[],[],[],[]]; gradw6  = [[],[],[],[],[]];\n",
    "\n",
    "gradp1  = [[],[],[],[],[]]; gradp2  = [[],[],[],[],[]]; gradp3  = [[],[],[],[],[]];\n",
    "gradp4  = [[],[],[],[],[]]; gradp5  = [[],[],[],[],[]]; gradp6  = [[],[],[],[],[]];\n",
    "\n",
    "gradup1  = [[],[],[],[],[]]; gradup2  = [[],[],[],[],[]]; gradup3  = [[],[],[],[],[]];\n",
    "gradup4  = [[],[],[],[],[]]; gradup5  = [[],[],[],[],[]]; gradup6  = [[],[],[],[],[]];\n",
    "\n",
    "list_of_outputs = [\n",
    "    layer1,layer2,layer3,layer4,layer5,layer6,\n",
    "    layer1a,layer2a,layer3a,layer4a,layer5a,layer6a,\n",
    "    l1.getw(),l2.getw(),l3.getw(),l4.getw(),l5.getw(),l6.getw(),\n",
    "    grad1w,grad2w,grad3w,grad4w,grad5w,grad6w,\n",
    "    grad1p,grad2p,grad3p,grad4p,grad5p,grad6p,\n",
    "    grad1_up[0],grad2_up[0],grad3_up[0],grad4_up[0],grad5_up[0],grad6_up[0]\n",
    "]\n",
    "\n",
    "for iter in range(num_epoch):\n",
    "\n",
    "    # Training Accuracy    \n",
    "    for current_batch_index in range(0,len(train_images),batch_size):\n",
    "        current_data  = train_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = train_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy,gradient_update],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(train_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_train = avg_acc_train + sess_results[0]\n",
    "        \n",
    "    # get the results\n",
    "    mid_stat = sess.run(list_of_outputs,feed_dict={x:current_data,y:current_label})\n",
    "    \n",
    "    # Test Accuracy    \n",
    "    for current_batch_index in range(0,len(test_images), batch_size):\n",
    "        current_data  = test_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = test_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(test_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_test = avg_acc_test + sess_results[0]   \n",
    "        \n",
    "    # ======================== extract stats ========================\n",
    "    llayer1 = append_stat(llayer1,mid_stat,0);  llayer2 = append_stat(llayer2,mid_stat,1);  llayer3 = append_stat(llayer3,mid_stat,2);\n",
    "    llayer4 = append_stat(llayer4,mid_stat,3);  llayer5 = append_stat(llayer5,mid_stat,4);  llayer6 = append_stat(llayer6,mid_stat,5);\n",
    "\n",
    "    llayer1a = append_stat(llayer1a,mid_stat,6);  llayer2a = append_stat(llayer2a,mid_stat,7);  llayer3a = append_stat(llayer3a,mid_stat,8);\n",
    "    llayer4a = append_stat(llayer4a,mid_stat,9);  llayer5a = append_stat(llayer5a,mid_stat,10); llayer6a = append_stat(llayer6a,mid_stat,11);\n",
    "    \n",
    "    weight1 = append_stat(weight1,mid_stat,12);  weight2 = append_stat(weight2,mid_stat,13);  weight3 = append_stat(weight3,mid_stat,14);\n",
    "    weight4 = append_stat(weight4,mid_stat,15);  weight5 = append_stat(weight5,mid_stat,16);  weight6 = append_stat(weight6,mid_stat,17);\n",
    "    \n",
    "    gradw1 = append_stat(gradw1,mid_stat,18); gradw2 = append_stat(gradw2,mid_stat,19); gradw3 = append_stat(gradw3,mid_stat,20);\n",
    "    gradw4 = append_stat(gradw4,mid_stat,21); gradw5 = append_stat(gradw5,mid_stat,22); gradw6 = append_stat(gradw6,mid_stat,23);\n",
    "    \n",
    "    gradp1 = append_stat(gradp1,mid_stat,24); gradp2 = append_stat(gradp2,mid_stat,25); gradp3 = append_stat(gradp3,mid_stat,26);\n",
    "    gradp4 = append_stat(gradp4,mid_stat,27); gradp5 = append_stat(gradp5,mid_stat,28); gradp6 = append_stat(gradp6,mid_stat,29);\n",
    "\n",
    "    gradup1 = append_stat(gradup1,mid_stat,30); gradup2 = append_stat(gradup2,mid_stat,31); gradup3 = append_stat(gradup3,mid_stat,32);\n",
    "    gradup4 = append_stat(gradup4,mid_stat,33); gradup5 = append_stat(gradup5,mid_stat,34); gradup6 = append_stat(gradup6,mid_stat,35);\n",
    "\n",
    "    train_acc.append(avg_acc_train/(len(train_images)/batch_size))\n",
    "    test_acc .append(avg_acc_test / (len(test_images)/batch_size))\n",
    "    # ======================== extract stats ========================\n",
    "    \n",
    "    # ======================== save to image ========================\n",
    "    save_to_image(mid_stat[0:6]   ,llayer1,llayer2,llayer3,llayer4,llayer5,llayer6,\"layer\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[6:12]  ,llayer1a,llayer2a,llayer3a,llayer4a,llayer5a,llayer6a,\"layera\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[12:18] ,weight1,weight2,weight3,weight4,weight5,weight6,\"weights\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[18:24] ,gradw1,gradw2,gradw3,gradw4,gradw5,gradw6,\"gradientw\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[24:30] ,gradp1,gradp2,gradp3,gradp4,gradp5,gradp6,\"gradientp\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[30:36] ,gradup1,gradup2,gradup3,gradup4,gradup5,gradup6,\"moment\",train_acc,test_acc,current_exp_name,iter)\n",
    "    # ======================== save to image ========================\n",
    "        \n",
    "    # ======================== print reset ========================\n",
    "    print(\"Current : \"+ str(iter) + \" Train Acc : \" + str(avg_acc_train/(len(train_images)/batch_size)) + \" Test Acc : \" + str(avg_acc_test/(len(test_images)/batch_size)) + '\\n')\n",
    "    avg_acc_train = 0 ; avg_acc_test  = 0\n",
    "    # ======================== print reset ========================\n",
    "\n",
    "np.save(current_exp_name+'/train_acc.npy',train_acc); np.save(current_exp_name+'/test_acc.npy', test_acc)    \n",
    "np.save(current_exp_name+'/llayer1.npy', llayer1);  np.save(current_exp_name+'/llayer2.npy', llayer2);  np.save(current_exp_name+'/llayer3.npy', llayer3); \n",
    "np.save(current_exp_name+'/llayer4.npy', llayer4);  np.save(current_exp_name+'/llayer5.npy', llayer5);  np.save(current_exp_name+'/llayer6.npy', llayer6); \n",
    "\n",
    "np.save(current_exp_name+'/llayer1a.npy', llayer1a);  np.save(current_exp_name+'/llayer2a.npy', llayer2a);  np.save(current_exp_name+'/llayer3a.npy', llayer3a); \n",
    "np.save(current_exp_name+'/llayer4a.npy', llayer4a);  np.save(current_exp_name+'/llayer5a.npy', llayer5a);  np.save(current_exp_name+'/llayer6a.npy', llayer6a); \n",
    "\n",
    "np.save(current_exp_name+'/weight1.npy', weight1);  np.save(current_exp_name+'/weight2.npy', weight2);  np.save(current_exp_name+'/weight3.npy', weight3);  \n",
    "np.save(current_exp_name+'/weight4.npy', weight4);  np.save(current_exp_name+'/weight5.npy', weight5);  np.save(current_exp_name+'/weight6.npy', weight6);  \n",
    "\n",
    "np.save(current_exp_name+'/gradw1.npy', gradw1); np.save(current_exp_name+'/gradw2.npy', gradw2); np.save(current_exp_name+'/gradw3.npy', gradw3);\n",
    "np.save(current_exp_name+'/gradw4.npy', gradw4); np.save(current_exp_name+'/gradw5.npy', gradw5); np.save(current_exp_name+'/gradw6.npy', gradw6);\n",
    "\n",
    "np.save(current_exp_name+'/gradp1.npy', gradp1); np.save(current_exp_name+'/gradp2.npy', gradp2); np.save(current_exp_name+'/gradp3.npy', gradp3);\n",
    "np.save(current_exp_name+'/gradp4.npy', gradp4); np.save(current_exp_name+'/gradp5.npy', gradp5); np.save(current_exp_name+'/gradp6.npy', gradp6);\n",
    "\n",
    "np.save(current_exp_name+'/gradup1.npy', gradup1); np.save(current_exp_name+'/gradup2.npy', gradup2); np.save(current_exp_name+'/gradup3.npy', gradup3);\n",
    "np.save(current_exp_name+'/gradup4.npy', gradup4); np.save(current_exp_name+'/gradup5.npy', gradup5); np.save(current_exp_name+'/gradup6.npy', gradup6);\n",
    "\n",
    "sess.close(); tf.reset_default_graph();\n",
    "\n",
    "%reset_selective -f l1,l2,l3,l4,l5,l6\n",
    "%reset_selective -f layer1,layer2,layer3,layer4,layer5,layer6\n",
    "%reset_selective -f layer1a,layer2a,layer3a,layer4a,layer5a,layer6a\n",
    "%reset_selective -f train_acc,test_acc,llayer1,llayer2,llayer3,llayer4,llayer5,llayer6,llayer1a,llayer2a,llayer3a,llayer4a,llayer5a,llayer6a\n",
    "%reset_selective -f weight1,weight2,weight3,weight4,weight5,weight6\n",
    "%reset_selective -f gradw1,gradw2,gradw3,gradw4,gradw5,gradw6\n",
    "%reset_selective -f gradp1,gradp2,gradp3,gradp4,gradp5,gradp6\n",
    "%reset_selective -f gradup1,gradup2,gradup3,gradup4,gradup5,gradup6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-05T01:05:02.802748Z",
     "start_time": "2019-01-05T00:53:26.617Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# B\n",
    "current_exp_name = 'B';\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# create layers\n",
    "l1 = CNN(3,3, 16,which_reg=current_exp_name); \n",
    "l2 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "l3 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "\n",
    "l4 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "l5 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "l6 = CNN(3,16,10,which_reg=current_exp_name); \n",
    "\n",
    "# 2. graph \n",
    "x = tf.placeholder(tf.float32,(batch_size,96,96,3))\n",
    "y = tf.placeholder(tf.float32,(batch_size,10))\n",
    "\n",
    "layer1, layer1a = l1. feedforward(x,stride=2)\n",
    "layer2, layer2a = l2. feedforward(layer1a,stride=2)\n",
    "layer3, layer3a = l3. feedforward(layer2a,stride=2)\n",
    "layer4, layer4a = l4. feedforward(layer3a,stride=2)\n",
    "layer5, layer5a = l5. feedforward(layer4a)\n",
    "layer6, layer6a = l6. feedforward(layer5a)\n",
    "\n",
    "final_layer   = tf.reduce_mean(layer6a,(1,2))\n",
    "final_softmax = tf_softmax(final_layer)\n",
    "cost          = -tf.reduce_mean(y * tf.log(final_softmax + 1e-8))\n",
    "correct_prediction = tf.equal(tf.argmax(final_softmax, 1), tf.argmax(y, 1))\n",
    "accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "gradient = tf.tile((final_softmax-y)[:,None,None,:],[1,6,6,1])/batch_size\n",
    "grad6p,grad6w,grad6_up = l6.backprop(gradient)\n",
    "grad5p,grad5w,grad5_up = l5.backprop(grad6p)\n",
    "grad4p,grad4w,grad4_up = l4.backprop(grad5p,stride=2)\n",
    "grad3p,grad3w,grad3_up = l3.backprop(grad4p,stride=2)\n",
    "grad2p,grad2w,grad2_up = l2.backprop(grad3p,stride=2)\n",
    "grad1p,grad1w,grad1_up = l1.backprop(grad2p,stride=2)\n",
    "\n",
    "gradient_update = grad6_up + grad5_up + grad4_up + grad3_up + grad2_up + grad1_up \n",
    "\n",
    "# train\n",
    "sess.run(tf.global_variables_initializer())\n",
    "avg_acc_train = 0; avg_acc_test  = 0; train_acc = [];test_acc = []\n",
    "\n",
    "# mean std skew kurt non-zero\n",
    "llayer1 = [[],[],[],[],[]]; llayer2 = [[],[],[],[],[]]; llayer3 = [[],[],[],[],[]]\n",
    "llayer4 = [[],[],[],[],[]]; llayer5 = [[],[],[],[],[]]; llayer6 = [[],[],[],[],[]]\n",
    "\n",
    "llayer1a = [[],[],[],[],[]]; llayer2a = [[],[],[],[],[]]; llayer3a = [[],[],[],[],[]]\n",
    "llayer4a = [[],[],[],[],[]]; llayer5a = [[],[],[],[],[]]; llayer6a = [[],[],[],[],[]]\n",
    "\n",
    "weight1 = [[],[],[],[],[]]; weight2 = [[],[],[],[],[]]; weight3 = [[],[],[],[],[]];\n",
    "weight4 = [[],[],[],[],[]]; weight5 = [[],[],[],[],[]]; weight6 = [[],[],[],[],[]];\n",
    "\n",
    "gradw1  = [[],[],[],[],[]]; gradw2  = [[],[],[],[],[]]; gradw3  = [[],[],[],[],[]];\n",
    "gradw4  = [[],[],[],[],[]]; gradw5  = [[],[],[],[],[]]; gradw6  = [[],[],[],[],[]];\n",
    "\n",
    "gradp1  = [[],[],[],[],[]]; gradp2  = [[],[],[],[],[]]; gradp3  = [[],[],[],[],[]];\n",
    "gradp4  = [[],[],[],[],[]]; gradp5  = [[],[],[],[],[]]; gradp6  = [[],[],[],[],[]];\n",
    "\n",
    "gradup1  = [[],[],[],[],[]]; gradup2  = [[],[],[],[],[]]; gradup3  = [[],[],[],[],[]];\n",
    "gradup4  = [[],[],[],[],[]]; gradup5  = [[],[],[],[],[]]; gradup6  = [[],[],[],[],[]];\n",
    "\n",
    "list_of_outputs = [\n",
    "    layer1,layer2,layer3,layer4,layer5,layer6,\n",
    "    layer1a,layer2a,layer3a,layer4a,layer5a,layer6a,\n",
    "    l1.getw(),l2.getw(),l3.getw(),l4.getw(),l5.getw(),l6.getw(),\n",
    "    grad1w,grad2w,grad3w,grad4w,grad5w,grad6w,\n",
    "    grad1p,grad2p,grad3p,grad4p,grad5p,grad6p,\n",
    "    grad1_up[0],grad2_up[0],grad3_up[0],grad4_up[0],grad5_up[0],grad6_up[0]\n",
    "]\n",
    "\n",
    "for iter in range(num_epoch):\n",
    "\n",
    "    # Training Accuracy    \n",
    "    for current_batch_index in range(0,len(train_images),batch_size):\n",
    "        current_data  = train_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = train_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy,gradient_update],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(train_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_train = avg_acc_train + sess_results[0]\n",
    "        \n",
    "    # get the results\n",
    "    mid_stat = sess.run(list_of_outputs,feed_dict={x:current_data,y:current_label})\n",
    "    \n",
    "    # Test Accuracy    \n",
    "    for current_batch_index in range(0,len(test_images), batch_size):\n",
    "        current_data  = test_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = test_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(test_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_test = avg_acc_test + sess_results[0]   \n",
    "        \n",
    "    # ======================== extract stats ========================\n",
    "    llayer1 = append_stat(llayer1,mid_stat,0);  llayer2 = append_stat(llayer2,mid_stat,1);  llayer3 = append_stat(llayer3,mid_stat,2);\n",
    "    llayer4 = append_stat(llayer4,mid_stat,3);  llayer5 = append_stat(llayer5,mid_stat,4);  llayer6 = append_stat(llayer6,mid_stat,5);\n",
    "\n",
    "    llayer1a = append_stat(llayer1a,mid_stat,6);  llayer2a = append_stat(llayer2a,mid_stat,7);  llayer3a = append_stat(llayer3a,mid_stat,8);\n",
    "    llayer4a = append_stat(llayer4a,mid_stat,9);  llayer5a = append_stat(llayer5a,mid_stat,10); llayer6a = append_stat(llayer6a,mid_stat,11);\n",
    "    \n",
    "    weight1 = append_stat(weight1,mid_stat,12);  weight2 = append_stat(weight2,mid_stat,13);  weight3 = append_stat(weight3,mid_stat,14);\n",
    "    weight4 = append_stat(weight4,mid_stat,15);  weight5 = append_stat(weight5,mid_stat,16);  weight6 = append_stat(weight6,mid_stat,17);\n",
    "    \n",
    "    gradw1 = append_stat(gradw1,mid_stat,18); gradw2 = append_stat(gradw2,mid_stat,19); gradw3 = append_stat(gradw3,mid_stat,20);\n",
    "    gradw4 = append_stat(gradw4,mid_stat,21); gradw5 = append_stat(gradw5,mid_stat,22); gradw6 = append_stat(gradw6,mid_stat,23);\n",
    "    \n",
    "    gradp1 = append_stat(gradp1,mid_stat,24); gradp2 = append_stat(gradp2,mid_stat,25); gradp3 = append_stat(gradp3,mid_stat,26);\n",
    "    gradp4 = append_stat(gradp4,mid_stat,27); gradp5 = append_stat(gradp5,mid_stat,28); gradp6 = append_stat(gradp6,mid_stat,29);\n",
    "\n",
    "    gradup1 = append_stat(gradup1,mid_stat,30); gradup2 = append_stat(gradup2,mid_stat,31); gradup3 = append_stat(gradup3,mid_stat,32);\n",
    "    gradup4 = append_stat(gradup4,mid_stat,33); gradup5 = append_stat(gradup5,mid_stat,34); gradup6 = append_stat(gradup6,mid_stat,35);\n",
    "\n",
    "    train_acc.append(avg_acc_train/(len(train_images)/batch_size))\n",
    "    test_acc .append(avg_acc_test / (len(test_images)/batch_size))\n",
    "    # ======================== extract stats ========================\n",
    "    \n",
    "    # ======================== save to image ========================\n",
    "    save_to_image(mid_stat[0:6]   ,llayer1,llayer2,llayer3,llayer4,llayer5,llayer6,\"layer\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[6:12]  ,llayer1a,llayer2a,llayer3a,llayer4a,llayer5a,llayer6a,\"layera\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[12:18] ,weight1,weight2,weight3,weight4,weight5,weight6,\"weights\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[18:24] ,gradw1,gradw2,gradw3,gradw4,gradw5,gradw6,\"gradientw\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[24:30] ,gradp1,gradp2,gradp3,gradp4,gradp5,gradp6,\"gradientp\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[30:36] ,gradup1,gradup2,gradup3,gradup4,gradup5,gradup6,\"moment\",train_acc,test_acc,current_exp_name,iter)\n",
    "    # ======================== save to image ========================\n",
    "        \n",
    "    # ======================== print reset ========================\n",
    "    print(\"Current : \"+ str(iter) + \" Train Acc : \" + str(avg_acc_train/(len(train_images)/batch_size)) + \" Test Acc : \" + str(avg_acc_test/(len(test_images)/batch_size)) + '\\n')\n",
    "    avg_acc_train = 0 ; avg_acc_test  = 0\n",
    "    # ======================== print reset ========================\n",
    "\n",
    "np.save(current_exp_name+'/train_acc.npy',train_acc); np.save(current_exp_name+'/test_acc.npy', test_acc)    \n",
    "np.save(current_exp_name+'/llayer1.npy', llayer1);  np.save(current_exp_name+'/llayer2.npy', llayer2);  np.save(current_exp_name+'/llayer3.npy', llayer3); \n",
    "np.save(current_exp_name+'/llayer4.npy', llayer4);  np.save(current_exp_name+'/llayer5.npy', llayer5);  np.save(current_exp_name+'/llayer6.npy', llayer6); \n",
    "\n",
    "np.save(current_exp_name+'/llayer1a.npy', llayer1a);  np.save(current_exp_name+'/llayer2a.npy', llayer2a);  np.save(current_exp_name+'/llayer3a.npy', llayer3a); \n",
    "np.save(current_exp_name+'/llayer4a.npy', llayer4a);  np.save(current_exp_name+'/llayer5a.npy', llayer5a);  np.save(current_exp_name+'/llayer6a.npy', llayer6a); \n",
    "\n",
    "np.save(current_exp_name+'/weight1.npy', weight1);  np.save(current_exp_name+'/weight2.npy', weight2);  np.save(current_exp_name+'/weight3.npy', weight3);  \n",
    "np.save(current_exp_name+'/weight4.npy', weight4);  np.save(current_exp_name+'/weight5.npy', weight5);  np.save(current_exp_name+'/weight6.npy', weight6);  \n",
    "\n",
    "np.save(current_exp_name+'/gradw1.npy', gradw1); np.save(current_exp_name+'/gradw2.npy', gradw2); np.save(current_exp_name+'/gradw3.npy', gradw3);\n",
    "np.save(current_exp_name+'/gradw4.npy', gradw4); np.save(current_exp_name+'/gradw5.npy', gradw5); np.save(current_exp_name+'/gradw6.npy', gradw6);\n",
    "\n",
    "np.save(current_exp_name+'/gradp1.npy', gradp1); np.save(current_exp_name+'/gradp2.npy', gradp2); np.save(current_exp_name+'/gradp3.npy', gradp3);\n",
    "np.save(current_exp_name+'/gradp4.npy', gradp4); np.save(current_exp_name+'/gradp5.npy', gradp5); np.save(current_exp_name+'/gradp6.npy', gradp6);\n",
    "\n",
    "np.save(current_exp_name+'/gradup1.npy', gradup1); np.save(current_exp_name+'/gradup2.npy', gradup2); np.save(current_exp_name+'/gradup3.npy', gradup3);\n",
    "np.save(current_exp_name+'/gradup4.npy', gradup4); np.save(current_exp_name+'/gradup5.npy', gradup5); np.save(current_exp_name+'/gradup6.npy', gradup6);\n",
    "\n",
    "sess.close(); tf.reset_default_graph();\n",
    "\n",
    "%reset_selective -f l1,l2,l3,l4,l5,l6\n",
    "%reset_selective -f layer1,layer2,layer3,layer4,layer5,layer6\n",
    "%reset_selective -f layer1a,layer2a,layer3a,layer4a,layer5a,layer6a\n",
    "%reset_selective -f train_acc,test_acc,llayer1,llayer2,llayer3,llayer4,llayer5,llayer6,llayer1a,llayer2a,llayer3a,llayer4a,llayer5a,llayer6a\n",
    "%reset_selective -f weight1,weight2,weight3,weight4,weight5,weight6\n",
    "%reset_selective -f gradw1,gradw2,gradw3,gradw4,gradw5,gradw6\n",
    "%reset_selective -f gradp1,gradp2,gradp3,gradp4,gradp5,gradp6\n",
    "%reset_selective -f gradup1,gradup2,gradup3,gradup4,gradup5,gradup6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-05T01:05:02.804743Z",
     "start_time": "2019-01-05T00:53:26.620Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# C\n",
    "current_exp_name = 'C';\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# create layers\n",
    "l1 = CNN(3,3, 16,which_reg=current_exp_name); \n",
    "l2 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "l3 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "\n",
    "l4 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "l5 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "l6 = CNN(3,16,10,which_reg=current_exp_name); \n",
    "\n",
    "# 2. graph \n",
    "x = tf.placeholder(tf.float32,(batch_size,96,96,3))\n",
    "y = tf.placeholder(tf.float32,(batch_size,10))\n",
    "\n",
    "layer1, layer1a = l1. feedforward(x,stride=2)\n",
    "layer2, layer2a = l2. feedforward(layer1a,stride=2)\n",
    "layer3, layer3a = l3. feedforward(layer2a,stride=2)\n",
    "layer4, layer4a = l4. feedforward(layer3a,stride=2)\n",
    "layer5, layer5a = l5. feedforward(layer4a)\n",
    "layer6, layer6a = l6. feedforward(layer5a)\n",
    "\n",
    "final_layer   = tf.reduce_mean(layer6a,(1,2))\n",
    "final_softmax = tf_softmax(final_layer)\n",
    "cost          = -tf.reduce_mean(y * tf.log(final_softmax + 1e-8))\n",
    "correct_prediction = tf.equal(tf.argmax(final_softmax, 1), tf.argmax(y, 1))\n",
    "accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "gradient = tf.tile((final_softmax-y)[:,None,None,:],[1,6,6,1])/batch_size\n",
    "grad6p,grad6w,grad6_up = l6.backprop(gradient)\n",
    "grad5p,grad5w,grad5_up = l5.backprop(grad6p)\n",
    "grad4p,grad4w,grad4_up = l4.backprop(grad5p,stride=2)\n",
    "grad3p,grad3w,grad3_up = l3.backprop(grad4p,stride=2)\n",
    "grad2p,grad2w,grad2_up = l2.backprop(grad3p,stride=2)\n",
    "grad1p,grad1w,grad1_up = l1.backprop(grad2p,stride=2)\n",
    "\n",
    "gradient_update = grad6_up + grad5_up + grad4_up + grad3_up + grad2_up + grad1_up \n",
    "\n",
    "# train\n",
    "sess.run(tf.global_variables_initializer())\n",
    "avg_acc_train = 0; avg_acc_test  = 0; train_acc = [];test_acc = []\n",
    "\n",
    "# mean std skew kurt non-zero\n",
    "llayer1 = [[],[],[],[],[]]; llayer2 = [[],[],[],[],[]]; llayer3 = [[],[],[],[],[]]\n",
    "llayer4 = [[],[],[],[],[]]; llayer5 = [[],[],[],[],[]]; llayer6 = [[],[],[],[],[]]\n",
    "\n",
    "llayer1a = [[],[],[],[],[]]; llayer2a = [[],[],[],[],[]]; llayer3a = [[],[],[],[],[]]\n",
    "llayer4a = [[],[],[],[],[]]; llayer5a = [[],[],[],[],[]]; llayer6a = [[],[],[],[],[]]\n",
    "\n",
    "weight1 = [[],[],[],[],[]]; weight2 = [[],[],[],[],[]]; weight3 = [[],[],[],[],[]];\n",
    "weight4 = [[],[],[],[],[]]; weight5 = [[],[],[],[],[]]; weight6 = [[],[],[],[],[]];\n",
    "\n",
    "gradw1  = [[],[],[],[],[]]; gradw2  = [[],[],[],[],[]]; gradw3  = [[],[],[],[],[]];\n",
    "gradw4  = [[],[],[],[],[]]; gradw5  = [[],[],[],[],[]]; gradw6  = [[],[],[],[],[]];\n",
    "\n",
    "gradp1  = [[],[],[],[],[]]; gradp2  = [[],[],[],[],[]]; gradp3  = [[],[],[],[],[]];\n",
    "gradp4  = [[],[],[],[],[]]; gradp5  = [[],[],[],[],[]]; gradp6  = [[],[],[],[],[]];\n",
    "\n",
    "gradup1  = [[],[],[],[],[]]; gradup2  = [[],[],[],[],[]]; gradup3  = [[],[],[],[],[]];\n",
    "gradup4  = [[],[],[],[],[]]; gradup5  = [[],[],[],[],[]]; gradup6  = [[],[],[],[],[]];\n",
    "\n",
    "list_of_outputs = [\n",
    "    layer1,layer2,layer3,layer4,layer5,layer6,\n",
    "    layer1a,layer2a,layer3a,layer4a,layer5a,layer6a,\n",
    "    l1.getw(),l2.getw(),l3.getw(),l4.getw(),l5.getw(),l6.getw(),\n",
    "    grad1w,grad2w,grad3w,grad4w,grad5w,grad6w,\n",
    "    grad1p,grad2p,grad3p,grad4p,grad5p,grad6p,\n",
    "    grad1_up[0],grad2_up[0],grad3_up[0],grad4_up[0],grad5_up[0],grad6_up[0]\n",
    "]\n",
    "\n",
    "for iter in range(num_epoch):\n",
    "\n",
    "    # Training Accuracy    \n",
    "    for current_batch_index in range(0,len(train_images),batch_size):\n",
    "        current_data  = train_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = train_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy,gradient_update],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(train_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_train = avg_acc_train + sess_results[0]\n",
    "        \n",
    "    # get the results\n",
    "    mid_stat = sess.run(list_of_outputs,feed_dict={x:current_data,y:current_label})\n",
    "    \n",
    "    # Test Accuracy    \n",
    "    for current_batch_index in range(0,len(test_images), batch_size):\n",
    "        current_data  = test_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = test_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(test_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_test = avg_acc_test + sess_results[0]   \n",
    "        \n",
    "    # ======================== extract stats ========================\n",
    "    llayer1 = append_stat(llayer1,mid_stat,0);  llayer2 = append_stat(llayer2,mid_stat,1);  llayer3 = append_stat(llayer3,mid_stat,2);\n",
    "    llayer4 = append_stat(llayer4,mid_stat,3);  llayer5 = append_stat(llayer5,mid_stat,4);  llayer6 = append_stat(llayer6,mid_stat,5);\n",
    "\n",
    "    llayer1a = append_stat(llayer1a,mid_stat,6);  llayer2a = append_stat(llayer2a,mid_stat,7);  llayer3a = append_stat(llayer3a,mid_stat,8);\n",
    "    llayer4a = append_stat(llayer4a,mid_stat,9);  llayer5a = append_stat(llayer5a,mid_stat,10); llayer6a = append_stat(llayer6a,mid_stat,11);\n",
    "    \n",
    "    weight1 = append_stat(weight1,mid_stat,12);  weight2 = append_stat(weight2,mid_stat,13);  weight3 = append_stat(weight3,mid_stat,14);\n",
    "    weight4 = append_stat(weight4,mid_stat,15);  weight5 = append_stat(weight5,mid_stat,16);  weight6 = append_stat(weight6,mid_stat,17);\n",
    "    \n",
    "    gradw1 = append_stat(gradw1,mid_stat,18); gradw2 = append_stat(gradw2,mid_stat,19); gradw3 = append_stat(gradw3,mid_stat,20);\n",
    "    gradw4 = append_stat(gradw4,mid_stat,21); gradw5 = append_stat(gradw5,mid_stat,22); gradw6 = append_stat(gradw6,mid_stat,23);\n",
    "    \n",
    "    gradp1 = append_stat(gradp1,mid_stat,24); gradp2 = append_stat(gradp2,mid_stat,25); gradp3 = append_stat(gradp3,mid_stat,26);\n",
    "    gradp4 = append_stat(gradp4,mid_stat,27); gradp5 = append_stat(gradp5,mid_stat,28); gradp6 = append_stat(gradp6,mid_stat,29);\n",
    "\n",
    "    gradup1 = append_stat(gradup1,mid_stat,30); gradup2 = append_stat(gradup2,mid_stat,31); gradup3 = append_stat(gradup3,mid_stat,32);\n",
    "    gradup4 = append_stat(gradup4,mid_stat,33); gradup5 = append_stat(gradup5,mid_stat,34); gradup6 = append_stat(gradup6,mid_stat,35);\n",
    "\n",
    "    train_acc.append(avg_acc_train/(len(train_images)/batch_size))\n",
    "    test_acc .append(avg_acc_test / (len(test_images)/batch_size))\n",
    "    # ======================== extract stats ========================\n",
    "    \n",
    "    # ======================== save to image ========================\n",
    "    save_to_image(mid_stat[0:6]   ,llayer1,llayer2,llayer3,llayer4,llayer5,llayer6,\"layer\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[6:12]  ,llayer1a,llayer2a,llayer3a,llayer4a,llayer5a,llayer6a,\"layera\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[12:18] ,weight1,weight2,weight3,weight4,weight5,weight6,\"weights\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[18:24] ,gradw1,gradw2,gradw3,gradw4,gradw5,gradw6,\"gradientw\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[24:30] ,gradp1,gradp2,gradp3,gradp4,gradp5,gradp6,\"gradientp\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[30:36] ,gradup1,gradup2,gradup3,gradup4,gradup5,gradup6,\"moment\",train_acc,test_acc,current_exp_name,iter)\n",
    "    # ======================== save to image ========================\n",
    "        \n",
    "    # ======================== print reset ========================\n",
    "    print(\"Current : \"+ str(iter) + \" Train Acc : \" + str(avg_acc_train/(len(train_images)/batch_size)) + \" Test Acc : \" + str(avg_acc_test/(len(test_images)/batch_size)) + '\\n')\n",
    "    avg_acc_train = 0 ; avg_acc_test  = 0\n",
    "    # ======================== print reset ========================\n",
    "\n",
    "np.save(current_exp_name+'/train_acc.npy',train_acc); np.save(current_exp_name+'/test_acc.npy', test_acc)    \n",
    "np.save(current_exp_name+'/llayer1.npy', llayer1);  np.save(current_exp_name+'/llayer2.npy', llayer2);  np.save(current_exp_name+'/llayer3.npy', llayer3); \n",
    "np.save(current_exp_name+'/llayer4.npy', llayer4);  np.save(current_exp_name+'/llayer5.npy', llayer5);  np.save(current_exp_name+'/llayer6.npy', llayer6); \n",
    "\n",
    "np.save(current_exp_name+'/llayer1a.npy', llayer1a);  np.save(current_exp_name+'/llayer2a.npy', llayer2a);  np.save(current_exp_name+'/llayer3a.npy', llayer3a); \n",
    "np.save(current_exp_name+'/llayer4a.npy', llayer4a);  np.save(current_exp_name+'/llayer5a.npy', llayer5a);  np.save(current_exp_name+'/llayer6a.npy', llayer6a); \n",
    "\n",
    "np.save(current_exp_name+'/weight1.npy', weight1);  np.save(current_exp_name+'/weight2.npy', weight2);  np.save(current_exp_name+'/weight3.npy', weight3);  \n",
    "np.save(current_exp_name+'/weight4.npy', weight4);  np.save(current_exp_name+'/weight5.npy', weight5);  np.save(current_exp_name+'/weight6.npy', weight6);  \n",
    "\n",
    "np.save(current_exp_name+'/gradw1.npy', gradw1); np.save(current_exp_name+'/gradw2.npy', gradw2); np.save(current_exp_name+'/gradw3.npy', gradw3);\n",
    "np.save(current_exp_name+'/gradw4.npy', gradw4); np.save(current_exp_name+'/gradw5.npy', gradw5); np.save(current_exp_name+'/gradw6.npy', gradw6);\n",
    "\n",
    "np.save(current_exp_name+'/gradp1.npy', gradp1); np.save(current_exp_name+'/gradp2.npy', gradp2); np.save(current_exp_name+'/gradp3.npy', gradp3);\n",
    "np.save(current_exp_name+'/gradp4.npy', gradp4); np.save(current_exp_name+'/gradp5.npy', gradp5); np.save(current_exp_name+'/gradp6.npy', gradp6);\n",
    "\n",
    "np.save(current_exp_name+'/gradup1.npy', gradup1); np.save(current_exp_name+'/gradup2.npy', gradup2); np.save(current_exp_name+'/gradup3.npy', gradup3);\n",
    "np.save(current_exp_name+'/gradup4.npy', gradup4); np.save(current_exp_name+'/gradup5.npy', gradup5); np.save(current_exp_name+'/gradup6.npy', gradup6);\n",
    "\n",
    "sess.close(); tf.reset_default_graph();\n",
    "\n",
    "%reset_selective -f train_acc,test_acc,llayer1,llayer2,llayer3,llayer4,llayer5,llayer6,llayer1a,llayer2a,llayer3a,llayer4a,llayer5a,llayer6a\n",
    "%reset_selective -f weight1,weight2,weight3,weight4,weight5,weight6\n",
    "%reset_selective -f gradw1,gradw2,gradw3,gradw4,gradw5,gradw6\n",
    "%reset_selective -f gradp1,gradp2,gradp3,gradp4,gradp5,gradp6\n",
    "%reset_selective -f gradup1,gradup2,gradup3,gradup4,gradup5,gradup6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-05T01:05:02.806740Z",
     "start_time": "2019-01-05T00:53:26.623Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# D\n",
    "current_exp_name = 'D';\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# create layers\n",
    "l1 = CNN(3,3, 16,which_reg=current_exp_name); \n",
    "l2 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "l3 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "\n",
    "l4 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "l5 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "l6 = CNN(3,16,10,which_reg=current_exp_name); \n",
    "\n",
    "# 2. graph \n",
    "x = tf.placeholder(tf.float32,(batch_size,96,96,3))\n",
    "y = tf.placeholder(tf.float32,(batch_size,10))\n",
    "\n",
    "layer1, layer1a = l1. feedforward(x,stride=2)\n",
    "layer2, layer2a = l2. feedforward(layer1a,stride=2)\n",
    "layer3, layer3a = l3. feedforward(layer2a,stride=2)\n",
    "layer4, layer4a = l4. feedforward(layer3a,stride=2)\n",
    "layer5, layer5a = l5. feedforward(layer4a)\n",
    "layer6, layer6a = l6. feedforward(layer5a)\n",
    "\n",
    "final_layer   = tf.reduce_mean(layer6a,(1,2))\n",
    "final_softmax = tf_softmax(final_layer)\n",
    "cost          = -tf.reduce_mean(y * tf.log(final_softmax + 1e-8))\n",
    "correct_prediction = tf.equal(tf.argmax(final_softmax, 1), tf.argmax(y, 1))\n",
    "accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "gradient = tf.tile((final_softmax-y)[:,None,None,:],[1,6,6,1])/batch_size\n",
    "grad6p,grad6w,grad6_up = l6.backprop(gradient)\n",
    "grad5p,grad5w,grad5_up = l5.backprop(grad6p)\n",
    "grad4p,grad4w,grad4_up = l4.backprop(grad5p,stride=2)\n",
    "grad3p,grad3w,grad3_up = l3.backprop(grad4p,stride=2)\n",
    "grad2p,grad2w,grad2_up = l2.backprop(grad3p,stride=2)\n",
    "grad1p,grad1w,grad1_up = l1.backprop(grad2p,stride=2)\n",
    "\n",
    "gradient_update = grad6_up + grad5_up + grad4_up + grad3_up + grad2_up + grad1_up \n",
    "\n",
    "# train\n",
    "sess.run(tf.global_variables_initializer())\n",
    "avg_acc_train = 0; avg_acc_test  = 0; train_acc = [];test_acc = []\n",
    "\n",
    "# mean std skew kurt non-zero\n",
    "llayer1 = [[],[],[],[],[]]; llayer2 = [[],[],[],[],[]]; llayer3 = [[],[],[],[],[]]\n",
    "llayer4 = [[],[],[],[],[]]; llayer5 = [[],[],[],[],[]]; llayer6 = [[],[],[],[],[]]\n",
    "\n",
    "llayer1a = [[],[],[],[],[]]; llayer2a = [[],[],[],[],[]]; llayer3a = [[],[],[],[],[]]\n",
    "llayer4a = [[],[],[],[],[]]; llayer5a = [[],[],[],[],[]]; llayer6a = [[],[],[],[],[]]\n",
    "\n",
    "weight1 = [[],[],[],[],[]]; weight2 = [[],[],[],[],[]]; weight3 = [[],[],[],[],[]];\n",
    "weight4 = [[],[],[],[],[]]; weight5 = [[],[],[],[],[]]; weight6 = [[],[],[],[],[]];\n",
    "\n",
    "gradw1  = [[],[],[],[],[]]; gradw2  = [[],[],[],[],[]]; gradw3  = [[],[],[],[],[]];\n",
    "gradw4  = [[],[],[],[],[]]; gradw5  = [[],[],[],[],[]]; gradw6  = [[],[],[],[],[]];\n",
    "\n",
    "gradp1  = [[],[],[],[],[]]; gradp2  = [[],[],[],[],[]]; gradp3  = [[],[],[],[],[]];\n",
    "gradp4  = [[],[],[],[],[]]; gradp5  = [[],[],[],[],[]]; gradp6  = [[],[],[],[],[]];\n",
    "\n",
    "gradup1  = [[],[],[],[],[]]; gradup2  = [[],[],[],[],[]]; gradup3  = [[],[],[],[],[]];\n",
    "gradup4  = [[],[],[],[],[]]; gradup5  = [[],[],[],[],[]]; gradup6  = [[],[],[],[],[]];\n",
    "\n",
    "list_of_outputs = [\n",
    "    layer1,layer2,layer3,layer4,layer5,layer6,\n",
    "    layer1a,layer2a,layer3a,layer4a,layer5a,layer6a,\n",
    "    l1.getw(),l2.getw(),l3.getw(),l4.getw(),l5.getw(),l6.getw(),\n",
    "    grad1w,grad2w,grad3w,grad4w,grad5w,grad6w,\n",
    "    grad1p,grad2p,grad3p,grad4p,grad5p,grad6p,\n",
    "    grad1_up[0],grad2_up[0],grad3_up[0],grad4_up[0],grad5_up[0],grad6_up[0]\n",
    "]\n",
    "\n",
    "for iter in range(num_epoch):\n",
    "\n",
    "    # Training Accuracy    \n",
    "    for current_batch_index in range(0,len(train_images),batch_size):\n",
    "        current_data  = train_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = train_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy,gradient_update],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(train_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_train = avg_acc_train + sess_results[0]\n",
    "        \n",
    "    # get the results\n",
    "    mid_stat = sess.run(list_of_outputs,feed_dict={x:current_data,y:current_label})\n",
    "    \n",
    "    # Test Accuracy    \n",
    "    for current_batch_index in range(0,len(test_images), batch_size):\n",
    "        current_data  = test_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = test_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(test_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_test = avg_acc_test + sess_results[0]   \n",
    "        \n",
    "    # ======================== extract stats ========================\n",
    "    llayer1 = append_stat(llayer1,mid_stat,0);  llayer2 = append_stat(llayer2,mid_stat,1);  llayer3 = append_stat(llayer3,mid_stat,2);\n",
    "    llayer4 = append_stat(llayer4,mid_stat,3);  llayer5 = append_stat(llayer5,mid_stat,4);  llayer6 = append_stat(llayer6,mid_stat,5);\n",
    "\n",
    "    llayer1a = append_stat(llayer1a,mid_stat,6);  llayer2a = append_stat(llayer2a,mid_stat,7);  llayer3a = append_stat(llayer3a,mid_stat,8);\n",
    "    llayer4a = append_stat(llayer4a,mid_stat,9);  llayer5a = append_stat(llayer5a,mid_stat,10); llayer6a = append_stat(llayer6a,mid_stat,11);\n",
    "    \n",
    "    weight1 = append_stat(weight1,mid_stat,12);  weight2 = append_stat(weight2,mid_stat,13);  weight3 = append_stat(weight3,mid_stat,14);\n",
    "    weight4 = append_stat(weight4,mid_stat,15);  weight5 = append_stat(weight5,mid_stat,16);  weight6 = append_stat(weight6,mid_stat,17);\n",
    "    \n",
    "    gradw1 = append_stat(gradw1,mid_stat,18); gradw2 = append_stat(gradw2,mid_stat,19); gradw3 = append_stat(gradw3,mid_stat,20);\n",
    "    gradw4 = append_stat(gradw4,mid_stat,21); gradw5 = append_stat(gradw5,mid_stat,22); gradw6 = append_stat(gradw6,mid_stat,23);\n",
    "    \n",
    "    gradp1 = append_stat(gradp1,mid_stat,24); gradp2 = append_stat(gradp2,mid_stat,25); gradp3 = append_stat(gradp3,mid_stat,26);\n",
    "    gradp4 = append_stat(gradp4,mid_stat,27); gradp5 = append_stat(gradp5,mid_stat,28); gradp6 = append_stat(gradp6,mid_stat,29);\n",
    "\n",
    "    gradup1 = append_stat(gradup1,mid_stat,30); gradup2 = append_stat(gradup2,mid_stat,31); gradup3 = append_stat(gradup3,mid_stat,32);\n",
    "    gradup4 = append_stat(gradup4,mid_stat,33); gradup5 = append_stat(gradup5,mid_stat,34); gradup6 = append_stat(gradup6,mid_stat,35);\n",
    "\n",
    "    train_acc.append(avg_acc_train/(len(train_images)/batch_size))\n",
    "    test_acc .append(avg_acc_test / (len(test_images)/batch_size))\n",
    "    # ======================== extract stats ========================\n",
    "    \n",
    "    # ======================== save to image ========================\n",
    "    save_to_image(mid_stat[0:6]   ,llayer1,llayer2,llayer3,llayer4,llayer5,llayer6,\"layer\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[6:12]  ,llayer1a,llayer2a,llayer3a,llayer4a,llayer5a,llayer6a,\"layera\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[12:18] ,weight1,weight2,weight3,weight4,weight5,weight6,\"weights\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[18:24] ,gradw1,gradw2,gradw3,gradw4,gradw5,gradw6,\"gradientw\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[24:30] ,gradp1,gradp2,gradp3,gradp4,gradp5,gradp6,\"gradientp\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[30:36] ,gradup1,gradup2,gradup3,gradup4,gradup5,gradup6,\"moment\",train_acc,test_acc,current_exp_name,iter)\n",
    "    # ======================== save to image ========================\n",
    "        \n",
    "    # ======================== print reset ========================\n",
    "    print(\"Current : \"+ str(iter) + \" Train Acc : \" + str(avg_acc_train/(len(train_images)/batch_size)) + \" Test Acc : \" + str(avg_acc_test/(len(test_images)/batch_size)) + '\\n')\n",
    "    avg_acc_train = 0 ; avg_acc_test  = 0\n",
    "    # ======================== print reset ========================\n",
    "\n",
    "np.save(current_exp_name+'/train_acc.npy',train_acc); np.save(current_exp_name+'/test_acc.npy', test_acc)    \n",
    "np.save(current_exp_name+'/llayer1.npy', llayer1);  np.save(current_exp_name+'/llayer2.npy', llayer2);  np.save(current_exp_name+'/llayer3.npy', llayer3); \n",
    "np.save(current_exp_name+'/llayer4.npy', llayer4);  np.save(current_exp_name+'/llayer5.npy', llayer5);  np.save(current_exp_name+'/llayer6.npy', llayer6); \n",
    "\n",
    "np.save(current_exp_name+'/llayer1a.npy', llayer1a);  np.save(current_exp_name+'/llayer2a.npy', llayer2a);  np.save(current_exp_name+'/llayer3a.npy', llayer3a); \n",
    "np.save(current_exp_name+'/llayer4a.npy', llayer4a);  np.save(current_exp_name+'/llayer5a.npy', llayer5a);  np.save(current_exp_name+'/llayer6a.npy', llayer6a); \n",
    "\n",
    "np.save(current_exp_name+'/weight1.npy', weight1);  np.save(current_exp_name+'/weight2.npy', weight2);  np.save(current_exp_name+'/weight3.npy', weight3);  \n",
    "np.save(current_exp_name+'/weight4.npy', weight4);  np.save(current_exp_name+'/weight5.npy', weight5);  np.save(current_exp_name+'/weight6.npy', weight6);  \n",
    "\n",
    "np.save(current_exp_name+'/gradw1.npy', gradw1); np.save(current_exp_name+'/gradw2.npy', gradw2); np.save(current_exp_name+'/gradw3.npy', gradw3);\n",
    "np.save(current_exp_name+'/gradw4.npy', gradw4); np.save(current_exp_name+'/gradw5.npy', gradw5); np.save(current_exp_name+'/gradw6.npy', gradw6);\n",
    "\n",
    "np.save(current_exp_name+'/gradp1.npy', gradp1); np.save(current_exp_name+'/gradp2.npy', gradp2); np.save(current_exp_name+'/gradp3.npy', gradp3);\n",
    "np.save(current_exp_name+'/gradp4.npy', gradp4); np.save(current_exp_name+'/gradp5.npy', gradp5); np.save(current_exp_name+'/gradp6.npy', gradp6);\n",
    "\n",
    "np.save(current_exp_name+'/gradup1.npy', gradup1); np.save(current_exp_name+'/gradup2.npy', gradup2); np.save(current_exp_name+'/gradup3.npy', gradup3);\n",
    "np.save(current_exp_name+'/gradup4.npy', gradup4); np.save(current_exp_name+'/gradup5.npy', gradup5); np.save(current_exp_name+'/gradup6.npy', gradup6);\n",
    "\n",
    "sess.close(); tf.reset_default_graph();\n",
    "\n",
    "%reset_selective -f train_acc,test_acc,llayer1,llayer2,llayer3,llayer4,llayer5,llayer6,llayer1a,llayer2a,llayer3a,llayer4a,llayer5a,llayer6a\n",
    "%reset_selective -f weight1,weight2,weight3,weight4,weight5,weight6\n",
    "%reset_selective -f gradw1,gradw2,gradw3,gradw4,gradw5,gradw6\n",
    "%reset_selective -f gradp1,gradp2,gradp3,gradp4,gradp5,gradp6\n",
    "%reset_selective -f gradup1,gradup2,gradup3,gradup4,gradup5,gradup6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-05T01:05:02.808733Z",
     "start_time": "2019-01-05T00:53:26.626Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# E\n",
    "current_exp_name = 'E';\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# create layers\n",
    "l1 = CNN(3,3, 16,which_reg=current_exp_name); \n",
    "l2 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "l3 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "\n",
    "l4 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "l5 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "l6 = CNN(3,16,10,which_reg=current_exp_name); \n",
    "\n",
    "# 2. graph \n",
    "x = tf.placeholder(tf.float32,(batch_size,96,96,3))\n",
    "y = tf.placeholder(tf.float32,(batch_size,10))\n",
    "\n",
    "layer1, layer1a = l1. feedforward(x,stride=2)\n",
    "layer2, layer2a = l2. feedforward(layer1a,stride=2)\n",
    "layer3, layer3a = l3. feedforward(layer2a,stride=2)\n",
    "layer4, layer4a = l4. feedforward(layer3a,stride=2)\n",
    "layer5, layer5a = l5. feedforward(layer4a)\n",
    "layer6, layer6a = l6. feedforward(layer5a)\n",
    "\n",
    "final_layer   = tf.reduce_mean(layer6a,(1,2))\n",
    "final_softmax = tf_softmax(final_layer)\n",
    "cost          = -tf.reduce_mean(y * tf.log(final_softmax + 1e-8))\n",
    "correct_prediction = tf.equal(tf.argmax(final_softmax, 1), tf.argmax(y, 1))\n",
    "accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "gradient = tf.tile((final_softmax-y)[:,None,None,:],[1,6,6,1])/batch_size\n",
    "grad6p,grad6w,grad6_up = l6.backprop(gradient)\n",
    "grad5p,grad5w,grad5_up = l5.backprop(grad6p)\n",
    "grad4p,grad4w,grad4_up = l4.backprop(grad5p,stride=2)\n",
    "grad3p,grad3w,grad3_up = l3.backprop(grad4p,stride=2)\n",
    "grad2p,grad2w,grad2_up = l2.backprop(grad3p,stride=2)\n",
    "grad1p,grad1w,grad1_up = l1.backprop(grad2p,stride=2)\n",
    "\n",
    "gradient_update = grad6_up + grad5_up + grad4_up + grad3_up + grad2_up + grad1_up \n",
    "\n",
    "# train\n",
    "sess.run(tf.global_variables_initializer())\n",
    "avg_acc_train = 0; avg_acc_test  = 0; train_acc = [];test_acc = []\n",
    "\n",
    "# mean std skew kurt non-zero\n",
    "llayer1 = [[],[],[],[],[]]; llayer2 = [[],[],[],[],[]]; llayer3 = [[],[],[],[],[]]\n",
    "llayer4 = [[],[],[],[],[]]; llayer5 = [[],[],[],[],[]]; llayer6 = [[],[],[],[],[]]\n",
    "\n",
    "llayer1a = [[],[],[],[],[]]; llayer2a = [[],[],[],[],[]]; llayer3a = [[],[],[],[],[]]\n",
    "llayer4a = [[],[],[],[],[]]; llayer5a = [[],[],[],[],[]]; llayer6a = [[],[],[],[],[]]\n",
    "\n",
    "weight1 = [[],[],[],[],[]]; weight2 = [[],[],[],[],[]]; weight3 = [[],[],[],[],[]];\n",
    "weight4 = [[],[],[],[],[]]; weight5 = [[],[],[],[],[]]; weight6 = [[],[],[],[],[]];\n",
    "\n",
    "gradw1  = [[],[],[],[],[]]; gradw2  = [[],[],[],[],[]]; gradw3  = [[],[],[],[],[]];\n",
    "gradw4  = [[],[],[],[],[]]; gradw5  = [[],[],[],[],[]]; gradw6  = [[],[],[],[],[]];\n",
    "\n",
    "gradp1  = [[],[],[],[],[]]; gradp2  = [[],[],[],[],[]]; gradp3  = [[],[],[],[],[]];\n",
    "gradp4  = [[],[],[],[],[]]; gradp5  = [[],[],[],[],[]]; gradp6  = [[],[],[],[],[]];\n",
    "\n",
    "gradup1  = [[],[],[],[],[]]; gradup2  = [[],[],[],[],[]]; gradup3  = [[],[],[],[],[]];\n",
    "gradup4  = [[],[],[],[],[]]; gradup5  = [[],[],[],[],[]]; gradup6  = [[],[],[],[],[]];\n",
    "\n",
    "list_of_outputs = [\n",
    "    layer1,layer2,layer3,layer4,layer5,layer6,\n",
    "    layer1a,layer2a,layer3a,layer4a,layer5a,layer6a,\n",
    "    l1.getw(),l2.getw(),l3.getw(),l4.getw(),l5.getw(),l6.getw(),\n",
    "    grad1w,grad2w,grad3w,grad4w,grad5w,grad6w,\n",
    "    grad1p,grad2p,grad3p,grad4p,grad5p,grad6p,\n",
    "    grad1_up[0],grad2_up[0],grad3_up[0],grad4_up[0],grad5_up[0],grad6_up[0]\n",
    "]\n",
    "\n",
    "for iter in range(num_epoch):\n",
    "\n",
    "    # Training Accuracy    \n",
    "    for current_batch_index in range(0,len(train_images),batch_size):\n",
    "        current_data  = train_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = train_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy,gradient_update],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(train_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_train = avg_acc_train + sess_results[0]\n",
    "        \n",
    "    # get the results\n",
    "    mid_stat = sess.run(list_of_outputs,feed_dict={x:current_data,y:current_label})\n",
    "    \n",
    "    # Test Accuracy    \n",
    "    for current_batch_index in range(0,len(test_images), batch_size):\n",
    "        current_data  = test_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = test_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(test_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_test = avg_acc_test + sess_results[0]   \n",
    "        \n",
    "    # ======================== extract stats ========================\n",
    "    llayer1 = append_stat(llayer1,mid_stat,0);  llayer2 = append_stat(llayer2,mid_stat,1);  llayer3 = append_stat(llayer3,mid_stat,2);\n",
    "    llayer4 = append_stat(llayer4,mid_stat,3);  llayer5 = append_stat(llayer5,mid_stat,4);  llayer6 = append_stat(llayer6,mid_stat,5);\n",
    "\n",
    "    llayer1a = append_stat(llayer1a,mid_stat,6);  llayer2a = append_stat(llayer2a,mid_stat,7);  llayer3a = append_stat(llayer3a,mid_stat,8);\n",
    "    llayer4a = append_stat(llayer4a,mid_stat,9);  llayer5a = append_stat(llayer5a,mid_stat,10); llayer6a = append_stat(llayer6a,mid_stat,11);\n",
    "    \n",
    "    weight1 = append_stat(weight1,mid_stat,12);  weight2 = append_stat(weight2,mid_stat,13);  weight3 = append_stat(weight3,mid_stat,14);\n",
    "    weight4 = append_stat(weight4,mid_stat,15);  weight5 = append_stat(weight5,mid_stat,16);  weight6 = append_stat(weight6,mid_stat,17);\n",
    "    \n",
    "    gradw1 = append_stat(gradw1,mid_stat,18); gradw2 = append_stat(gradw2,mid_stat,19); gradw3 = append_stat(gradw3,mid_stat,20);\n",
    "    gradw4 = append_stat(gradw4,mid_stat,21); gradw5 = append_stat(gradw5,mid_stat,22); gradw6 = append_stat(gradw6,mid_stat,23);\n",
    "    \n",
    "    gradp1 = append_stat(gradp1,mid_stat,24); gradp2 = append_stat(gradp2,mid_stat,25); gradp3 = append_stat(gradp3,mid_stat,26);\n",
    "    gradp4 = append_stat(gradp4,mid_stat,27); gradp5 = append_stat(gradp5,mid_stat,28); gradp6 = append_stat(gradp6,mid_stat,29);\n",
    "\n",
    "    gradup1 = append_stat(gradup1,mid_stat,30); gradup2 = append_stat(gradup2,mid_stat,31); gradup3 = append_stat(gradup3,mid_stat,32);\n",
    "    gradup4 = append_stat(gradup4,mid_stat,33); gradup5 = append_stat(gradup5,mid_stat,34); gradup6 = append_stat(gradup6,mid_stat,35);\n",
    "\n",
    "    train_acc.append(avg_acc_train/(len(train_images)/batch_size))\n",
    "    test_acc .append(avg_acc_test / (len(test_images)/batch_size))\n",
    "    # ======================== extract stats ========================\n",
    "    \n",
    "    # ======================== save to image ========================\n",
    "    save_to_image(mid_stat[0:6]   ,llayer1,llayer2,llayer3,llayer4,llayer5,llayer6,\"layer\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[6:12]  ,llayer1a,llayer2a,llayer3a,llayer4a,llayer5a,llayer6a,\"layera\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[12:18] ,weight1,weight2,weight3,weight4,weight5,weight6,\"weights\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[18:24] ,gradw1,gradw2,gradw3,gradw4,gradw5,gradw6,\"gradientw\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[24:30] ,gradp1,gradp2,gradp3,gradp4,gradp5,gradp6,\"gradientp\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[30:36] ,gradup1,gradup2,gradup3,gradup4,gradup5,gradup6,\"moment\",train_acc,test_acc,current_exp_name,iter)\n",
    "    # ======================== save to image ========================\n",
    "        \n",
    "    # ======================== print reset ========================\n",
    "    print(\"Current : \"+ str(iter) + \" Train Acc : \" + str(avg_acc_train/(len(train_images)/batch_size)) + \" Test Acc : \" + str(avg_acc_test/(len(test_images)/batch_size)) + '\\n')\n",
    "    avg_acc_train = 0 ; avg_acc_test  = 0\n",
    "    # ======================== print reset ========================\n",
    "\n",
    "np.save(current_exp_name+'/train_acc.npy',train_acc); np.save(current_exp_name+'/test_acc.npy', test_acc)    \n",
    "np.save(current_exp_name+'/llayer1.npy', llayer1);  np.save(current_exp_name+'/llayer2.npy', llayer2);  np.save(current_exp_name+'/llayer3.npy', llayer3); \n",
    "np.save(current_exp_name+'/llayer4.npy', llayer4);  np.save(current_exp_name+'/llayer5.npy', llayer5);  np.save(current_exp_name+'/llayer6.npy', llayer6); \n",
    "\n",
    "np.save(current_exp_name+'/llayer1a.npy', llayer1a);  np.save(current_exp_name+'/llayer2a.npy', llayer2a);  np.save(current_exp_name+'/llayer3a.npy', llayer3a); \n",
    "np.save(current_exp_name+'/llayer4a.npy', llayer4a);  np.save(current_exp_name+'/llayer5a.npy', llayer5a);  np.save(current_exp_name+'/llayer6a.npy', llayer6a); \n",
    "\n",
    "np.save(current_exp_name+'/weight1.npy', weight1);  np.save(current_exp_name+'/weight2.npy', weight2);  np.save(current_exp_name+'/weight3.npy', weight3);  \n",
    "np.save(current_exp_name+'/weight4.npy', weight4);  np.save(current_exp_name+'/weight5.npy', weight5);  np.save(current_exp_name+'/weight6.npy', weight6);  \n",
    "\n",
    "np.save(current_exp_name+'/gradw1.npy', gradw1); np.save(current_exp_name+'/gradw2.npy', gradw2); np.save(current_exp_name+'/gradw3.npy', gradw3);\n",
    "np.save(current_exp_name+'/gradw4.npy', gradw4); np.save(current_exp_name+'/gradw5.npy', gradw5); np.save(current_exp_name+'/gradw6.npy', gradw6);\n",
    "\n",
    "np.save(current_exp_name+'/gradp1.npy', gradp1); np.save(current_exp_name+'/gradp2.npy', gradp2); np.save(current_exp_name+'/gradp3.npy', gradp3);\n",
    "np.save(current_exp_name+'/gradp4.npy', gradp4); np.save(current_exp_name+'/gradp5.npy', gradp5); np.save(current_exp_name+'/gradp6.npy', gradp6);\n",
    "\n",
    "np.save(current_exp_name+'/gradup1.npy', gradup1); np.save(current_exp_name+'/gradup2.npy', gradup2); np.save(current_exp_name+'/gradup3.npy', gradup3);\n",
    "np.save(current_exp_name+'/gradup4.npy', gradup4); np.save(current_exp_name+'/gradup5.npy', gradup5); np.save(current_exp_name+'/gradup6.npy', gradup6);\n",
    "\n",
    "sess.close(); tf.reset_default_graph();\n",
    "\n",
    "%reset_selective -f train_acc,test_acc,llayer1,llayer2,llayer3,llayer4,llayer5,llayer6,llayer1a,llayer2a,llayer3a,llayer4a,llayer5a,llayer6a\n",
    "%reset_selective -f weight1,weight2,weight3,weight4,weight5,weight6\n",
    "%reset_selective -f gradw1,gradw2,gradw3,gradw4,gradw5,gradw6\n",
    "%reset_selective -f gradp1,gradp2,gradp3,gradp4,gradp5,gradp6\n",
    "%reset_selective -f gradup1,gradup2,gradup3,gradup4,gradup5,gradup6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-05T01:05:02.809765Z",
     "start_time": "2019-01-05T00:53:26.630Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# F\n",
    "current_exp_name = 'F';\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# create layers\n",
    "l1 = CNN(3,3, 16,which_reg=current_exp_name); \n",
    "l2 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "l3 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "\n",
    "l4 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "l5 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "l6 = CNN(3,16,10,which_reg=current_exp_name); \n",
    "\n",
    "# 2. graph \n",
    "x = tf.placeholder(tf.float32,(batch_size,96,96,3))\n",
    "y = tf.placeholder(tf.float32,(batch_size,10))\n",
    "\n",
    "layer1, layer1a = l1. feedforward(x,stride=2)\n",
    "layer2, layer2a = l2. feedforward(layer1a,stride=2)\n",
    "layer3, layer3a = l3. feedforward(layer2a,stride=2)\n",
    "layer4, layer4a = l4. feedforward(layer3a,stride=2)\n",
    "layer5, layer5a = l5. feedforward(layer4a)\n",
    "layer6, layer6a = l6. feedforward(layer5a)\n",
    "\n",
    "final_layer   = tf.reduce_mean(layer6a,(1,2))\n",
    "final_softmax = tf_softmax(final_layer)\n",
    "cost          = -tf.reduce_mean(y * tf.log(final_softmax + 1e-8))\n",
    "correct_prediction = tf.equal(tf.argmax(final_softmax, 1), tf.argmax(y, 1))\n",
    "accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "gradient = tf.tile((final_softmax-y)[:,None,None,:],[1,6,6,1])/batch_size\n",
    "grad6p,grad6w,grad6_up = l6.backprop(gradient)\n",
    "grad5p,grad5w,grad5_up = l5.backprop(grad6p)\n",
    "grad4p,grad4w,grad4_up = l4.backprop(grad5p,stride=2)\n",
    "grad3p,grad3w,grad3_up = l3.backprop(grad4p,stride=2)\n",
    "grad2p,grad2w,grad2_up = l2.backprop(grad3p,stride=2)\n",
    "grad1p,grad1w,grad1_up = l1.backprop(grad2p,stride=2)\n",
    "\n",
    "gradient_update = grad6_up + grad5_up + grad4_up + grad3_up + grad2_up + grad1_up \n",
    "\n",
    "# train\n",
    "sess.run(tf.global_variables_initializer())\n",
    "avg_acc_train = 0; avg_acc_test  = 0; train_acc = [];test_acc = []\n",
    "\n",
    "# mean std skew kurt non-zero\n",
    "llayer1 = [[],[],[],[],[]]; llayer2 = [[],[],[],[],[]]; llayer3 = [[],[],[],[],[]]\n",
    "llayer4 = [[],[],[],[],[]]; llayer5 = [[],[],[],[],[]]; llayer6 = [[],[],[],[],[]]\n",
    "\n",
    "llayer1a = [[],[],[],[],[]]; llayer2a = [[],[],[],[],[]]; llayer3a = [[],[],[],[],[]]\n",
    "llayer4a = [[],[],[],[],[]]; llayer5a = [[],[],[],[],[]]; llayer6a = [[],[],[],[],[]]\n",
    "\n",
    "weight1 = [[],[],[],[],[]]; weight2 = [[],[],[],[],[]]; weight3 = [[],[],[],[],[]];\n",
    "weight4 = [[],[],[],[],[]]; weight5 = [[],[],[],[],[]]; weight6 = [[],[],[],[],[]];\n",
    "\n",
    "gradw1  = [[],[],[],[],[]]; gradw2  = [[],[],[],[],[]]; gradw3  = [[],[],[],[],[]];\n",
    "gradw4  = [[],[],[],[],[]]; gradw5  = [[],[],[],[],[]]; gradw6  = [[],[],[],[],[]];\n",
    "\n",
    "gradp1  = [[],[],[],[],[]]; gradp2  = [[],[],[],[],[]]; gradp3  = [[],[],[],[],[]];\n",
    "gradp4  = [[],[],[],[],[]]; gradp5  = [[],[],[],[],[]]; gradp6  = [[],[],[],[],[]];\n",
    "\n",
    "gradup1  = [[],[],[],[],[]]; gradup2  = [[],[],[],[],[]]; gradup3  = [[],[],[],[],[]];\n",
    "gradup4  = [[],[],[],[],[]]; gradup5  = [[],[],[],[],[]]; gradup6  = [[],[],[],[],[]];\n",
    "\n",
    "list_of_outputs = [\n",
    "    layer1,layer2,layer3,layer4,layer5,layer6,\n",
    "    layer1a,layer2a,layer3a,layer4a,layer5a,layer6a,\n",
    "    l1.getw(),l2.getw(),l3.getw(),l4.getw(),l5.getw(),l6.getw(),\n",
    "    grad1w,grad2w,grad3w,grad4w,grad5w,grad6w,\n",
    "    grad1p,grad2p,grad3p,grad4p,grad5p,grad6p,\n",
    "    grad1_up[0],grad2_up[0],grad3_up[0],grad4_up[0],grad5_up[0],grad6_up[0]\n",
    "]\n",
    "\n",
    "for iter in range(num_epoch):\n",
    "\n",
    "    # Training Accuracy    \n",
    "    for current_batch_index in range(0,len(train_images),batch_size):\n",
    "        current_data  = train_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = train_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy,gradient_update],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(train_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_train = avg_acc_train + sess_results[0]\n",
    "        \n",
    "    # get the results\n",
    "    mid_stat = sess.run(list_of_outputs,feed_dict={x:current_data,y:current_label})\n",
    "    \n",
    "    # Test Accuracy    \n",
    "    for current_batch_index in range(0,len(test_images), batch_size):\n",
    "        current_data  = test_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = test_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(test_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_test = avg_acc_test + sess_results[0]   \n",
    "        \n",
    "    # ======================== extract stats ========================\n",
    "    llayer1 = append_stat(llayer1,mid_stat,0);  llayer2 = append_stat(llayer2,mid_stat,1);  llayer3 = append_stat(llayer3,mid_stat,2);\n",
    "    llayer4 = append_stat(llayer4,mid_stat,3);  llayer5 = append_stat(llayer5,mid_stat,4);  llayer6 = append_stat(llayer6,mid_stat,5);\n",
    "\n",
    "    llayer1a = append_stat(llayer1a,mid_stat,6);  llayer2a = append_stat(llayer2a,mid_stat,7);  llayer3a = append_stat(llayer3a,mid_stat,8);\n",
    "    llayer4a = append_stat(llayer4a,mid_stat,9);  llayer5a = append_stat(llayer5a,mid_stat,10); llayer6a = append_stat(llayer6a,mid_stat,11);\n",
    "    \n",
    "    weight1 = append_stat(weight1,mid_stat,12);  weight2 = append_stat(weight2,mid_stat,13);  weight3 = append_stat(weight3,mid_stat,14);\n",
    "    weight4 = append_stat(weight4,mid_stat,15);  weight5 = append_stat(weight5,mid_stat,16);  weight6 = append_stat(weight6,mid_stat,17);\n",
    "    \n",
    "    gradw1 = append_stat(gradw1,mid_stat,18); gradw2 = append_stat(gradw2,mid_stat,19); gradw3 = append_stat(gradw3,mid_stat,20);\n",
    "    gradw4 = append_stat(gradw4,mid_stat,21); gradw5 = append_stat(gradw5,mid_stat,22); gradw6 = append_stat(gradw6,mid_stat,23);\n",
    "    \n",
    "    gradp1 = append_stat(gradp1,mid_stat,24); gradp2 = append_stat(gradp2,mid_stat,25); gradp3 = append_stat(gradp3,mid_stat,26);\n",
    "    gradp4 = append_stat(gradp4,mid_stat,27); gradp5 = append_stat(gradp5,mid_stat,28); gradp6 = append_stat(gradp6,mid_stat,29);\n",
    "\n",
    "    gradup1 = append_stat(gradup1,mid_stat,30); gradup2 = append_stat(gradup2,mid_stat,31); gradup3 = append_stat(gradup3,mid_stat,32);\n",
    "    gradup4 = append_stat(gradup4,mid_stat,33); gradup5 = append_stat(gradup5,mid_stat,34); gradup6 = append_stat(gradup6,mid_stat,35);\n",
    "\n",
    "    train_acc.append(avg_acc_train/(len(train_images)/batch_size))\n",
    "    test_acc .append(avg_acc_test / (len(test_images)/batch_size))\n",
    "    # ======================== extract stats ========================\n",
    "    \n",
    "    # ======================== save to image ========================\n",
    "    save_to_image(mid_stat[0:6]   ,llayer1,llayer2,llayer3,llayer4,llayer5,llayer6,\"layer\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[6:12]  ,llayer1a,llayer2a,llayer3a,llayer4a,llayer5a,llayer6a,\"layera\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[12:18] ,weight1,weight2,weight3,weight4,weight5,weight6,\"weights\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[18:24] ,gradw1,gradw2,gradw3,gradw4,gradw5,gradw6,\"gradientw\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[24:30] ,gradp1,gradp2,gradp3,gradp4,gradp5,gradp6,\"gradientp\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[30:36] ,gradup1,gradup2,gradup3,gradup4,gradup5,gradup6,\"moment\",train_acc,test_acc,current_exp_name,iter)\n",
    "    # ======================== save to image ========================\n",
    "        \n",
    "    # ======================== print reset ========================\n",
    "    print(\"Current : \"+ str(iter) + \" Train Acc : \" + str(avg_acc_train/(len(train_images)/batch_size)) + \" Test Acc : \" + str(avg_acc_test/(len(test_images)/batch_size)) + '\\n')\n",
    "    avg_acc_train = 0 ; avg_acc_test  = 0\n",
    "    # ======================== print reset ========================\n",
    "\n",
    "np.save(current_exp_name+'/train_acc.npy',train_acc); np.save(current_exp_name+'/test_acc.npy', test_acc)    \n",
    "np.save(current_exp_name+'/llayer1.npy', llayer1);  np.save(current_exp_name+'/llayer2.npy', llayer2);  np.save(current_exp_name+'/llayer3.npy', llayer3); \n",
    "np.save(current_exp_name+'/llayer4.npy', llayer4);  np.save(current_exp_name+'/llayer5.npy', llayer5);  np.save(current_exp_name+'/llayer6.npy', llayer6); \n",
    "\n",
    "np.save(current_exp_name+'/llayer1a.npy', llayer1a);  np.save(current_exp_name+'/llayer2a.npy', llayer2a);  np.save(current_exp_name+'/llayer3a.npy', llayer3a); \n",
    "np.save(current_exp_name+'/llayer4a.npy', llayer4a);  np.save(current_exp_name+'/llayer5a.npy', llayer5a);  np.save(current_exp_name+'/llayer6a.npy', llayer6a); \n",
    "\n",
    "np.save(current_exp_name+'/weight1.npy', weight1);  np.save(current_exp_name+'/weight2.npy', weight2);  np.save(current_exp_name+'/weight3.npy', weight3);  \n",
    "np.save(current_exp_name+'/weight4.npy', weight4);  np.save(current_exp_name+'/weight5.npy', weight5);  np.save(current_exp_name+'/weight6.npy', weight6);  \n",
    "\n",
    "np.save(current_exp_name+'/gradw1.npy', gradw1); np.save(current_exp_name+'/gradw2.npy', gradw2); np.save(current_exp_name+'/gradw3.npy', gradw3);\n",
    "np.save(current_exp_name+'/gradw4.npy', gradw4); np.save(current_exp_name+'/gradw5.npy', gradw5); np.save(current_exp_name+'/gradw6.npy', gradw6);\n",
    "\n",
    "np.save(current_exp_name+'/gradp1.npy', gradp1); np.save(current_exp_name+'/gradp2.npy', gradp2); np.save(current_exp_name+'/gradp3.npy', gradp3);\n",
    "np.save(current_exp_name+'/gradp4.npy', gradp4); np.save(current_exp_name+'/gradp5.npy', gradp5); np.save(current_exp_name+'/gradp6.npy', gradp6);\n",
    "\n",
    "np.save(current_exp_name+'/gradup1.npy', gradup1); np.save(current_exp_name+'/gradup2.npy', gradup2); np.save(current_exp_name+'/gradup3.npy', gradup3);\n",
    "np.save(current_exp_name+'/gradup4.npy', gradup4); np.save(current_exp_name+'/gradup5.npy', gradup5); np.save(current_exp_name+'/gradup6.npy', gradup6);\n",
    "\n",
    "sess.close(); tf.reset_default_graph();\n",
    "\n",
    "%reset_selective -f train_acc,test_acc,llayer1,llayer2,llayer3,llayer4,llayer5,llayer6,llayer1a,llayer2a,llayer3a,llayer4a,llayer5a,llayer6a\n",
    "%reset_selective -f weight1,weight2,weight3,weight4,weight5,weight6\n",
    "%reset_selective -f gradw1,gradw2,gradw3,gradw4,gradw5,gradw6\n",
    "%reset_selective -f gradp1,gradp2,gradp3,gradp4,gradp5,gradp6\n",
    "%reset_selective -f gradup1,gradup2,gradup3,gradup4,gradup5,gradup6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-05T01:05:02.810728Z",
     "start_time": "2019-01-05T00:53:26.633Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# G\n",
    "current_exp_name = 'G';\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# create layers\n",
    "l1 = CNN(3,3, 16,which_reg=current_exp_name); \n",
    "l2 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "l3 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "\n",
    "l4 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "l5 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "l6 = CNN(3,16,10,which_reg=current_exp_name); \n",
    "\n",
    "# 2. graph \n",
    "x = tf.placeholder(tf.float32,(batch_size,96,96,3))\n",
    "y = tf.placeholder(tf.float32,(batch_size,10))\n",
    "\n",
    "layer1, layer1a = l1. feedforward(x,stride=2)\n",
    "layer2, layer2a = l2. feedforward(layer1a,stride=2)\n",
    "layer3, layer3a = l3. feedforward(layer2a,stride=2)\n",
    "layer4, layer4a = l4. feedforward(layer3a,stride=2)\n",
    "layer5, layer5a = l5. feedforward(layer4a)\n",
    "layer6, layer6a = l6. feedforward(layer5a)\n",
    "\n",
    "final_layer   = tf.reduce_mean(layer6a,(1,2))\n",
    "final_softmax = tf_softmax(final_layer)\n",
    "cost          = -tf.reduce_mean(y * tf.log(final_softmax + 1e-8))\n",
    "correct_prediction = tf.equal(tf.argmax(final_softmax, 1), tf.argmax(y, 1))\n",
    "accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "gradient = tf.tile((final_softmax-y)[:,None,None,:],[1,6,6,1])/batch_size\n",
    "grad6p,grad6w,grad6_up = l6.backprop(gradient)\n",
    "grad5p,grad5w,grad5_up = l5.backprop(grad6p)\n",
    "grad4p,grad4w,grad4_up = l4.backprop(grad5p,stride=2)\n",
    "grad3p,grad3w,grad3_up = l3.backprop(grad4p,stride=2)\n",
    "grad2p,grad2w,grad2_up = l2.backprop(grad3p,stride=2)\n",
    "grad1p,grad1w,grad1_up = l1.backprop(grad2p,stride=2)\n",
    "\n",
    "gradient_update = grad6_up + grad5_up + grad4_up + grad3_up + grad2_up + grad1_up \n",
    "\n",
    "# train\n",
    "sess.run(tf.global_variables_initializer())\n",
    "avg_acc_train = 0; avg_acc_test  = 0; train_acc = [];test_acc = []\n",
    "\n",
    "# mean std skew kurt non-zero\n",
    "llayer1 = [[],[],[],[],[]]; llayer2 = [[],[],[],[],[]]; llayer3 = [[],[],[],[],[]]\n",
    "llayer4 = [[],[],[],[],[]]; llayer5 = [[],[],[],[],[]]; llayer6 = [[],[],[],[],[]]\n",
    "\n",
    "llayer1a = [[],[],[],[],[]]; llayer2a = [[],[],[],[],[]]; llayer3a = [[],[],[],[],[]]\n",
    "llayer4a = [[],[],[],[],[]]; llayer5a = [[],[],[],[],[]]; llayer6a = [[],[],[],[],[]]\n",
    "\n",
    "weight1 = [[],[],[],[],[]]; weight2 = [[],[],[],[],[]]; weight3 = [[],[],[],[],[]];\n",
    "weight4 = [[],[],[],[],[]]; weight5 = [[],[],[],[],[]]; weight6 = [[],[],[],[],[]];\n",
    "\n",
    "gradw1  = [[],[],[],[],[]]; gradw2  = [[],[],[],[],[]]; gradw3  = [[],[],[],[],[]];\n",
    "gradw4  = [[],[],[],[],[]]; gradw5  = [[],[],[],[],[]]; gradw6  = [[],[],[],[],[]];\n",
    "\n",
    "gradp1  = [[],[],[],[],[]]; gradp2  = [[],[],[],[],[]]; gradp3  = [[],[],[],[],[]];\n",
    "gradp4  = [[],[],[],[],[]]; gradp5  = [[],[],[],[],[]]; gradp6  = [[],[],[],[],[]];\n",
    "\n",
    "gradup1  = [[],[],[],[],[]]; gradup2  = [[],[],[],[],[]]; gradup3  = [[],[],[],[],[]];\n",
    "gradup4  = [[],[],[],[],[]]; gradup5  = [[],[],[],[],[]]; gradup6  = [[],[],[],[],[]];\n",
    "\n",
    "list_of_outputs = [\n",
    "    layer1,layer2,layer3,layer4,layer5,layer6,\n",
    "    layer1a,layer2a,layer3a,layer4a,layer5a,layer6a,\n",
    "    l1.getw(),l2.getw(),l3.getw(),l4.getw(),l5.getw(),l6.getw(),\n",
    "    grad1w,grad2w,grad3w,grad4w,grad5w,grad6w,\n",
    "    grad1p,grad2p,grad3p,grad4p,grad5p,grad6p,\n",
    "    grad1_up[0],grad2_up[0],grad3_up[0],grad4_up[0],grad5_up[0],grad6_up[0]\n",
    "]\n",
    "\n",
    "for iter in range(num_epoch):\n",
    "\n",
    "    # Training Accuracy    \n",
    "    for current_batch_index in range(0,len(train_images),batch_size):\n",
    "        current_data  = train_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = train_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy,gradient_update],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(train_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_train = avg_acc_train + sess_results[0]\n",
    "        \n",
    "    # get the results\n",
    "    mid_stat = sess.run(list_of_outputs,feed_dict={x:current_data,y:current_label})\n",
    "    \n",
    "    # Test Accuracy    \n",
    "    for current_batch_index in range(0,len(test_images), batch_size):\n",
    "        current_data  = test_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = test_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(test_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_test = avg_acc_test + sess_results[0]   \n",
    "        \n",
    "    # ======================== extract stats ========================\n",
    "    llayer1 = append_stat(llayer1,mid_stat,0);  llayer2 = append_stat(llayer2,mid_stat,1);  llayer3 = append_stat(llayer3,mid_stat,2);\n",
    "    llayer4 = append_stat(llayer4,mid_stat,3);  llayer5 = append_stat(llayer5,mid_stat,4);  llayer6 = append_stat(llayer6,mid_stat,5);\n",
    "\n",
    "    llayer1a = append_stat(llayer1a,mid_stat,6);  llayer2a = append_stat(llayer2a,mid_stat,7);  llayer3a = append_stat(llayer3a,mid_stat,8);\n",
    "    llayer4a = append_stat(llayer4a,mid_stat,9);  llayer5a = append_stat(llayer5a,mid_stat,10); llayer6a = append_stat(llayer6a,mid_stat,11);\n",
    "    \n",
    "    weight1 = append_stat(weight1,mid_stat,12);  weight2 = append_stat(weight2,mid_stat,13);  weight3 = append_stat(weight3,mid_stat,14);\n",
    "    weight4 = append_stat(weight4,mid_stat,15);  weight5 = append_stat(weight5,mid_stat,16);  weight6 = append_stat(weight6,mid_stat,17);\n",
    "    \n",
    "    gradw1 = append_stat(gradw1,mid_stat,18); gradw2 = append_stat(gradw2,mid_stat,19); gradw3 = append_stat(gradw3,mid_stat,20);\n",
    "    gradw4 = append_stat(gradw4,mid_stat,21); gradw5 = append_stat(gradw5,mid_stat,22); gradw6 = append_stat(gradw6,mid_stat,23);\n",
    "    \n",
    "    gradp1 = append_stat(gradp1,mid_stat,24); gradp2 = append_stat(gradp2,mid_stat,25); gradp3 = append_stat(gradp3,mid_stat,26);\n",
    "    gradp4 = append_stat(gradp4,mid_stat,27); gradp5 = append_stat(gradp5,mid_stat,28); gradp6 = append_stat(gradp6,mid_stat,29);\n",
    "\n",
    "    gradup1 = append_stat(gradup1,mid_stat,30); gradup2 = append_stat(gradup2,mid_stat,31); gradup3 = append_stat(gradup3,mid_stat,32);\n",
    "    gradup4 = append_stat(gradup4,mid_stat,33); gradup5 = append_stat(gradup5,mid_stat,34); gradup6 = append_stat(gradup6,mid_stat,35);\n",
    "\n",
    "    train_acc.append(avg_acc_train/(len(train_images)/batch_size))\n",
    "    test_acc .append(avg_acc_test / (len(test_images)/batch_size))\n",
    "    # ======================== extract stats ========================\n",
    "    \n",
    "    # ======================== save to image ========================\n",
    "    save_to_image(mid_stat[0:6]   ,llayer1,llayer2,llayer3,llayer4,llayer5,llayer6,\"layer\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[6:12]  ,llayer1a,llayer2a,llayer3a,llayer4a,llayer5a,llayer6a,\"layera\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[12:18] ,weight1,weight2,weight3,weight4,weight5,weight6,\"weights\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[18:24] ,gradw1,gradw2,gradw3,gradw4,gradw5,gradw6,\"gradientw\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[24:30] ,gradp1,gradp2,gradp3,gradp4,gradp5,gradp6,\"gradientp\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[30:36] ,gradup1,gradup2,gradup3,gradup4,gradup5,gradup6,\"moment\",train_acc,test_acc,current_exp_name,iter)\n",
    "    # ======================== save to image ========================\n",
    "        \n",
    "    # ======================== print reset ========================\n",
    "    print(\"Current : \"+ str(iter) + \" Train Acc : \" + str(avg_acc_train/(len(train_images)/batch_size)) + \" Test Acc : \" + str(avg_acc_test/(len(test_images)/batch_size)) + '\\n')\n",
    "    avg_acc_train = 0 ; avg_acc_test  = 0\n",
    "    # ======================== print reset ========================\n",
    "\n",
    "np.save(current_exp_name+'/train_acc.npy',train_acc); np.save(current_exp_name+'/test_acc.npy', test_acc)    \n",
    "np.save(current_exp_name+'/llayer1.npy', llayer1);  np.save(current_exp_name+'/llayer2.npy', llayer2);  np.save(current_exp_name+'/llayer3.npy', llayer3); \n",
    "np.save(current_exp_name+'/llayer4.npy', llayer4);  np.save(current_exp_name+'/llayer5.npy', llayer5);  np.save(current_exp_name+'/llayer6.npy', llayer6); \n",
    "\n",
    "np.save(current_exp_name+'/llayer1a.npy', llayer1a);  np.save(current_exp_name+'/llayer2a.npy', llayer2a);  np.save(current_exp_name+'/llayer3a.npy', llayer3a); \n",
    "np.save(current_exp_name+'/llayer4a.npy', llayer4a);  np.save(current_exp_name+'/llayer5a.npy', llayer5a);  np.save(current_exp_name+'/llayer6a.npy', llayer6a); \n",
    "\n",
    "np.save(current_exp_name+'/weight1.npy', weight1);  np.save(current_exp_name+'/weight2.npy', weight2);  np.save(current_exp_name+'/weight3.npy', weight3);  \n",
    "np.save(current_exp_name+'/weight4.npy', weight4);  np.save(current_exp_name+'/weight5.npy', weight5);  np.save(current_exp_name+'/weight6.npy', weight6);  \n",
    "\n",
    "np.save(current_exp_name+'/gradw1.npy', gradw1); np.save(current_exp_name+'/gradw2.npy', gradw2); np.save(current_exp_name+'/gradw3.npy', gradw3);\n",
    "np.save(current_exp_name+'/gradw4.npy', gradw4); np.save(current_exp_name+'/gradw5.npy', gradw5); np.save(current_exp_name+'/gradw6.npy', gradw6);\n",
    "\n",
    "np.save(current_exp_name+'/gradp1.npy', gradp1); np.save(current_exp_name+'/gradp2.npy', gradp2); np.save(current_exp_name+'/gradp3.npy', gradp3);\n",
    "np.save(current_exp_name+'/gradp4.npy', gradp4); np.save(current_exp_name+'/gradp5.npy', gradp5); np.save(current_exp_name+'/gradp6.npy', gradp6);\n",
    "\n",
    "np.save(current_exp_name+'/gradup1.npy', gradup1); np.save(current_exp_name+'/gradup2.npy', gradup2); np.save(current_exp_name+'/gradup3.npy', gradup3);\n",
    "np.save(current_exp_name+'/gradup4.npy', gradup4); np.save(current_exp_name+'/gradup5.npy', gradup5); np.save(current_exp_name+'/gradup6.npy', gradup6);\n",
    "\n",
    "sess.close(); tf.reset_default_graph();\n",
    "\n",
    "%reset_selective -f train_acc,test_acc,llayer1,llayer2,llayer3,llayer4,llayer5,llayer6,llayer1a,llayer2a,llayer3a,llayer4a,llayer5a,llayer6a\n",
    "%reset_selective -f weight1,weight2,weight3,weight4,weight5,weight6\n",
    "%reset_selective -f gradw1,gradw2,gradw3,gradw4,gradw5,gradw6\n",
    "%reset_selective -f gradp1,gradp2,gradp3,gradp4,gradp5,gradp6\n",
    "%reset_selective -f gradup1,gradup2,gradup3,gradup4,gradup5,gradup6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-05T01:05:02.812733Z",
     "start_time": "2019-01-05T00:53:26.636Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# H\n",
    "current_exp_name = 'H';\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# create layers\n",
    "l1 = CNN(3,3, 16,which_reg=current_exp_name); \n",
    "l2 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "l3 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "\n",
    "l4 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "l5 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "l6 = CNN(3,16,10,which_reg=current_exp_name); \n",
    "\n",
    "# 2. graph \n",
    "x = tf.placeholder(tf.float32,(batch_size,96,96,3))\n",
    "y = tf.placeholder(tf.float32,(batch_size,10))\n",
    "\n",
    "layer1, layer1a = l1. feedforward(x,stride=2)\n",
    "layer2, layer2a = l2. feedforward(layer1a,stride=2)\n",
    "layer3, layer3a = l3. feedforward(layer2a,stride=2)\n",
    "layer4, layer4a = l4. feedforward(layer3a,stride=2)\n",
    "layer5, layer5a = l5. feedforward(layer4a)\n",
    "layer6, layer6a = l6. feedforward(layer5a)\n",
    "\n",
    "final_layer   = tf.reduce_mean(layer6a,(1,2))\n",
    "final_softmax = tf_softmax(final_layer)\n",
    "cost          = -tf.reduce_mean(y * tf.log(final_softmax + 1e-8))\n",
    "correct_prediction = tf.equal(tf.argmax(final_softmax, 1), tf.argmax(y, 1))\n",
    "accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "gradient = tf.tile((final_softmax-y)[:,None,None,:],[1,6,6,1])/batch_size\n",
    "grad6p,grad6w,grad6_up = l6.backprop(gradient)\n",
    "grad5p,grad5w,grad5_up = l5.backprop(grad6p)\n",
    "grad4p,grad4w,grad4_up = l4.backprop(grad5p,stride=2)\n",
    "grad3p,grad3w,grad3_up = l3.backprop(grad4p,stride=2)\n",
    "grad2p,grad2w,grad2_up = l2.backprop(grad3p,stride=2)\n",
    "grad1p,grad1w,grad1_up = l1.backprop(grad2p,stride=2)\n",
    "\n",
    "gradient_update = grad6_up + grad5_up + grad4_up + grad3_up + grad2_up + grad1_up \n",
    "\n",
    "# train\n",
    "sess.run(tf.global_variables_initializer())\n",
    "avg_acc_train = 0; avg_acc_test  = 0; train_acc = [];test_acc = []\n",
    "\n",
    "# mean std skew kurt non-zero\n",
    "llayer1 = [[],[],[],[],[]]; llayer2 = [[],[],[],[],[]]; llayer3 = [[],[],[],[],[]]\n",
    "llayer4 = [[],[],[],[],[]]; llayer5 = [[],[],[],[],[]]; llayer6 = [[],[],[],[],[]]\n",
    "\n",
    "llayer1a = [[],[],[],[],[]]; llayer2a = [[],[],[],[],[]]; llayer3a = [[],[],[],[],[]]\n",
    "llayer4a = [[],[],[],[],[]]; llayer5a = [[],[],[],[],[]]; llayer6a = [[],[],[],[],[]]\n",
    "\n",
    "weight1 = [[],[],[],[],[]]; weight2 = [[],[],[],[],[]]; weight3 = [[],[],[],[],[]];\n",
    "weight4 = [[],[],[],[],[]]; weight5 = [[],[],[],[],[]]; weight6 = [[],[],[],[],[]];\n",
    "\n",
    "gradw1  = [[],[],[],[],[]]; gradw2  = [[],[],[],[],[]]; gradw3  = [[],[],[],[],[]];\n",
    "gradw4  = [[],[],[],[],[]]; gradw5  = [[],[],[],[],[]]; gradw6  = [[],[],[],[],[]];\n",
    "\n",
    "gradp1  = [[],[],[],[],[]]; gradp2  = [[],[],[],[],[]]; gradp3  = [[],[],[],[],[]];\n",
    "gradp4  = [[],[],[],[],[]]; gradp5  = [[],[],[],[],[]]; gradp6  = [[],[],[],[],[]];\n",
    "\n",
    "gradup1  = [[],[],[],[],[]]; gradup2  = [[],[],[],[],[]]; gradup3  = [[],[],[],[],[]];\n",
    "gradup4  = [[],[],[],[],[]]; gradup5  = [[],[],[],[],[]]; gradup6  = [[],[],[],[],[]];\n",
    "\n",
    "list_of_outputs = [\n",
    "    layer1,layer2,layer3,layer4,layer5,layer6,\n",
    "    layer1a,layer2a,layer3a,layer4a,layer5a,layer6a,\n",
    "    l1.getw(),l2.getw(),l3.getw(),l4.getw(),l5.getw(),l6.getw(),\n",
    "    grad1w,grad2w,grad3w,grad4w,grad5w,grad6w,\n",
    "    grad1p,grad2p,grad3p,grad4p,grad5p,grad6p,\n",
    "    grad1_up[0],grad2_up[0],grad3_up[0],grad4_up[0],grad5_up[0],grad6_up[0]\n",
    "]\n",
    "\n",
    "for iter in range(num_epoch):\n",
    "\n",
    "    # Training Accuracy    \n",
    "    for current_batch_index in range(0,len(train_images),batch_size):\n",
    "        current_data  = train_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = train_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy,gradient_update],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(train_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_train = avg_acc_train + sess_results[0]\n",
    "        \n",
    "    # get the results\n",
    "    mid_stat = sess.run(list_of_outputs,feed_dict={x:current_data,y:current_label})\n",
    "    \n",
    "    # Test Accuracy    \n",
    "    for current_batch_index in range(0,len(test_images), batch_size):\n",
    "        current_data  = test_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = test_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(test_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_test = avg_acc_test + sess_results[0]   \n",
    "        \n",
    "    # ======================== extract stats ========================\n",
    "    llayer1 = append_stat(llayer1,mid_stat,0);  llayer2 = append_stat(llayer2,mid_stat,1);  llayer3 = append_stat(llayer3,mid_stat,2);\n",
    "    llayer4 = append_stat(llayer4,mid_stat,3);  llayer5 = append_stat(llayer5,mid_stat,4);  llayer6 = append_stat(llayer6,mid_stat,5);\n",
    "\n",
    "    llayer1a = append_stat(llayer1a,mid_stat,6);  llayer2a = append_stat(llayer2a,mid_stat,7);  llayer3a = append_stat(llayer3a,mid_stat,8);\n",
    "    llayer4a = append_stat(llayer4a,mid_stat,9);  llayer5a = append_stat(llayer5a,mid_stat,10); llayer6a = append_stat(llayer6a,mid_stat,11);\n",
    "    \n",
    "    weight1 = append_stat(weight1,mid_stat,12);  weight2 = append_stat(weight2,mid_stat,13);  weight3 = append_stat(weight3,mid_stat,14);\n",
    "    weight4 = append_stat(weight4,mid_stat,15);  weight5 = append_stat(weight5,mid_stat,16);  weight6 = append_stat(weight6,mid_stat,17);\n",
    "    \n",
    "    gradw1 = append_stat(gradw1,mid_stat,18); gradw2 = append_stat(gradw2,mid_stat,19); gradw3 = append_stat(gradw3,mid_stat,20);\n",
    "    gradw4 = append_stat(gradw4,mid_stat,21); gradw5 = append_stat(gradw5,mid_stat,22); gradw6 = append_stat(gradw6,mid_stat,23);\n",
    "    \n",
    "    gradp1 = append_stat(gradp1,mid_stat,24); gradp2 = append_stat(gradp2,mid_stat,25); gradp3 = append_stat(gradp3,mid_stat,26);\n",
    "    gradp4 = append_stat(gradp4,mid_stat,27); gradp5 = append_stat(gradp5,mid_stat,28); gradp6 = append_stat(gradp6,mid_stat,29);\n",
    "\n",
    "    gradup1 = append_stat(gradup1,mid_stat,30); gradup2 = append_stat(gradup2,mid_stat,31); gradup3 = append_stat(gradup3,mid_stat,32);\n",
    "    gradup4 = append_stat(gradup4,mid_stat,33); gradup5 = append_stat(gradup5,mid_stat,34); gradup6 = append_stat(gradup6,mid_stat,35);\n",
    "\n",
    "    train_acc.append(avg_acc_train/(len(train_images)/batch_size))\n",
    "    test_acc .append(avg_acc_test / (len(test_images)/batch_size))\n",
    "    # ======================== extract stats ========================\n",
    "    \n",
    "    # ======================== save to image ========================\n",
    "    save_to_image(mid_stat[0:6]   ,llayer1,llayer2,llayer3,llayer4,llayer5,llayer6,\"layer\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[6:12]  ,llayer1a,llayer2a,llayer3a,llayer4a,llayer5a,llayer6a,\"layera\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[12:18] ,weight1,weight2,weight3,weight4,weight5,weight6,\"weights\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[18:24] ,gradw1,gradw2,gradw3,gradw4,gradw5,gradw6,\"gradientw\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[24:30] ,gradp1,gradp2,gradp3,gradp4,gradp5,gradp6,\"gradientp\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[30:36] ,gradup1,gradup2,gradup3,gradup4,gradup5,gradup6,\"moment\",train_acc,test_acc,current_exp_name,iter)\n",
    "    # ======================== save to image ========================\n",
    "        \n",
    "    # ======================== print reset ========================\n",
    "    print(\"Current : \"+ str(iter) + \" Train Acc : \" + str(avg_acc_train/(len(train_images)/batch_size)) + \" Test Acc : \" + str(avg_acc_test/(len(test_images)/batch_size)) + '\\n')\n",
    "    avg_acc_train = 0 ; avg_acc_test  = 0\n",
    "    # ======================== print reset ========================\n",
    "\n",
    "np.save(current_exp_name+'/train_acc.npy',train_acc); np.save(current_exp_name+'/test_acc.npy', test_acc)    \n",
    "np.save(current_exp_name+'/llayer1.npy', llayer1);  np.save(current_exp_name+'/llayer2.npy', llayer2);  np.save(current_exp_name+'/llayer3.npy', llayer3); \n",
    "np.save(current_exp_name+'/llayer4.npy', llayer4);  np.save(current_exp_name+'/llayer5.npy', llayer5);  np.save(current_exp_name+'/llayer6.npy', llayer6); \n",
    "\n",
    "np.save(current_exp_name+'/llayer1a.npy', llayer1a);  np.save(current_exp_name+'/llayer2a.npy', llayer2a);  np.save(current_exp_name+'/llayer3a.npy', llayer3a); \n",
    "np.save(current_exp_name+'/llayer4a.npy', llayer4a);  np.save(current_exp_name+'/llayer5a.npy', llayer5a);  np.save(current_exp_name+'/llayer6a.npy', llayer6a); \n",
    "\n",
    "np.save(current_exp_name+'/weight1.npy', weight1);  np.save(current_exp_name+'/weight2.npy', weight2);  np.save(current_exp_name+'/weight3.npy', weight3);  \n",
    "np.save(current_exp_name+'/weight4.npy', weight4);  np.save(current_exp_name+'/weight5.npy', weight5);  np.save(current_exp_name+'/weight6.npy', weight6);  \n",
    "\n",
    "np.save(current_exp_name+'/gradw1.npy', gradw1); np.save(current_exp_name+'/gradw2.npy', gradw2); np.save(current_exp_name+'/gradw3.npy', gradw3);\n",
    "np.save(current_exp_name+'/gradw4.npy', gradw4); np.save(current_exp_name+'/gradw5.npy', gradw5); np.save(current_exp_name+'/gradw6.npy', gradw6);\n",
    "\n",
    "np.save(current_exp_name+'/gradp1.npy', gradp1); np.save(current_exp_name+'/gradp2.npy', gradp2); np.save(current_exp_name+'/gradp3.npy', gradp3);\n",
    "np.save(current_exp_name+'/gradp4.npy', gradp4); np.save(current_exp_name+'/gradp5.npy', gradp5); np.save(current_exp_name+'/gradp6.npy', gradp6);\n",
    "\n",
    "np.save(current_exp_name+'/gradup1.npy', gradup1); np.save(current_exp_name+'/gradup2.npy', gradup2); np.save(current_exp_name+'/gradup3.npy', gradup3);\n",
    "np.save(current_exp_name+'/gradup4.npy', gradup4); np.save(current_exp_name+'/gradup5.npy', gradup5); np.save(current_exp_name+'/gradup6.npy', gradup6);\n",
    "\n",
    "sess.close(); tf.reset_default_graph();\n",
    "\n",
    "%reset_selective -f train_acc,test_acc,llayer1,llayer2,llayer3,llayer4,llayer5,llayer6,llayer1a,llayer2a,llayer3a,llayer4a,llayer5a,llayer6a\n",
    "%reset_selective -f weight1,weight2,weight3,weight4,weight5,weight6\n",
    "%reset_selective -f gradw1,gradw2,gradw3,gradw4,gradw5,gradw6\n",
    "%reset_selective -f gradp1,gradp2,gradp3,gradp4,gradp5,gradp6\n",
    "%reset_selective -f gradup1,gradup2,gradup3,gradup4,gradup5,gradup6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-05T01:05:02.813721Z",
     "start_time": "2019-01-05T00:53:26.640Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# I\n",
    "current_exp_name = 'I';\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# create layers\n",
    "l1 = CNN(3,3, 16,which_reg=current_exp_name); \n",
    "l2 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "l3 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "\n",
    "l4 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "l5 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "l6 = CNN(3,16,10,which_reg=current_exp_name); \n",
    "\n",
    "# 2. graph \n",
    "x = tf.placeholder(tf.float32,(batch_size,96,96,3))\n",
    "y = tf.placeholder(tf.float32,(batch_size,10))\n",
    "\n",
    "layer1, layer1a = l1. feedforward(x,stride=2)\n",
    "layer2, layer2a = l2. feedforward(layer1a,stride=2)\n",
    "layer3, layer3a = l3. feedforward(layer2a,stride=2)\n",
    "layer4, layer4a = l4. feedforward(layer3a,stride=2)\n",
    "layer5, layer5a = l5. feedforward(layer4a)\n",
    "layer6, layer6a = l6. feedforward(layer5a)\n",
    "\n",
    "final_layer   = tf.reduce_mean(layer6a,(1,2))\n",
    "final_softmax = tf_softmax(final_layer)\n",
    "cost          = -tf.reduce_mean(y * tf.log(final_softmax + 1e-8))\n",
    "correct_prediction = tf.equal(tf.argmax(final_softmax, 1), tf.argmax(y, 1))\n",
    "accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "gradient = tf.tile((final_softmax-y)[:,None,None,:],[1,6,6,1])/batch_size\n",
    "grad6p,grad6w,grad6_up = l6.backprop(gradient)\n",
    "grad5p,grad5w,grad5_up = l5.backprop(grad6p)\n",
    "grad4p,grad4w,grad4_up = l4.backprop(grad5p,stride=2)\n",
    "grad3p,grad3w,grad3_up = l3.backprop(grad4p,stride=2)\n",
    "grad2p,grad2w,grad2_up = l2.backprop(grad3p,stride=2)\n",
    "grad1p,grad1w,grad1_up = l1.backprop(grad2p,stride=2)\n",
    "\n",
    "gradient_update = grad6_up + grad5_up + grad4_up + grad3_up + grad2_up + grad1_up \n",
    "\n",
    "# train\n",
    "sess.run(tf.global_variables_initializer())\n",
    "avg_acc_train = 0; avg_acc_test  = 0; train_acc = [];test_acc = []\n",
    "\n",
    "# mean std skew kurt non-zero\n",
    "llayer1 = [[],[],[],[],[]]; llayer2 = [[],[],[],[],[]]; llayer3 = [[],[],[],[],[]]\n",
    "llayer4 = [[],[],[],[],[]]; llayer5 = [[],[],[],[],[]]; llayer6 = [[],[],[],[],[]]\n",
    "\n",
    "llayer1a = [[],[],[],[],[]]; llayer2a = [[],[],[],[],[]]; llayer3a = [[],[],[],[],[]]\n",
    "llayer4a = [[],[],[],[],[]]; llayer5a = [[],[],[],[],[]]; llayer6a = [[],[],[],[],[]]\n",
    "\n",
    "weight1 = [[],[],[],[],[]]; weight2 = [[],[],[],[],[]]; weight3 = [[],[],[],[],[]];\n",
    "weight4 = [[],[],[],[],[]]; weight5 = [[],[],[],[],[]]; weight6 = [[],[],[],[],[]];\n",
    "\n",
    "gradw1  = [[],[],[],[],[]]; gradw2  = [[],[],[],[],[]]; gradw3  = [[],[],[],[],[]];\n",
    "gradw4  = [[],[],[],[],[]]; gradw5  = [[],[],[],[],[]]; gradw6  = [[],[],[],[],[]];\n",
    "\n",
    "gradp1  = [[],[],[],[],[]]; gradp2  = [[],[],[],[],[]]; gradp3  = [[],[],[],[],[]];\n",
    "gradp4  = [[],[],[],[],[]]; gradp5  = [[],[],[],[],[]]; gradp6  = [[],[],[],[],[]];\n",
    "\n",
    "gradup1  = [[],[],[],[],[]]; gradup2  = [[],[],[],[],[]]; gradup3  = [[],[],[],[],[]];\n",
    "gradup4  = [[],[],[],[],[]]; gradup5  = [[],[],[],[],[]]; gradup6  = [[],[],[],[],[]];\n",
    "\n",
    "list_of_outputs = [\n",
    "    layer1,layer2,layer3,layer4,layer5,layer6,\n",
    "    layer1a,layer2a,layer3a,layer4a,layer5a,layer6a,\n",
    "    l1.getw(),l2.getw(),l3.getw(),l4.getw(),l5.getw(),l6.getw(),\n",
    "    grad1w,grad2w,grad3w,grad4w,grad5w,grad6w,\n",
    "    grad1p,grad2p,grad3p,grad4p,grad5p,grad6p,\n",
    "    grad1_up[0],grad2_up[0],grad3_up[0],grad4_up[0],grad5_up[0],grad6_up[0]\n",
    "]\n",
    "\n",
    "for iter in range(num_epoch):\n",
    "\n",
    "    # Training Accuracy    \n",
    "    for current_batch_index in range(0,len(train_images),batch_size):\n",
    "        current_data  = train_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = train_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy,gradient_update],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(train_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_train = avg_acc_train + sess_results[0]\n",
    "        \n",
    "    # get the results\n",
    "    mid_stat = sess.run(list_of_outputs,feed_dict={x:current_data,y:current_label})\n",
    "    \n",
    "    # Test Accuracy    \n",
    "    for current_batch_index in range(0,len(test_images), batch_size):\n",
    "        current_data  = test_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = test_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(test_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_test = avg_acc_test + sess_results[0]   \n",
    "        \n",
    "    # ======================== extract stats ========================\n",
    "    llayer1 = append_stat(llayer1,mid_stat,0);  llayer2 = append_stat(llayer2,mid_stat,1);  llayer3 = append_stat(llayer3,mid_stat,2);\n",
    "    llayer4 = append_stat(llayer4,mid_stat,3);  llayer5 = append_stat(llayer5,mid_stat,4);  llayer6 = append_stat(llayer6,mid_stat,5);\n",
    "\n",
    "    llayer1a = append_stat(llayer1a,mid_stat,6);  llayer2a = append_stat(llayer2a,mid_stat,7);  llayer3a = append_stat(llayer3a,mid_stat,8);\n",
    "    llayer4a = append_stat(llayer4a,mid_stat,9);  llayer5a = append_stat(llayer5a,mid_stat,10); llayer6a = append_stat(llayer6a,mid_stat,11);\n",
    "    \n",
    "    weight1 = append_stat(weight1,mid_stat,12);  weight2 = append_stat(weight2,mid_stat,13);  weight3 = append_stat(weight3,mid_stat,14);\n",
    "    weight4 = append_stat(weight4,mid_stat,15);  weight5 = append_stat(weight5,mid_stat,16);  weight6 = append_stat(weight6,mid_stat,17);\n",
    "    \n",
    "    gradw1 = append_stat(gradw1,mid_stat,18); gradw2 = append_stat(gradw2,mid_stat,19); gradw3 = append_stat(gradw3,mid_stat,20);\n",
    "    gradw4 = append_stat(gradw4,mid_stat,21); gradw5 = append_stat(gradw5,mid_stat,22); gradw6 = append_stat(gradw6,mid_stat,23);\n",
    "    \n",
    "    gradp1 = append_stat(gradp1,mid_stat,24); gradp2 = append_stat(gradp2,mid_stat,25); gradp3 = append_stat(gradp3,mid_stat,26);\n",
    "    gradp4 = append_stat(gradp4,mid_stat,27); gradp5 = append_stat(gradp5,mid_stat,28); gradp6 = append_stat(gradp6,mid_stat,29);\n",
    "\n",
    "    gradup1 = append_stat(gradup1,mid_stat,30); gradup2 = append_stat(gradup2,mid_stat,31); gradup3 = append_stat(gradup3,mid_stat,32);\n",
    "    gradup4 = append_stat(gradup4,mid_stat,33); gradup5 = append_stat(gradup5,mid_stat,34); gradup6 = append_stat(gradup6,mid_stat,35);\n",
    "\n",
    "    train_acc.append(avg_acc_train/(len(train_images)/batch_size))\n",
    "    test_acc .append(avg_acc_test / (len(test_images)/batch_size))\n",
    "    # ======================== extract stats ========================\n",
    "    \n",
    "    # ======================== save to image ========================\n",
    "    save_to_image(mid_stat[0:6]   ,llayer1,llayer2,llayer3,llayer4,llayer5,llayer6,\"layer\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[6:12]  ,llayer1a,llayer2a,llayer3a,llayer4a,llayer5a,llayer6a,\"layera\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[12:18] ,weight1,weight2,weight3,weight4,weight5,weight6,\"weights\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[18:24] ,gradw1,gradw2,gradw3,gradw4,gradw5,gradw6,\"gradientw\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[24:30] ,gradp1,gradp2,gradp3,gradp4,gradp5,gradp6,\"gradientp\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[30:36] ,gradup1,gradup2,gradup3,gradup4,gradup5,gradup6,\"moment\",train_acc,test_acc,current_exp_name,iter)\n",
    "    # ======================== save to image ========================\n",
    "        \n",
    "    # ======================== print reset ========================\n",
    "    print(\"Current : \"+ str(iter) + \" Train Acc : \" + str(avg_acc_train/(len(train_images)/batch_size)) + \" Test Acc : \" + str(avg_acc_test/(len(test_images)/batch_size)) + '\\n')\n",
    "    avg_acc_train = 0 ; avg_acc_test  = 0\n",
    "    # ======================== print reset ========================\n",
    "\n",
    "np.save(current_exp_name+'/train_acc.npy',train_acc); np.save(current_exp_name+'/test_acc.npy', test_acc)    \n",
    "np.save(current_exp_name+'/llayer1.npy', llayer1);  np.save(current_exp_name+'/llayer2.npy', llayer2);  np.save(current_exp_name+'/llayer3.npy', llayer3); \n",
    "np.save(current_exp_name+'/llayer4.npy', llayer4);  np.save(current_exp_name+'/llayer5.npy', llayer5);  np.save(current_exp_name+'/llayer6.npy', llayer6); \n",
    "\n",
    "np.save(current_exp_name+'/llayer1a.npy', llayer1a);  np.save(current_exp_name+'/llayer2a.npy', llayer2a);  np.save(current_exp_name+'/llayer3a.npy', llayer3a); \n",
    "np.save(current_exp_name+'/llayer4a.npy', llayer4a);  np.save(current_exp_name+'/llayer5a.npy', llayer5a);  np.save(current_exp_name+'/llayer6a.npy', llayer6a); \n",
    "\n",
    "np.save(current_exp_name+'/weight1.npy', weight1);  np.save(current_exp_name+'/weight2.npy', weight2);  np.save(current_exp_name+'/weight3.npy', weight3);  \n",
    "np.save(current_exp_name+'/weight4.npy', weight4);  np.save(current_exp_name+'/weight5.npy', weight5);  np.save(current_exp_name+'/weight6.npy', weight6);  \n",
    "\n",
    "np.save(current_exp_name+'/gradw1.npy', gradw1); np.save(current_exp_name+'/gradw2.npy', gradw2); np.save(current_exp_name+'/gradw3.npy', gradw3);\n",
    "np.save(current_exp_name+'/gradw4.npy', gradw4); np.save(current_exp_name+'/gradw5.npy', gradw5); np.save(current_exp_name+'/gradw6.npy', gradw6);\n",
    "\n",
    "np.save(current_exp_name+'/gradp1.npy', gradp1); np.save(current_exp_name+'/gradp2.npy', gradp2); np.save(current_exp_name+'/gradp3.npy', gradp3);\n",
    "np.save(current_exp_name+'/gradp4.npy', gradp4); np.save(current_exp_name+'/gradp5.npy', gradp5); np.save(current_exp_name+'/gradp6.npy', gradp6);\n",
    "\n",
    "np.save(current_exp_name+'/gradup1.npy', gradup1); np.save(current_exp_name+'/gradup2.npy', gradup2); np.save(current_exp_name+'/gradup3.npy', gradup3);\n",
    "np.save(current_exp_name+'/gradup4.npy', gradup4); np.save(current_exp_name+'/gradup5.npy', gradup5); np.save(current_exp_name+'/gradup6.npy', gradup6);\n",
    "\n",
    "sess.close(); tf.reset_default_graph();\n",
    "\n",
    "%reset_selective -f train_acc,test_acc,llayer1,llayer2,llayer3,llayer4,llayer5,llayer6,llayer1a,llayer2a,llayer3a,llayer4a,llayer5a,llayer6a\n",
    "%reset_selective -f weight1,weight2,weight3,weight4,weight5,weight6\n",
    "%reset_selective -f gradw1,gradw2,gradw3,gradw4,gradw5,gradw6\n",
    "%reset_selective -f gradp1,gradp2,gradp3,gradp4,gradp5,gradp6\n",
    "%reset_selective -f gradup1,gradup2,gradup3,gradup4,gradup5,gradup6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-05T01:05:02.815715Z",
     "start_time": "2019-01-05T00:53:26.643Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# J\n",
    "current_exp_name = 'J';\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# create layers\n",
    "l1 = CNN(3,3, 16,which_reg=current_exp_name); \n",
    "l2 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "l3 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "\n",
    "l4 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "l5 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "l6 = CNN(3,16,10,which_reg=current_exp_name); \n",
    "\n",
    "# 2. graph \n",
    "x = tf.placeholder(tf.float32,(batch_size,96,96,3))\n",
    "y = tf.placeholder(tf.float32,(batch_size,10))\n",
    "\n",
    "layer1, layer1a = l1. feedforward(x,stride=2)\n",
    "layer2, layer2a = l2. feedforward(layer1a,stride=2)\n",
    "layer3, layer3a = l3. feedforward(layer2a,stride=2)\n",
    "layer4, layer4a = l4. feedforward(layer3a,stride=2)\n",
    "layer5, layer5a = l5. feedforward(layer4a)\n",
    "layer6, layer6a = l6. feedforward(layer5a)\n",
    "\n",
    "final_layer   = tf.reduce_mean(layer6a,(1,2))\n",
    "final_softmax = tf_softmax(final_layer)\n",
    "cost          = -tf.reduce_mean(y * tf.log(final_softmax + 1e-8))\n",
    "correct_prediction = tf.equal(tf.argmax(final_softmax, 1), tf.argmax(y, 1))\n",
    "accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "gradient = tf.tile((final_softmax-y)[:,None,None,:],[1,6,6,1])/batch_size\n",
    "grad6p,grad6w,grad6_up = l6.backprop(gradient)\n",
    "grad5p,grad5w,grad5_up = l5.backprop(grad6p)\n",
    "grad4p,grad4w,grad4_up = l4.backprop(grad5p,stride=2)\n",
    "grad3p,grad3w,grad3_up = l3.backprop(grad4p,stride=2)\n",
    "grad2p,grad2w,grad2_up = l2.backprop(grad3p,stride=2)\n",
    "grad1p,grad1w,grad1_up = l1.backprop(grad2p,stride=2)\n",
    "\n",
    "gradient_update = grad6_up + grad5_up + grad4_up + grad3_up + grad2_up + grad1_up \n",
    "\n",
    "# train\n",
    "sess.run(tf.global_variables_initializer())\n",
    "avg_acc_train = 0; avg_acc_test  = 0; train_acc = [];test_acc = []\n",
    "\n",
    "# mean std skew kurt non-zero\n",
    "llayer1 = [[],[],[],[],[]]; llayer2 = [[],[],[],[],[]]; llayer3 = [[],[],[],[],[]]\n",
    "llayer4 = [[],[],[],[],[]]; llayer5 = [[],[],[],[],[]]; llayer6 = [[],[],[],[],[]]\n",
    "\n",
    "llayer1a = [[],[],[],[],[]]; llayer2a = [[],[],[],[],[]]; llayer3a = [[],[],[],[],[]]\n",
    "llayer4a = [[],[],[],[],[]]; llayer5a = [[],[],[],[],[]]; llayer6a = [[],[],[],[],[]]\n",
    "\n",
    "weight1 = [[],[],[],[],[]]; weight2 = [[],[],[],[],[]]; weight3 = [[],[],[],[],[]];\n",
    "weight4 = [[],[],[],[],[]]; weight5 = [[],[],[],[],[]]; weight6 = [[],[],[],[],[]];\n",
    "\n",
    "gradw1  = [[],[],[],[],[]]; gradw2  = [[],[],[],[],[]]; gradw3  = [[],[],[],[],[]];\n",
    "gradw4  = [[],[],[],[],[]]; gradw5  = [[],[],[],[],[]]; gradw6  = [[],[],[],[],[]];\n",
    "\n",
    "gradp1  = [[],[],[],[],[]]; gradp2  = [[],[],[],[],[]]; gradp3  = [[],[],[],[],[]];\n",
    "gradp4  = [[],[],[],[],[]]; gradp5  = [[],[],[],[],[]]; gradp6  = [[],[],[],[],[]];\n",
    "\n",
    "gradup1  = [[],[],[],[],[]]; gradup2  = [[],[],[],[],[]]; gradup3  = [[],[],[],[],[]];\n",
    "gradup4  = [[],[],[],[],[]]; gradup5  = [[],[],[],[],[]]; gradup6  = [[],[],[],[],[]];\n",
    "\n",
    "list_of_outputs = [\n",
    "    layer1,layer2,layer3,layer4,layer5,layer6,\n",
    "    layer1a,layer2a,layer3a,layer4a,layer5a,layer6a,\n",
    "    l1.getw(),l2.getw(),l3.getw(),l4.getw(),l5.getw(),l6.getw(),\n",
    "    grad1w,grad2w,grad3w,grad4w,grad5w,grad6w,\n",
    "    grad1p,grad2p,grad3p,grad4p,grad5p,grad6p,\n",
    "    grad1_up[0],grad2_up[0],grad3_up[0],grad4_up[0],grad5_up[0],grad6_up[0]\n",
    "]\n",
    "\n",
    "for iter in range(num_epoch):\n",
    "\n",
    "    # Training Accuracy    \n",
    "    for current_batch_index in range(0,len(train_images),batch_size):\n",
    "        current_data  = train_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = train_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy,gradient_update],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(train_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_train = avg_acc_train + sess_results[0]\n",
    "        \n",
    "    # get the results\n",
    "    mid_stat = sess.run(list_of_outputs,feed_dict={x:current_data,y:current_label})\n",
    "    \n",
    "    # Test Accuracy    \n",
    "    for current_batch_index in range(0,len(test_images), batch_size):\n",
    "        current_data  = test_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = test_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(test_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_test = avg_acc_test + sess_results[0]   \n",
    "        \n",
    "    # ======================== extract stats ========================\n",
    "    llayer1 = append_stat(llayer1,mid_stat,0);  llayer2 = append_stat(llayer2,mid_stat,1);  llayer3 = append_stat(llayer3,mid_stat,2);\n",
    "    llayer4 = append_stat(llayer4,mid_stat,3);  llayer5 = append_stat(llayer5,mid_stat,4);  llayer6 = append_stat(llayer6,mid_stat,5);\n",
    "\n",
    "    llayer1a = append_stat(llayer1a,mid_stat,6);  llayer2a = append_stat(llayer2a,mid_stat,7);  llayer3a = append_stat(llayer3a,mid_stat,8);\n",
    "    llayer4a = append_stat(llayer4a,mid_stat,9);  llayer5a = append_stat(llayer5a,mid_stat,10); llayer6a = append_stat(llayer6a,mid_stat,11);\n",
    "    \n",
    "    weight1 = append_stat(weight1,mid_stat,12);  weight2 = append_stat(weight2,mid_stat,13);  weight3 = append_stat(weight3,mid_stat,14);\n",
    "    weight4 = append_stat(weight4,mid_stat,15);  weight5 = append_stat(weight5,mid_stat,16);  weight6 = append_stat(weight6,mid_stat,17);\n",
    "    \n",
    "    gradw1 = append_stat(gradw1,mid_stat,18); gradw2 = append_stat(gradw2,mid_stat,19); gradw3 = append_stat(gradw3,mid_stat,20);\n",
    "    gradw4 = append_stat(gradw4,mid_stat,21); gradw5 = append_stat(gradw5,mid_stat,22); gradw6 = append_stat(gradw6,mid_stat,23);\n",
    "    \n",
    "    gradp1 = append_stat(gradp1,mid_stat,24); gradp2 = append_stat(gradp2,mid_stat,25); gradp3 = append_stat(gradp3,mid_stat,26);\n",
    "    gradp4 = append_stat(gradp4,mid_stat,27); gradp5 = append_stat(gradp5,mid_stat,28); gradp6 = append_stat(gradp6,mid_stat,29);\n",
    "\n",
    "    gradup1 = append_stat(gradup1,mid_stat,30); gradup2 = append_stat(gradup2,mid_stat,31); gradup3 = append_stat(gradup3,mid_stat,32);\n",
    "    gradup4 = append_stat(gradup4,mid_stat,33); gradup5 = append_stat(gradup5,mid_stat,34); gradup6 = append_stat(gradup6,mid_stat,35);\n",
    "\n",
    "    train_acc.append(avg_acc_train/(len(train_images)/batch_size))\n",
    "    test_acc .append(avg_acc_test / (len(test_images)/batch_size))\n",
    "    # ======================== extract stats ========================\n",
    "    \n",
    "    # ======================== save to image ========================\n",
    "    save_to_image(mid_stat[0:6]   ,llayer1,llayer2,llayer3,llayer4,llayer5,llayer6,\"layer\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[6:12]  ,llayer1a,llayer2a,llayer3a,llayer4a,llayer5a,llayer6a,\"layera\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[12:18] ,weight1,weight2,weight3,weight4,weight5,weight6,\"weights\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[18:24] ,gradw1,gradw2,gradw3,gradw4,gradw5,gradw6,\"gradientw\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[24:30] ,gradp1,gradp2,gradp3,gradp4,gradp5,gradp6,\"gradientp\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[30:36] ,gradup1,gradup2,gradup3,gradup4,gradup5,gradup6,\"moment\",train_acc,test_acc,current_exp_name,iter)\n",
    "    # ======================== save to image ========================\n",
    "        \n",
    "    # ======================== print reset ========================\n",
    "    print(\"Current : \"+ str(iter) + \" Train Acc : \" + str(avg_acc_train/(len(train_images)/batch_size)) + \" Test Acc : \" + str(avg_acc_test/(len(test_images)/batch_size)) + '\\n')\n",
    "    avg_acc_train = 0 ; avg_acc_test  = 0\n",
    "    # ======================== print reset ========================\n",
    "\n",
    "np.save(current_exp_name+'/train_acc.npy',train_acc); np.save(current_exp_name+'/test_acc.npy', test_acc)    \n",
    "np.save(current_exp_name+'/llayer1.npy', llayer1);  np.save(current_exp_name+'/llayer2.npy', llayer2);  np.save(current_exp_name+'/llayer3.npy', llayer3); \n",
    "np.save(current_exp_name+'/llayer4.npy', llayer4);  np.save(current_exp_name+'/llayer5.npy', llayer5);  np.save(current_exp_name+'/llayer6.npy', llayer6); \n",
    "\n",
    "np.save(current_exp_name+'/llayer1a.npy', llayer1a);  np.save(current_exp_name+'/llayer2a.npy', llayer2a);  np.save(current_exp_name+'/llayer3a.npy', llayer3a); \n",
    "np.save(current_exp_name+'/llayer4a.npy', llayer4a);  np.save(current_exp_name+'/llayer5a.npy', llayer5a);  np.save(current_exp_name+'/llayer6a.npy', llayer6a); \n",
    "\n",
    "np.save(current_exp_name+'/weight1.npy', weight1);  np.save(current_exp_name+'/weight2.npy', weight2);  np.save(current_exp_name+'/weight3.npy', weight3);  \n",
    "np.save(current_exp_name+'/weight4.npy', weight4);  np.save(current_exp_name+'/weight5.npy', weight5);  np.save(current_exp_name+'/weight6.npy', weight6);  \n",
    "\n",
    "np.save(current_exp_name+'/gradw1.npy', gradw1); np.save(current_exp_name+'/gradw2.npy', gradw2); np.save(current_exp_name+'/gradw3.npy', gradw3);\n",
    "np.save(current_exp_name+'/gradw4.npy', gradw4); np.save(current_exp_name+'/gradw5.npy', gradw5); np.save(current_exp_name+'/gradw6.npy', gradw6);\n",
    "\n",
    "np.save(current_exp_name+'/gradp1.npy', gradp1); np.save(current_exp_name+'/gradp2.npy', gradp2); np.save(current_exp_name+'/gradp3.npy', gradp3);\n",
    "np.save(current_exp_name+'/gradp4.npy', gradp4); np.save(current_exp_name+'/gradp5.npy', gradp5); np.save(current_exp_name+'/gradp6.npy', gradp6);\n",
    "\n",
    "np.save(current_exp_name+'/gradup1.npy', gradup1); np.save(current_exp_name+'/gradup2.npy', gradup2); np.save(current_exp_name+'/gradup3.npy', gradup3);\n",
    "np.save(current_exp_name+'/gradup4.npy', gradup4); np.save(current_exp_name+'/gradup5.npy', gradup5); np.save(current_exp_name+'/gradup6.npy', gradup6);\n",
    "\n",
    "sess.close(); tf.reset_default_graph();\n",
    "\n",
    "%reset_selective -f train_acc,test_acc,llayer1,llayer2,llayer3,llayer4,llayer5,llayer6,llayer1a,llayer2a,llayer3a,llayer4a,llayer5a,llayer6a\n",
    "%reset_selective -f weight1,weight2,weight3,weight4,weight5,weight6\n",
    "%reset_selective -f gradw1,gradw2,gradw3,gradw4,gradw5,gradw6\n",
    "%reset_selective -f gradp1,gradp2,gradp3,gradp4,gradp5,gradp6\n",
    "%reset_selective -f gradup1,gradup2,gradup3,gradup4,gradup5,gradup6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-05T01:05:02.817709Z",
     "start_time": "2019-01-05T00:53:26.645Z"
    }
   },
   "outputs": [],
   "source": [
    "! start ."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
