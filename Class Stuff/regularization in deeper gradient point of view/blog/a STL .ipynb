{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-17T17:48:29.363742Z",
     "start_time": "2019-01-17T17:48:29.328848Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "if (!(\"Notification\" in window)) {\n",
       "    alert(\"This browser does not support desktop notifications, so the %%notify magic will not work.\");\n",
       "} else if (Notification.permission !== 'granted' && Notification.permission !== 'denied') {\n",
       "    Notification.requestPermission(function (permission) {\n",
       "        if(!('permission' in Notification)) {\n",
       "            Notification.permission = permission;\n",
       "        }\n",
       "    })\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import Library and some random image data set\n",
    "import tensorflow as tf\n",
    "import numpy      as np\n",
    "import seaborn    as sns \n",
    "import pandas     as pd\n",
    "import os,sys\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "np.random.seed(78); tf.set_random_seed(78)\n",
    "\n",
    "# get some of the STL data set\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from skimage import util \n",
    "from skimage.transform import resize\n",
    "from skimage.io import imread\n",
    "import warnings\n",
    "from numpy import inf\n",
    "\n",
    "from scipy.stats import kurtosis,skew\n",
    "\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import gc\n",
    "from IPython.display import display, clear_output\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from matplotlib import animation\n",
    "%load_ext jupyternotify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Things to compare</h1>\n",
    "<div>\n",
    "<ol>\n",
    "  <li>Z: Baseline </li>\n",
    "  <li>A: abs(Theta)  </li>\n",
    "  <li>B: Theta ^ 2</li>\n",
    "  <li>C: sqrt(Theta ^ 2) </li>\n",
    "  <li>D: - log(1+Theta ^ 2)</li>\n",
    "  <li>E: - tanh(Theta) </li>\n",
    "  <li>F: - tanh(Theta^2)</li>\n",
    "  <li>G: - tanh(abs(Theta))</li>\n",
    "  <li>H: - tanh(abs(Theta)^2)</li>\n",
    "  <li>I:   sin(Theta)</li>\n",
    "  <li>J:   abs(sin(Theta))</li>\n",
    "  <li>K:   log(Theta^2)</li>\n",
    "  <li>L:   Theta * log(Theta^2)</li>\n",
    "</ol>\n",
    "</div>\n",
    "\n",
    "<div>\n",
    "<ol>\n",
    "  <li>CNN - no batch </li>\n",
    "  <li>CNN - batch </li>\n",
    "  <li>FNN - no batch </li>\n",
    "  <li>FNN - batch </li>\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-17T17:32:47.321711Z",
     "start_time": "2019-01-17T17:32:44.900395Z"
    },
    "code_folding": [
     0,
     3,
     30,
     38
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 96, 96, 3) 1.0 0.0\n",
      "(5000, 10) 1.0 0.0\n",
      "(8000, 96, 96, 3) 1.0 0.0\n",
      "(8000, 10) 1.0 0.0\n"
     ]
    }
   ],
   "source": [
    "# read all of the data\n",
    "# https://github.com/mttk/STL10\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "def read_all_images(path_to_data):\n",
    "    \"\"\"\n",
    "    :param path_to_data: the file containing the binary images from the STL-10 dataset\n",
    "    :return: an array containing all the images\n",
    "    \"\"\"\n",
    "\n",
    "    with open(path_to_data, 'rb') as f:\n",
    "        # read whole file in uint8 chunks\n",
    "        everything = np.fromfile(f, dtype=np.uint8)\n",
    "\n",
    "        # We force the data into 3x96x96 chunks, since the\n",
    "        # images are stored in \"column-major order\", meaning\n",
    "        # that \"the first 96*96 values are the red channel,\n",
    "        # the next 96*96 are green, and the last are blue.\"\n",
    "        # The -1 is since the size of the pictures depends\n",
    "        # on the input file, and this way numpy determines\n",
    "        # the size on its own.\n",
    "\n",
    "        images = np.reshape(everything, (-1, 3, 96, 96))\n",
    "\n",
    "        # Now transpose the images into a standard image format\n",
    "        # readable by, for example, matplotlib.imshow\n",
    "        # You might want to comment this line or reverse the shuffle\n",
    "        # if you will use a learning algorithm like CNN, since they like\n",
    "        # their channels separated.\n",
    "        images = np.transpose(images, (0, 3, 2, 1))\n",
    "        return images\n",
    "def read_labels(path_to_labels):\n",
    "    \"\"\"\n",
    "    :param path_to_labels: path to the binary file containing labels from the STL-10 dataset\n",
    "    :return: an array containing the labels\n",
    "    \"\"\"\n",
    "    with open(path_to_labels, 'rb') as f:\n",
    "        labels = np.fromfile(f, dtype=np.uint8)\n",
    "        return labels\n",
    "def show_images(data,row=1,col=1):\n",
    "    fig=plt.figure(figsize=(10,10))\n",
    "    columns = col; rows = row\n",
    "    for i in range(1, columns*rows +1):\n",
    "        fig.add_subplot(rows, columns, i)\n",
    "        plt.imshow(data[i-1])\n",
    "    plt.show()\n",
    "\n",
    "train_images = read_all_images(\"../../../DataSet/STL10/stl10_binary/train_X.bin\") / 255.0\n",
    "train_labels = read_labels    (\"../../../DataSet/STL10/stl10_binary/train_Y.bin\")\n",
    "test_images  = read_all_images(\"../../../DataSet/STL10/stl10_binary/test_X.bin\")  / 255.0\n",
    "test_labels  = read_labels    (\"../../../DataSet/STL10/stl10_binary/test_y.bin\")\n",
    "\n",
    "label_encoder= OneHotEncoder(sparse=False,categories='auto')\n",
    "train_labels = label_encoder.fit_transform(train_labels.reshape((-1,1)))\n",
    "test_labels  = label_encoder.fit_transform(test_labels.reshape((-1,1)))\n",
    "\n",
    "print(train_images.shape,train_images.max(),train_images.min())\n",
    "print(train_labels.shape,train_labels.max(),train_labels.min())\n",
    "print(test_images.shape,test_images.max(),test_images.min())\n",
    "print(test_labels.shape,test_labels.max(),test_labels.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-17T17:58:26.052489Z",
     "start_time": "2019-01-17T17:58:25.967475Z"
    },
    "code_folding": [
     6,
     53,
     67,
     74,
     77
    ]
   },
   "outputs": [],
   "source": [
    "# create the layers\n",
    "def tf_softmax(x): return tf.nn.softmax(x)\n",
    "\n",
    "def tf_relu(x):   return tf.nn.relu(x)\n",
    "def d_tf_relu(x): return tf.cast(tf.greater(x,0),tf.float32)\n",
    "\n",
    "class CNN():\n",
    "\n",
    "    def __init__(self,k,inc,out, stddev=0.05,which_reg=0,act=tf_relu,d_act=d_tf_relu):\n",
    "        self.w              = tf.Variable(tf.random_normal([k,k,inc,out],stddev=stddev,seed=2,dtype=tf.float32))\n",
    "        self.m,self.v       = tf.Variable(tf.zeros_like(self.w)),tf.Variable(tf.zeros_like(self.w))\n",
    "        self.act,self.d_act = act,d_act\n",
    "        self.regularizer    = which_reg\n",
    "        \n",
    "    def getw(self): return self.w\n",
    "\n",
    "    def feedforward(self,input,stride=1,padding='SAME'):\n",
    "        self.input  = input\n",
    "        self.layer  = tf.nn.conv2d(input,self.w,strides=[1,stride,stride,1],padding=padding) \n",
    "        self.layerA = self.act(self.layer)\n",
    "        return self.layer, self.layerA\n",
    "    \n",
    "    def backprop(self,gradient,stride=1,padding='SAME'):\n",
    "        grad_part_1 = gradient\n",
    "        grad_part_2 = self.d_act(self.layer)\n",
    "        grad_part_3 = self.input\n",
    "\n",
    "        grad_middle = grad_part_1 * grad_part_2\n",
    "        grad        = tf.nn.conv2d_backprop_filter(input = grad_part_3,filter_sizes = tf.shape(self.w),  out_backprop = grad_middle,strides=[1,stride,stride,1],padding=padding) / batch_size\n",
    "        grad_pass   = tf.nn.conv2d_backprop_input (input_sizes = tf.shape(self.input),filter= self.w,out_backprop = grad_middle,strides=[1,stride,stride,1],padding=padding)\n",
    "\n",
    "        if self.regularizer == 'A': grad = grad + lamda * 2.0 * self.w\n",
    "        if self.regularizer == 'B': grad = grad + lamda * 0.5 * (tf.sign(self.w) + (1.0/tf.sqrt(tf.square(tf.abs(self.w))+ 10e-8)) * tf.abs(self.w) * tf.sign(self.w))\n",
    "        if self.regularizer == 'C': grad = grad + lamda * tf.ones_like(self.w)\n",
    "        if self.regularizer == 'D': grad = grad + lamda * (1.0/tf.sqrt(tf.square(self.w)+ 10e-8)) * self.w\n",
    "        if self.regularizer == 'E': grad = grad + lamda * tf.sign(self.w)\n",
    "        if self.regularizer == 'F': grad = grad + lamda * (1.0/tf.sqrt(tf.square(tf.abs(self.w))+ 10e-8)) * tf.abs(self.w) * tf.sign(self.w)\n",
    "        if self.regularizer == 'G': grad = grad + lamda * ( self.w**2/(tf.sqrt(self.w**2)+10e-8) - tf.sqrt(self.w ** 2))/(self.w**2 + 10e-8)\n",
    "        if self.regularizer == 'H': grad = grad + lamda * -1.0 * (1.0-tf.tanh(self.w) ** 2)\n",
    "        if self.regularizer == 'I': grad = grad + lamda * -1.0 * (1.0-tf.tanh(self.w ** 2) ** 2) * 2.0 * self.w\n",
    "        if self.regularizer == 'J': grad = grad + lamda * -1.0 * (1/(1+self.w ** 2)) * 2.0 * self.w\n",
    "        if self.regularizer == 'K': grad = grad + lamda * -1.0 * (1/(1+self.w ** 2)) * 2.0 * self.w\n",
    "        if self.regularizer == 'L': grad = grad + lamda * -1.0 * (1/(1+self.w ** 2)) * 2.0 * self.w\n",
    "        \n",
    "        update_w = []\n",
    "        update_w.append(tf.assign( self.m,self.m*beta1 + (1-beta1) * (grad)   ))\n",
    "        update_w.append(tf.assign( self.v,self.v*beta2 + (1-beta2) * (grad ** 2)   ))\n",
    "        m_hat = self.m / (1-beta1) ; v_hat = self.v / (1-beta2)\n",
    "        adam_middle = m_hat * learning_rate/(tf.sqrt(v_hat) + adam_e)\n",
    "        update_w.append(tf.assign(self.w,tf.subtract(self.w,adam_middle  )))\n",
    "        \n",
    "        return grad_pass,grad,update_w\n",
    "    \n",
    "def save_to_image(data,name):\n",
    "    l1g,l2g,l3g,l4g,l5g,l6g = data\n",
    "    l1g,l2g,l3g,l4g,l5g,l6g = np.asarray(l1g),np.asarray(l2g),np.asarray(l3g),np.asarray(l4g),np.asarray(l5g),np.asarray(l6g)\n",
    "    plt.figure(figsize=(25,15))\n",
    "    plt.suptitle('Current Iter : ' + str(iter))\n",
    "    plt.subplot(231); plt.hist(l1g.ravel(),50); plt.title('layer 1')\n",
    "    plt.subplot(232); plt.hist(l2g.ravel(),50); plt.title('layer 2')\n",
    "    plt.subplot(233); plt.hist(l3g.ravel(),50); plt.title('layer 3')\n",
    "    plt.subplot(234); plt.hist(l4g.ravel(),50); plt.title('layer 4')\n",
    "    plt.subplot(235); plt.hist(l5g.ravel(),50); plt.title('layer 5')\n",
    "    plt.subplot(236); plt.hist(l6g.ravel(),50); plt.title('layer 6')\n",
    "    plt.savefig(name + str(iter)+'.png')\n",
    "    plt.tight_layout()\n",
    "    plt.close('all')     \n",
    "def append_stat(current_list,data,number):\n",
    "    current_list[0].append(data[number].mean())\n",
    "    current_list[1].append(data[number].std())\n",
    "    current_list[2].append(skew    (data[number].ravel()))\n",
    "    current_list[3].append(kurtosis(data[number].ravel()))\n",
    "    current_list[4].append(np.count_nonzero(data[number]))\n",
    "    return current_list\n",
    "def transform_to_2d(data):\n",
    "    batch,width,height,chan = data.shape\n",
    "    return data.reshape((batch*width,height*chan))\n",
    "def save_to_image(main_data,one,two,three,four,five,six,experiment_name,tran_acc,test_acc,current_exp,iter):\n",
    "    plt.figure(figsize=(20,40))\n",
    "    G = gridspec.GridSpec(8, 6)\n",
    "\n",
    "    plt.figtext(0.5,1.0,\"Iter: \" + str(iter) + \" Histogram Per \" + experiment_name,ha=\"center\", va=\"top\", fontsize=35, color=\"black\")\n",
    "    plt.subplot(G[0, 0]).hist(main_data[0].ravel(),50,color='red');       plt.subplot(G[0, 0]).set_title(experiment_name+' 1')\n",
    "    plt.subplot(G[0, 1]).hist(main_data[1].ravel(),50,color='orange');    plt.subplot(G[0, 1]).set_title(experiment_name+' 2')\n",
    "    plt.subplot(G[0, 2]).hist(main_data[2].ravel(),50,color='yellow');  plt.subplot(G[0, 2]).set_title(experiment_name+' 3')\n",
    "    plt.subplot(G[0, 3]).hist(main_data[3].ravel(),50,color='green');    plt.subplot(G[0, 3]).set_title(experiment_name+' 4')\n",
    "    plt.subplot(G[0, 4]).hist(main_data[4].ravel(),50,color='blue');     plt.subplot(G[0, 4]).set_title(experiment_name+' 5')\n",
    "    plt.subplot(G[0, 5]).hist(main_data[5].ravel(),50,color='black');     plt.subplot(G[0, 5]).set_title(experiment_name+' 6')\n",
    "\n",
    "    plt.subplot(G[1, :]).set_title(\"Mean Per \"+ experiment_name)\n",
    "    plt.subplot(G[1, :]).plot(one[0]  ,c='red',alpha=0.9,label='1')\n",
    "    plt.subplot(G[1, :]).plot(two[0]  ,c='orange',alpha=0.9,label='2')\n",
    "    plt.subplot(G[1, :]).plot(three[0],c='yellow',alpha=0.9,label='3')\n",
    "    plt.subplot(G[1, :]).plot(four[0],c='green',alpha=0.9,label='4')\n",
    "    plt.subplot(G[1, :]).plot(five[0],c='blue',alpha=0.9,label='5')\n",
    "    plt.subplot(G[1, :]).plot(six[0],c='black',alpha=0.9,label='6')\n",
    "    plt.legend(bbox_to_anchor=(0., 0.95, 1., .05), loc=9,ncol=6, mode=\"expand\", borderaxespad=0.)\n",
    "\n",
    "    plt.subplot(G[2, :]).set_title(\"Standard Deviation Per \"+ experiment_name)\n",
    "    plt.subplot(G[2, :]).plot(one[1]  ,c='red',alpha=0.9,label='1')\n",
    "    plt.subplot(G[2, :]).plot(two[1]  ,c='orange',alpha=0.9,label='2')\n",
    "    plt.subplot(G[2, :]).plot(three[1],c='yellow',alpha=0.9,label='3')\n",
    "    plt.subplot(G[2, :]).plot(four[1],c='green',alpha=0.9,label='4')\n",
    "    plt.subplot(G[2, :]).plot(five[1],c='blue',alpha=0.9,label='5')\n",
    "    plt.subplot(G[2, :]).plot(six[1],c='black',alpha=0.9,label='6')\n",
    "    plt.legend(bbox_to_anchor=(0., 0.95, 1., .05), loc=9,ncol=6, mode=\"expand\", borderaxespad=0.)\n",
    "\n",
    "    plt.subplot(G[3, :]).set_title(\"Skewness Per \"+ experiment_name)\n",
    "    plt.subplot(G[3, :]).plot(one[2]  ,c='red',alpha=0.9,label='1')\n",
    "    plt.subplot(G[3, :]).plot(two[2]  ,c='orange',alpha=0.9,label='2')\n",
    "    plt.subplot(G[3, :]).plot(three[2],c='yellow',alpha=0.9,label='3')\n",
    "    plt.subplot(G[3, :]).plot(four[2],c='green',alpha=0.9,label='4')\n",
    "    plt.subplot(G[3, :]).plot(five[2],c='blue',alpha=0.9,label='5')\n",
    "    plt.subplot(G[3, :]).plot(six[2],c='black',alpha=0.9,label='6')\n",
    "    plt.legend(bbox_to_anchor=(0., 0.95, 1., .05), loc=9,ncol=6, mode=\"expand\", borderaxespad=0.)\n",
    "\n",
    "    plt.subplot(G[4, :]).set_title(\"Kurtosis Per \"+ experiment_name)\n",
    "    plt.subplot(G[4, :]).plot(one[3]  ,c='red',alpha=0.9,label='1')\n",
    "    plt.subplot(G[4, :]).plot(two[3]  ,c='orange',alpha=0.9,label='2')\n",
    "    plt.subplot(G[4, :]).plot(three[3],c='yellow',alpha=0.9,label='3')\n",
    "    plt.subplot(G[4, :]).plot(four[3],c='green',alpha=0.9,label='4')\n",
    "    plt.subplot(G[4, :]).plot(five[3],c='blue',alpha=0.9,label='5')\n",
    "    plt.subplot(G[4, :]).plot(six[3],c='black',alpha=0.9,label='6')\n",
    "    plt.legend(bbox_to_anchor=(0., 0.95, 1., .05), loc=9,ncol=6, mode=\"expand\", borderaxespad=0.)\n",
    "\n",
    "    plt.subplot(G[5, :]).set_title(\"# Non-Zero Per \"+ experiment_name)\n",
    "    plt.subplot(G[5, :]).plot(one[4]  ,c='red',alpha=0.9   ,label='1')\n",
    "    plt.subplot(G[5, :]).plot(two[4]  ,c='orange',alpha=0.9,label='2')\n",
    "    plt.subplot(G[5, :]).plot(three[4],c='yellow',alpha=0.9,label='3')\n",
    "    plt.subplot(G[5, :]).plot(four[4],c='green',alpha=0.9  ,label='4')\n",
    "    plt.subplot(G[5, :]).plot(five[4],c='blue',alpha=0.9   ,label='5')\n",
    "    plt.subplot(G[5, :]).plot(six[4],c='black',alpha=0.9   ,label='6')\n",
    "    plt.legend(bbox_to_anchor=(0., 0.95, 1., .05), loc=9,ncol=6, mode=\"expand\", borderaxespad=0.)\n",
    "\n",
    "    plt.subplot(G[6, :]).set_title(\"Train/Test accuracy\")\n",
    "    plt.subplot(G[6, :]).plot(train_acc  ,c='red',alpha=0.9, label='Train')\n",
    "    plt.subplot(G[6, :]).plot(test_acc   ,c='blue',alpha=0.9,label='Test')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.figtext(0.5,0,\"Correlation Matrix Per \"+ experiment_name,ha=\"center\", va=\"bottom\", fontsize=30, color=\"black\")\n",
    "    plt.subplot(G[7, 0]).imshow(np.corrcoef(transform_to_2d(main_data[0])),cmap='gray')\n",
    "    plt.subplot(G[7, 1]).imshow(np.corrcoef(transform_to_2d(main_data[1])),cmap='gray')\n",
    "    plt.subplot(G[7, 2]).imshow(np.corrcoef(transform_to_2d(main_data[2])),cmap='gray')\n",
    "    plt.subplot(G[7, 3]).imshow(np.corrcoef(transform_to_2d(main_data[3])),cmap='gray')\n",
    "    plt.subplot(G[7, 4]).imshow(np.corrcoef(transform_to_2d(main_data[4])),cmap='gray')\n",
    "    plt.subplot(G[7, 5]).imshow(np.corrcoef(transform_to_2d(main_data[5])),cmap='gray')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(current_exp + '/' + experiment_name + '/' + str(iter) + '.png')\n",
    "    plt.close('all')\n",
    "def plot_rotation_weight(current_layers,current_layer_number,current_batch_norm_type,current_exp_name): \n",
    "\n",
    "    def rotate(angle):ax.view_init(azim=angle)\n",
    "    plt.rcParams.update({'font.size': 8})\n",
    "    colors = ['red','orange','yellow','green','blue','purple','black','gold','silver','cyan','pink']\n",
    "    number_of_eps = [1,2,3,4,5,6,7,8,9,10,11]\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax  = fig.add_subplot(111, projection='3d')\n",
    "    count = 0\n",
    "    for episode in range(num_eps+1):\n",
    "        if episode == num_eps:\n",
    "            ys   = current_layers.mean(0).flatten()\n",
    "            xmin = ys.min(); xmax = ys.max(); step = 0.005\n",
    "            hist,bins = np.histogram(ys, bins=np.linspace(xmin, xmax, (xmax-xmin)/step))\n",
    "            ax.bar(bins[:-1], hist, width=0.01,zs=episode, zdir='y', color=colors[episode], alpha=0.8)\n",
    "        else:\n",
    "            ys   = current_layers[episode].flatten()\n",
    "            xmin = ys.min(); xmax = ys.max(); step = 0.005\n",
    "            hist,bins = np.histogram(ys, bins=np.linspace(xmin, xmax, (xmax-xmin)/step))\n",
    "            ax.bar(bins[:-1], hist, width=0.01,zs=episode, zdir='y', color=colors[episode], alpha=0.4)\n",
    "    ax.set_xlabel('Values')\n",
    "    ax.set_ylabel('Episode')\n",
    "    ax.get_yaxis().set_ticks(np.arange(num_eps+1))\n",
    "    ax.set_zlabel('Histogram')\n",
    "\n",
    "    ax.w_xaxis.set_pane_color((1.0, 1.0, 1.0, 1.0))\n",
    "    ax.w_yaxis.set_pane_color((1.0, 1.0, 1.0, 1.0))\n",
    "    ax.w_zaxis.set_pane_color((1.0, 1.0, 1.0, 1.0))\n",
    "\n",
    "    # make the grid lines transparent\n",
    "    ax.xaxis._axinfo[\"grid\"]['color'] =  (1,1,1,0)\n",
    "    ax.yaxis._axinfo[\"grid\"]['color'] =  (1,1,1,0)\n",
    "    # ax.zaxis._axinfo[\"grid\"]['color'] =  (1,1,1,0)\n",
    "    #plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    animation.FuncAnimation(fig, rotate, frames=np.arange(0,362,2),interval=100) \\\n",
    "    .save(str(current_batch_norm_type)+'/'+str(current_exp_name)+'/weight_'+str(current_layer_number)+'.gif', dpi=80, writer='imagemagick')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-17T17:32:56.181157Z",
     "start_time": "2019-01-17T17:32:56.176169Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# set hyper parameter\n",
    "plt.rcParams.update({'font.size': 25})\n",
    "current_batch_norm_type = 'no_batch_cnn'\n",
    "num_eps   = 10; num_epoch = 2; learning_rate = 0.0008; batch_size = 20; beta1,beta2,adam_e = 0.9,0.999,1e-9; lamda = 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-17T17:34:56.268684Z",
     "start_time": "2019-01-17T17:32:58.704694Z"
    },
    "code_folding": [
     59
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    ================================================\n",
      "                Starting Episode: 0\n",
      "    ================================================\n",
      "\n",
      "Current : 0\t Train Acc : 0.121\t Test Acc : 0.175\t\n",
      "Current Iter : 1/2 batch : 7980/8000 acc : 0.15\n",
      "    ================================================\n",
      "                Starting Episode: 1\n",
      "    ================================================\n",
      "\n",
      "Current : 0\t Train Acc : 0.137\t Test Acc : 0.189\t\n",
      "Current Iter : 1/2 batch : 7980/8000 acc : 0.15\n",
      "    ================================================\n",
      "                Starting Episode: 2\n",
      "    ================================================\n",
      "\n",
      "Current : 0\t Train Acc : 0.14\t Test Acc : 0.199\t\n",
      "Current Iter : 1/2 batch : 7980/8000 acc : 0.35\n",
      "    ================================================\n",
      "                Starting Episode: 3\n",
      "    ================================================\n",
      "\n",
      "Current : 0\t Train Acc : 0.133\t Test Acc : 0.199\t\n",
      "Current Iter : 1/2 batch : 7980/8000 acc : 0.35\n",
      "    ================================================\n",
      "                Starting Episode: 4\n",
      "    ================================================\n",
      "\n",
      "Current : 0\t Train Acc : 0.123\t Test Acc : 0.172\t\n",
      "Current Iter : 1/2 batch : 7980/8000 acc : 0.15\n",
      "    ================================================\n",
      "                Starting Episode: 5\n",
      "    ================================================\n",
      "\n",
      "Current : 0\t Train Acc : 0.12\t Test Acc : 0.178\t\n",
      "Current Iter : 1/2 batch : 7980/8000 acc : 0.25\n",
      "    ================================================\n",
      "                Starting Episode: 6\n",
      "    ================================================\n",
      "\n",
      "Current : 0\t Train Acc : 0.116\t Test Acc : 0.169\t\n",
      "Current Iter : 1/2 batch : 7980/8000 acc : 0.35\n",
      "    ================================================\n",
      "                Starting Episode: 7\n",
      "    ================================================\n",
      "\n",
      "Current : 0\t Train Acc : 0.133\t Test Acc : 0.142\t\n",
      "Current Iter : 1/2 batch : 7980/8000 acc : 0.25\n",
      "    ================================================\n",
      "                Starting Episode: 8\n",
      "    ================================================\n",
      "\n",
      "Current : 0\t Train Acc : 0.116\t Test Acc : 0.17\t\n",
      "Current Iter : 1/2 batch : 7980/8000 acc : 0.35\n",
      "    ================================================\n",
      "                Starting Episode: 9\n",
      "    ================================================\n",
      "\n",
      "Current : 0\t Train Acc : 0.126\t Test Acc : 0.211\t\n",
      "Current Iter : 1/2 batch : 7980/8000 acc : 0.35\r"
     ]
    }
   ],
   "source": [
    "# Z\n",
    "current_exp_name = 'Z'; \n",
    "sess = tf.InteractiveSession()\n",
    "current_exp_train_accuracy = np.zeros((num_eps,num_epoch))\n",
    "current_exp_test_accuracy  = np.zeros((num_eps,num_epoch))\n",
    "current_weights_layer1 = np.zeros((num_eps,3,3,3,16))\n",
    "current_weights_layer2 = np.zeros((num_eps,3,3,16,16))\n",
    "current_weights_layer3 = np.zeros((num_eps,3,3,16,16))\n",
    "current_weights_layer4 = np.zeros((num_eps,3,3,16,16))\n",
    "current_weights_layer5 = np.zeros((num_eps,3,3,16,16))\n",
    "current_weights_layer6 = np.zeros((num_eps,3,3,16,10))\n",
    "\n",
    "for episode in range(num_eps):\n",
    "    sys.stdout.write(\"\"\"\n",
    "    ================================================\n",
    "                Starting Episode: \"\"\" + str(episode) +  \"\"\"\n",
    "    ================================================\\n\n",
    "    \"\"\");sys.stdout.flush();\n",
    "    \n",
    "    # create layers\n",
    "    l1 = CNN(3,3, 16,which_reg=current_exp_name); \n",
    "    l2 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "    l3 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "\n",
    "    l4 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "    l5 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "    l6 = CNN(3,16,10,which_reg=current_exp_name); \n",
    "\n",
    "    # 2. graph \n",
    "    x = tf.placeholder(tf.float32,(batch_size,96,96,3))\n",
    "    y = tf.placeholder(tf.float32,(batch_size,10))\n",
    "\n",
    "    layer1, layer1a = l1. feedforward(x,stride=2)\n",
    "    layer2, layer2a = l2. feedforward(layer1a,stride=2)\n",
    "    layer3, layer3a = l3. feedforward(layer2a,stride=2)\n",
    "    layer4, layer4a = l4. feedforward(layer3a,stride=2)\n",
    "    layer5, layer5a = l5. feedforward(layer4a)\n",
    "    layer6, layer6a = l6. feedforward(layer5a)\n",
    "\n",
    "    final_layer   = tf.reduce_mean(layer6a,(1,2))\n",
    "    final_softmax = tf_softmax(final_layer)\n",
    "    cost          = -tf.reduce_mean(y * tf.log(final_softmax + 1e-8))\n",
    "    correct_prediction = tf.equal(tf.argmax(final_softmax, 1), tf.argmax(y, 1))\n",
    "    accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    gradient = tf.tile((final_softmax-y)[:,None,None,:],[1,6,6,1])/batch_size\n",
    "    grad6p,grad6w,grad6_up = l6.backprop(gradient)\n",
    "    grad5p,grad5w,grad5_up = l5.backprop(grad6p)\n",
    "    grad4p,grad4w,grad4_up = l4.backprop(grad5p,stride=2)\n",
    "    grad3p,grad3w,grad3_up = l3.backprop(grad4p,stride=2)\n",
    "    grad2p,grad2w,grad2_up = l2.backprop(grad3p,stride=2)\n",
    "    grad1p,grad1w,grad1_up = l1.backprop(grad2p,stride=2)\n",
    "\n",
    "    gradient_update = grad6_up + grad5_up + grad4_up + grad3_up + grad2_up + grad1_up \n",
    "\n",
    "    # train\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    avg_acc_train = 0; avg_acc_test  = 0; train_acc = [];test_acc = []\n",
    "\n",
    "    for iter in range(num_epoch):\n",
    "\n",
    "        # Training Accuracy    \n",
    "        for current_batch_index in range(0,len(train_images),batch_size):\n",
    "            current_data  = train_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "            current_label = train_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "            sess_results  = sess.run([accuracy,gradient_update],feed_dict={x:current_data,y:current_label})\n",
    "            sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(train_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "            sys.stdout.flush(); avg_acc_train = avg_acc_train + sess_results[0]\n",
    "\n",
    "        # Test Accuracy    \n",
    "        for current_batch_index in range(0,len(test_images), batch_size):\n",
    "            current_data  = test_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "            current_label = test_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "            sess_results  = sess.run([accuracy],feed_dict={x:current_data,y:current_label})\n",
    "            sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(test_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "            sys.stdout.flush(); avg_acc_test = avg_acc_test + sess_results[0]   \n",
    "\n",
    "        # ======================== print reset ========================\n",
    "        train_acc.append(avg_acc_train/(len(train_images)/batch_size))\n",
    "        test_acc .append(avg_acc_test / (len(test_images)/batch_size))\n",
    "        if iter%2 == 0 :\n",
    "            sys.stdout.write(\"Current : \"+ str(iter) + \"\\t\" +\n",
    "                  \" Train Acc : \" + str(np.around(avg_acc_train/(len(train_images)/batch_size),3)) + \"\\t\" +\n",
    "                  \" Test Acc : \"  + str(np.around(avg_acc_test/(len(test_images)/batch_size),3)) + \"\\t\\n\")\n",
    "            sys.stdout.flush();\n",
    "        avg_acc_train = 0 ; avg_acc_test  = 0\n",
    "        # ======================== print reset ========================\n",
    "\n",
    "    # save to episode all of them\n",
    "    current_exp_train_accuracy[episode,:] = train_acc\n",
    "    current_exp_test_accuracy[episode,:] = test_acc\n",
    "    current_weights_layer1[episode] = l1.getw().eval()\n",
    "    current_weights_layer2[episode] = l2.getw().eval()\n",
    "    current_weights_layer3[episode] = l3.getw().eval()\n",
    "    current_weights_layer4[episode] = l4.getw().eval()\n",
    "    current_weights_layer5[episode] = l5.getw().eval()\n",
    "    current_weights_layer6[episode] = l6.getw().eval()\n",
    "\n",
    "sess.close()\n",
    "\n",
    "# save to gif\n",
    "# plot_rotation_weight(current_weights_layer1,'1',current_batch_norm_type,current_exp_name)\n",
    "# plot_rotation_weight(current_weights_layer2,'2',current_batch_norm_type,current_exp_name)\n",
    "# plot_rotation_weight(current_weights_layer3,'3',current_batch_norm_type,current_exp_name)\n",
    "# plot_rotation_weight(current_weights_layer4,'4',current_batch_norm_type,current_exp_name)\n",
    "# plot_rotation_weight(current_weights_layer5,'5',current_batch_norm_type,current_exp_name)\n",
    "# plot_rotation_weight(current_weights_layer6,'6',current_batch_norm_type,current_exp_name)\n",
    "\n",
    "# save to image\n",
    "\n",
    "# save all average and accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-17T17:35:25.248137Z",
     "start_time": "2019-01-17T17:35:25.130462Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the accuracy min max and mean\n",
    "plt.plot(current_exp_train_accuracy.max(0),  ' ' ,color='blue')\n",
    "plt.plot(current_exp_train_accuracy.mean(0), '-' ,color='blue')\n",
    "plt.plot(current_exp_train_accuracy.min(0) , ' ' ,color='blue')\n",
    "plt.fill_between(range(num_epoch),current_exp_train_accuracy.max(0),current_exp_train_accuracy.min(0),facecolor='blue', alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-17T18:28:39.770903Z",
     "start_time": "2019-01-17T18:28:36.036529Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 12 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the histogram\n",
    "def plot_image_weight(current_layers,current_layer_number,current_batch_norm_type,current_exp_name,current_exp_train_accuracy,current_exp_test_accuracy):\n",
    "    colors = ['red','orange','yellow','green','blue','purple','black','gold','silver','cyan','pink']\n",
    "    def rt(number): return np.around(number,4)\n",
    "    plt.style.use('seaborn')\n",
    "    plt.rcParams.update({'font.size': 30})\n",
    "    fig = plt.figure(figsize=(15,15))\n",
    "    \n",
    "    fig.add_subplot(341); plt.hist(current_layers[0].flatten(),50,color=colors[0],alpha=0.8);plt.title('EPS: 1 Mean: '+str(rt(current_layers[0].mean())) +' STD: '+str(rt(current_layers[0].std())))\n",
    "    fig.add_subplot(342); plt.hist(current_layers[1].flatten(),50,color=colors[1],alpha=0.8);plt.title('EPS: 2 Mean: '+str(rt(current_layers[1].mean())) +' STD: '+str(rt(current_layers[1].std())))\n",
    "    fig.add_subplot(343); plt.hist(current_layers[2].flatten(),50,color=colors[2],alpha=0.8);plt.title('EPS: 3 Mean: '+str(rt(current_layers[2].mean())) +' STD: '+str(rt(current_layers[2].std())))\n",
    "    fig.add_subplot(344); plt.hist(current_layers[3].flatten(),50,color=colors[3],alpha=0.8);plt.title('EPS: 4 Mean: '+str(rt(current_layers[3].mean())) +' STD: '+str(rt(current_layers[3].std())))\n",
    "    \n",
    "    fig.add_subplot(345); plt.hist(current_layers[4].flatten(),50,color=colors[4],alpha=0.8);plt.title('EPS: 5 Mean: '+str(rt(current_layers[4].mean())) +' STD: '+str(rt(current_layers[4].std())))\n",
    "    fig.add_subplot(346); plt.hist(current_layers[5].flatten(),50,color=colors[5],alpha=0.8);plt.title('EPS: 6 Mean: '+str(rt(current_layers[5].mean())) +' STD: '+str(rt(current_layers[5].std())))\n",
    "    fig.add_subplot(347); plt.hist(current_layers[6].flatten(),50,color=colors[6],alpha=0.8);plt.title('EPS: 7 Mean: '+str(rt(current_layers[6].mean())) +' STD: '+str(rt(current_layers[6].std())))\n",
    "    fig.add_subplot(348); plt.hist(current_layers[7].flatten(),50,color=colors[7],alpha=0.8);plt.title('EPS: 8 Mean: '+str(rt(current_layers[7].mean())) +' STD: '+str(rt(current_layers[7].std())))\n",
    "    \n",
    "    fig.add_subplot(3,4,9);  plt.hist(current_layers[8].flatten(),50,color=colors[9],alpha=0.8);plt.title('EPS: 9 Mean: '+str(rt(current_layers[8].mean())) +' STD: '+str(rt(current_layers[8].std())))\n",
    "    fig.add_subplot(3,4,10); plt.hist(current_layers[9].flatten(),50,color=colors[10],alpha=0.8);plt.title('EPS: 10 Mean: '+str(rt(current_layers[9].mean())) +' STD: '+str(rt(current_layers[9].std())))\n",
    "    fig.add_subplot(3,4,11); plt.hist(current_layers.mean(0).flatten(),50,color=colors[0],alpha=0.8);plt.title('EPS: All Mean: '+str(rt(current_layers.mean(0).mean())) +' STD: '+str(rt(current_layers.mean(0).std())))\n",
    "    fig.add_subplot(3,4,12); \n",
    "    plt.plot(current_exp_train_accuracy.max(0),  ' ' ,color='red')\n",
    "    plt.plot(current_exp_train_accuracy.mean(0), '-' ,color='red',label='train mean')\n",
    "    plt.plot(current_exp_train_accuracy.min(0) , ' ' ,color='red')\n",
    "    plt.fill_between(range(num_epoch),current_exp_train_accuracy.max(0),current_exp_train_accuracy.min(0),facecolor='red', alpha=0.2)\n",
    "\n",
    "    plt.plot(current_exp_test_accuracy.max(0),  ' ' ,color='blue')\n",
    "    plt.plot(current_exp_test_accuracy.mean(0), '-' ,color='blue',label='test mean')\n",
    "    plt.plot(current_exp_test_accuracy.min(0) , ' ' ,color='blue')\n",
    "    plt.fill_between(range(num_epoch),current_exp_test_accuracy.max(0),current_exp_test_accuracy.min(0),facecolor='blue', alpha=0.2)\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(str(current_batch_norm_type)+'/'+str(current_exp_name)+'/weight_'+str(current_layer_number)+'.png')\n",
    "    plt.show()\n",
    "\n",
    "plot_image_weight(current_weights_layer1,'1',current_batch_norm_type,current_exp_name,current_exp_train_accuracy,current_exp_test_accuracy)\n",
    "# %notify -m \"done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-17T14:13:35.181047Z",
     "start_time": "2019-01-17T14:11:36.570361Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-17T18:27:09.352448Z",
     "start_time": "2019-01-17T18:27:09.099123Z"
    }
   },
   "outputs": [],
   "source": [
    "! start ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-17T17:27:01.263846Z",
     "start_time": "2019-01-17T17:27:01.226912Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-17T14:06:39.500262Z",
     "start_time": "2019-01-17T14:06:39.460356Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-17T14:08:45.021606Z",
     "start_time": "2019-01-17T14:08:45.017619Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-17T14:09:20.308018Z",
     "start_time": "2019-01-17T14:09:20.259162Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-05T02:14:30.085275Z",
     "start_time": "2019-01-05T01:28:38.463294Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current : 0 Train Acc : 0.11660000236332417 Test Acc : 0.159750002855435\n",
      "\n",
      "Current : 1 Train Acc : 0.16240000292658807 Test Acc : 0.20162500322796403\n",
      "\n",
      "Current : 2 Train Acc : 0.232800002977252 Test Acc : 0.27775000233203173\n",
      "\n",
      "Current : 3 Train Acc : 0.2702000027447939 Test Acc : 0.29712500181980434\n",
      "\n",
      "Current : 4 Train Acc : 0.2896000024676323 Test Acc : 0.3038750021997839\n",
      "\n",
      "Current : 5 Train Acc : 0.30340000185370447 Test Acc : 0.31387500192038714\n",
      "\n",
      "Current : 6 Train Acc : 0.3138000020235777 Test Acc : 0.3185000020544976\n",
      "\n",
      "Current : 7 Train Acc : 0.320400001257658 Test Acc : 0.3266250020265579\n",
      "\n",
      "Current : 8 Train Acc : 0.3264000009596348 Test Acc : 0.3303750018402934\n",
      "\n",
      "Current : 9 Train Acc : 0.3310000019520521 Test Acc : 0.334375001937151\n",
      "\n",
      "Current : 10 Train Acc : 0.3350000016838312 Test Acc : 0.33825000187382104\n",
      "\n",
      "Current : 11 Train Acc : 0.33620000129938127 Test Acc : 0.34137500166893003\n",
      "\n",
      "Current : 12 Train Acc : 0.34300000116229057 Test Acc : 0.34500000135041775\n",
      "\n",
      "Current : 13 Train Acc : 0.3468000015169382 Test Acc : 0.3485000017378479\n",
      "\n",
      "Current : 14 Train Acc : 0.34820000137388707 Test Acc : 0.34962500142864883\n",
      "\n",
      "Current : 15 Train Acc : 0.35120000164210796 Test Acc : 0.35387500139884653\n",
      "\n",
      "Current : 16 Train Acc : 0.35820000107586386 Test Acc : 0.3567500016931444\n",
      "\n",
      "Current : 17 Train Acc : 0.36120000110566614 Test Acc : 0.3571250015962869\n",
      "\n",
      "Current : 18 Train Acc : 0.364800000667572 Test Acc : 0.3612500016950071\n",
      "\n",
      "Current : 19 Train Acc : 0.37140000075101853 Test Acc : 0.36200000158511103\n",
      "\n",
      "Current : 20 Train Acc : 0.37540000024437903 Test Acc : 0.3633750007394701\n",
      "\n",
      "Current : 21 Train Acc : 0.37679999992251395 Test Acc : 0.3651250004488975\n",
      "\n",
      "Current : 22 Train Acc : 0.3792000004947185 Test Acc : 0.3678750005830079\n",
      "\n",
      "Current : 23 Train Acc : 0.38000000080466273 Test Acc : 0.3696250004414469\n",
      "\n",
      "Current : 24 Train Acc : 0.383000000923872 Test Acc : 0.37100000041536985\n",
      "\n",
      "Current : 25 Train Acc : 0.383800000756979 Test Acc : 0.3726250005047768\n",
      "\n",
      "Current : 26 Train Acc : 0.3842000014483929 Test Acc : 0.37450000026263297\n",
      "\n",
      "Current : 27 Train Acc : 0.3850000014305115 Test Acc : 0.37475000069476666\n",
      "\n",
      "Current : 28 Train Acc : 0.3874000018239021 Test Acc : 0.3783750008139759\n",
      "\n",
      "Current : 29 Train Acc : 0.38780000180006025 Test Acc : 0.3803750006295741\n",
      "\n",
      "Current : 30 Train Acc : 0.3914000019431114 Test Acc : 0.3796250007860362\n",
      "\n",
      "Current : 31 Train Acc : 0.3916000018119812 Test Acc : 0.3805000007338822\n",
      "\n",
      "Current : 32 Train Acc : 0.3942000014185906 Test Acc : 0.38112500083632767\n",
      "\n",
      "Current : 33 Train Acc : 0.3932000014185906 Test Acc : 0.38400000114925203\n",
      "\n",
      "Current : 34 Train Acc : 0.39620000144839285 Test Acc : 0.3861250013206154\n",
      "\n",
      "Current : 35 Train Acc : 0.400800001591444 Test Acc : 0.3876250005606562\n",
      "\n",
      "Current : 36 Train Acc : 0.3996000018417835 Test Acc : 0.38750000034458937\n",
      "\n",
      "Current : 37 Train Acc : 0.40380000206828115 Test Acc : 0.38912500071339307\n",
      "\n",
      "Current : 38 Train Acc : 0.4050000015795231 Test Acc : 0.3906250009592622\n",
      "\n",
      "Current : 39 Train Acc : 0.4064000011384487 Test Acc : 0.3911250015255064\n",
      "\n",
      "Current : 40 Train Acc : 0.40740000107884405 Test Acc : 0.39250000099651516\n",
      "\n",
      "Current : 41 Train Acc : 0.41020000067353246 Test Acc : 0.3933750013913959\n",
      "\n",
      "Current : 42 Train Acc : 0.40880000093579294 Test Acc : 0.39475000138394534\n",
      "\n",
      "Current : 43 Train Acc : 0.4128000008761883 Test Acc : 0.3946250013168901\n",
      "\n",
      "Current : 44 Train Acc : 0.4154000011384487 Test Acc : 0.39450000098906457\n",
      "\n",
      "Current : 45 Train Acc : 0.41700000086426736 Test Acc : 0.39500000077299774\n",
      "\n",
      "Current : 46 Train Acc : 0.4188000017106533 Test Acc : 0.3958750009071082\n",
      "\n",
      "Current : 47 Train Acc : 0.4188000021278858 Test Acc : 0.3962500010896474\n",
      "\n",
      "Current : 48 Train Acc : 0.4202000022828579 Test Acc : 0.3975000007171184\n",
      "\n",
      "Current : 49 Train Acc : 0.4224000024199486 Test Acc : 0.39787500069476667\n",
      "\n",
      "Current : 50 Train Acc : 0.4236000021100044 Test Acc : 0.39837500036694107\n",
      "\n",
      "Current : 51 Train Acc : 0.42660000270605086 Test Acc : 0.4002500003110617\n",
      "\n",
      "Current : 52 Train Acc : 0.42740000212192536 Test Acc : 0.3991250001359731\n",
      "\n",
      "Current : 53 Train Acc : 0.4298000026345253 Test Acc : 0.4012500003632158\n",
      "\n",
      "Current : 54 Train Acc : 0.4292000025510788 Test Acc : 0.40137500054202974\n",
      "\n",
      "Current : 55 Train Acc : 0.4322000021338463 Test Acc : 0.40262500059790907\n",
      "\n",
      "Current : 56 Train Acc : 0.430800002515316 Test Acc : 0.40300000114366413\n",
      "\n",
      "Current : 57 Train Acc : 0.43380000251531603 Test Acc : 0.4048750007897615\n",
      "\n",
      "Current : 58 Train Acc : 0.4352000025510788 Test Acc : 0.4057499997969717\n",
      "\n",
      "Current : 59 Train Acc : 0.4376000028252602 Test Acc : 0.40675000052899124\n",
      "\n",
      "Current : 60 Train Acc : 0.441000002682209 Test Acc : 0.4282499995827675\n",
      "\n",
      "Current : 61 Train Acc : 0.46840000194311143 Test Acc : 0.43012500047683716\n",
      "\n",
      "Current : 62 Train Acc : 0.4732000017762184 Test Acc : 0.43400000032037495\n",
      "\n",
      "Current : 63 Train Acc : 0.4770000013709068 Test Acc : 0.4351250001043081\n",
      "\n",
      "Current : 64 Train Acc : 0.47960000091791155 Test Acc : 0.4350000002235174\n",
      "\n",
      "Current : 65 Train Acc : 0.4778000013232231 Test Acc : 0.43600000090897084\n",
      "\n",
      "Current : 66 Train Acc : 0.47920000141859054 Test Acc : 0.4375000010430813\n",
      "\n",
      "Current : 67 Train Acc : 0.4818000019788742 Test Acc : 0.4376250011846423\n",
      "\n",
      "Current : 68 Train Acc : 0.4832000018954277 Test Acc : 0.43675000075250864\n",
      "\n",
      "Current : 69 Train Acc : 0.4852000021338463 Test Acc : 0.4383750006183982\n",
      "\n",
      "Current : 70 Train Acc : 0.4868000018000603 Test Acc : 0.4392500006407499\n",
      "\n",
      "Current : 71 Train Acc : 0.48740000182390214 Test Acc : 0.44012500047683717\n",
      "\n",
      "Current : 72 Train Acc : 0.486200001180172 Test Acc : 0.4411250007711351\n",
      "\n",
      "Current : 73 Train Acc : 0.4872000016570091 Test Acc : 0.44087500078603625\n",
      "\n",
      "Current : 74 Train Acc : 0.48840000182390214 Test Acc : 0.44200000083073976\n",
      "\n",
      "Current : 75 Train Acc : 0.4916000019907951 Test Acc : 0.44287500077858566\n",
      "\n",
      "Current : 76 Train Acc : 0.4926000021696091 Test Acc : 0.44262500109151004\n",
      "\n",
      "Current : 77 Train Acc : 0.49340000212192536 Test Acc : 0.4425000006891787\n",
      "\n",
      "Current : 78 Train Acc : 0.4926000015735626 Test Acc : 0.44362500090152024\n",
      "\n",
      "Current : 79 Train Acc : 0.49560000133514404 Test Acc : 0.44362500108778474\n",
      "\n",
      "Current : 80 Train Acc : 0.4978000019788742 Test Acc : 0.4441250004991889\n",
      "\n",
      "Current : 81 Train Acc : 0.4972000012397766 Test Acc : 0.4436250007897615\n",
      "\n",
      "Current : 82 Train Acc : 0.49980000138282776 Test Acc : 0.4453750005364418\n",
      "\n",
      "Current : 83 Train Acc : 0.5046000015735627 Test Acc : 0.4463750004395843\n",
      "\n",
      "Current : 84 Train Acc : 0.5054000009298325 Test Acc : 0.44637500058859586\n",
      "\n",
      "Current : 85 Train Acc : 0.5058000018596649 Test Acc : 0.44737500049173834\n",
      "\n",
      "Current : 86 Train Acc : 0.5080000017881393 Test Acc : 0.448500000461936\n",
      "\n",
      "Current : 87 Train Acc : 0.5068000015020371 Test Acc : 0.4483750003576279\n",
      "\n",
      "Current : 88 Train Acc : 0.5092000017166137 Test Acc : 0.4482499999180436\n",
      "\n",
      "Current : 89 Train Acc : 0.51200000166893 Test Acc : 0.44762500025331975\n",
      "\n",
      "Current : 90 Train Acc : 0.5120000020861626 Test Acc : 0.44899999994784595\n",
      "\n",
      "Current : 91 Train Acc : 0.5146000012755394 Test Acc : 0.4472499999403954\n",
      "\n",
      "Current : 92 Train Acc : 0.5156000012159347 Test Acc : 0.44837500028312205\n",
      "\n",
      "Current : 93 Train Acc : 0.517200001835823 Test Acc : 0.44950000029057263\n",
      "\n",
      "Current : 94 Train Acc : 0.5166000016927719 Test Acc : 0.45037499990314245\n",
      "\n",
      "Current : 95 Train Acc : 0.5206000016927719 Test Acc : 0.45275000043213365\n",
      "\n",
      "Current : 96 Train Acc : 0.521400001525879 Test Acc : 0.45325000006705524\n",
      "\n",
      "Current : 97 Train Acc : 0.5232000012397766 Test Acc : 0.4555000011250377\n",
      "\n",
      "Current : 98 Train Acc : 0.5244000009298324 Test Acc : 0.4542500003054738\n",
      "\n",
      "Current : 99 Train Acc : 0.5258000013828278 Test Acc : 0.45637500025331973\n",
      "\n",
      "Current : 100 Train Acc : 0.5272000007629395 Test Acc : 0.4575000004470348\n",
      "\n",
      "Current : 101 Train Acc : 0.5308000016212463 Test Acc : 0.4588750008866191\n",
      "\n",
      "Current : 102 Train Acc : 0.5314000005722046 Test Acc : 0.45887500084936617\n",
      "\n",
      "Current : 103 Train Acc : 0.5322000006437302 Test Acc : 0.4593750010058284\n",
      "\n",
      "Current : 104 Train Acc : 0.5338000015020371 Test Acc : 0.4601250008493662\n",
      "\n",
      "Current : 105 Train Acc : 0.5340000010728836 Test Acc : 0.46112500071525575\n",
      "\n",
      "Current : 106 Train Acc : 0.5356000009775161 Test Acc : 0.46325000081211326\n",
      "\n",
      "Current : 107 Train Acc : 0.5372000012397766 Test Acc : 0.4628750006854534\n",
      "\n",
      "Current : 108 Train Acc : 0.5380000007152558 Test Acc : 0.46337500028312206\n",
      "\n",
      "Current : 109 Train Acc : 0.5390000010728836 Test Acc : 0.46287500083446503\n",
      "\n",
      "Current : 110 Train Acc : 0.5396000009775161 Test Acc : 0.463875000923872\n",
      "\n",
      "Current : 111 Train Acc : 0.5418000009059906 Test Acc : 0.4652500012516975\n",
      "\n",
      "Current : 112 Train Acc : 0.5428000011444092 Test Acc : 0.4662500011175871\n",
      "\n",
      "Current : 113 Train Acc : 0.5436000019311905 Test Acc : 0.4663750010728836\n",
      "\n",
      "Current : 114 Train Acc : 0.5444000016450882 Test Acc : 0.4668750012665987\n",
      "\n",
      "Current : 115 Train Acc : 0.545800001502037 Test Acc : 0.4672500006109476\n",
      "\n",
      "Current : 116 Train Acc : 0.5464000009298324 Test Acc : 0.46800000056624413\n",
      "\n",
      "Current : 117 Train Acc : 0.5460000010728836 Test Acc : 0.468250000923872\n",
      "\n",
      "Current : 118 Train Acc : 0.5468000011444092 Test Acc : 0.4695000005140901\n",
      "\n",
      "Current : 119 Train Acc : 0.5478000006675721 Test Acc : 0.4701250006258488\n",
      "\n",
      "Current : 120 Train Acc : 0.548600000500679 Test Acc : 0.4712500000372529\n",
      "\n",
      "Current : 121 Train Acc : 0.5496000003814697 Test Acc : 0.47250000029802325\n",
      "\n",
      "Current : 122 Train Acc : 0.5503999999761582 Test Acc : 0.47275000043213367\n",
      "\n",
      "Current : 123 Train Acc : 0.5501999998092652 Test Acc : 0.47325000021606684\n",
      "\n",
      "Current : 124 Train Acc : 0.553200001001358 Test Acc : 0.47325000036507847\n",
      "\n",
      "Current : 125 Train Acc : 0.5558000005483628 Test Acc : 0.475375000461936\n",
      "\n",
      "Current : 126 Train Acc : 0.5564000002145767 Test Acc : 0.4737500002980232\n",
      "\n",
      "Current : 127 Train Acc : 0.5576000010967255 Test Acc : 0.47524999983608723\n",
      "\n",
      "Current : 128 Train Acc : 0.5606000003814697 Test Acc : 0.47399999998509884\n",
      "\n",
      "Current : 129 Train Acc : 0.5596000001430511 Test Acc : 0.4758750000223517\n",
      "\n",
      "Current : 130 Train Acc : 0.5620000001192093 Test Acc : 0.4748749998211861\n",
      "\n",
      "Current : 131 Train Acc : 0.5634000003933907 Test Acc : 0.4757499998062849\n",
      "\n",
      "Current : 132 Train Acc : 0.5650000014305114 Test Acc : 0.47637499995529653\n",
      "\n",
      "Current : 133 Train Acc : 0.5642000010609627 Test Acc : 0.47662499994039537\n",
      "\n",
      "Current : 134 Train Acc : 0.5644000014662742 Test Acc : 0.4767499999701977\n",
      "\n",
      "Current : 135 Train Acc : 0.566000001847744 Test Acc : 0.4791250001639128\n",
      "\n",
      "Current : 136 Train Acc : 0.5680000013709068 Test Acc : 0.47975000031292436\n",
      "\n",
      "Current : 137 Train Acc : 0.5682000009417534 Test Acc : 0.47887500017881396\n",
      "\n",
      "Current : 138 Train Acc : 0.570000001013279 Test Acc : 0.48025000013411046\n",
      "\n",
      "Current : 139 Train Acc : 0.5700000013709068 Test Acc : 0.480375000089407\n",
      "\n",
      "Current : 140 Train Acc : 0.5694000012278557 Test Acc : 0.4797500001639128\n",
      "\n",
      "Current : 141 Train Acc : 0.5716000016331673 Test Acc : 0.4793750002235174\n",
      "\n",
      "Current : 142 Train Acc : 0.5714000013470649 Test Acc : 0.4812500000372529\n",
      "\n",
      "Current : 143 Train Acc : 0.573600001513958 Test Acc : 0.48187500033527614\n",
      "\n",
      "Current : 144 Train Acc : 0.5740000013709068 Test Acc : 0.48212500009685755\n",
      "\n",
      "Current : 145 Train Acc : 0.5730000016689301 Test Acc : 0.4828750004246831\n",
      "\n",
      "Current : 146 Train Acc : 0.5738000018000603 Test Acc : 0.48225000012665986\n",
      "\n",
      "Current : 147 Train Acc : 0.5776000018715859 Test Acc : 0.4828750008717179\n",
      "\n",
      "Current : 148 Train Acc : 0.5778000012040139 Test Acc : 0.483000000603497\n",
      "\n",
      "Current : 149 Train Acc : 0.5780000017285347 Test Acc : 0.48187500026077035\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Z\n",
    "current_exp_name = 'Z';\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# create layers\n",
    "l1 = CNN(3,3, 16,which_reg=current_exp_name); \n",
    "l2 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "l3 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "\n",
    "l4 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "l5 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "l6 = CNN(3,16,10,which_reg=current_exp_name); \n",
    "\n",
    "# 2. graph \n",
    "x = tf.placeholder(tf.float32,(batch_size,96,96,3))\n",
    "y = tf.placeholder(tf.float32,(batch_size,10))\n",
    "\n",
    "layer1, layer1a = l1. feedforward(x,stride=2)\n",
    "layer2, layer2a = l2. feedforward(layer1a,stride=2)\n",
    "layer3, layer3a = l3. feedforward(layer2a,stride=2)\n",
    "layer4, layer4a = l4. feedforward(layer3a,stride=2)\n",
    "layer5, layer5a = l5. feedforward(layer4a)\n",
    "layer6, layer6a = l6. feedforward(layer5a)\n",
    "\n",
    "final_layer   = tf.reduce_mean(layer6a,(1,2))\n",
    "final_softmax = tf_softmax(final_layer)\n",
    "cost          = -tf.reduce_mean(y * tf.log(final_softmax + 1e-8))\n",
    "correct_prediction = tf.equal(tf.argmax(final_softmax, 1), tf.argmax(y, 1))\n",
    "accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "gradient = tf.tile((final_softmax-y)[:,None,None,:],[1,6,6,1])/batch_size\n",
    "grad6p,grad6w,grad6_up = l6.backprop(gradient)\n",
    "grad5p,grad5w,grad5_up = l5.backprop(grad6p)\n",
    "grad4p,grad4w,grad4_up = l4.backprop(grad5p,stride=2)\n",
    "grad3p,grad3w,grad3_up = l3.backprop(grad4p,stride=2)\n",
    "grad2p,grad2w,grad2_up = l2.backprop(grad3p,stride=2)\n",
    "grad1p,grad1w,grad1_up = l1.backprop(grad2p,stride=2)\n",
    "\n",
    "gradient_update = grad6_up + grad5_up + grad4_up + grad3_up + grad2_up + grad1_up \n",
    "\n",
    "# train\n",
    "sess.run(tf.global_variables_initializer())\n",
    "avg_acc_train = 0; avg_acc_test  = 0; train_acc = [];test_acc = []\n",
    "\n",
    "# mean std skew kurt non-zero\n",
    "llayer1 = [[],[],[],[],[]]; llayer2 = [[],[],[],[],[]]; llayer3 = [[],[],[],[],[]]\n",
    "llayer4 = [[],[],[],[],[]]; llayer5 = [[],[],[],[],[]]; llayer6 = [[],[],[],[],[]]\n",
    "\n",
    "llayer1a = [[],[],[],[],[]]; llayer2a = [[],[],[],[],[]]; llayer3a = [[],[],[],[],[]]\n",
    "llayer4a = [[],[],[],[],[]]; llayer5a = [[],[],[],[],[]]; llayer6a = [[],[],[],[],[]]\n",
    "\n",
    "weight1 = [[],[],[],[],[]]; weight2 = [[],[],[],[],[]]; weight3 = [[],[],[],[],[]];\n",
    "weight4 = [[],[],[],[],[]]; weight5 = [[],[],[],[],[]]; weight6 = [[],[],[],[],[]];\n",
    "\n",
    "gradw1  = [[],[],[],[],[]]; gradw2  = [[],[],[],[],[]]; gradw3  = [[],[],[],[],[]];\n",
    "gradw4  = [[],[],[],[],[]]; gradw5  = [[],[],[],[],[]]; gradw6  = [[],[],[],[],[]];\n",
    "\n",
    "gradp1  = [[],[],[],[],[]]; gradp2  = [[],[],[],[],[]]; gradp3  = [[],[],[],[],[]];\n",
    "gradp4  = [[],[],[],[],[]]; gradp5  = [[],[],[],[],[]]; gradp6  = [[],[],[],[],[]];\n",
    "\n",
    "gradup1  = [[],[],[],[],[]]; gradup2  = [[],[],[],[],[]]; gradup3  = [[],[],[],[],[]];\n",
    "gradup4  = [[],[],[],[],[]]; gradup5  = [[],[],[],[],[]]; gradup6  = [[],[],[],[],[]];\n",
    "\n",
    "list_of_outputs = [\n",
    "    layer1,layer2,layer3,layer4,layer5,layer6,\n",
    "    layer1a,layer2a,layer3a,layer4a,layer5a,layer6a,\n",
    "    l1.getw(),l2.getw(),l3.getw(),l4.getw(),l5.getw(),l6.getw(),\n",
    "    grad1w,grad2w,grad3w,grad4w,grad5w,grad6w,\n",
    "    grad1p,grad2p,grad3p,grad4p,grad5p,grad6p,\n",
    "    grad1_up[0],grad2_up[0],grad3_up[0],grad4_up[0],grad5_up[0],grad6_up[0]\n",
    "]\n",
    "\n",
    "for iter in range(num_epoch):\n",
    "\n",
    "    # Training Accuracy    \n",
    "    for current_batch_index in range(0,len(train_images),batch_size):\n",
    "        current_data  = train_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = train_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy,gradient_update],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(train_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_train = avg_acc_train + sess_results[0]\n",
    "        \n",
    "    # get the results\n",
    "    mid_stat = sess.run(list_of_outputs,feed_dict={x:current_data,y:current_label})\n",
    "    \n",
    "    # Test Accuracy    \n",
    "    for current_batch_index in range(0,len(test_images), batch_size):\n",
    "        current_data  = test_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = test_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(test_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_test = avg_acc_test + sess_results[0]   \n",
    "        \n",
    "    # ======================== extract stats ========================\n",
    "    llayer1 = append_stat(llayer1,mid_stat,0);  llayer2 = append_stat(llayer2,mid_stat,1);  llayer3 = append_stat(llayer3,mid_stat,2);\n",
    "    llayer4 = append_stat(llayer4,mid_stat,3);  llayer5 = append_stat(llayer5,mid_stat,4);  llayer6 = append_stat(llayer6,mid_stat,5);\n",
    "\n",
    "    llayer1a = append_stat(llayer1a,mid_stat,6);  llayer2a = append_stat(llayer2a,mid_stat,7);  llayer3a = append_stat(llayer3a,mid_stat,8);\n",
    "    llayer4a = append_stat(llayer4a,mid_stat,9);  llayer5a = append_stat(llayer5a,mid_stat,10); llayer6a = append_stat(llayer6a,mid_stat,11);\n",
    "    \n",
    "    weight1 = append_stat(weight1,mid_stat,12);  weight2 = append_stat(weight2,mid_stat,13);  weight3 = append_stat(weight3,mid_stat,14);\n",
    "    weight4 = append_stat(weight4,mid_stat,15);  weight5 = append_stat(weight5,mid_stat,16);  weight6 = append_stat(weight6,mid_stat,17);\n",
    "    \n",
    "    gradw1 = append_stat(gradw1,mid_stat,18); gradw2 = append_stat(gradw2,mid_stat,19); gradw3 = append_stat(gradw3,mid_stat,20);\n",
    "    gradw4 = append_stat(gradw4,mid_stat,21); gradw5 = append_stat(gradw5,mid_stat,22); gradw6 = append_stat(gradw6,mid_stat,23);\n",
    "    \n",
    "    gradp1 = append_stat(gradp1,mid_stat,24); gradp2 = append_stat(gradp2,mid_stat,25); gradp3 = append_stat(gradp3,mid_stat,26);\n",
    "    gradp4 = append_stat(gradp4,mid_stat,27); gradp5 = append_stat(gradp5,mid_stat,28); gradp6 = append_stat(gradp6,mid_stat,29);\n",
    "\n",
    "    gradup1 = append_stat(gradup1,mid_stat,30); gradup2 = append_stat(gradup2,mid_stat,31); gradup3 = append_stat(gradup3,mid_stat,32);\n",
    "    gradup4 = append_stat(gradup4,mid_stat,33); gradup5 = append_stat(gradup5,mid_stat,34); gradup6 = append_stat(gradup6,mid_stat,35);\n",
    "\n",
    "    train_acc.append(avg_acc_train/(len(train_images)/batch_size))\n",
    "    test_acc .append(avg_acc_test / (len(test_images)/batch_size))\n",
    "    # ======================== extract stats ========================\n",
    "    \n",
    "    # ======================== save to image ========================\n",
    "    save_to_image(mid_stat[0:6]   ,llayer1,llayer2,llayer3,llayer4,llayer5,llayer6,\"layer\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[6:12]  ,llayer1a,llayer2a,llayer3a,llayer4a,llayer5a,llayer6a,\"layera\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[12:18] ,weight1,weight2,weight3,weight4,weight5,weight6,\"weights\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[18:24] ,gradw1,gradw2,gradw3,gradw4,gradw5,gradw6,\"gradientw\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[24:30] ,gradp1,gradp2,gradp3,gradp4,gradp5,gradp6,\"gradientp\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[30:36] ,gradup1,gradup2,gradup3,gradup4,gradup5,gradup6,\"moment\",train_acc,test_acc,current_exp_name,iter)\n",
    "    # ======================== save to image ========================\n",
    "        \n",
    "    # ======================== print reset ========================\n",
    "    print(\"Current : \"+ str(iter) + \" Train Acc : \" + str(avg_acc_train/(len(train_images)/batch_size)) + \" Test Acc : \" + str(avg_acc_test/(len(test_images)/batch_size)) + '\\n')\n",
    "    avg_acc_train = 0 ; avg_acc_test  = 0\n",
    "    # ======================== print reset ========================\n",
    "\n",
    "np.save(current_exp_name+'/train_acc.npy',train_acc); np.save(current_exp_name+'/test_acc.npy', test_acc)    \n",
    "np.save(current_exp_name+'/llayer1.npy', llayer1);  np.save(current_exp_name+'/llayer2.npy', llayer2);  np.save(current_exp_name+'/llayer3.npy', llayer3); \n",
    "np.save(current_exp_name+'/llayer4.npy', llayer4);  np.save(current_exp_name+'/llayer5.npy', llayer5);  np.save(current_exp_name+'/llayer6.npy', llayer6); \n",
    "\n",
    "np.save(current_exp_name+'/llayer1a.npy', llayer1a);  np.save(current_exp_name+'/llayer2a.npy', llayer2a);  np.save(current_exp_name+'/llayer3a.npy', llayer3a); \n",
    "np.save(current_exp_name+'/llayer4a.npy', llayer4a);  np.save(current_exp_name+'/llayer5a.npy', llayer5a);  np.save(current_exp_name+'/llayer6a.npy', llayer6a); \n",
    "\n",
    "np.save(current_exp_name+'/weight1.npy', weight1);  np.save(current_exp_name+'/weight2.npy', weight2);  np.save(current_exp_name+'/weight3.npy', weight3);  \n",
    "np.save(current_exp_name+'/weight4.npy', weight4);  np.save(current_exp_name+'/weight5.npy', weight5);  np.save(current_exp_name+'/weight6.npy', weight6);  \n",
    "\n",
    "np.save(current_exp_name+'/gradw1.npy', gradw1); np.save(current_exp_name+'/gradw2.npy', gradw2); np.save(current_exp_name+'/gradw3.npy', gradw3);\n",
    "np.save(current_exp_name+'/gradw4.npy', gradw4); np.save(current_exp_name+'/gradw5.npy', gradw5); np.save(current_exp_name+'/gradw6.npy', gradw6);\n",
    "\n",
    "np.save(current_exp_name+'/gradp1.npy', gradp1); np.save(current_exp_name+'/gradp2.npy', gradp2); np.save(current_exp_name+'/gradp3.npy', gradp3);\n",
    "np.save(current_exp_name+'/gradp4.npy', gradp4); np.save(current_exp_name+'/gradp5.npy', gradp5); np.save(current_exp_name+'/gradp6.npy', gradp6);\n",
    "\n",
    "np.save(current_exp_name+'/gradup1.npy', gradup1); np.save(current_exp_name+'/gradup2.npy', gradup2); np.save(current_exp_name+'/gradup3.npy', gradup3);\n",
    "np.save(current_exp_name+'/gradup4.npy', gradup4); np.save(current_exp_name+'/gradup5.npy', gradup5); np.save(current_exp_name+'/gradup6.npy', gradup6);\n",
    "\n",
    "sess.close(); tf.reset_default_graph();\n",
    "\n",
    "%reset_selective -f l1,l2,l3,l4,l5,l6\n",
    "%reset_selective -f layer1,layer2,layer3,layer4,layer5,layer6\n",
    "%reset_selective -f layer1a,layer2a,layer3a,layer4a,layer5a,layer6a\n",
    "%reset_selective -f train_acc,test_acc,llayer1,llayer2,llayer3,llayer4,llayer5,llayer6,llayer1a,llayer2a,llayer3a,llayer4a,llayer5a,llayer6a\n",
    "%reset_selective -f weight1,weight2,weight3,weight4,weight5,weight6\n",
    "%reset_selective -f gradw1,gradw2,gradw3,gradw4,gradw5,gradw6\n",
    "%reset_selective -f gradp1,gradp2,gradp3,gradp4,gradp5,gradp6\n",
    "%reset_selective -f gradup1,gradup2,gradup3,gradup4,gradup5,gradup6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-05T03:04:25.512552Z",
     "start_time": "2019-01-05T02:14:30.111207Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current : 0 Train Acc : 0.12520000219345093 Test Acc : 0.17137500314973295\n",
      "\n",
      "Current : 1 Train Acc : 0.16260000313818454 Test Acc : 0.16837500305846334\n",
      "\n",
      "Current : 2 Train Acc : 0.20100000366568566 Test Acc : 0.2147500032838434\n",
      "\n",
      "Current : 3 Train Acc : 0.2406000030785799 Test Acc : 0.2551250029914081\n",
      "\n",
      "Current : 4 Train Acc : 0.2748000023216009 Test Acc : 0.2888750022184104\n",
      "\n",
      "Current : 5 Train Acc : 0.29140000288188456 Test Acc : 0.3045000023022294\n",
      "\n",
      "Current : 6 Train Acc : 0.3048000024408102 Test Acc : 0.31912500218488277\n",
      "\n",
      "Current : 7 Train Acc : 0.31300000251829624 Test Acc : 0.3307500015571713\n",
      "\n",
      "Current : 8 Train Acc : 0.3224000021219254 Test Acc : 0.33987500217743216\n",
      "\n",
      "Current : 9 Train Acc : 0.33340000224113464 Test Acc : 0.34937500187195836\n",
      "\n",
      "Current : 10 Train Acc : 0.34400000230968 Test Acc : 0.36100000262260434\n",
      "\n",
      "Current : 11 Train Acc : 0.35580000130832196 Test Acc : 0.36737500195391476\n",
      "\n",
      "Current : 12 Train Acc : 0.3604000013321638 Test Acc : 0.37512500140815974\n",
      "\n",
      "Current : 13 Train Acc : 0.3678000010997057 Test Acc : 0.3797500015422702\n",
      "\n",
      "Current : 14 Train Acc : 0.3739999998062849 Test Acc : 0.37950000146403906\n",
      "\n",
      "Current : 15 Train Acc : 0.3818000003248453 Test Acc : 0.3861250020749867\n",
      "\n",
      "Current : 16 Train Acc : 0.3859999998509884 Test Acc : 0.3892500012740493\n",
      "\n",
      "Current : 17 Train Acc : 0.3934000009596348 Test Acc : 0.3905000018700957\n",
      "\n",
      "Current : 18 Train Acc : 0.397400001257658 Test Acc : 0.3926250018179417\n",
      "\n",
      "Current : 19 Train Acc : 0.40280000123381615 Test Acc : 0.3980000016093254\n",
      "\n",
      "Current : 20 Train Acc : 0.4074000011384487 Test Acc : 0.40062500115484\n",
      "\n",
      "Current : 21 Train Acc : 0.40860000094771387 Test Acc : 0.4036250016093254\n",
      "\n",
      "Current : 22 Train Acc : 0.4112000012099743 Test Acc : 0.40850000113248824\n",
      "\n",
      "Current : 23 Train Acc : 0.4154000008404255 Test Acc : 0.4100000010803342\n",
      "\n",
      "Current : 24 Train Acc : 0.4166000008881092 Test Acc : 0.4126250009983778\n",
      "\n",
      "Current : 25 Train Acc : 0.42160000121593477 Test Acc : 0.41300000093877315\n",
      "\n",
      "Current : 26 Train Acc : 0.42380000126361844 Test Acc : 0.4141250007599592\n",
      "\n",
      "Current : 27 Train Acc : 0.42840000087022784 Test Acc : 0.4151250010356307\n",
      "\n",
      "Current : 28 Train Acc : 0.4292000004053116 Test Acc : 0.4182500006631017\n",
      "\n",
      "Current : 29 Train Acc : 0.431200001001358 Test Acc : 0.4173750011622906\n",
      "\n",
      "Current : 30 Train Acc : 0.4324000009894371 Test Acc : 0.4187500010058284\n",
      "\n",
      "Current : 31 Train Acc : 0.43400000149011614 Test Acc : 0.42050000056624415\n",
      "\n",
      "Current : 32 Train Acc : 0.43460000115633013 Test Acc : 0.42387500137090683\n",
      "\n",
      "Current : 33 Train Acc : 0.4426000002324581 Test Acc : 0.4235000008717179\n",
      "\n",
      "Current : 34 Train Acc : 0.4454000000059605 Test Acc : 0.424000000320375\n",
      "\n",
      "Current : 35 Train Acc : 0.44880000039935114 Test Acc : 0.42525000132620333\n",
      "\n",
      "Current : 36 Train Acc : 0.45340000104904177 Test Acc : 0.42662500094622374\n",
      "\n",
      "Current : 37 Train Acc : 0.4548000007271767 Test Acc : 0.4288750008866191\n",
      "\n",
      "Current : 38 Train Acc : 0.45960000044107435 Test Acc : 0.4296250008046627\n",
      "\n",
      "Current : 39 Train Acc : 0.45980000001192095 Test Acc : 0.4298750006407499\n",
      "\n",
      "Current : 40 Train Acc : 0.4606000007987022 Test Acc : 0.4307500007003546\n",
      "\n",
      "Current : 41 Train Acc : 0.4626000003218651 Test Acc : 0.4333750004693866\n",
      "\n",
      "Current : 42 Train Acc : 0.4656000007390976 Test Acc : 0.4343750005587935\n",
      "\n",
      "Current : 43 Train Acc : 0.46600000113248824 Test Acc : 0.43687500070780516\n",
      "\n",
      "Current : 44 Train Acc : 0.4690000007152557 Test Acc : 0.4366250004246831\n",
      "\n",
      "Current : 45 Train Acc : 0.47000000077486037 Test Acc : 0.44037500079721215\n",
      "\n",
      "Current : 46 Train Acc : 0.47260000139474867 Test Acc : 0.4388750007376075\n",
      "\n",
      "Current : 47 Train Acc : 0.4736000016331673 Test Acc : 0.4403750012069941\n",
      "\n",
      "Current : 48 Train Acc : 0.47240000194311144 Test Acc : 0.441625001244247\n",
      "\n",
      "Current : 49 Train Acc : 0.4748000019192696 Test Acc : 0.441625001244247\n",
      "\n",
      "Current : 50 Train Acc : 0.4736000022292137 Test Acc : 0.4450000013411045\n",
      "\n",
      "Current : 51 Train Acc : 0.4740000024437904 Test Acc : 0.4471250006183982\n",
      "\n",
      "Current : 52 Train Acc : 0.4744000017642975 Test Acc : 0.44675000093877315\n",
      "\n",
      "Current : 53 Train Acc : 0.47600000143051147 Test Acc : 0.4458750006556511\n",
      "\n",
      "Current : 54 Train Acc : 0.4772000016570091 Test Acc : 0.4440000004693866\n",
      "\n",
      "Current : 55 Train Acc : 0.47520000201463697 Test Acc : 0.4451250010728836\n",
      "\n",
      "Current : 56 Train Acc : 0.4780000020265579 Test Acc : 0.4465000008419156\n",
      "\n",
      "Current : 57 Train Acc : 0.4810000017285347 Test Acc : 0.4448750011995435\n",
      "\n",
      "Current : 58 Train Acc : 0.48280000203847884 Test Acc : 0.4442500013485551\n",
      "\n",
      "Current : 59 Train Acc : 0.48360000240802764 Test Acc : 0.445000001527369\n",
      "\n",
      "Current : 60 Train Acc : 0.4860000023841858 Test Acc : 0.4463750006258488\n",
      "\n",
      "Current : 61 Train Acc : 0.4880000022053719 Test Acc : 0.44537500105798244\n",
      "\n",
      "Current : 62 Train Acc : 0.4872000025510788 Test Acc : 0.4475000012293458\n",
      "\n",
      "Current : 63 Train Acc : 0.48900000262260435 Test Acc : 0.4445000011473894\n",
      "\n",
      "Current : 64 Train Acc : 0.49120000207424164 Test Acc : 0.44687500096857546\n",
      "\n",
      "Current : 65 Train Acc : 0.4940000023841858 Test Acc : 0.44750000096857545\n",
      "\n",
      "Current : 66 Train Acc : 0.49400000166893004 Test Acc : 0.44725000083446503\n",
      "\n",
      "Current : 67 Train Acc : 0.4962000021338463 Test Acc : 0.44887500125914814\n",
      "\n",
      "Current : 68 Train Acc : 0.49700000214576723 Test Acc : 0.4488750011101365\n",
      "\n",
      "Current : 69 Train Acc : 0.4976000016927719 Test Acc : 0.4486250012740493\n",
      "\n",
      "Current : 70 Train Acc : 0.5006000017523765 Test Acc : 0.4486250012740493\n",
      "\n",
      "Current : 71 Train Acc : 0.5000000008940697 Test Acc : 0.44837500106543304\n",
      "\n",
      "Current : 72 Train Acc : 0.5012000016570092 Test Acc : 0.45062500115484\n",
      "\n",
      "Current : 73 Train Acc : 0.5026000011563301 Test Acc : 0.45062500100582836\n",
      "\n",
      "Current : 74 Train Acc : 0.5030000014901161 Test Acc : 0.4526250011473894\n",
      "\n",
      "Current : 75 Train Acc : 0.5036000013947487 Test Acc : 0.4532500009983778\n",
      "\n",
      "Current : 76 Train Acc : 0.505600001513958 Test Acc : 0.4561250016465783\n",
      "\n",
      "Current : 77 Train Acc : 0.5076000011563301 Test Acc : 0.45412500139325856\n",
      "\n",
      "Current : 78 Train Acc : 0.5074000014662743 Test Acc : 0.45762500148266555\n",
      "\n",
      "Current : 79 Train Acc : 0.5098000014424324 Test Acc : 0.4552500016614795\n",
      "\n",
      "Current : 80 Train Acc : 0.5100000020861626 Test Acc : 0.4568750014528632\n",
      "\n",
      "Current : 81 Train Acc : 0.5116000013947487 Test Acc : 0.45487500179558993\n",
      "\n",
      "Current : 82 Train Acc : 0.5104000015854836 Test Acc : 0.4545000022277236\n",
      "\n",
      "Current : 83 Train Acc : 0.5126000009179116 Test Acc : 0.4565000020340085\n",
      "\n",
      "Current : 84 Train Acc : 0.5132000014185906 Test Acc : 0.4572500018402934\n",
      "\n",
      "Current : 85 Train Acc : 0.5164000012874603 Test Acc : 0.4566250020265579\n",
      "\n",
      "Current : 86 Train Acc : 0.5178000015020371 Test Acc : 0.4570000022649765\n",
      "\n",
      "Current : 87 Train Acc : 0.5200000021457672 Test Acc : 0.45800000227987764\n",
      "\n",
      "Current : 88 Train Acc : 0.5198000020980835 Test Acc : 0.46200000174343586\n",
      "\n",
      "Current : 89 Train Acc : 0.5210000022649764 Test Acc : 0.4612500015646219\n",
      "\n",
      "Current : 90 Train Acc : 0.5246000025272369 Test Acc : 0.46125000193715093\n",
      "\n",
      "Current : 91 Train Acc : 0.5230000025033951 Test Acc : 0.4645000016689301\n",
      "\n",
      "Current : 92 Train Acc : 0.5266000014543534 Test Acc : 0.461875001937151\n",
      "\n",
      "Current : 93 Train Acc : 0.5262000019550324 Test Acc : 0.46625000160187485\n",
      "\n",
      "Current : 94 Train Acc : 0.5266000019311905 Test Acc : 0.4612500014528632\n",
      "\n",
      "Current : 95 Train Acc : 0.5298000022172927 Test Acc : 0.4620000015944242\n",
      "\n",
      "Current : 96 Train Acc : 0.5314000029563903 Test Acc : 0.46387500163167716\n",
      "\n",
      "Current : 97 Train Acc : 0.5300000021457673 Test Acc : 0.4660000015050173\n",
      "\n",
      "Current : 98 Train Acc : 0.5326000024080276 Test Acc : 0.4655000014975667\n",
      "\n",
      "Current : 99 Train Acc : 0.5328000029325485 Test Acc : 0.4666250018030405\n",
      "\n",
      "Current : 100 Train Acc : 0.5346000019311905 Test Acc : 0.4675000021606684\n",
      "\n",
      "Current : 101 Train Acc : 0.5350000026226044 Test Acc : 0.4688750022649765\n",
      "\n",
      "Current : 102 Train Acc : 0.5346000019311905 Test Acc : 0.47187500182539227\n",
      "\n",
      "Current : 103 Train Acc : 0.5376000014543534 Test Acc : 0.47137500148266553\n",
      "\n",
      "Current : 104 Train Acc : 0.5390000021457673 Test Acc : 0.4723750014975667\n",
      "\n",
      "Current : 105 Train Acc : 0.5408000018596649 Test Acc : 0.47425000175833704\n",
      "\n",
      "Current : 106 Train Acc : 0.5422000018358231 Test Acc : 0.4747500017285347\n",
      "\n",
      "Current : 107 Train Acc : 0.5436000024080276 Test Acc : 0.477125001847744\n",
      "\n",
      "Current : 108 Train Acc : 0.5464000012874604 Test Acc : 0.4765000019967556\n",
      "\n",
      "Current : 109 Train Acc : 0.5448000007867813 Test Acc : 0.4780000018328428\n",
      "\n",
      "Current : 110 Train Acc : 0.5446000013351441 Test Acc : 0.47775000117719174\n",
      "\n",
      "Current : 111 Train Acc : 0.5468000005483628 Test Acc : 0.48062500171363354\n",
      "\n",
      "Current : 112 Train Acc : 0.5470000015497207 Test Acc : 0.47950000133365395\n",
      "\n",
      "Current : 113 Train Acc : 0.5476000014543533 Test Acc : 0.4817500017210841\n",
      "\n",
      "Current : 114 Train Acc : 0.5494000004529953 Test Acc : 0.4825000014901161\n",
      "\n",
      "Current : 115 Train Acc : 0.5492000006437302 Test Acc : 0.4823750014603138\n",
      "\n",
      "Current : 116 Train Acc : 0.5542000004053116 Test Acc : 0.48287500120699406\n",
      "\n",
      "Current : 117 Train Acc : 0.5526000003814697 Test Acc : 0.48650000140070915\n",
      "\n",
      "Current : 118 Train Acc : 0.5534000018835068 Test Acc : 0.4853750010579824\n",
      "\n",
      "Current : 119 Train Acc : 0.5574000016450882 Test Acc : 0.4852500015869737\n",
      "\n",
      "Current : 120 Train Acc : 0.5572000019550324 Test Acc : 0.487125001475215\n",
      "\n",
      "Current : 121 Train Acc : 0.5586000020503998 Test Acc : 0.48512500125914815\n",
      "\n",
      "Current : 122 Train Acc : 0.55880000269413 Test Acc : 0.4877500012144446\n",
      "\n",
      "Current : 123 Train Acc : 0.5596000015735626 Test Acc : 0.48962500162422656\n",
      "\n",
      "Current : 124 Train Acc : 0.559800001502037 Test Acc : 0.4886250015348196\n",
      "\n",
      "Current : 125 Train Acc : 0.55900000166893 Test Acc : 0.49025000173598526\n",
      "\n",
      "Current : 126 Train Acc : 0.5592000017166138 Test Acc : 0.49087500154972075\n",
      "\n",
      "Current : 127 Train Acc : 0.5586000019311905 Test Acc : 0.4895000013709068\n",
      "\n",
      "Current : 128 Train Acc : 0.5604000010490418 Test Acc : 0.4905000017210841\n",
      "\n",
      "Current : 129 Train Acc : 0.5628000020980835 Test Acc : 0.49062500182539226\n",
      "\n",
      "Current : 130 Train Acc : 0.5624000015258789 Test Acc : 0.4908750017359853\n",
      "\n",
      "Current : 131 Train Acc : 0.5642000013589858 Test Acc : 0.4925000014528632\n",
      "\n",
      "Current : 132 Train Acc : 0.5648000017404556 Test Acc : 0.491500001065433\n",
      "\n",
      "Current : 133 Train Acc : 0.5662000014781952 Test Acc : 0.49100000131875277\n",
      "\n",
      "Current : 134 Train Acc : 0.567400000333786 Test Acc : 0.49175000105053185\n",
      "\n",
      "Current : 135 Train Acc : 0.5676000008583069 Test Acc : 0.4915000008419156\n",
      "\n",
      "Current : 136 Train Acc : 0.5696000002622604 Test Acc : 0.49225000113248824\n",
      "\n",
      "Current : 137 Train Acc : 0.5714000004529953 Test Acc : 0.4926250019669533\n",
      "\n",
      "Current : 138 Train Acc : 0.5704000008106231 Test Acc : 0.49275000162422655\n",
      "\n",
      "Current : 139 Train Acc : 0.5720000014305114 Test Acc : 0.4935000018030405\n",
      "\n",
      "Current : 140 Train Acc : 0.5722000014781952 Test Acc : 0.49112500160932543\n",
      "\n",
      "Current : 141 Train Acc : 0.5750000011920929 Test Acc : 0.4897500013187528\n",
      "\n",
      "Current : 142 Train Acc : 0.5752000007629394 Test Acc : 0.4922500012069941\n",
      "\n",
      "Current : 143 Train Acc : 0.5734000011682511 Test Acc : 0.49175000201910735\n",
      "\n",
      "Current : 144 Train Acc : 0.5766000010967255 Test Acc : 0.492500001527369\n",
      "\n",
      "Current : 145 Train Acc : 0.5778000020980835 Test Acc : 0.4931250013783574\n",
      "\n",
      "Current : 146 Train Acc : 0.577600001335144 Test Acc : 0.4928750018775463\n",
      "\n",
      "Current : 147 Train Acc : 0.5784000015258789 Test Acc : 0.4935000013560057\n",
      "\n",
      "Current : 148 Train Acc : 0.5810000015497208 Test Acc : 0.49387500144541263\n",
      "\n",
      "Current : 149 Train Acc : 0.5808000031709671 Test Acc : 0.4960000018030405\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# A\n",
    "current_exp_name = 'A';\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# create layers\n",
    "l1 = CNN(3,3, 16,which_reg=current_exp_name); \n",
    "l2 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "l3 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "\n",
    "l4 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "l5 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "l6 = CNN(3,16,10,which_reg=current_exp_name); \n",
    "\n",
    "# 2. graph \n",
    "x = tf.placeholder(tf.float32,(batch_size,96,96,3))\n",
    "y = tf.placeholder(tf.float32,(batch_size,10))\n",
    "\n",
    "layer1, layer1a = l1. feedforward(x,stride=2)\n",
    "layer2, layer2a = l2. feedforward(layer1a,stride=2)\n",
    "layer3, layer3a = l3. feedforward(layer2a,stride=2)\n",
    "layer4, layer4a = l4. feedforward(layer3a,stride=2)\n",
    "layer5, layer5a = l5. feedforward(layer4a)\n",
    "layer6, layer6a = l6. feedforward(layer5a)\n",
    "\n",
    "final_layer   = tf.reduce_mean(layer6a,(1,2))\n",
    "final_softmax = tf_softmax(final_layer)\n",
    "cost          = -tf.reduce_mean(y * tf.log(final_softmax + 1e-8))\n",
    "correct_prediction = tf.equal(tf.argmax(final_softmax, 1), tf.argmax(y, 1))\n",
    "accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "gradient = tf.tile((final_softmax-y)[:,None,None,:],[1,6,6,1])/batch_size\n",
    "grad6p,grad6w,grad6_up = l6.backprop(gradient)\n",
    "grad5p,grad5w,grad5_up = l5.backprop(grad6p)\n",
    "grad4p,grad4w,grad4_up = l4.backprop(grad5p,stride=2)\n",
    "grad3p,grad3w,grad3_up = l3.backprop(grad4p,stride=2)\n",
    "grad2p,grad2w,grad2_up = l2.backprop(grad3p,stride=2)\n",
    "grad1p,grad1w,grad1_up = l1.backprop(grad2p,stride=2)\n",
    "\n",
    "gradient_update = grad6_up + grad5_up + grad4_up + grad3_up + grad2_up + grad1_up \n",
    "\n",
    "# train\n",
    "sess.run(tf.global_variables_initializer())\n",
    "avg_acc_train = 0; avg_acc_test  = 0; train_acc = [];test_acc = []\n",
    "\n",
    "# mean std skew kurt non-zero\n",
    "llayer1 = [[],[],[],[],[]]; llayer2 = [[],[],[],[],[]]; llayer3 = [[],[],[],[],[]]\n",
    "llayer4 = [[],[],[],[],[]]; llayer5 = [[],[],[],[],[]]; llayer6 = [[],[],[],[],[]]\n",
    "\n",
    "llayer1a = [[],[],[],[],[]]; llayer2a = [[],[],[],[],[]]; llayer3a = [[],[],[],[],[]]\n",
    "llayer4a = [[],[],[],[],[]]; llayer5a = [[],[],[],[],[]]; llayer6a = [[],[],[],[],[]]\n",
    "\n",
    "weight1 = [[],[],[],[],[]]; weight2 = [[],[],[],[],[]]; weight3 = [[],[],[],[],[]];\n",
    "weight4 = [[],[],[],[],[]]; weight5 = [[],[],[],[],[]]; weight6 = [[],[],[],[],[]];\n",
    "\n",
    "gradw1  = [[],[],[],[],[]]; gradw2  = [[],[],[],[],[]]; gradw3  = [[],[],[],[],[]];\n",
    "gradw4  = [[],[],[],[],[]]; gradw5  = [[],[],[],[],[]]; gradw6  = [[],[],[],[],[]];\n",
    "\n",
    "gradp1  = [[],[],[],[],[]]; gradp2  = [[],[],[],[],[]]; gradp3  = [[],[],[],[],[]];\n",
    "gradp4  = [[],[],[],[],[]]; gradp5  = [[],[],[],[],[]]; gradp6  = [[],[],[],[],[]];\n",
    "\n",
    "gradup1  = [[],[],[],[],[]]; gradup2  = [[],[],[],[],[]]; gradup3  = [[],[],[],[],[]];\n",
    "gradup4  = [[],[],[],[],[]]; gradup5  = [[],[],[],[],[]]; gradup6  = [[],[],[],[],[]];\n",
    "\n",
    "list_of_outputs = [\n",
    "    layer1,layer2,layer3,layer4,layer5,layer6,\n",
    "    layer1a,layer2a,layer3a,layer4a,layer5a,layer6a,\n",
    "    l1.getw(),l2.getw(),l3.getw(),l4.getw(),l5.getw(),l6.getw(),\n",
    "    grad1w,grad2w,grad3w,grad4w,grad5w,grad6w,\n",
    "    grad1p,grad2p,grad3p,grad4p,grad5p,grad6p,\n",
    "    grad1_up[0],grad2_up[0],grad3_up[0],grad4_up[0],grad5_up[0],grad6_up[0]\n",
    "]\n",
    "\n",
    "for iter in range(num_epoch):\n",
    "\n",
    "    # Training Accuracy    \n",
    "    for current_batch_index in range(0,len(train_images),batch_size):\n",
    "        current_data  = train_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = train_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy,gradient_update],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(train_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_train = avg_acc_train + sess_results[0]\n",
    "        \n",
    "    # get the results\n",
    "    mid_stat = sess.run(list_of_outputs,feed_dict={x:current_data,y:current_label})\n",
    "    \n",
    "    # Test Accuracy    \n",
    "    for current_batch_index in range(0,len(test_images), batch_size):\n",
    "        current_data  = test_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = test_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(test_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_test = avg_acc_test + sess_results[0]   \n",
    "        \n",
    "    # ======================== extract stats ========================\n",
    "    llayer1 = append_stat(llayer1,mid_stat,0);  llayer2 = append_stat(llayer2,mid_stat,1);  llayer3 = append_stat(llayer3,mid_stat,2);\n",
    "    llayer4 = append_stat(llayer4,mid_stat,3);  llayer5 = append_stat(llayer5,mid_stat,4);  llayer6 = append_stat(llayer6,mid_stat,5);\n",
    "\n",
    "    llayer1a = append_stat(llayer1a,mid_stat,6);  llayer2a = append_stat(llayer2a,mid_stat,7);  llayer3a = append_stat(llayer3a,mid_stat,8);\n",
    "    llayer4a = append_stat(llayer4a,mid_stat,9);  llayer5a = append_stat(llayer5a,mid_stat,10); llayer6a = append_stat(llayer6a,mid_stat,11);\n",
    "    \n",
    "    weight1 = append_stat(weight1,mid_stat,12);  weight2 = append_stat(weight2,mid_stat,13);  weight3 = append_stat(weight3,mid_stat,14);\n",
    "    weight4 = append_stat(weight4,mid_stat,15);  weight5 = append_stat(weight5,mid_stat,16);  weight6 = append_stat(weight6,mid_stat,17);\n",
    "    \n",
    "    gradw1 = append_stat(gradw1,mid_stat,18); gradw2 = append_stat(gradw2,mid_stat,19); gradw3 = append_stat(gradw3,mid_stat,20);\n",
    "    gradw4 = append_stat(gradw4,mid_stat,21); gradw5 = append_stat(gradw5,mid_stat,22); gradw6 = append_stat(gradw6,mid_stat,23);\n",
    "    \n",
    "    gradp1 = append_stat(gradp1,mid_stat,24); gradp2 = append_stat(gradp2,mid_stat,25); gradp3 = append_stat(gradp3,mid_stat,26);\n",
    "    gradp4 = append_stat(gradp4,mid_stat,27); gradp5 = append_stat(gradp5,mid_stat,28); gradp6 = append_stat(gradp6,mid_stat,29);\n",
    "\n",
    "    gradup1 = append_stat(gradup1,mid_stat,30); gradup2 = append_stat(gradup2,mid_stat,31); gradup3 = append_stat(gradup3,mid_stat,32);\n",
    "    gradup4 = append_stat(gradup4,mid_stat,33); gradup5 = append_stat(gradup5,mid_stat,34); gradup6 = append_stat(gradup6,mid_stat,35);\n",
    "\n",
    "    train_acc.append(avg_acc_train/(len(train_images)/batch_size))\n",
    "    test_acc .append(avg_acc_test / (len(test_images)/batch_size))\n",
    "    # ======================== extract stats ========================\n",
    "    \n",
    "    # ======================== save to image ========================\n",
    "    save_to_image(mid_stat[0:6]   ,llayer1,llayer2,llayer3,llayer4,llayer5,llayer6,\"layer\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[6:12]  ,llayer1a,llayer2a,llayer3a,llayer4a,llayer5a,llayer6a,\"layera\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[12:18] ,weight1,weight2,weight3,weight4,weight5,weight6,\"weights\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[18:24] ,gradw1,gradw2,gradw3,gradw4,gradw5,gradw6,\"gradientw\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[24:30] ,gradp1,gradp2,gradp3,gradp4,gradp5,gradp6,\"gradientp\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[30:36] ,gradup1,gradup2,gradup3,gradup4,gradup5,gradup6,\"moment\",train_acc,test_acc,current_exp_name,iter)\n",
    "    # ======================== save to image ========================\n",
    "        \n",
    "    # ======================== print reset ========================\n",
    "    print(\"Current : \"+ str(iter) + \" Train Acc : \" + str(avg_acc_train/(len(train_images)/batch_size)) + \" Test Acc : \" + str(avg_acc_test/(len(test_images)/batch_size)) + '\\n')\n",
    "    avg_acc_train = 0 ; avg_acc_test  = 0\n",
    "    # ======================== print reset ========================\n",
    "\n",
    "np.save(current_exp_name+'/train_acc.npy',train_acc); np.save(current_exp_name+'/test_acc.npy', test_acc)    \n",
    "np.save(current_exp_name+'/llayer1.npy', llayer1);  np.save(current_exp_name+'/llayer2.npy', llayer2);  np.save(current_exp_name+'/llayer3.npy', llayer3); \n",
    "np.save(current_exp_name+'/llayer4.npy', llayer4);  np.save(current_exp_name+'/llayer5.npy', llayer5);  np.save(current_exp_name+'/llayer6.npy', llayer6); \n",
    "\n",
    "np.save(current_exp_name+'/llayer1a.npy', llayer1a);  np.save(current_exp_name+'/llayer2a.npy', llayer2a);  np.save(current_exp_name+'/llayer3a.npy', llayer3a); \n",
    "np.save(current_exp_name+'/llayer4a.npy', llayer4a);  np.save(current_exp_name+'/llayer5a.npy', llayer5a);  np.save(current_exp_name+'/llayer6a.npy', llayer6a); \n",
    "\n",
    "np.save(current_exp_name+'/weight1.npy', weight1);  np.save(current_exp_name+'/weight2.npy', weight2);  np.save(current_exp_name+'/weight3.npy', weight3);  \n",
    "np.save(current_exp_name+'/weight4.npy', weight4);  np.save(current_exp_name+'/weight5.npy', weight5);  np.save(current_exp_name+'/weight6.npy', weight6);  \n",
    "\n",
    "np.save(current_exp_name+'/gradw1.npy', gradw1); np.save(current_exp_name+'/gradw2.npy', gradw2); np.save(current_exp_name+'/gradw3.npy', gradw3);\n",
    "np.save(current_exp_name+'/gradw4.npy', gradw4); np.save(current_exp_name+'/gradw5.npy', gradw5); np.save(current_exp_name+'/gradw6.npy', gradw6);\n",
    "\n",
    "np.save(current_exp_name+'/gradp1.npy', gradp1); np.save(current_exp_name+'/gradp2.npy', gradp2); np.save(current_exp_name+'/gradp3.npy', gradp3);\n",
    "np.save(current_exp_name+'/gradp4.npy', gradp4); np.save(current_exp_name+'/gradp5.npy', gradp5); np.save(current_exp_name+'/gradp6.npy', gradp6);\n",
    "\n",
    "np.save(current_exp_name+'/gradup1.npy', gradup1); np.save(current_exp_name+'/gradup2.npy', gradup2); np.save(current_exp_name+'/gradup3.npy', gradup3);\n",
    "np.save(current_exp_name+'/gradup4.npy', gradup4); np.save(current_exp_name+'/gradup5.npy', gradup5); np.save(current_exp_name+'/gradup6.npy', gradup6);\n",
    "\n",
    "sess.close(); tf.reset_default_graph();\n",
    "\n",
    "%reset_selective -f l1,l2,l3,l4,l5,l6\n",
    "%reset_selective -f layer1,layer2,layer3,layer4,layer5,layer6\n",
    "%reset_selective -f layer1a,layer2a,layer3a,layer4a,layer5a,layer6a\n",
    "%reset_selective -f train_acc,test_acc,llayer1,llayer2,llayer3,llayer4,llayer5,llayer6,llayer1a,llayer2a,llayer3a,llayer4a,llayer5a,llayer6a\n",
    "%reset_selective -f weight1,weight2,weight3,weight4,weight5,weight6\n",
    "%reset_selective -f gradw1,gradw2,gradw3,gradw4,gradw5,gradw6\n",
    "%reset_selective -f gradp1,gradp2,gradp3,gradp4,gradp5,gradp6\n",
    "%reset_selective -f gradup1,gradup2,gradup3,gradup4,gradup5,gradup6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-05T03:53:47.443406Z",
     "start_time": "2019-01-05T03:04:25.541597Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current : 0 Train Acc : 0.13880000250041485 Test Acc : 0.14112500302493572\n",
      "\n",
      "Current : 1 Train Acc : 0.16340000315010547 Test Acc : 0.1870000028796494\n",
      "\n",
      "Current : 2 Train Acc : 0.18060000301897525 Test Acc : 0.20100000320002437\n",
      "\n",
      "Current : 3 Train Acc : 0.19960000337660314 Test Acc : 0.2270000031311065\n",
      "\n",
      "Current : 4 Train Acc : 0.2302000036239624 Test Acc : 0.25800000298768283\n",
      "\n",
      "Current : 5 Train Acc : 0.2560000031143427 Test Acc : 0.27962500222027303\n",
      "\n",
      "Current : 6 Train Acc : 0.27820000299811365 Test Acc : 0.29662500210106374\n",
      "\n",
      "Current : 7 Train Acc : 0.29180000261962413 Test Acc : 0.3046250015497208\n",
      "\n",
      "Current : 8 Train Acc : 0.3062000022083521 Test Acc : 0.31175000201910735\n",
      "\n",
      "Current : 9 Train Acc : 0.3142000021189451 Test Acc : 0.32212500132620336\n",
      "\n",
      "Current : 10 Train Acc : 0.31940000122785567 Test Acc : 0.32762500174343584\n",
      "\n",
      "Current : 11 Train Acc : 0.3250000011920929 Test Acc : 0.3311250019259751\n",
      "\n",
      "Current : 12 Train Acc : 0.33360000175237653 Test Acc : 0.3353750019147992\n",
      "\n",
      "Current : 13 Train Acc : 0.3376000009775162 Test Acc : 0.3425000018067658\n",
      "\n",
      "Current : 14 Train Acc : 0.3416000011712313 Test Acc : 0.3462500011362135\n",
      "\n",
      "Current : 15 Train Acc : 0.34420000171661375 Test Acc : 0.3525000012014061\n",
      "\n",
      "Current : 16 Train Acc : 0.34660000133514407 Test Acc : 0.3558750010468066\n",
      "\n",
      "Current : 17 Train Acc : 0.3500000008940697 Test Acc : 0.35812500076368453\n",
      "\n",
      "Current : 18 Train Acc : 0.3538000007867813 Test Acc : 0.3615000013634562\n",
      "\n",
      "Current : 19 Train Acc : 0.3576000013947487 Test Acc : 0.36525000113993883\n",
      "\n",
      "Current : 20 Train Acc : 0.3606000010371208 Test Acc : 0.3676250011473894\n",
      "\n",
      "Current : 21 Train Acc : 0.362200001180172 Test Acc : 0.3680000012367964\n",
      "\n",
      "Current : 22 Train Acc : 0.36620000141859055 Test Acc : 0.36887500070035456\n",
      "\n",
      "Current : 23 Train Acc : 0.36760000079870225 Test Acc : 0.37037500079721214\n",
      "\n",
      "Current : 24 Train Acc : 0.37040000057220457 Test Acc : 0.3746250010840595\n",
      "\n",
      "Current : 25 Train Acc : 0.3726000007390976 Test Acc : 0.3758750015683472\n",
      "\n",
      "Current : 26 Train Acc : 0.37380000060796736 Test Acc : 0.37900000140070916\n",
      "\n",
      "Current : 27 Train Acc : 0.37560000145435335 Test Acc : 0.3801250018179417\n",
      "\n",
      "Current : 28 Train Acc : 0.3800000010728836 Test Acc : 0.3800000022165477\n",
      "\n",
      "Current : 29 Train Acc : 0.38440000075101854 Test Acc : 0.38075000237673523\n",
      "\n",
      "Current : 30 Train Acc : 0.3828000009059906 Test Acc : 0.3820000021159649\n",
      "\n",
      "Current : 31 Train Acc : 0.38520000165700913 Test Acc : 0.3835000020824373\n",
      "\n",
      "Current : 32 Train Acc : 0.38700000175833704 Test Acc : 0.3852500022388995\n",
      "\n",
      "Current : 33 Train Acc : 0.3898000013828278 Test Acc : 0.3852500023134053\n",
      "\n",
      "Current : 34 Train Acc : 0.39320000129938126 Test Acc : 0.3862500022351742\n",
      "\n",
      "Current : 35 Train Acc : 0.3952000012397766 Test Acc : 0.3886250017955899\n",
      "\n",
      "Current : 36 Train Acc : 0.3978000009059906 Test Acc : 0.3888750007376075\n",
      "\n",
      "Current : 37 Train Acc : 0.39960000073909757 Test Acc : 0.39187500078231097\n",
      "\n",
      "Current : 38 Train Acc : 0.4018000010848045 Test Acc : 0.392000000923872\n",
      "\n",
      "Current : 39 Train Acc : 0.4066000006794929 Test Acc : 0.39400000097230076\n",
      "\n",
      "Current : 40 Train Acc : 0.40900000101327894 Test Acc : 0.39462500099092723\n",
      "\n",
      "Current : 41 Train Acc : 0.4096000016927719 Test Acc : 0.39462500089779495\n",
      "\n",
      "Current : 42 Train Acc : 0.4114000017940998 Test Acc : 0.39537500085309146\n",
      "\n",
      "Current : 43 Train Acc : 0.4142000017464161 Test Acc : 0.39637500101700424\n",
      "\n",
      "Current : 44 Train Acc : 0.41520000126957896 Test Acc : 0.3971250016056001\n",
      "\n",
      "Current : 45 Train Acc : 0.4176000012457371 Test Acc : 0.39862500118091704\n",
      "\n",
      "Current : 46 Train Acc : 0.41660000187158586 Test Acc : 0.3996250008791685\n",
      "\n",
      "Current : 47 Train Acc : 0.4190000021159649 Test Acc : 0.40025000166147945\n",
      "\n",
      "Current : 48 Train Acc : 0.4206000017821789 Test Acc : 0.4012500012293458\n",
      "\n",
      "Current : 49 Train Acc : 0.42120000192523005 Test Acc : 0.40175000090152024\n",
      "\n",
      "Current : 50 Train Acc : 0.423200001180172 Test Acc : 0.40450000066310166\n",
      "\n",
      "Current : 51 Train Acc : 0.42320000153779985 Test Acc : 0.4058750007301569\n",
      "\n",
      "Current : 52 Train Acc : 0.4262000016570091 Test Acc : 0.40912500105798244\n",
      "\n",
      "Current : 53 Train Acc : 0.4260000016093254 Test Acc : 0.4106250009313226\n",
      "\n",
      "Current : 54 Train Acc : 0.4300000012516975 Test Acc : 0.411500000692904\n",
      "\n",
      "Current : 55 Train Acc : 0.43040000152587893 Test Acc : 0.413125000372529\n",
      "\n",
      "Current : 56 Train Acc : 0.43120000094175337 Test Acc : 0.41450000073760745\n",
      "\n",
      "Current : 57 Train Acc : 0.433200001001358 Test Acc : 0.41562500074505804\n",
      "\n",
      "Current : 58 Train Acc : 0.43660000067949295 Test Acc : 0.4173750008642674\n",
      "\n",
      "Current : 59 Train Acc : 0.4388000007271767 Test Acc : 0.41825000088661907\n",
      "\n",
      "Current : 60 Train Acc : 0.440800000667572 Test Acc : 0.4192500009387732\n",
      "\n",
      "Current : 61 Train Acc : 0.4398000010251999 Test Acc : 0.4182500013336539\n",
      "\n",
      "Current : 62 Train Acc : 0.44280000060796737 Test Acc : 0.4191250013932586\n",
      "\n",
      "Current : 63 Train Acc : 0.4452000012397766 Test Acc : 0.4207500013336539\n",
      "\n",
      "Current : 64 Train Acc : 0.4480000007748604 Test Acc : 0.42075000118464234\n",
      "\n",
      "Current : 65 Train Acc : 0.44880000084638594 Test Acc : 0.4230000015348196\n",
      "\n",
      "Current : 66 Train Acc : 0.4502000010609627 Test Acc : 0.42337500158697366\n",
      "\n",
      "Current : 67 Train Acc : 0.44960000067949296 Test Acc : 0.4248750017955899\n",
      "\n",
      "Current : 68 Train Acc : 0.45200000125169754 Test Acc : 0.4248750016838312\n",
      "\n",
      "Current : 69 Train Acc : 0.4526000009179115 Test Acc : 0.427125001847744\n",
      "\n",
      "Current : 70 Train Acc : 0.4564000012278557 Test Acc : 0.4291250020638108\n",
      "\n",
      "Current : 71 Train Acc : 0.4584000009894371 Test Acc : 0.42850000157952306\n",
      "\n",
      "Current : 72 Train Acc : 0.4608000010251999 Test Acc : 0.4313750014081597\n",
      "\n",
      "Current : 73 Train Acc : 0.4628000012040138 Test Acc : 0.43050000112503767\n",
      "\n",
      "Current : 74 Train Acc : 0.46360000103712085 Test Acc : 0.4311250016838312\n",
      "\n",
      "Current : 75 Train Acc : 0.4656000008583069 Test Acc : 0.4335000008717179\n",
      "\n",
      "Current : 76 Train Acc : 0.4670000010728836 Test Acc : 0.43300000108778475\n",
      "\n",
      "Current : 77 Train Acc : 0.4692000012397766 Test Acc : 0.4330000014603138\n",
      "\n",
      "Current : 78 Train Acc : 0.4704000011086464 Test Acc : 0.4347500013932586\n",
      "\n",
      "Current : 79 Train Acc : 0.4726000010371208 Test Acc : 0.4326250008121133\n",
      "\n",
      "Current : 80 Train Acc : 0.4736000009775162 Test Acc : 0.43350000090897084\n",
      "\n",
      "Current : 81 Train Acc : 0.4744000012278557 Test Acc : 0.43375000085681675\n",
      "\n",
      "Current : 82 Train Acc : 0.4752000009417534 Test Acc : 0.4340000007674098\n",
      "\n",
      "Current : 83 Train Acc : 0.47600000029802325 Test Acc : 0.4333750006183982\n",
      "\n",
      "Current : 84 Train Acc : 0.47720000010728836 Test Acc : 0.4338750008493662\n",
      "\n",
      "Current : 85 Train Acc : 0.4784000009894371 Test Acc : 0.43275000125169755\n",
      "\n",
      "Current : 86 Train Acc : 0.4798000009059906 Test Acc : 0.431500001065433\n",
      "\n",
      "Current : 87 Train Acc : 0.48080000120401384 Test Acc : 0.4323750013113022\n",
      "\n",
      "Current : 88 Train Acc : 0.4806000011563301 Test Acc : 0.43387500125914813\n",
      "\n",
      "Current : 89 Train Acc : 0.483600001513958 Test Acc : 0.43525000140070913\n",
      "\n",
      "Current : 90 Train Acc : 0.48540000092983243 Test Acc : 0.43450000148266554\n",
      "\n",
      "Current : 91 Train Acc : 0.4848000011444092 Test Acc : 0.4360000015050173\n",
      "\n",
      "Current : 92 Train Acc : 0.48520000141859054 Test Acc : 0.437375001385808\n",
      "\n",
      "Current : 93 Train Acc : 0.4878000007867813 Test Acc : 0.4372500014677644\n",
      "\n",
      "Current : 94 Train Acc : 0.48860000133514403 Test Acc : 0.4383750018104911\n",
      "\n",
      "Current : 95 Train Acc : 0.48920000112056733 Test Acc : 0.4387500014528632\n",
      "\n",
      "Current : 96 Train Acc : 0.48920000088214877 Test Acc : 0.44100000195205213\n",
      "\n",
      "Current : 97 Train Acc : 0.4896000007390976 Test Acc : 0.4422500015422702\n",
      "\n",
      "Current : 98 Train Acc : 0.49100000131130217 Test Acc : 0.44287500232458116\n",
      "\n",
      "Current : 99 Train Acc : 0.4930000015497208 Test Acc : 0.4442500020563602\n",
      "\n",
      "Current : 100 Train Acc : 0.4942000013589859 Test Acc : 0.4455000023543835\n",
      "\n",
      "Current : 101 Train Acc : 0.4942000005245209 Test Acc : 0.4463750017806888\n",
      "\n",
      "Current : 102 Train Acc : 0.49559999990463255 Test Acc : 0.4483750020712614\n",
      "\n",
      "Current : 103 Train Acc : 0.4939999998807907 Test Acc : 0.44950000163167714\n",
      "\n",
      "Current : 104 Train Acc : 0.49639999914169314 Test Acc : 0.4505000017955899\n",
      "\n",
      "Current : 105 Train Acc : 0.4966000002622604 Test Acc : 0.44987500172108413\n",
      "\n",
      "Current : 106 Train Acc : 0.4980000005364418 Test Acc : 0.450000001527369\n",
      "\n",
      "Current : 107 Train Acc : 0.4990000006556511 Test Acc : 0.4508750016987324\n",
      "\n",
      "Current : 108 Train Acc : 0.49960000038146973 Test Acc : 0.4516250018030405\n",
      "\n",
      "Current : 109 Train Acc : 0.5014000002741814 Test Acc : 0.45137500151991844\n",
      "\n",
      "Current : 110 Train Acc : 0.5022000005841255 Test Acc : 0.45087500136345626\n",
      "\n",
      "Current : 111 Train Acc : 0.5018000004887581 Test Acc : 0.45150000099092724\n",
      "\n",
      "Current : 112 Train Acc : 0.5032000007033348 Test Acc : 0.45175000112503766\n",
      "\n",
      "Current : 113 Train Acc : 0.5046000009179116 Test Acc : 0.4507500008493662\n",
      "\n",
      "Current : 114 Train Acc : 0.5054000008702279 Test Acc : 0.44950000125914813\n",
      "\n",
      "Current : 115 Train Acc : 0.5072000010609626 Test Acc : 0.44800000105053184\n",
      "\n",
      "Current : 116 Train Acc : 0.507600000679493 Test Acc : 0.4502500008791685\n",
      "\n",
      "Current : 117 Train Acc : 0.5102000015377999 Test Acc : 0.4508750008791685\n",
      "\n",
      "Current : 118 Train Acc : 0.5120000014901162 Test Acc : 0.4506250008568168\n",
      "\n",
      "Current : 119 Train Acc : 0.5110000008940697 Test Acc : 0.45112500082701446\n",
      "\n",
      "Current : 120 Train Acc : 0.5142000010609626 Test Acc : 0.4507500008493662\n",
      "\n",
      "Current : 121 Train Acc : 0.5154000014662743 Test Acc : 0.45287500131875275\n",
      "\n",
      "Current : 122 Train Acc : 0.5160000016093254 Test Acc : 0.4536250013485551\n",
      "\n",
      "Current : 123 Train Acc : 0.5166000009775161 Test Acc : 0.45150000110268595\n",
      "\n",
      "Current : 124 Train Acc : 0.5178000018000603 Test Acc : 0.4557500006631017\n",
      "\n",
      "Current : 125 Train Acc : 0.5162000026106834 Test Acc : 0.45437500029802325\n",
      "\n",
      "Current : 126 Train Acc : 0.5172000016570091 Test Acc : 0.45412500072270634\n",
      "\n",
      "Current : 127 Train Acc : 0.516400001347065 Test Acc : 0.45312500115484\n",
      "\n",
      "Current : 128 Train Acc : 0.5176000017523765 Test Acc : 0.45300000097602605\n",
      "\n",
      "Current : 129 Train Acc : 0.5184000014662743 Test Acc : 0.4532500008121133\n",
      "\n",
      "Current : 130 Train Acc : 0.5208000016808509 Test Acc : 0.4570000013336539\n",
      "\n",
      "Current : 131 Train Acc : 0.520600002348423 Test Acc : 0.4590000012144446\n",
      "\n",
      "Current : 132 Train Acc : 0.5222000017762184 Test Acc : 0.4575000008195639\n",
      "\n",
      "Current : 133 Train Acc : 0.5226000017523765 Test Acc : 0.45837500106543305\n",
      "\n",
      "Current : 134 Train Acc : 0.5226000010967254 Test Acc : 0.459500000551343\n",
      "\n",
      "Current : 135 Train Acc : 0.5236000016331672 Test Acc : 0.45987500105053186\n",
      "\n",
      "Current : 136 Train Acc : 0.5256000013947487 Test Acc : 0.4607500011846423\n",
      "\n",
      "Current : 137 Train Acc : 0.5276000013947487 Test Acc : 0.46100000105798244\n",
      "\n",
      "Current : 138 Train Acc : 0.527600001513958 Test Acc : 0.4656250007264316\n",
      "\n",
      "Current : 139 Train Acc : 0.5274000014662743 Test Acc : 0.4645000007376075\n",
      "\n",
      "Current : 140 Train Acc : 0.5284000014662743 Test Acc : 0.46462500052526595\n",
      "\n",
      "Current : 141 Train Acc : 0.5298000014424324 Test Acc : 0.4645000002719462\n",
      "\n",
      "Current : 142 Train Acc : 0.5296000009179115 Test Acc : 0.4650000005401671\n",
      "\n",
      "Current : 143 Train Acc : 0.530200001001358 Test Acc : 0.4658750001899898\n",
      "\n",
      "Current : 144 Train Acc : 0.5324000009298324 Test Acc : 0.46737500010058286\n",
      "\n",
      "Current : 145 Train Acc : 0.5346000014543534 Test Acc : 0.4671250003390014\n",
      "\n",
      "Current : 146 Train Acc : 0.5362000014781952 Test Acc : 0.46675000060349703\n",
      "\n",
      "Current : 147 Train Acc : 0.5382000019550324 Test Acc : 0.46725000102072956\n",
      "\n",
      "Current : 148 Train Acc : 0.5412000013589859 Test Acc : 0.469125000257045\n",
      "\n",
      "Current : 149 Train Acc : 0.5398000005483627 Test Acc : 0.46937500055879355\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# B\n",
    "current_exp_name = 'B';\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# create layers\n",
    "l1 = CNN(3,3, 16,which_reg=current_exp_name); \n",
    "l2 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "l3 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "\n",
    "l4 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "l5 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "l6 = CNN(3,16,10,which_reg=current_exp_name); \n",
    "\n",
    "# 2. graph \n",
    "x = tf.placeholder(tf.float32,(batch_size,96,96,3))\n",
    "y = tf.placeholder(tf.float32,(batch_size,10))\n",
    "\n",
    "layer1, layer1a = l1. feedforward(x,stride=2)\n",
    "layer2, layer2a = l2. feedforward(layer1a,stride=2)\n",
    "layer3, layer3a = l3. feedforward(layer2a,stride=2)\n",
    "layer4, layer4a = l4. feedforward(layer3a,stride=2)\n",
    "layer5, layer5a = l5. feedforward(layer4a)\n",
    "layer6, layer6a = l6. feedforward(layer5a)\n",
    "\n",
    "final_layer   = tf.reduce_mean(layer6a,(1,2))\n",
    "final_softmax = tf_softmax(final_layer)\n",
    "cost          = -tf.reduce_mean(y * tf.log(final_softmax + 1e-8))\n",
    "correct_prediction = tf.equal(tf.argmax(final_softmax, 1), tf.argmax(y, 1))\n",
    "accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "gradient = tf.tile((final_softmax-y)[:,None,None,:],[1,6,6,1])/batch_size\n",
    "grad6p,grad6w,grad6_up = l6.backprop(gradient)\n",
    "grad5p,grad5w,grad5_up = l5.backprop(grad6p)\n",
    "grad4p,grad4w,grad4_up = l4.backprop(grad5p,stride=2)\n",
    "grad3p,grad3w,grad3_up = l3.backprop(grad4p,stride=2)\n",
    "grad2p,grad2w,grad2_up = l2.backprop(grad3p,stride=2)\n",
    "grad1p,grad1w,grad1_up = l1.backprop(grad2p,stride=2)\n",
    "\n",
    "gradient_update = grad6_up + grad5_up + grad4_up + grad3_up + grad2_up + grad1_up \n",
    "\n",
    "# train\n",
    "sess.run(tf.global_variables_initializer())\n",
    "avg_acc_train = 0; avg_acc_test  = 0; train_acc = [];test_acc = []\n",
    "\n",
    "# mean std skew kurt non-zero\n",
    "llayer1 = [[],[],[],[],[]]; llayer2 = [[],[],[],[],[]]; llayer3 = [[],[],[],[],[]]\n",
    "llayer4 = [[],[],[],[],[]]; llayer5 = [[],[],[],[],[]]; llayer6 = [[],[],[],[],[]]\n",
    "\n",
    "llayer1a = [[],[],[],[],[]]; llayer2a = [[],[],[],[],[]]; llayer3a = [[],[],[],[],[]]\n",
    "llayer4a = [[],[],[],[],[]]; llayer5a = [[],[],[],[],[]]; llayer6a = [[],[],[],[],[]]\n",
    "\n",
    "weight1 = [[],[],[],[],[]]; weight2 = [[],[],[],[],[]]; weight3 = [[],[],[],[],[]];\n",
    "weight4 = [[],[],[],[],[]]; weight5 = [[],[],[],[],[]]; weight6 = [[],[],[],[],[]];\n",
    "\n",
    "gradw1  = [[],[],[],[],[]]; gradw2  = [[],[],[],[],[]]; gradw3  = [[],[],[],[],[]];\n",
    "gradw4  = [[],[],[],[],[]]; gradw5  = [[],[],[],[],[]]; gradw6  = [[],[],[],[],[]];\n",
    "\n",
    "gradp1  = [[],[],[],[],[]]; gradp2  = [[],[],[],[],[]]; gradp3  = [[],[],[],[],[]];\n",
    "gradp4  = [[],[],[],[],[]]; gradp5  = [[],[],[],[],[]]; gradp6  = [[],[],[],[],[]];\n",
    "\n",
    "gradup1  = [[],[],[],[],[]]; gradup2  = [[],[],[],[],[]]; gradup3  = [[],[],[],[],[]];\n",
    "gradup4  = [[],[],[],[],[]]; gradup5  = [[],[],[],[],[]]; gradup6  = [[],[],[],[],[]];\n",
    "\n",
    "list_of_outputs = [\n",
    "    layer1,layer2,layer3,layer4,layer5,layer6,\n",
    "    layer1a,layer2a,layer3a,layer4a,layer5a,layer6a,\n",
    "    l1.getw(),l2.getw(),l3.getw(),l4.getw(),l5.getw(),l6.getw(),\n",
    "    grad1w,grad2w,grad3w,grad4w,grad5w,grad6w,\n",
    "    grad1p,grad2p,grad3p,grad4p,grad5p,grad6p,\n",
    "    grad1_up[0],grad2_up[0],grad3_up[0],grad4_up[0],grad5_up[0],grad6_up[0]\n",
    "]\n",
    "\n",
    "for iter in range(num_epoch):\n",
    "\n",
    "    # Training Accuracy    \n",
    "    for current_batch_index in range(0,len(train_images),batch_size):\n",
    "        current_data  = train_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = train_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy,gradient_update],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(train_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_train = avg_acc_train + sess_results[0]\n",
    "        \n",
    "    # get the results\n",
    "    mid_stat = sess.run(list_of_outputs,feed_dict={x:current_data,y:current_label})\n",
    "    \n",
    "    # Test Accuracy    \n",
    "    for current_batch_index in range(0,len(test_images), batch_size):\n",
    "        current_data  = test_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = test_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(test_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_test = avg_acc_test + sess_results[0]   \n",
    "        \n",
    "    # ======================== extract stats ========================\n",
    "    llayer1 = append_stat(llayer1,mid_stat,0);  llayer2 = append_stat(llayer2,mid_stat,1);  llayer3 = append_stat(llayer3,mid_stat,2);\n",
    "    llayer4 = append_stat(llayer4,mid_stat,3);  llayer5 = append_stat(llayer5,mid_stat,4);  llayer6 = append_stat(llayer6,mid_stat,5);\n",
    "\n",
    "    llayer1a = append_stat(llayer1a,mid_stat,6);  llayer2a = append_stat(llayer2a,mid_stat,7);  llayer3a = append_stat(llayer3a,mid_stat,8);\n",
    "    llayer4a = append_stat(llayer4a,mid_stat,9);  llayer5a = append_stat(llayer5a,mid_stat,10); llayer6a = append_stat(llayer6a,mid_stat,11);\n",
    "    \n",
    "    weight1 = append_stat(weight1,mid_stat,12);  weight2 = append_stat(weight2,mid_stat,13);  weight3 = append_stat(weight3,mid_stat,14);\n",
    "    weight4 = append_stat(weight4,mid_stat,15);  weight5 = append_stat(weight5,mid_stat,16);  weight6 = append_stat(weight6,mid_stat,17);\n",
    "    \n",
    "    gradw1 = append_stat(gradw1,mid_stat,18); gradw2 = append_stat(gradw2,mid_stat,19); gradw3 = append_stat(gradw3,mid_stat,20);\n",
    "    gradw4 = append_stat(gradw4,mid_stat,21); gradw5 = append_stat(gradw5,mid_stat,22); gradw6 = append_stat(gradw6,mid_stat,23);\n",
    "    \n",
    "    gradp1 = append_stat(gradp1,mid_stat,24); gradp2 = append_stat(gradp2,mid_stat,25); gradp3 = append_stat(gradp3,mid_stat,26);\n",
    "    gradp4 = append_stat(gradp4,mid_stat,27); gradp5 = append_stat(gradp5,mid_stat,28); gradp6 = append_stat(gradp6,mid_stat,29);\n",
    "\n",
    "    gradup1 = append_stat(gradup1,mid_stat,30); gradup2 = append_stat(gradup2,mid_stat,31); gradup3 = append_stat(gradup3,mid_stat,32);\n",
    "    gradup4 = append_stat(gradup4,mid_stat,33); gradup5 = append_stat(gradup5,mid_stat,34); gradup6 = append_stat(gradup6,mid_stat,35);\n",
    "\n",
    "    train_acc.append(avg_acc_train/(len(train_images)/batch_size))\n",
    "    test_acc .append(avg_acc_test / (len(test_images)/batch_size))\n",
    "    # ======================== extract stats ========================\n",
    "    \n",
    "    # ======================== save to image ========================\n",
    "    save_to_image(mid_stat[0:6]   ,llayer1,llayer2,llayer3,llayer4,llayer5,llayer6,\"layer\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[6:12]  ,llayer1a,llayer2a,llayer3a,llayer4a,llayer5a,llayer6a,\"layera\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[12:18] ,weight1,weight2,weight3,weight4,weight5,weight6,\"weights\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[18:24] ,gradw1,gradw2,gradw3,gradw4,gradw5,gradw6,\"gradientw\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[24:30] ,gradp1,gradp2,gradp3,gradp4,gradp5,gradp6,\"gradientp\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[30:36] ,gradup1,gradup2,gradup3,gradup4,gradup5,gradup6,\"moment\",train_acc,test_acc,current_exp_name,iter)\n",
    "    # ======================== save to image ========================\n",
    "        \n",
    "    # ======================== print reset ========================\n",
    "    print(\"Current : \"+ str(iter) + \" Train Acc : \" + str(avg_acc_train/(len(train_images)/batch_size)) + \" Test Acc : \" + str(avg_acc_test/(len(test_images)/batch_size)) + '\\n')\n",
    "    avg_acc_train = 0 ; avg_acc_test  = 0\n",
    "    # ======================== print reset ========================\n",
    "\n",
    "np.save(current_exp_name+'/train_acc.npy',train_acc); np.save(current_exp_name+'/test_acc.npy', test_acc)    \n",
    "np.save(current_exp_name+'/llayer1.npy', llayer1);  np.save(current_exp_name+'/llayer2.npy', llayer2);  np.save(current_exp_name+'/llayer3.npy', llayer3); \n",
    "np.save(current_exp_name+'/llayer4.npy', llayer4);  np.save(current_exp_name+'/llayer5.npy', llayer5);  np.save(current_exp_name+'/llayer6.npy', llayer6); \n",
    "\n",
    "np.save(current_exp_name+'/llayer1a.npy', llayer1a);  np.save(current_exp_name+'/llayer2a.npy', llayer2a);  np.save(current_exp_name+'/llayer3a.npy', llayer3a); \n",
    "np.save(current_exp_name+'/llayer4a.npy', llayer4a);  np.save(current_exp_name+'/llayer5a.npy', llayer5a);  np.save(current_exp_name+'/llayer6a.npy', llayer6a); \n",
    "\n",
    "np.save(current_exp_name+'/weight1.npy', weight1);  np.save(current_exp_name+'/weight2.npy', weight2);  np.save(current_exp_name+'/weight3.npy', weight3);  \n",
    "np.save(current_exp_name+'/weight4.npy', weight4);  np.save(current_exp_name+'/weight5.npy', weight5);  np.save(current_exp_name+'/weight6.npy', weight6);  \n",
    "\n",
    "np.save(current_exp_name+'/gradw1.npy', gradw1); np.save(current_exp_name+'/gradw2.npy', gradw2); np.save(current_exp_name+'/gradw3.npy', gradw3);\n",
    "np.save(current_exp_name+'/gradw4.npy', gradw4); np.save(current_exp_name+'/gradw5.npy', gradw5); np.save(current_exp_name+'/gradw6.npy', gradw6);\n",
    "\n",
    "np.save(current_exp_name+'/gradp1.npy', gradp1); np.save(current_exp_name+'/gradp2.npy', gradp2); np.save(current_exp_name+'/gradp3.npy', gradp3);\n",
    "np.save(current_exp_name+'/gradp4.npy', gradp4); np.save(current_exp_name+'/gradp5.npy', gradp5); np.save(current_exp_name+'/gradp6.npy', gradp6);\n",
    "\n",
    "np.save(current_exp_name+'/gradup1.npy', gradup1); np.save(current_exp_name+'/gradup2.npy', gradup2); np.save(current_exp_name+'/gradup3.npy', gradup3);\n",
    "np.save(current_exp_name+'/gradup4.npy', gradup4); np.save(current_exp_name+'/gradup5.npy', gradup5); np.save(current_exp_name+'/gradup6.npy', gradup6);\n",
    "\n",
    "sess.close(); tf.reset_default_graph();\n",
    "\n",
    "%reset_selective -f l1,l2,l3,l4,l5,l6\n",
    "%reset_selective -f layer1,layer2,layer3,layer4,layer5,layer6\n",
    "%reset_selective -f layer1a,layer2a,layer3a,layer4a,layer5a,layer6a\n",
    "%reset_selective -f train_acc,test_acc,llayer1,llayer2,llayer3,llayer4,llayer5,llayer6,llayer1a,llayer2a,llayer3a,llayer4a,llayer5a,llayer6a\n",
    "%reset_selective -f weight1,weight2,weight3,weight4,weight5,weight6\n",
    "%reset_selective -f gradw1,gradw2,gradw3,gradw4,gradw5,gradw6\n",
    "%reset_selective -f gradp1,gradp2,gradp3,gradp4,gradp5,gradp6\n",
    "%reset_selective -f gradup1,gradup2,gradup3,gradup4,gradup5,gradup6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-05T04:46:24.454272Z",
     "start_time": "2019-01-05T03:53:47.474382Z"
    },
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current : 0 Train Acc : 0.10000000189244747 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 1 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 2 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 3 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 4 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 5 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 6 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 7 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 8 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 9 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 10 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 11 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 12 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 13 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 14 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 15 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 16 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 17 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 18 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 19 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 20 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 21 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 22 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 23 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 24 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 25 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 26 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 27 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 28 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 29 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 30 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 31 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 32 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 33 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 34 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 35 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 36 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 37 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 38 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 39 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 40 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 41 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 42 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 43 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 44 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 45 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 46 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 47 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 48 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 49 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 50 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 51 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 52 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 53 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 54 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 55 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 56 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 57 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 58 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 59 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 60 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 61 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 62 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 63 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 64 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 65 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 66 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 67 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 68 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 69 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 70 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 71 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 72 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 73 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 74 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 75 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 76 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 77 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 78 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 79 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 80 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 81 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 82 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 83 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 84 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 85 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 86 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 87 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 88 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 89 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 90 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 91 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 92 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 93 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 94 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 95 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 96 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 97 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 98 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 99 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 100 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 101 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 102 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 103 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 104 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 105 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 106 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 107 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current : 108 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 109 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 110 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 111 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 112 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 113 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 114 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 115 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 116 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 117 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 118 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 119 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 120 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 121 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 122 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 123 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 124 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 125 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 126 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 127 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 128 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 129 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 130 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 131 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 132 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 133 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 134 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 135 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 136 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 137 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 138 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 139 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 140 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 141 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 142 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 143 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 144 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 145 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 146 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 147 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 148 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current : 149 Train Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# C\n",
    "current_exp_name = 'C';\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# create layers\n",
    "l1 = CNN(3,3, 16,which_reg=current_exp_name); \n",
    "l2 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "l3 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "\n",
    "l4 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "l5 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "l6 = CNN(3,16,10,which_reg=current_exp_name); \n",
    "\n",
    "# 2. graph \n",
    "x = tf.placeholder(tf.float32,(batch_size,96,96,3))\n",
    "y = tf.placeholder(tf.float32,(batch_size,10))\n",
    "\n",
    "layer1, layer1a = l1. feedforward(x,stride=2)\n",
    "layer2, layer2a = l2. feedforward(layer1a,stride=2)\n",
    "layer3, layer3a = l3. feedforward(layer2a,stride=2)\n",
    "layer4, layer4a = l4. feedforward(layer3a,stride=2)\n",
    "layer5, layer5a = l5. feedforward(layer4a)\n",
    "layer6, layer6a = l6. feedforward(layer5a)\n",
    "\n",
    "final_layer   = tf.reduce_mean(layer6a,(1,2))\n",
    "final_softmax = tf_softmax(final_layer)\n",
    "cost          = -tf.reduce_mean(y * tf.log(final_softmax + 1e-8))\n",
    "correct_prediction = tf.equal(tf.argmax(final_softmax, 1), tf.argmax(y, 1))\n",
    "accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "gradient = tf.tile((final_softmax-y)[:,None,None,:],[1,6,6,1])/batch_size\n",
    "grad6p,grad6w,grad6_up = l6.backprop(gradient)\n",
    "grad5p,grad5w,grad5_up = l5.backprop(grad6p)\n",
    "grad4p,grad4w,grad4_up = l4.backprop(grad5p,stride=2)\n",
    "grad3p,grad3w,grad3_up = l3.backprop(grad4p,stride=2)\n",
    "grad2p,grad2w,grad2_up = l2.backprop(grad3p,stride=2)\n",
    "grad1p,grad1w,grad1_up = l1.backprop(grad2p,stride=2)\n",
    "\n",
    "gradient_update = grad6_up + grad5_up + grad4_up + grad3_up + grad2_up + grad1_up \n",
    "\n",
    "# train\n",
    "sess.run(tf.global_variables_initializer())\n",
    "avg_acc_train = 0; avg_acc_test  = 0; train_acc = [];test_acc = []\n",
    "\n",
    "# mean std skew kurt non-zero\n",
    "llayer1 = [[],[],[],[],[]]; llayer2 = [[],[],[],[],[]]; llayer3 = [[],[],[],[],[]]\n",
    "llayer4 = [[],[],[],[],[]]; llayer5 = [[],[],[],[],[]]; llayer6 = [[],[],[],[],[]]\n",
    "\n",
    "llayer1a = [[],[],[],[],[]]; llayer2a = [[],[],[],[],[]]; llayer3a = [[],[],[],[],[]]\n",
    "llayer4a = [[],[],[],[],[]]; llayer5a = [[],[],[],[],[]]; llayer6a = [[],[],[],[],[]]\n",
    "\n",
    "weight1 = [[],[],[],[],[]]; weight2 = [[],[],[],[],[]]; weight3 = [[],[],[],[],[]];\n",
    "weight4 = [[],[],[],[],[]]; weight5 = [[],[],[],[],[]]; weight6 = [[],[],[],[],[]];\n",
    "\n",
    "gradw1  = [[],[],[],[],[]]; gradw2  = [[],[],[],[],[]]; gradw3  = [[],[],[],[],[]];\n",
    "gradw4  = [[],[],[],[],[]]; gradw5  = [[],[],[],[],[]]; gradw6  = [[],[],[],[],[]];\n",
    "\n",
    "gradp1  = [[],[],[],[],[]]; gradp2  = [[],[],[],[],[]]; gradp3  = [[],[],[],[],[]];\n",
    "gradp4  = [[],[],[],[],[]]; gradp5  = [[],[],[],[],[]]; gradp6  = [[],[],[],[],[]];\n",
    "\n",
    "gradup1  = [[],[],[],[],[]]; gradup2  = [[],[],[],[],[]]; gradup3  = [[],[],[],[],[]];\n",
    "gradup4  = [[],[],[],[],[]]; gradup5  = [[],[],[],[],[]]; gradup6  = [[],[],[],[],[]];\n",
    "\n",
    "list_of_outputs = [\n",
    "    layer1,layer2,layer3,layer4,layer5,layer6,\n",
    "    layer1a,layer2a,layer3a,layer4a,layer5a,layer6a,\n",
    "    l1.getw(),l2.getw(),l3.getw(),l4.getw(),l5.getw(),l6.getw(),\n",
    "    grad1w,grad2w,grad3w,grad4w,grad5w,grad6w,\n",
    "    grad1p,grad2p,grad3p,grad4p,grad5p,grad6p,\n",
    "    grad1_up[0],grad2_up[0],grad3_up[0],grad4_up[0],grad5_up[0],grad6_up[0]\n",
    "]\n",
    "\n",
    "for iter in range(num_epoch):\n",
    "\n",
    "    # Training Accuracy    \n",
    "    for current_batch_index in range(0,len(train_images),batch_size):\n",
    "        current_data  = train_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = train_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy,gradient_update],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(train_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_train = avg_acc_train + sess_results[0]\n",
    "        \n",
    "    # get the results\n",
    "    mid_stat = sess.run(list_of_outputs,feed_dict={x:current_data,y:current_label})\n",
    "    \n",
    "    # Test Accuracy    \n",
    "    for current_batch_index in range(0,len(test_images), batch_size):\n",
    "        current_data  = test_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = test_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(test_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_test = avg_acc_test + sess_results[0]   \n",
    "        \n",
    "    # ======================== extract stats ========================\n",
    "    llayer1 = append_stat(llayer1,mid_stat,0);  llayer2 = append_stat(llayer2,mid_stat,1);  llayer3 = append_stat(llayer3,mid_stat,2);\n",
    "    llayer4 = append_stat(llayer4,mid_stat,3);  llayer5 = append_stat(llayer5,mid_stat,4);  llayer6 = append_stat(llayer6,mid_stat,5);\n",
    "\n",
    "    llayer1a = append_stat(llayer1a,mid_stat,6);  llayer2a = append_stat(llayer2a,mid_stat,7);  llayer3a = append_stat(llayer3a,mid_stat,8);\n",
    "    llayer4a = append_stat(llayer4a,mid_stat,9);  llayer5a = append_stat(llayer5a,mid_stat,10); llayer6a = append_stat(llayer6a,mid_stat,11);\n",
    "    \n",
    "    weight1 = append_stat(weight1,mid_stat,12);  weight2 = append_stat(weight2,mid_stat,13);  weight3 = append_stat(weight3,mid_stat,14);\n",
    "    weight4 = append_stat(weight4,mid_stat,15);  weight5 = append_stat(weight5,mid_stat,16);  weight6 = append_stat(weight6,mid_stat,17);\n",
    "    \n",
    "    gradw1 = append_stat(gradw1,mid_stat,18); gradw2 = append_stat(gradw2,mid_stat,19); gradw3 = append_stat(gradw3,mid_stat,20);\n",
    "    gradw4 = append_stat(gradw4,mid_stat,21); gradw5 = append_stat(gradw5,mid_stat,22); gradw6 = append_stat(gradw6,mid_stat,23);\n",
    "    \n",
    "    gradp1 = append_stat(gradp1,mid_stat,24); gradp2 = append_stat(gradp2,mid_stat,25); gradp3 = append_stat(gradp3,mid_stat,26);\n",
    "    gradp4 = append_stat(gradp4,mid_stat,27); gradp5 = append_stat(gradp5,mid_stat,28); gradp6 = append_stat(gradp6,mid_stat,29);\n",
    "\n",
    "    gradup1 = append_stat(gradup1,mid_stat,30); gradup2 = append_stat(gradup2,mid_stat,31); gradup3 = append_stat(gradup3,mid_stat,32);\n",
    "    gradup4 = append_stat(gradup4,mid_stat,33); gradup5 = append_stat(gradup5,mid_stat,34); gradup6 = append_stat(gradup6,mid_stat,35);\n",
    "\n",
    "    train_acc.append(avg_acc_train/(len(train_images)/batch_size))\n",
    "    test_acc .append(avg_acc_test / (len(test_images)/batch_size))\n",
    "    # ======================== extract stats ========================\n",
    "    \n",
    "    # ======================== save to image ========================\n",
    "    save_to_image(mid_stat[0:6]   ,llayer1,llayer2,llayer3,llayer4,llayer5,llayer6,\"layer\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[6:12]  ,llayer1a,llayer2a,llayer3a,llayer4a,llayer5a,llayer6a,\"layera\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[12:18] ,weight1,weight2,weight3,weight4,weight5,weight6,\"weights\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[18:24] ,gradw1,gradw2,gradw3,gradw4,gradw5,gradw6,\"gradientw\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[24:30] ,gradp1,gradp2,gradp3,gradp4,gradp5,gradp6,\"gradientp\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[30:36] ,gradup1,gradup2,gradup3,gradup4,gradup5,gradup6,\"moment\",train_acc,test_acc,current_exp_name,iter)\n",
    "    # ======================== save to image ========================\n",
    "        \n",
    "    # ======================== print reset ========================\n",
    "    print(\"Current : \"+ str(iter) + \" Train Acc : \" + str(avg_acc_train/(len(train_images)/batch_size)) + \" Test Acc : \" + str(avg_acc_test/(len(test_images)/batch_size)) + '\\n')\n",
    "    avg_acc_train = 0 ; avg_acc_test  = 0\n",
    "    # ======================== print reset ========================\n",
    "\n",
    "np.save(current_exp_name+'/train_acc.npy',train_acc); np.save(current_exp_name+'/test_acc.npy', test_acc)    \n",
    "np.save(current_exp_name+'/llayer1.npy', llayer1);  np.save(current_exp_name+'/llayer2.npy', llayer2);  np.save(current_exp_name+'/llayer3.npy', llayer3); \n",
    "np.save(current_exp_name+'/llayer4.npy', llayer4);  np.save(current_exp_name+'/llayer5.npy', llayer5);  np.save(current_exp_name+'/llayer6.npy', llayer6); \n",
    "\n",
    "np.save(current_exp_name+'/llayer1a.npy', llayer1a);  np.save(current_exp_name+'/llayer2a.npy', llayer2a);  np.save(current_exp_name+'/llayer3a.npy', llayer3a); \n",
    "np.save(current_exp_name+'/llayer4a.npy', llayer4a);  np.save(current_exp_name+'/llayer5a.npy', llayer5a);  np.save(current_exp_name+'/llayer6a.npy', llayer6a); \n",
    "\n",
    "np.save(current_exp_name+'/weight1.npy', weight1);  np.save(current_exp_name+'/weight2.npy', weight2);  np.save(current_exp_name+'/weight3.npy', weight3);  \n",
    "np.save(current_exp_name+'/weight4.npy', weight4);  np.save(current_exp_name+'/weight5.npy', weight5);  np.save(current_exp_name+'/weight6.npy', weight6);  \n",
    "\n",
    "np.save(current_exp_name+'/gradw1.npy', gradw1); np.save(current_exp_name+'/gradw2.npy', gradw2); np.save(current_exp_name+'/gradw3.npy', gradw3);\n",
    "np.save(current_exp_name+'/gradw4.npy', gradw4); np.save(current_exp_name+'/gradw5.npy', gradw5); np.save(current_exp_name+'/gradw6.npy', gradw6);\n",
    "\n",
    "np.save(current_exp_name+'/gradp1.npy', gradp1); np.save(current_exp_name+'/gradp2.npy', gradp2); np.save(current_exp_name+'/gradp3.npy', gradp3);\n",
    "np.save(current_exp_name+'/gradp4.npy', gradp4); np.save(current_exp_name+'/gradp5.npy', gradp5); np.save(current_exp_name+'/gradp6.npy', gradp6);\n",
    "\n",
    "np.save(current_exp_name+'/gradup1.npy', gradup1); np.save(current_exp_name+'/gradup2.npy', gradup2); np.save(current_exp_name+'/gradup3.npy', gradup3);\n",
    "np.save(current_exp_name+'/gradup4.npy', gradup4); np.save(current_exp_name+'/gradup5.npy', gradup5); np.save(current_exp_name+'/gradup6.npy', gradup6);\n",
    "\n",
    "sess.close(); tf.reset_default_graph();\n",
    "\n",
    "%reset_selective -f l1,l2,l3,l4,l5,l6\n",
    "%reset_selective -f layer1,layer2,layer3,layer4,layer5,layer6\n",
    "%reset_selective -f layer1a,layer2a,layer3a,layer4a,layer5a,layer6a\n",
    "%reset_selective -f train_acc,test_acc,llayer1,llayer2,llayer3,llayer4,llayer5,llayer6,llayer1a,llayer2a,llayer3a,llayer4a,llayer5a,llayer6a\n",
    "%reset_selective -f weight1,weight2,weight3,weight4,weight5,weight6\n",
    "%reset_selective -f gradw1,gradw2,gradw3,gradw4,gradw5,gradw6\n",
    "%reset_selective -f gradp1,gradp2,gradp3,gradp4,gradp5,gradp6\n",
    "%reset_selective -f gradup1,gradup2,gradup3,gradup4,gradup5,gradup6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-05T05:35:39.460841Z",
     "start_time": "2019-01-05T04:46:24.478435Z"
    },
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current : 0 Train Acc : 0.12040000247955322 Test Acc : 0.16450000292621553\n",
      "\n",
      "Current : 1 Train Acc : 0.16380000235140324 Test Acc : 0.20400000305846333\n",
      "\n",
      "Current : 2 Train Acc : 0.1922000028192997 Test Acc : 0.21750000334344805\n",
      "\n",
      "Current : 3 Train Acc : 0.21020000356435775 Test Acc : 0.23487500294111668\n",
      "\n",
      "Current : 4 Train Acc : 0.230600002810359 Test Acc : 0.24850000254809856\n",
      "\n",
      "Current : 5 Train Acc : 0.2346000032275915 Test Acc : 0.2565000024624169\n",
      "\n",
      "Current : 6 Train Acc : 0.2450000032633543 Test Acc : 0.26362500195391475\n",
      "\n",
      "Current : 7 Train Acc : 0.2528000033199787 Test Acc : 0.26987500249408186\n",
      "\n",
      "Current : 8 Train Acc : 0.26040000274777414 Test Acc : 0.275500002540648\n",
      "\n",
      "Current : 9 Train Acc : 0.2636000022739172 Test Acc : 0.2796250029001385\n",
      "\n",
      "Current : 10 Train Acc : 0.2662000025063753 Test Acc : 0.2858750028256327\n",
      "\n",
      "Current : 11 Train Acc : 0.28240000233054163 Test Acc : 0.2950000028125942\n",
      "\n",
      "Current : 12 Train Acc : 0.2936000028699636 Test Acc : 0.29837500195018946\n",
      "\n",
      "Current : 13 Train Acc : 0.3002000014036894 Test Acc : 0.30550000197254124\n",
      "\n",
      "Current : 14 Train Acc : 0.3052000017762184 Test Acc : 0.30962500187568365\n",
      "\n",
      "Current : 15 Train Acc : 0.3106000017374754 Test Acc : 0.31337500164285303\n",
      "\n",
      "Current : 16 Train Acc : 0.3158000017851591 Test Acc : 0.31600000204518436\n",
      "\n",
      "Current : 17 Train Acc : 0.3198000022917986 Test Acc : 0.3226250022929162\n",
      "\n",
      "Current : 18 Train Acc : 0.3266000021994114 Test Acc : 0.3256250019930303\n",
      "\n",
      "Current : 19 Train Acc : 0.3316000012606382 Test Acc : 0.3306250016111881\n",
      "\n",
      "Current : 20 Train Acc : 0.33500000251829626 Test Acc : 0.33525000161491336\n",
      "\n",
      "Current : 21 Train Acc : 0.34020000183582305 Test Acc : 0.3403750022035092\n",
      "\n",
      "Current : 22 Train Acc : 0.34620000171661375 Test Acc : 0.3433750019222498\n",
      "\n",
      "Current : 23 Train Acc : 0.35500000120699404 Test Acc : 0.34750000180676577\n",
      "\n",
      "Current : 24 Train Acc : 0.3588000008761883 Test Acc : 0.3513750014733523\n",
      "\n",
      "Current : 25 Train Acc : 0.36000000159442425 Test Acc : 0.3566250012814999\n",
      "\n",
      "Current : 26 Train Acc : 0.3628000005632639 Test Acc : 0.3622500019147992\n",
      "\n",
      "Current : 27 Train Acc : 0.3704000002145767 Test Acc : 0.3665000016242266\n",
      "\n",
      "Current : 28 Train Acc : 0.3762000013291836 Test Acc : 0.37212500125169756\n",
      "\n",
      "Current : 29 Train Acc : 0.37600000175833703 Test Acc : 0.37700000176206233\n",
      "\n",
      "Current : 30 Train Acc : 0.378400001347065 Test Acc : 0.38062500124797227\n",
      "\n",
      "Current : 31 Train Acc : 0.3814000015258789 Test Acc : 0.38187500113621353\n",
      "\n",
      "Current : 32 Train Acc : 0.3886000013947487 Test Acc : 0.3827500007115304\n",
      "\n",
      "Current : 33 Train Acc : 0.393400001347065 Test Acc : 0.38250000078231094\n",
      "\n",
      "Current : 34 Train Acc : 0.3966000013649464 Test Acc : 0.3853750004991889\n",
      "\n",
      "Current : 35 Train Acc : 0.40100000098347666 Test Acc : 0.38425000096671286\n",
      "\n",
      "Current : 36 Train Acc : 0.405400002092123 Test Acc : 0.3880000008549541\n",
      "\n",
      "Current : 37 Train Acc : 0.40580000230669977 Test Acc : 0.38787500056438146\n",
      "\n",
      "Current : 38 Train Acc : 0.4102000017762184 Test Acc : 0.3900000011082739\n",
      "\n",
      "Current : 39 Train Acc : 0.41180000227689745 Test Acc : 0.3883750005345792\n",
      "\n",
      "Current : 40 Train Acc : 0.4132000021338463 Test Acc : 0.39100000051781536\n",
      "\n",
      "Current : 41 Train Acc : 0.41520000231266024 Test Acc : 0.39212500082328916\n",
      "\n",
      "Current : 42 Train Acc : 0.41760000163316724 Test Acc : 0.3933750003017485\n",
      "\n",
      "Current : 43 Train Acc : 0.4218000007867813 Test Acc : 0.39787500055506825\n",
      "\n",
      "Current : 44 Train Acc : 0.422200001180172 Test Acc : 0.39787500036880374\n",
      "\n",
      "Current : 45 Train Acc : 0.4238000011444092 Test Acc : 0.3992500001005828\n",
      "\n",
      "Current : 46 Train Acc : 0.42560000163316725 Test Acc : 0.4011250001564622\n",
      "\n",
      "Current : 47 Train Acc : 0.42920000189542773 Test Acc : 0.40225000040605663\n",
      "\n",
      "Current : 48 Train Acc : 0.43420000141859055 Test Acc : 0.4053750005923212\n",
      "\n",
      "Current : 49 Train Acc : 0.43600000178813936 Test Acc : 0.4048750003986061\n",
      "\n",
      "Current : 50 Train Acc : 0.43700000166893005 Test Acc : 0.40762500030919907\n",
      "\n",
      "Current : 51 Train Acc : 0.4378000015020371 Test Acc : 0.4095000009052455\n",
      "\n",
      "Current : 52 Train Acc : 0.44040000063180923 Test Acc : 0.40887500083073974\n",
      "\n",
      "Current : 53 Train Acc : 0.4412000014781952 Test Acc : 0.4088750007003546\n",
      "\n",
      "Current : 54 Train Acc : 0.4442000009417534 Test Acc : 0.4085000005178154\n",
      "\n",
      "Current : 55 Train Acc : 0.44720000052452086 Test Acc : 0.41000000063329933\n",
      "\n",
      "Current : 56 Train Acc : 0.44780000001192094 Test Acc : 0.41125000121071936\n",
      "\n",
      "Current : 57 Train Acc : 0.4488000007271767 Test Acc : 0.40987500056624415\n",
      "\n",
      "Current : 58 Train Acc : 0.45060000026226044 Test Acc : 0.41450000073760745\n",
      "\n",
      "Current : 59 Train Acc : 0.4514000006914139 Test Acc : 0.4143750000372529\n",
      "\n",
      "Current : 60 Train Acc : 0.452600000500679 Test Acc : 0.4170000000298023\n",
      "\n",
      "Current : 61 Train Acc : 0.45600000059604645 Test Acc : 0.41650000028312206\n",
      "\n",
      "Current : 62 Train Acc : 0.45940000033378603 Test Acc : 0.4180000003799796\n",
      "\n",
      "Current : 63 Train Acc : 0.46020000070333483 Test Acc : 0.41962500020861626\n",
      "\n",
      "Current : 64 Train Acc : 0.4632000006437302 Test Acc : 0.4207500005699694\n",
      "\n",
      "Current : 65 Train Acc : 0.4656000007390976 Test Acc : 0.42100000051781533\n",
      "\n",
      "Current : 66 Train Acc : 0.4668000012636185 Test Acc : 0.42262500079348686\n",
      "\n",
      "Current : 67 Train Acc : 0.46680000114440917 Test Acc : 0.42400000074878336\n",
      "\n",
      "Current : 68 Train Acc : 0.4674000015258789 Test Acc : 0.42537500100210307\n",
      "\n",
      "Current : 69 Train Acc : 0.4680000019669533 Test Acc : 0.42575000124052165\n",
      "\n",
      "Current : 70 Train Acc : 0.4718000019192696 Test Acc : 0.4261250011064112\n",
      "\n",
      "Current : 71 Train Acc : 0.47160000133514407 Test Acc : 0.42650000093504786\n",
      "\n",
      "Current : 72 Train Acc : 0.4728000011444092 Test Acc : 0.426000001039356\n",
      "\n",
      "Current : 73 Train Acc : 0.475200001001358 Test Acc : 0.42825000112876294\n",
      "\n",
      "Current : 74 Train Acc : 0.47640000069141386 Test Acc : 0.4290000006183982\n",
      "\n",
      "Current : 75 Train Acc : 0.478600000500679 Test Acc : 0.42875000059604645\n",
      "\n",
      "Current : 76 Train Acc : 0.4806000006198883 Test Acc : 0.42912500094622374\n",
      "\n",
      "Current : 77 Train Acc : 0.4804000009298325 Test Acc : 0.42887500081211327\n",
      "\n",
      "Current : 78 Train Acc : 0.4812000013589859 Test Acc : 0.43050000090152024\n",
      "\n",
      "Current : 79 Train Acc : 0.48360000109672546 Test Acc : 0.4283750005438924\n",
      "\n",
      "Current : 80 Train Acc : 0.4834000008106232 Test Acc : 0.42925000086426734\n",
      "\n",
      "Current : 81 Train Acc : 0.4860000011920929 Test Acc : 0.4306250003352761\n",
      "\n",
      "Current : 82 Train Acc : 0.48740000146627427 Test Acc : 0.43062500037252904\n",
      "\n",
      "Current : 83 Train Acc : 0.48720000129938124 Test Acc : 0.43224999997764824\n",
      "\n",
      "Current : 84 Train Acc : 0.4884000019431114 Test Acc : 0.43275000013411047\n",
      "\n",
      "Current : 85 Train Acc : 0.4906000021696091 Test Acc : 0.43325000047683715\n",
      "\n",
      "Current : 86 Train Acc : 0.49280000191926954 Test Acc : 0.4330000008642674\n",
      "\n",
      "Current : 87 Train Acc : 0.49360000222921374 Test Acc : 0.4336250001564622\n",
      "\n",
      "Current : 88 Train Acc : 0.49520000153779986 Test Acc : 0.4347500006482005\n",
      "\n",
      "Current : 89 Train Acc : 0.49640000122785566 Test Acc : 0.4371250006556511\n",
      "\n",
      "Current : 90 Train Acc : 0.4972000016570091 Test Acc : 0.43787500001490115\n",
      "\n",
      "Current : 91 Train Acc : 0.4982000017762184 Test Acc : 0.43974999990314245\n",
      "\n",
      "Current : 92 Train Acc : 0.4984000020623207 Test Acc : 0.44024999979883433\n",
      "\n",
      "Current : 93 Train Acc : 0.49960000211000444 Test Acc : 0.44075000029057265\n",
      "\n",
      "Current : 94 Train Acc : 0.5012000021338463 Test Acc : 0.4412499999627471\n",
      "\n",
      "Current : 95 Train Acc : 0.5006000016331673 Test Acc : 0.44062499951571227\n",
      "\n",
      "Current : 96 Train Acc : 0.4998000013232231 Test Acc : 0.4421249996125698\n",
      "\n",
      "Current : 97 Train Acc : 0.5052000009417534 Test Acc : 0.4421249997615814\n",
      "\n",
      "Current : 98 Train Acc : 0.5044000006318092 Test Acc : 0.44424999982118607\n",
      "\n",
      "Current : 99 Train Acc : 0.505000001847744 Test Acc : 0.44412499994039534\n",
      "\n",
      "Current : 100 Train Acc : 0.507600001513958 Test Acc : 0.4452499998360872\n",
      "\n",
      "Current : 101 Train Acc : 0.5066000013947487 Test Acc : 0.4462499998137355\n",
      "\n",
      "Current : 102 Train Acc : 0.5080000011324882 Test Acc : 0.44699999984353783\n",
      "\n",
      "Current : 103 Train Acc : 0.5080000011920929 Test Acc : 0.4480000000447035\n",
      "\n",
      "Current : 104 Train Acc : 0.5108000022172928 Test Acc : 0.44862499997019767\n",
      "\n",
      "Current : 105 Train Acc : 0.511200002014637 Test Acc : 0.4497500008717179\n",
      "\n",
      "Current : 106 Train Acc : 0.5146000021100045 Test Acc : 0.451000000461936\n",
      "\n",
      "Current : 107 Train Acc : 0.5154000023007392 Test Acc : 0.45325000055134296\n",
      "\n",
      "Current : 108 Train Acc : 0.5140000016093255 Test Acc : 0.45450000010430813\n",
      "\n",
      "Current : 109 Train Acc : 0.5190000022053719 Test Acc : 0.45487500008195636\n",
      "\n",
      "Current : 110 Train Acc : 0.5184000023007392 Test Acc : 0.4561249999701977\n",
      "\n",
      "Current : 111 Train Acc : 0.5198000015020371 Test Acc : 0.45662500005215406\n",
      "\n",
      "Current : 112 Train Acc : 0.5218000020980835 Test Acc : 0.4578749997541308\n",
      "\n",
      "Current : 113 Train Acc : 0.5240000014305115 Test Acc : 0.45950000070035457\n",
      "\n",
      "Current : 114 Train Acc : 0.5242000017166137 Test Acc : 0.45950000047683714\n",
      "\n",
      "Current : 115 Train Acc : 0.5270000013113022 Test Acc : 0.4600000003352761\n",
      "\n",
      "Current : 116 Train Acc : 0.5278000015020371 Test Acc : 0.46049999993294477\n",
      "\n",
      "Current : 117 Train Acc : 0.5278000016212463 Test Acc : 0.46062499966472387\n",
      "\n",
      "Current : 118 Train Acc : 0.53000000166893 Test Acc : 0.46200000021606685\n",
      "\n",
      "Current : 119 Train Acc : 0.5320000013113022 Test Acc : 0.4626250000670552\n",
      "\n",
      "Current : 120 Train Acc : 0.5334000018239021 Test Acc : 0.4632500005885959\n",
      "\n",
      "Current : 121 Train Acc : 0.5342000011205673 Test Acc : 0.4637500006705523\n",
      "\n",
      "Current : 122 Train Acc : 0.5344000015258789 Test Acc : 0.4655000006407499\n",
      "\n",
      "Current : 123 Train Acc : 0.5358000018000603 Test Acc : 0.4666250002756715\n",
      "\n",
      "Current : 124 Train Acc : 0.5378000010251999 Test Acc : 0.4672500004991889\n",
      "\n",
      "Current : 125 Train Acc : 0.5382000017762184 Test Acc : 0.46737500082701444\n",
      "\n",
      "Current : 126 Train Acc : 0.5392000010609627 Test Acc : 0.4672500007599592\n",
      "\n",
      "Current : 127 Train Acc : 0.5414000015854835 Test Acc : 0.46862500082701447\n",
      "\n",
      "Current : 128 Train Acc : 0.5432000016570091 Test Acc : 0.46862500067800283\n",
      "\n",
      "Current : 129 Train Acc : 0.5456000004410744 Test Acc : 0.4707500006631017\n",
      "\n",
      "Current : 130 Train Acc : 0.546600000679493 Test Acc : 0.46900000143796206\n",
      "\n",
      "Current : 131 Train Acc : 0.548200000822544 Test Acc : 0.4696250011399388\n",
      "\n",
      "Current : 132 Train Acc : 0.5482000004649162 Test Acc : 0.469125000834465\n",
      "\n",
      "Current : 133 Train Acc : 0.5486000009179115 Test Acc : 0.47137500129640103\n",
      "\n",
      "Current : 134 Train Acc : 0.5510000013709069 Test Acc : 0.4715000006183982\n",
      "\n",
      "Current : 135 Train Acc : 0.5510000001788139 Test Acc : 0.4713750011101365\n",
      "\n",
      "Current : 136 Train Acc : 0.5517999992966652 Test Acc : 0.47237500112503766\n",
      "\n",
      "Current : 137 Train Acc : 0.5504000000357628 Test Acc : 0.47212500002235175\n",
      "\n",
      "Current : 138 Train Acc : 0.5528000001311302 Test Acc : 0.47175000082701446\n",
      "\n",
      "Current : 139 Train Acc : 0.5531999997496605 Test Acc : 0.47312500067055224\n",
      "\n",
      "Current : 140 Train Acc : 0.5565999993681907 Test Acc : 0.47300000075250864\n",
      "\n",
      "Current : 141 Train Acc : 0.5559999991059303 Test Acc : 0.4733750008791685\n",
      "\n",
      "Current : 142 Train Acc : 0.5573999989628792 Test Acc : 0.47400000065565107\n",
      "\n",
      "Current : 143 Train Acc : 0.5569999992251397 Test Acc : 0.47512500047683714\n",
      "\n",
      "Current : 144 Train Acc : 0.5571999989151954 Test Acc : 0.4745000005140901\n",
      "\n",
      "Current : 145 Train Acc : 0.557599999845028 Test Acc : 0.47400000046938656\n",
      "\n",
      "Current : 146 Train Acc : 0.558599999845028 Test Acc : 0.4748750001937151\n",
      "\n",
      "Current : 147 Train Acc : 0.5593999997973442 Test Acc : 0.47474999975413085\n",
      "\n",
      "Current : 148 Train Acc : 0.5603999989628792 Test Acc : 0.4752500005438924\n",
      "\n",
      "Current : 149 Train Acc : 0.5599999997019768 Test Acc : 0.4767500003799796\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# D\n",
    "current_exp_name = 'D';\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# create layers\n",
    "l1 = CNN(3,3, 16,which_reg=current_exp_name); \n",
    "l2 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "l3 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "\n",
    "l4 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "l5 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "l6 = CNN(3,16,10,which_reg=current_exp_name); \n",
    "\n",
    "# 2. graph \n",
    "x = tf.placeholder(tf.float32,(batch_size,96,96,3))\n",
    "y = tf.placeholder(tf.float32,(batch_size,10))\n",
    "\n",
    "layer1, layer1a = l1. feedforward(x,stride=2)\n",
    "layer2, layer2a = l2. feedforward(layer1a,stride=2)\n",
    "layer3, layer3a = l3. feedforward(layer2a,stride=2)\n",
    "layer4, layer4a = l4. feedforward(layer3a,stride=2)\n",
    "layer5, layer5a = l5. feedforward(layer4a)\n",
    "layer6, layer6a = l6. feedforward(layer5a)\n",
    "\n",
    "final_layer   = tf.reduce_mean(layer6a,(1,2))\n",
    "final_softmax = tf_softmax(final_layer)\n",
    "cost          = -tf.reduce_mean(y * tf.log(final_softmax + 1e-8))\n",
    "correct_prediction = tf.equal(tf.argmax(final_softmax, 1), tf.argmax(y, 1))\n",
    "accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "gradient = tf.tile((final_softmax-y)[:,None,None,:],[1,6,6,1])/batch_size\n",
    "grad6p,grad6w,grad6_up = l6.backprop(gradient)\n",
    "grad5p,grad5w,grad5_up = l5.backprop(grad6p)\n",
    "grad4p,grad4w,grad4_up = l4.backprop(grad5p,stride=2)\n",
    "grad3p,grad3w,grad3_up = l3.backprop(grad4p,stride=2)\n",
    "grad2p,grad2w,grad2_up = l2.backprop(grad3p,stride=2)\n",
    "grad1p,grad1w,grad1_up = l1.backprop(grad2p,stride=2)\n",
    "\n",
    "gradient_update = grad6_up + grad5_up + grad4_up + grad3_up + grad2_up + grad1_up \n",
    "\n",
    "# train\n",
    "sess.run(tf.global_variables_initializer())\n",
    "avg_acc_train = 0; avg_acc_test  = 0; train_acc = [];test_acc = []\n",
    "\n",
    "# mean std skew kurt non-zero\n",
    "llayer1 = [[],[],[],[],[]]; llayer2 = [[],[],[],[],[]]; llayer3 = [[],[],[],[],[]]\n",
    "llayer4 = [[],[],[],[],[]]; llayer5 = [[],[],[],[],[]]; llayer6 = [[],[],[],[],[]]\n",
    "\n",
    "llayer1a = [[],[],[],[],[]]; llayer2a = [[],[],[],[],[]]; llayer3a = [[],[],[],[],[]]\n",
    "llayer4a = [[],[],[],[],[]]; llayer5a = [[],[],[],[],[]]; llayer6a = [[],[],[],[],[]]\n",
    "\n",
    "weight1 = [[],[],[],[],[]]; weight2 = [[],[],[],[],[]]; weight3 = [[],[],[],[],[]];\n",
    "weight4 = [[],[],[],[],[]]; weight5 = [[],[],[],[],[]]; weight6 = [[],[],[],[],[]];\n",
    "\n",
    "gradw1  = [[],[],[],[],[]]; gradw2  = [[],[],[],[],[]]; gradw3  = [[],[],[],[],[]];\n",
    "gradw4  = [[],[],[],[],[]]; gradw5  = [[],[],[],[],[]]; gradw6  = [[],[],[],[],[]];\n",
    "\n",
    "gradp1  = [[],[],[],[],[]]; gradp2  = [[],[],[],[],[]]; gradp3  = [[],[],[],[],[]];\n",
    "gradp4  = [[],[],[],[],[]]; gradp5  = [[],[],[],[],[]]; gradp6  = [[],[],[],[],[]];\n",
    "\n",
    "gradup1  = [[],[],[],[],[]]; gradup2  = [[],[],[],[],[]]; gradup3  = [[],[],[],[],[]];\n",
    "gradup4  = [[],[],[],[],[]]; gradup5  = [[],[],[],[],[]]; gradup6  = [[],[],[],[],[]];\n",
    "\n",
    "list_of_outputs = [\n",
    "    layer1,layer2,layer3,layer4,layer5,layer6,\n",
    "    layer1a,layer2a,layer3a,layer4a,layer5a,layer6a,\n",
    "    l1.getw(),l2.getw(),l3.getw(),l4.getw(),l5.getw(),l6.getw(),\n",
    "    grad1w,grad2w,grad3w,grad4w,grad5w,grad6w,\n",
    "    grad1p,grad2p,grad3p,grad4p,grad5p,grad6p,\n",
    "    grad1_up[0],grad2_up[0],grad3_up[0],grad4_up[0],grad5_up[0],grad6_up[0]\n",
    "]\n",
    "\n",
    "for iter in range(num_epoch):\n",
    "\n",
    "    # Training Accuracy    \n",
    "    for current_batch_index in range(0,len(train_images),batch_size):\n",
    "        current_data  = train_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = train_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy,gradient_update],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(train_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_train = avg_acc_train + sess_results[0]\n",
    "        \n",
    "    # get the results\n",
    "    mid_stat = sess.run(list_of_outputs,feed_dict={x:current_data,y:current_label})\n",
    "    \n",
    "    # Test Accuracy    \n",
    "    for current_batch_index in range(0,len(test_images), batch_size):\n",
    "        current_data  = test_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = test_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(test_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_test = avg_acc_test + sess_results[0]   \n",
    "        \n",
    "    # ======================== extract stats ========================\n",
    "    llayer1 = append_stat(llayer1,mid_stat,0);  llayer2 = append_stat(llayer2,mid_stat,1);  llayer3 = append_stat(llayer3,mid_stat,2);\n",
    "    llayer4 = append_stat(llayer4,mid_stat,3);  llayer5 = append_stat(llayer5,mid_stat,4);  llayer6 = append_stat(llayer6,mid_stat,5);\n",
    "\n",
    "    llayer1a = append_stat(llayer1a,mid_stat,6);  llayer2a = append_stat(llayer2a,mid_stat,7);  llayer3a = append_stat(llayer3a,mid_stat,8);\n",
    "    llayer4a = append_stat(llayer4a,mid_stat,9);  llayer5a = append_stat(llayer5a,mid_stat,10); llayer6a = append_stat(llayer6a,mid_stat,11);\n",
    "    \n",
    "    weight1 = append_stat(weight1,mid_stat,12);  weight2 = append_stat(weight2,mid_stat,13);  weight3 = append_stat(weight3,mid_stat,14);\n",
    "    weight4 = append_stat(weight4,mid_stat,15);  weight5 = append_stat(weight5,mid_stat,16);  weight6 = append_stat(weight6,mid_stat,17);\n",
    "    \n",
    "    gradw1 = append_stat(gradw1,mid_stat,18); gradw2 = append_stat(gradw2,mid_stat,19); gradw3 = append_stat(gradw3,mid_stat,20);\n",
    "    gradw4 = append_stat(gradw4,mid_stat,21); gradw5 = append_stat(gradw5,mid_stat,22); gradw6 = append_stat(gradw6,mid_stat,23);\n",
    "    \n",
    "    gradp1 = append_stat(gradp1,mid_stat,24); gradp2 = append_stat(gradp2,mid_stat,25); gradp3 = append_stat(gradp3,mid_stat,26);\n",
    "    gradp4 = append_stat(gradp4,mid_stat,27); gradp5 = append_stat(gradp5,mid_stat,28); gradp6 = append_stat(gradp6,mid_stat,29);\n",
    "\n",
    "    gradup1 = append_stat(gradup1,mid_stat,30); gradup2 = append_stat(gradup2,mid_stat,31); gradup3 = append_stat(gradup3,mid_stat,32);\n",
    "    gradup4 = append_stat(gradup4,mid_stat,33); gradup5 = append_stat(gradup5,mid_stat,34); gradup6 = append_stat(gradup6,mid_stat,35);\n",
    "\n",
    "    train_acc.append(avg_acc_train/(len(train_images)/batch_size))\n",
    "    test_acc .append(avg_acc_test / (len(test_images)/batch_size))\n",
    "    # ======================== extract stats ========================\n",
    "    \n",
    "    # ======================== save to image ========================\n",
    "    save_to_image(mid_stat[0:6]   ,llayer1,llayer2,llayer3,llayer4,llayer5,llayer6,\"layer\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[6:12]  ,llayer1a,llayer2a,llayer3a,llayer4a,llayer5a,llayer6a,\"layera\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[12:18] ,weight1,weight2,weight3,weight4,weight5,weight6,\"weights\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[18:24] ,gradw1,gradw2,gradw3,gradw4,gradw5,gradw6,\"gradientw\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[24:30] ,gradp1,gradp2,gradp3,gradp4,gradp5,gradp6,\"gradientp\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[30:36] ,gradup1,gradup2,gradup3,gradup4,gradup5,gradup6,\"moment\",train_acc,test_acc,current_exp_name,iter)\n",
    "    # ======================== save to image ========================\n",
    "        \n",
    "    # ======================== print reset ========================\n",
    "    print(\"Current : \"+ str(iter) + \" Train Acc : \" + str(avg_acc_train/(len(train_images)/batch_size)) + \" Test Acc : \" + str(avg_acc_test/(len(test_images)/batch_size)) + '\\n')\n",
    "    avg_acc_train = 0 ; avg_acc_test  = 0\n",
    "    # ======================== print reset ========================\n",
    "\n",
    "np.save(current_exp_name+'/train_acc.npy',train_acc); np.save(current_exp_name+'/test_acc.npy', test_acc)    \n",
    "np.save(current_exp_name+'/llayer1.npy', llayer1);  np.save(current_exp_name+'/llayer2.npy', llayer2);  np.save(current_exp_name+'/llayer3.npy', llayer3); \n",
    "np.save(current_exp_name+'/llayer4.npy', llayer4);  np.save(current_exp_name+'/llayer5.npy', llayer5);  np.save(current_exp_name+'/llayer6.npy', llayer6); \n",
    "\n",
    "np.save(current_exp_name+'/llayer1a.npy', llayer1a);  np.save(current_exp_name+'/llayer2a.npy', llayer2a);  np.save(current_exp_name+'/llayer3a.npy', llayer3a); \n",
    "np.save(current_exp_name+'/llayer4a.npy', llayer4a);  np.save(current_exp_name+'/llayer5a.npy', llayer5a);  np.save(current_exp_name+'/llayer6a.npy', llayer6a); \n",
    "\n",
    "np.save(current_exp_name+'/weight1.npy', weight1);  np.save(current_exp_name+'/weight2.npy', weight2);  np.save(current_exp_name+'/weight3.npy', weight3);  \n",
    "np.save(current_exp_name+'/weight4.npy', weight4);  np.save(current_exp_name+'/weight5.npy', weight5);  np.save(current_exp_name+'/weight6.npy', weight6);  \n",
    "\n",
    "np.save(current_exp_name+'/gradw1.npy', gradw1); np.save(current_exp_name+'/gradw2.npy', gradw2); np.save(current_exp_name+'/gradw3.npy', gradw3);\n",
    "np.save(current_exp_name+'/gradw4.npy', gradw4); np.save(current_exp_name+'/gradw5.npy', gradw5); np.save(current_exp_name+'/gradw6.npy', gradw6);\n",
    "\n",
    "np.save(current_exp_name+'/gradp1.npy', gradp1); np.save(current_exp_name+'/gradp2.npy', gradp2); np.save(current_exp_name+'/gradp3.npy', gradp3);\n",
    "np.save(current_exp_name+'/gradp4.npy', gradp4); np.save(current_exp_name+'/gradp5.npy', gradp5); np.save(current_exp_name+'/gradp6.npy', gradp6);\n",
    "\n",
    "np.save(current_exp_name+'/gradup1.npy', gradup1); np.save(current_exp_name+'/gradup2.npy', gradup2); np.save(current_exp_name+'/gradup3.npy', gradup3);\n",
    "np.save(current_exp_name+'/gradup4.npy', gradup4); np.save(current_exp_name+'/gradup5.npy', gradup5); np.save(current_exp_name+'/gradup6.npy', gradup6);\n",
    "\n",
    "sess.close(); tf.reset_default_graph();\n",
    "\n",
    "%reset_selective -f l1,l2,l3,l4,l5,l6\n",
    "%reset_selective -f layer1,layer2,layer3,layer4,layer5,layer6\n",
    "%reset_selective -f layer1a,layer2a,layer3a,layer4a,layer5a,layer6a\n",
    "%reset_selective -f train_acc,test_acc,llayer1,llayer2,llayer3,llayer4,llayer5,llayer6,llayer1a,llayer2a,llayer3a,llayer4a,llayer5a,llayer6a\n",
    "%reset_selective -f weight1,weight2,weight3,weight4,weight5,weight6\n",
    "%reset_selective -f gradw1,gradw2,gradw3,gradw4,gradw5,gradw6\n",
    "%reset_selective -f gradp1,gradp2,gradp3,gradp4,gradp5,gradp6\n",
    "%reset_selective -f gradup1,gradup2,gradup3,gradup4,gradup5,gradup6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-05T06:24:55.247444Z",
     "start_time": "2019-01-05T05:35:39.490738Z"
    },
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current : 0 Train Acc : 0.13200000239908696 Test Acc : 0.15575000283308327\n",
      "\n",
      "Current : 1 Train Acc : 0.1644000030308962 Test Acc : 0.1888750031683594\n",
      "\n",
      "Current : 2 Train Acc : 0.18620000295341016 Test Acc : 0.2131250027101487\n",
      "\n",
      "Current : 3 Train Acc : 0.20900000317394735 Test Acc : 0.2491250034701079\n",
      "\n",
      "Current : 4 Train Acc : 0.23600000295042992 Test Acc : 0.2661250026524067\n",
      "\n",
      "Current : 5 Train Acc : 0.25700000290572644 Test Acc : 0.284000002052635\n",
      "\n",
      "Current : 6 Train Acc : 0.2758000028282404 Test Acc : 0.29875000251457096\n",
      "\n",
      "Current : 7 Train Acc : 0.2930000028908253 Test Acc : 0.3052500028163195\n",
      "\n",
      "Current : 8 Train Acc : 0.3082000017464161 Test Acc : 0.31075000249780715\n",
      "\n",
      "Current : 9 Train Acc : 0.31800000178813936 Test Acc : 0.31600000252947213\n",
      "\n",
      "Current : 10 Train Acc : 0.322600001975894 Test Acc : 0.3217500022891909\n",
      "\n",
      "Current : 11 Train Acc : 0.3292000019699335 Test Acc : 0.32487500216811893\n",
      "\n",
      "Current : 12 Train Acc : 0.3326000013798475 Test Acc : 0.3263750022277236\n",
      "\n",
      "Current : 13 Train Acc : 0.3368000008761883 Test Acc : 0.33075000228360296\n",
      "\n",
      "Current : 14 Train Acc : 0.34120000106096265 Test Acc : 0.3307500024046749\n",
      "\n",
      "Current : 15 Train Acc : 0.3436000012457371 Test Acc : 0.33225000186823306\n",
      "\n",
      "Current : 16 Train Acc : 0.34420000195503236 Test Acc : 0.33462500184774396\n",
      "\n",
      "Current : 17 Train Acc : 0.34200000232458116 Test Acc : 0.3338750022277236\n",
      "\n",
      "Current : 18 Train Acc : 0.3448000018596649 Test Acc : 0.3375000024214387\n",
      "\n",
      "Current : 19 Train Acc : 0.34720000141859053 Test Acc : 0.3380000026524067\n",
      "\n",
      "Current : 20 Train Acc : 0.3518000014424324 Test Acc : 0.34112500252202155\n",
      "\n",
      "Current : 21 Train Acc : 0.3522000012993813 Test Acc : 0.3471250025555491\n",
      "\n",
      "Current : 22 Train Acc : 0.3574000008106232 Test Acc : 0.3507500019669533\n",
      "\n",
      "Current : 23 Train Acc : 0.36140000051259996 Test Acc : 0.3547500020638108\n",
      "\n",
      "Current : 24 Train Acc : 0.3612000008970499 Test Acc : 0.35675000147894026\n",
      "\n",
      "Current : 25 Train Acc : 0.36480000060796736 Test Acc : 0.3591250016540289\n",
      "\n",
      "Current : 26 Train Acc : 0.36740000122785565 Test Acc : 0.3612500020675361\n",
      "\n",
      "Current : 27 Train Acc : 0.37040000087022784 Test Acc : 0.36650000190362336\n",
      "\n",
      "Current : 28 Train Acc : 0.3728000023663044 Test Acc : 0.36687500163912773\n",
      "\n",
      "Current : 29 Train Acc : 0.3784000023305416 Test Acc : 0.37050000093877317\n",
      "\n",
      "Current : 30 Train Acc : 0.3826000015735626 Test Acc : 0.3767500013485551\n",
      "\n",
      "Current : 31 Train Acc : 0.3866000018119812 Test Acc : 0.37937500078231096\n",
      "\n",
      "Current : 32 Train Acc : 0.3904000023007393 Test Acc : 0.38362500062212346\n",
      "\n",
      "Current : 33 Train Acc : 0.3918000019788742 Test Acc : 0.38662500156089663\n",
      "\n",
      "Current : 34 Train Acc : 0.3916000017523766 Test Acc : 0.3885000006482005\n",
      "\n",
      "Current : 35 Train Acc : 0.3988000012040138 Test Acc : 0.38850000079721214\n",
      "\n",
      "Current : 36 Train Acc : 0.4034000007510185 Test Acc : 0.3906250007264316\n",
      "\n",
      "Current : 37 Train Acc : 0.4052000006437302 Test Acc : 0.39250000050291417\n",
      "\n",
      "Current : 38 Train Acc : 0.40400000029802324 Test Acc : 0.39400000078603625\n",
      "\n",
      "Current : 39 Train Acc : 0.40720000040531157 Test Acc : 0.39462500059977174\n",
      "\n",
      "Current : 40 Train Acc : 0.4096000006198883 Test Acc : 0.3960000009275973\n",
      "\n",
      "Current : 41 Train Acc : 0.4108000003993511 Test Acc : 0.3988750007934868\n",
      "\n",
      "Current : 42 Train Acc : 0.4134000011086464 Test Acc : 0.40137500075623395\n",
      "\n",
      "Current : 43 Train Acc : 0.4152000015079975 Test Acc : 0.4021250004507601\n",
      "\n",
      "Current : 44 Train Acc : 0.42000000083446504 Test Acc : 0.40225000051781534\n",
      "\n",
      "Current : 45 Train Acc : 0.42060000175237655 Test Acc : 0.40350000038743017\n",
      "\n",
      "Current : 46 Train Acc : 0.42200000190734865 Test Acc : 0.4031250009313226\n",
      "\n",
      "Current : 47 Train Acc : 0.4258000019192696 Test Acc : 0.4050000007078052\n",
      "\n",
      "Current : 48 Train Acc : 0.4272000022530556 Test Acc : 0.4055000005103648\n",
      "\n",
      "Current : 49 Train Acc : 0.4302000024318695 Test Acc : 0.40625000031664965\n",
      "\n",
      "Current : 50 Train Acc : 0.43040000200271605 Test Acc : 0.4078750002942979\n",
      "\n",
      "Current : 51 Train Acc : 0.43400000196695326 Test Acc : 0.40912500070407987\n",
      "\n",
      "Current : 52 Train Acc : 0.43480000191926954 Test Acc : 0.4096250003390014\n",
      "\n",
      "Current : 53 Train Acc : 0.43940000182390215 Test Acc : 0.40925000021234154\n",
      "\n",
      "Current : 54 Train Acc : 0.4424000014662743 Test Acc : 0.4105000003054738\n",
      "\n",
      "Current : 55 Train Acc : 0.4452000014781952 Test Acc : 0.41175000041723253\n",
      "\n",
      "Current : 56 Train Acc : 0.4482000015974045 Test Acc : 0.41187500026077034\n",
      "\n",
      "Current : 57 Train Acc : 0.44760000145435336 Test Acc : 0.41275000024586916\n",
      "\n",
      "Current : 58 Train Acc : 0.45020000064373017 Test Acc : 0.41450000036507845\n",
      "\n",
      "Current : 59 Train Acc : 0.4504000011086464 Test Acc : 0.4148750005662441\n",
      "\n",
      "Current : 60 Train Acc : 0.45040000185370443 Test Acc : 0.4156250002607703\n",
      "\n",
      "Current : 61 Train Acc : 0.45280000111460683 Test Acc : 0.41650000065565107\n",
      "\n",
      "Current : 62 Train Acc : 0.4538000011742115 Test Acc : 0.4157500009983778\n",
      "\n",
      "Current : 63 Train Acc : 0.4560000013411045 Test Acc : 0.4177500008791685\n",
      "\n",
      "Current : 64 Train Acc : 0.4576000012457371 Test Acc : 0.41762500096112487\n",
      "\n",
      "Current : 65 Train Acc : 0.4568000005185604 Test Acc : 0.4182500006258488\n",
      "\n",
      "Current : 66 Train Acc : 0.4574000021368265 Test Acc : 0.41962500046938656\n",
      "\n",
      "Current : 67 Train Acc : 0.4604000010639429 Test Acc : 0.4208750008046627\n",
      "\n",
      "Current : 68 Train Acc : 0.46180000199377536 Test Acc : 0.42125000096857546\n",
      "\n",
      "Current : 69 Train Acc : 0.4644000010639429 Test Acc : 0.42175000071525576\n",
      "\n",
      "Current : 70 Train Acc : 0.4664000006467104 Test Acc : 0.42112500086426735\n",
      "\n",
      "Current : 71 Train Acc : 0.46600000132620334 Test Acc : 0.4225000004097819\n",
      "\n",
      "Current : 72 Train Acc : 0.4668000012785196 Test Acc : 0.4241250004991889\n",
      "\n",
      "Current : 73 Train Acc : 0.4704000014215708 Test Acc : 0.424625000320375\n",
      "\n",
      "Current : 74 Train Acc : 0.47180000199377536 Test Acc : 0.4267500006593764\n",
      "\n",
      "Current : 75 Train Acc : 0.47080000199377536 Test Acc : 0.427125000897795\n",
      "\n",
      "Current : 76 Train Acc : 0.4750000016838312 Test Acc : 0.4270000009797513\n",
      "\n",
      "Current : 77 Train Acc : 0.4740000019222498 Test Acc : 0.4298750009573996\n",
      "\n",
      "Current : 78 Train Acc : 0.4758000014573336 Test Acc : 0.42975000128149987\n",
      "\n",
      "Current : 79 Train Acc : 0.47440000173449515 Test Acc : 0.4331250013038516\n",
      "\n",
      "Current : 80 Train Acc : 0.47640000107884406 Test Acc : 0.43475000087171795\n",
      "\n",
      "Current : 81 Train Acc : 0.47740000090003015 Test Acc : 0.4353750007227063\n",
      "\n",
      "Current : 82 Train Acc : 0.47940000000596045 Test Acc : 0.43637500040233135\n",
      "\n",
      "Current : 83 Train Acc : 0.48040000012516976 Test Acc : 0.4350000006519258\n",
      "\n",
      "Current : 84 Train Acc : 0.4823999999761581 Test Acc : 0.4365000007301569\n",
      "\n",
      "Current : 85 Train Acc : 0.48200000017881395 Test Acc : 0.43687500052154066\n",
      "\n",
      "Current : 86 Train Acc : 0.48460000050067903 Test Acc : 0.4398750011995435\n",
      "\n",
      "Current : 87 Train Acc : 0.4850000005960464 Test Acc : 0.44075000084936616\n",
      "\n",
      "Current : 88 Train Acc : 0.48480000084638597 Test Acc : 0.4397500005178154\n",
      "\n",
      "Current : 89 Train Acc : 0.486200001001358 Test Acc : 0.4412500008940697\n",
      "\n",
      "Current : 90 Train Acc : 0.48780000054836276 Test Acc : 0.4415000002831221\n",
      "\n",
      "Current : 91 Train Acc : 0.4880000005960464 Test Acc : 0.442750000692904\n",
      "\n",
      "Current : 92 Train Acc : 0.48940000087022784 Test Acc : 0.4448750003799796\n",
      "\n",
      "Current : 93 Train Acc : 0.4896000006198883 Test Acc : 0.4455000011250377\n",
      "\n",
      "Current : 94 Train Acc : 0.4914000014066696 Test Acc : 0.44525000095367434\n",
      "\n",
      "Current : 95 Train Acc : 0.48920000094175337 Test Acc : 0.44650000020861624\n",
      "\n",
      "Current : 96 Train Acc : 0.49180000060796736 Test Acc : 0.447375000230968\n",
      "\n",
      "Current : 97 Train Acc : 0.49320000112056733 Test Acc : 0.44650000009685753\n",
      "\n",
      "Current : 98 Train Acc : 0.49220000094175337 Test Acc : 0.4477500005811453\n",
      "\n",
      "Current : 99 Train Acc : 0.49400000083446505 Test Acc : 0.44925000112503766\n",
      "\n",
      "Current : 100 Train Acc : 0.4914000006914139 Test Acc : 0.45000000115484\n",
      "\n",
      "Current : 101 Train Acc : 0.4936000015735626 Test Acc : 0.4510000009462237\n",
      "\n",
      "Current : 102 Train Acc : 0.4948000011444092 Test Acc : 0.4522500015422702\n",
      "\n",
      "Current : 103 Train Acc : 0.4962000012397766 Test Acc : 0.45262500137090683\n",
      "\n",
      "Current : 104 Train Acc : 0.4964000011086464 Test Acc : 0.4536250010505319\n",
      "\n",
      "Current : 105 Train Acc : 0.4976000015735626 Test Acc : 0.45537500105798245\n",
      "\n",
      "Current : 106 Train Acc : 0.5026000013947487 Test Acc : 0.4567500006780028\n",
      "\n",
      "Current : 107 Train Acc : 0.5004000017046928 Test Acc : 0.45737500093877315\n",
      "\n",
      "Current : 108 Train Acc : 0.5040000012516975 Test Acc : 0.4581250006519258\n",
      "\n",
      "Current : 109 Train Acc : 0.5032000015377999 Test Acc : 0.4580000011250377\n",
      "\n",
      "Current : 110 Train Acc : 0.5034000014662743 Test Acc : 0.45925000045448544\n",
      "\n",
      "Current : 111 Train Acc : 0.5032000010609626 Test Acc : 0.4593750011920929\n",
      "\n",
      "Current : 112 Train Acc : 0.502600000679493 Test Acc : 0.45837500074878335\n",
      "\n",
      "Current : 113 Train Acc : 0.5044000011086464 Test Acc : 0.46012500032782555\n",
      "\n",
      "Current : 114 Train Acc : 0.5066000011563301 Test Acc : 0.46037500085309147\n",
      "\n",
      "Current : 115 Train Acc : 0.5064000003933906 Test Acc : 0.46100000077858566\n",
      "\n",
      "Current : 116 Train Acc : 0.506400001168251 Test Acc : 0.46250000052154067\n",
      "\n",
      "Current : 117 Train Acc : 0.507600000500679 Test Acc : 0.4618750007078052\n",
      "\n",
      "Current : 118 Train Acc : 0.5070000007748604 Test Acc : 0.4625000006332993\n",
      "\n",
      "Current : 119 Train Acc : 0.5082000010609626 Test Acc : 0.463625000230968\n",
      "\n",
      "Current : 120 Train Acc : 0.5090000004470349 Test Acc : 0.4620000004582107\n",
      "\n",
      "Current : 121 Train Acc : 0.5122000007033348 Test Acc : 0.4623750003241003\n",
      "\n",
      "Current : 122 Train Acc : 0.5132000000476837 Test Acc : 0.46225000023841856\n",
      "\n",
      "Current : 123 Train Acc : 0.5116000000834465 Test Acc : 0.4627500004135072\n",
      "\n",
      "Current : 124 Train Acc : 0.5144000005424023 Test Acc : 0.462250000461936\n",
      "\n",
      "Current : 125 Train Acc : 0.517200000166893 Test Acc : 0.4617500004731119\n",
      "\n",
      "Current : 126 Train Acc : 0.5142000004649162 Test Acc : 0.4640000009909272\n",
      "\n",
      "Current : 127 Train Acc : 0.5152000002264977 Test Acc : 0.4616250005736947\n",
      "\n",
      "Current : 128 Train Acc : 0.5146000009179116 Test Acc : 0.46125000074505806\n",
      "\n",
      "Current : 129 Train Acc : 0.5155999999344348 Test Acc : 0.4626250007376075\n",
      "\n",
      "Current : 130 Train Acc : 0.5182000011503697 Test Acc : 0.46225000053644183\n",
      "\n",
      "Current : 131 Train Acc : 0.5198000010251999 Test Acc : 0.4626250008121133\n",
      "\n",
      "Current : 132 Train Acc : 0.5198000009655952 Test Acc : 0.4636250006407499\n",
      "\n",
      "Current : 133 Train Acc : 0.5204000009894371 Test Acc : 0.4631250010430813\n",
      "\n",
      "Current : 134 Train Acc : 0.5202000006437302 Test Acc : 0.4643750013038516\n",
      "\n",
      "Current : 135 Train Acc : 0.5208000015020371 Test Acc : 0.4667500010505319\n",
      "\n",
      "Current : 136 Train Acc : 0.5212000013589859 Test Acc : 0.4662500011920929\n",
      "\n",
      "Current : 137 Train Acc : 0.5232000014781952 Test Acc : 0.4666250013932586\n",
      "\n",
      "Current : 138 Train Acc : 0.5252000008821487 Test Acc : 0.4676250013336539\n",
      "\n",
      "Current : 139 Train Acc : 0.5254000012874603 Test Acc : 0.4655000012740493\n",
      "\n",
      "Current : 140 Train Acc : 0.5266000012159348 Test Acc : 0.4666250013932586\n",
      "\n",
      "Current : 141 Train Acc : 0.5264000016450882 Test Acc : 0.46812500167638066\n",
      "\n",
      "Current : 142 Train Acc : 0.5278000012636185 Test Acc : 0.4667500014230609\n",
      "\n",
      "Current : 143 Train Acc : 0.5286000008583069 Test Acc : 0.4673750014603138\n",
      "\n",
      "Current : 144 Train Acc : 0.5306000009179115 Test Acc : 0.468000001013279\n",
      "\n",
      "Current : 145 Train Acc : 0.530800000846386 Test Acc : 0.46900000140070913\n",
      "\n",
      "Current : 146 Train Acc : 0.5310000010728836 Test Acc : 0.4693750015646219\n",
      "\n",
      "Current : 147 Train Acc : 0.531200001835823 Test Acc : 0.4701250015944243\n",
      "\n",
      "Current : 148 Train Acc : 0.5344000019431114 Test Acc : 0.4686250014230609\n",
      "\n",
      "Current : 149 Train Acc : 0.5338000017404556 Test Acc : 0.469750001244247\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# E\n",
    "current_exp_name = 'E';\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# create layers\n",
    "l1 = CNN(3,3, 16,which_reg=current_exp_name); \n",
    "l2 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "l3 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "\n",
    "l4 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "l5 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "l6 = CNN(3,16,10,which_reg=current_exp_name); \n",
    "\n",
    "# 2. graph \n",
    "x = tf.placeholder(tf.float32,(batch_size,96,96,3))\n",
    "y = tf.placeholder(tf.float32,(batch_size,10))\n",
    "\n",
    "layer1, layer1a = l1. feedforward(x,stride=2)\n",
    "layer2, layer2a = l2. feedforward(layer1a,stride=2)\n",
    "layer3, layer3a = l3. feedforward(layer2a,stride=2)\n",
    "layer4, layer4a = l4. feedforward(layer3a,stride=2)\n",
    "layer5, layer5a = l5. feedforward(layer4a)\n",
    "layer6, layer6a = l6. feedforward(layer5a)\n",
    "\n",
    "final_layer   = tf.reduce_mean(layer6a,(1,2))\n",
    "final_softmax = tf_softmax(final_layer)\n",
    "cost          = -tf.reduce_mean(y * tf.log(final_softmax + 1e-8))\n",
    "correct_prediction = tf.equal(tf.argmax(final_softmax, 1), tf.argmax(y, 1))\n",
    "accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "gradient = tf.tile((final_softmax-y)[:,None,None,:],[1,6,6,1])/batch_size\n",
    "grad6p,grad6w,grad6_up = l6.backprop(gradient)\n",
    "grad5p,grad5w,grad5_up = l5.backprop(grad6p)\n",
    "grad4p,grad4w,grad4_up = l4.backprop(grad5p,stride=2)\n",
    "grad3p,grad3w,grad3_up = l3.backprop(grad4p,stride=2)\n",
    "grad2p,grad2w,grad2_up = l2.backprop(grad3p,stride=2)\n",
    "grad1p,grad1w,grad1_up = l1.backprop(grad2p,stride=2)\n",
    "\n",
    "gradient_update = grad6_up + grad5_up + grad4_up + grad3_up + grad2_up + grad1_up \n",
    "\n",
    "# train\n",
    "sess.run(tf.global_variables_initializer())\n",
    "avg_acc_train = 0; avg_acc_test  = 0; train_acc = [];test_acc = []\n",
    "\n",
    "# mean std skew kurt non-zero\n",
    "llayer1 = [[],[],[],[],[]]; llayer2 = [[],[],[],[],[]]; llayer3 = [[],[],[],[],[]]\n",
    "llayer4 = [[],[],[],[],[]]; llayer5 = [[],[],[],[],[]]; llayer6 = [[],[],[],[],[]]\n",
    "\n",
    "llayer1a = [[],[],[],[],[]]; llayer2a = [[],[],[],[],[]]; llayer3a = [[],[],[],[],[]]\n",
    "llayer4a = [[],[],[],[],[]]; llayer5a = [[],[],[],[],[]]; llayer6a = [[],[],[],[],[]]\n",
    "\n",
    "weight1 = [[],[],[],[],[]]; weight2 = [[],[],[],[],[]]; weight3 = [[],[],[],[],[]];\n",
    "weight4 = [[],[],[],[],[]]; weight5 = [[],[],[],[],[]]; weight6 = [[],[],[],[],[]];\n",
    "\n",
    "gradw1  = [[],[],[],[],[]]; gradw2  = [[],[],[],[],[]]; gradw3  = [[],[],[],[],[]];\n",
    "gradw4  = [[],[],[],[],[]]; gradw5  = [[],[],[],[],[]]; gradw6  = [[],[],[],[],[]];\n",
    "\n",
    "gradp1  = [[],[],[],[],[]]; gradp2  = [[],[],[],[],[]]; gradp3  = [[],[],[],[],[]];\n",
    "gradp4  = [[],[],[],[],[]]; gradp5  = [[],[],[],[],[]]; gradp6  = [[],[],[],[],[]];\n",
    "\n",
    "gradup1  = [[],[],[],[],[]]; gradup2  = [[],[],[],[],[]]; gradup3  = [[],[],[],[],[]];\n",
    "gradup4  = [[],[],[],[],[]]; gradup5  = [[],[],[],[],[]]; gradup6  = [[],[],[],[],[]];\n",
    "\n",
    "list_of_outputs = [\n",
    "    layer1,layer2,layer3,layer4,layer5,layer6,\n",
    "    layer1a,layer2a,layer3a,layer4a,layer5a,layer6a,\n",
    "    l1.getw(),l2.getw(),l3.getw(),l4.getw(),l5.getw(),l6.getw(),\n",
    "    grad1w,grad2w,grad3w,grad4w,grad5w,grad6w,\n",
    "    grad1p,grad2p,grad3p,grad4p,grad5p,grad6p,\n",
    "    grad1_up[0],grad2_up[0],grad3_up[0],grad4_up[0],grad5_up[0],grad6_up[0]\n",
    "]\n",
    "\n",
    "for iter in range(num_epoch):\n",
    "\n",
    "    # Training Accuracy    \n",
    "    for current_batch_index in range(0,len(train_images),batch_size):\n",
    "        current_data  = train_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = train_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy,gradient_update],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(train_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_train = avg_acc_train + sess_results[0]\n",
    "        \n",
    "    # get the results\n",
    "    mid_stat = sess.run(list_of_outputs,feed_dict={x:current_data,y:current_label})\n",
    "    \n",
    "    # Test Accuracy    \n",
    "    for current_batch_index in range(0,len(test_images), batch_size):\n",
    "        current_data  = test_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = test_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(test_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_test = avg_acc_test + sess_results[0]   \n",
    "        \n",
    "    # ======================== extract stats ========================\n",
    "    llayer1 = append_stat(llayer1,mid_stat,0);  llayer2 = append_stat(llayer2,mid_stat,1);  llayer3 = append_stat(llayer3,mid_stat,2);\n",
    "    llayer4 = append_stat(llayer4,mid_stat,3);  llayer5 = append_stat(llayer5,mid_stat,4);  llayer6 = append_stat(llayer6,mid_stat,5);\n",
    "\n",
    "    llayer1a = append_stat(llayer1a,mid_stat,6);  llayer2a = append_stat(llayer2a,mid_stat,7);  llayer3a = append_stat(llayer3a,mid_stat,8);\n",
    "    llayer4a = append_stat(llayer4a,mid_stat,9);  llayer5a = append_stat(llayer5a,mid_stat,10); llayer6a = append_stat(llayer6a,mid_stat,11);\n",
    "    \n",
    "    weight1 = append_stat(weight1,mid_stat,12);  weight2 = append_stat(weight2,mid_stat,13);  weight3 = append_stat(weight3,mid_stat,14);\n",
    "    weight4 = append_stat(weight4,mid_stat,15);  weight5 = append_stat(weight5,mid_stat,16);  weight6 = append_stat(weight6,mid_stat,17);\n",
    "    \n",
    "    gradw1 = append_stat(gradw1,mid_stat,18); gradw2 = append_stat(gradw2,mid_stat,19); gradw3 = append_stat(gradw3,mid_stat,20);\n",
    "    gradw4 = append_stat(gradw4,mid_stat,21); gradw5 = append_stat(gradw5,mid_stat,22); gradw6 = append_stat(gradw6,mid_stat,23);\n",
    "    \n",
    "    gradp1 = append_stat(gradp1,mid_stat,24); gradp2 = append_stat(gradp2,mid_stat,25); gradp3 = append_stat(gradp3,mid_stat,26);\n",
    "    gradp4 = append_stat(gradp4,mid_stat,27); gradp5 = append_stat(gradp5,mid_stat,28); gradp6 = append_stat(gradp6,mid_stat,29);\n",
    "\n",
    "    gradup1 = append_stat(gradup1,mid_stat,30); gradup2 = append_stat(gradup2,mid_stat,31); gradup3 = append_stat(gradup3,mid_stat,32);\n",
    "    gradup4 = append_stat(gradup4,mid_stat,33); gradup5 = append_stat(gradup5,mid_stat,34); gradup6 = append_stat(gradup6,mid_stat,35);\n",
    "\n",
    "    train_acc.append(avg_acc_train/(len(train_images)/batch_size))\n",
    "    test_acc .append(avg_acc_test / (len(test_images)/batch_size))\n",
    "    # ======================== extract stats ========================\n",
    "    \n",
    "    # ======================== save to image ========================\n",
    "    save_to_image(mid_stat[0:6]   ,llayer1,llayer2,llayer3,llayer4,llayer5,llayer6,\"layer\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[6:12]  ,llayer1a,llayer2a,llayer3a,llayer4a,llayer5a,llayer6a,\"layera\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[12:18] ,weight1,weight2,weight3,weight4,weight5,weight6,\"weights\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[18:24] ,gradw1,gradw2,gradw3,gradw4,gradw5,gradw6,\"gradientw\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[24:30] ,gradp1,gradp2,gradp3,gradp4,gradp5,gradp6,\"gradientp\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[30:36] ,gradup1,gradup2,gradup3,gradup4,gradup5,gradup6,\"moment\",train_acc,test_acc,current_exp_name,iter)\n",
    "    # ======================== save to image ========================\n",
    "        \n",
    "    # ======================== print reset ========================\n",
    "    print(\"Current : \"+ str(iter) + \" Train Acc : \" + str(avg_acc_train/(len(train_images)/batch_size)) + \" Test Acc : \" + str(avg_acc_test/(len(test_images)/batch_size)) + '\\n')\n",
    "    avg_acc_train = 0 ; avg_acc_test  = 0\n",
    "    # ======================== print reset ========================\n",
    "\n",
    "np.save(current_exp_name+'/train_acc.npy',train_acc); np.save(current_exp_name+'/test_acc.npy', test_acc)    \n",
    "np.save(current_exp_name+'/llayer1.npy', llayer1);  np.save(current_exp_name+'/llayer2.npy', llayer2);  np.save(current_exp_name+'/llayer3.npy', llayer3); \n",
    "np.save(current_exp_name+'/llayer4.npy', llayer4);  np.save(current_exp_name+'/llayer5.npy', llayer5);  np.save(current_exp_name+'/llayer6.npy', llayer6); \n",
    "\n",
    "np.save(current_exp_name+'/llayer1a.npy', llayer1a);  np.save(current_exp_name+'/llayer2a.npy', llayer2a);  np.save(current_exp_name+'/llayer3a.npy', llayer3a); \n",
    "np.save(current_exp_name+'/llayer4a.npy', llayer4a);  np.save(current_exp_name+'/llayer5a.npy', llayer5a);  np.save(current_exp_name+'/llayer6a.npy', llayer6a); \n",
    "\n",
    "np.save(current_exp_name+'/weight1.npy', weight1);  np.save(current_exp_name+'/weight2.npy', weight2);  np.save(current_exp_name+'/weight3.npy', weight3);  \n",
    "np.save(current_exp_name+'/weight4.npy', weight4);  np.save(current_exp_name+'/weight5.npy', weight5);  np.save(current_exp_name+'/weight6.npy', weight6);  \n",
    "\n",
    "np.save(current_exp_name+'/gradw1.npy', gradw1); np.save(current_exp_name+'/gradw2.npy', gradw2); np.save(current_exp_name+'/gradw3.npy', gradw3);\n",
    "np.save(current_exp_name+'/gradw4.npy', gradw4); np.save(current_exp_name+'/gradw5.npy', gradw5); np.save(current_exp_name+'/gradw6.npy', gradw6);\n",
    "\n",
    "np.save(current_exp_name+'/gradp1.npy', gradp1); np.save(current_exp_name+'/gradp2.npy', gradp2); np.save(current_exp_name+'/gradp3.npy', gradp3);\n",
    "np.save(current_exp_name+'/gradp4.npy', gradp4); np.save(current_exp_name+'/gradp5.npy', gradp5); np.save(current_exp_name+'/gradp6.npy', gradp6);\n",
    "\n",
    "np.save(current_exp_name+'/gradup1.npy', gradup1); np.save(current_exp_name+'/gradup2.npy', gradup2); np.save(current_exp_name+'/gradup3.npy', gradup3);\n",
    "np.save(current_exp_name+'/gradup4.npy', gradup4); np.save(current_exp_name+'/gradup5.npy', gradup5); np.save(current_exp_name+'/gradup6.npy', gradup6);\n",
    "\n",
    "sess.close(); tf.reset_default_graph();\n",
    "\n",
    "%reset_selective -f l1,l2,l3,l4,l5,l6\n",
    "%reset_selective -f layer1,layer2,layer3,layer4,layer5,layer6\n",
    "%reset_selective -f layer1a,layer2a,layer3a,layer4a,layer5a,layer6a\n",
    "%reset_selective -f train_acc,test_acc,llayer1,llayer2,llayer3,llayer4,llayer5,llayer6,llayer1a,llayer2a,llayer3a,llayer4a,llayer5a,llayer6a\n",
    "%reset_selective -f weight1,weight2,weight3,weight4,weight5,weight6\n",
    "%reset_selective -f gradw1,gradw2,gradw3,gradw4,gradw5,gradw6\n",
    "%reset_selective -f gradp1,gradp2,gradp3,gradp4,gradp5,gradp6\n",
    "%reset_selective -f gradup1,gradup2,gradup3,gradup4,gradup5,gradup6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-05T07:13:34.136557Z",
     "start_time": "2019-01-05T06:24:55.278360Z"
    },
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current : 0 Train Acc : 0.1306000026911497 Test Acc : 0.1513750028051436\n",
      "\n",
      "Current : 1 Train Acc : 0.16440000282227993 Test Acc : 0.18000000331550836\n",
      "\n",
      "Current : 2 Train Acc : 0.16740000273287298 Test Acc : 0.18512500330805778\n",
      "\n",
      "Current : 3 Train Acc : 0.1822000031322241 Test Acc : 0.20650000332854687\n",
      "\n",
      "Current : 4 Train Acc : 0.2102000034302473 Test Acc : 0.2323750031646341\n",
      "\n",
      "Current : 5 Train Acc : 0.23520000298321248 Test Acc : 0.25650000273250045\n",
      "\n",
      "Current : 6 Train Acc : 0.24180000299215318 Test Acc : 0.2665000023879111\n",
      "\n",
      "Current : 7 Train Acc : 0.255400003015995 Test Acc : 0.2702500027231872\n",
      "\n",
      "Current : 8 Train Acc : 0.2594000026732683 Test Acc : 0.2755000024382025\n",
      "\n",
      "Current : 9 Train Acc : 0.2652000028640032 Test Acc : 0.2793750024866313\n",
      "\n",
      "Current : 10 Train Acc : 0.27000000336766244 Test Acc : 0.28562500251457096\n",
      "\n",
      "Current : 11 Train Acc : 0.2710000034272671 Test Acc : 0.2848750032763928\n",
      "\n",
      "Current : 12 Train Acc : 0.2724000031650066 Test Acc : 0.29062500288709997\n",
      "\n",
      "Current : 13 Train Acc : 0.27720000271499157 Test Acc : 0.2942500021960586\n",
      "\n",
      "Current : 14 Train Acc : 0.2760000022500753 Test Acc : 0.3058750022109598\n",
      "\n",
      "Current : 15 Train Acc : 0.2888000023961067 Test Acc : 0.310250002136454\n",
      "\n",
      "Current : 16 Train Acc : 0.3018000024706125 Test Acc : 0.31012500210665167\n",
      "\n",
      "Current : 17 Train Acc : 0.31140000294148923 Test Acc : 0.3131250020116568\n",
      "\n",
      "Current : 18 Train Acc : 0.31860000255703924 Test Acc : 0.3162500018719584\n",
      "\n",
      "Current : 19 Train Acc : 0.3222000015974045 Test Acc : 0.3286250023264438\n",
      "\n",
      "Current : 20 Train Acc : 0.3354000014364719 Test Acc : 0.32950000199489293\n",
      "\n",
      "Current : 21 Train Acc : 0.3328000014722347 Test Acc : 0.33300000220537185\n",
      "\n",
      "Current : 22 Train Acc : 0.3346000015735626 Test Acc : 0.338375001642853\n",
      "\n",
      "Current : 23 Train Acc : 0.3366000009775162 Test Acc : 0.3388750019390136\n",
      "\n",
      "Current : 24 Train Acc : 0.33800000077486037 Test Acc : 0.3407500021997839\n",
      "\n",
      "Current : 25 Train Acc : 0.3412000014185905 Test Acc : 0.341625001905486\n",
      "\n",
      "Current : 26 Train Acc : 0.34380000162124635 Test Acc : 0.3453750016447157\n",
      "\n",
      "Current : 27 Train Acc : 0.3472000018954277 Test Acc : 0.34837500198744237\n",
      "\n",
      "Current : 28 Train Acc : 0.3524000014066696 Test Acc : 0.3513750019203872\n",
      "\n",
      "Current : 29 Train Acc : 0.3558000011444092 Test Acc : 0.35512500188313423\n",
      "\n",
      "Current : 30 Train Acc : 0.3586000016927719 Test Acc : 0.35875000145286323\n",
      "\n",
      "Current : 31 Train Acc : 0.36180000159144404 Test Acc : 0.3627500011026859\n",
      "\n",
      "Current : 32 Train Acc : 0.3632000018656254 Test Acc : 0.3652500009536743\n",
      "\n",
      "Current : 33 Train Acc : 0.3694000016748905 Test Acc : 0.3685000014305115\n",
      "\n",
      "Current : 34 Train Acc : 0.37160000124573705 Test Acc : 0.37037500102072957\n",
      "\n",
      "Current : 35 Train Acc : 0.37280000177025796 Test Acc : 0.3705000014416873\n",
      "\n",
      "Current : 36 Train Acc : 0.37300000149011614 Test Acc : 0.37175000134855507\n",
      "\n",
      "Current : 37 Train Acc : 0.3758000013232231 Test Acc : 0.3746250014938414\n",
      "\n",
      "Current : 38 Train Acc : 0.3760000014305115 Test Acc : 0.3763750010356307\n",
      "\n",
      "Current : 39 Train Acc : 0.37800000175833703 Test Acc : 0.3767500013485551\n",
      "\n",
      "Current : 40 Train Acc : 0.3826000018715858 Test Acc : 0.3795000011101365\n",
      "\n",
      "Current : 41 Train Acc : 0.38680000221729277 Test Acc : 0.3798750012926757\n",
      "\n",
      "Current : 42 Train Acc : 0.38900000232458115 Test Acc : 0.3818750014156103\n",
      "\n",
      "Current : 43 Train Acc : 0.39000000232458115 Test Acc : 0.3847500014677644\n",
      "\n",
      "Current : 44 Train Acc : 0.39200000220537184 Test Acc : 0.3887500016391277\n",
      "\n",
      "Current : 45 Train Acc : 0.3952000017166138 Test Acc : 0.3915000015869737\n",
      "\n",
      "Current : 46 Train Acc : 0.39800000190734863 Test Acc : 0.39300000086426734\n",
      "\n",
      "Current : 47 Train Acc : 0.400800001680851 Test Acc : 0.3961250006780028\n",
      "\n",
      "Current : 48 Train Acc : 0.4040000013709068 Test Acc : 0.3966250005736947\n",
      "\n",
      "Current : 49 Train Acc : 0.40580000162124635 Test Acc : 0.39762500058859584\n",
      "\n",
      "Current : 50 Train Acc : 0.40880000120401383 Test Acc : 0.3986250006780028\n",
      "\n",
      "Current : 51 Train Acc : 0.41160000121593476 Test Acc : 0.4016250004991889\n",
      "\n",
      "Current : 52 Train Acc : 0.41140000185370446 Test Acc : 0.4036250007525086\n",
      "\n",
      "Current : 53 Train Acc : 0.4114000013768673 Test Acc : 0.4041250006482005\n",
      "\n",
      "Current : 54 Train Acc : 0.41400000151991845 Test Acc : 0.4053750004060566\n",
      "\n",
      "Current : 55 Train Acc : 0.41680000159144404 Test Acc : 0.4083750005997717\n",
      "\n",
      "Current : 56 Train Acc : 0.4208000011742115 Test Acc : 0.40937500005587935\n",
      "\n",
      "Current : 57 Train Acc : 0.421200001180172 Test Acc : 0.41087500052526593\n",
      "\n",
      "Current : 58 Train Acc : 0.4228000013232231 Test Acc : 0.41125000042840837\n",
      "\n",
      "Current : 59 Train Acc : 0.42740000104904174 Test Acc : 0.41200000086799266\n",
      "\n",
      "Current : 60 Train Acc : 0.4288000010251999 Test Acc : 0.4115000013820827\n",
      "\n",
      "Current : 61 Train Acc : 0.43180000185966494 Test Acc : 0.41287500141188505\n",
      "\n",
      "Current : 62 Train Acc : 0.4336000010073185 Test Acc : 0.4140000014938414\n",
      "\n",
      "Current : 63 Train Acc : 0.4334000016748905 Test Acc : 0.41412500167265537\n",
      "\n",
      "Current : 64 Train Acc : 0.4340000014603138 Test Acc : 0.4128750014305115\n",
      "\n",
      "Current : 65 Train Acc : 0.43300000098347663 Test Acc : 0.4143750013411045\n",
      "\n",
      "Current : 66 Train Acc : 0.4336000007092953 Test Acc : 0.41500000081956384\n",
      "\n",
      "Current : 67 Train Acc : 0.43720000120997426 Test Acc : 0.41675000090152026\n",
      "\n",
      "Current : 68 Train Acc : 0.43940000066161156 Test Acc : 0.41687500081956386\n",
      "\n",
      "Current : 69 Train Acc : 0.4402000009715557 Test Acc : 0.41725000116974115\n",
      "\n",
      "Current : 70 Train Acc : 0.4420000014603138 Test Acc : 0.4172500006575137\n",
      "\n",
      "Current : 71 Train Acc : 0.4458000009357929 Test Acc : 0.4176250008214265\n",
      "\n",
      "Current : 72 Train Acc : 0.44740000185370443 Test Acc : 0.4206250007078052\n",
      "\n",
      "Current : 73 Train Acc : 0.44800000163912773 Test Acc : 0.4218750004842877\n",
      "\n",
      "Current : 74 Train Acc : 0.4488000017106533 Test Acc : 0.4237500005308539\n",
      "\n",
      "Current : 75 Train Acc : 0.45140000149607656 Test Acc : 0.4246250007022172\n",
      "\n",
      "Current : 76 Train Acc : 0.451800001591444 Test Acc : 0.4252500006277114\n",
      "\n",
      "Current : 77 Train Acc : 0.45440000090003013 Test Acc : 0.4266250008158386\n",
      "\n",
      "Current : 78 Train Acc : 0.45680000153183936 Test Acc : 0.42650000085122886\n",
      "\n",
      "Current : 79 Train Acc : 0.4578000014126301 Test Acc : 0.42812500086613\n",
      "\n",
      "Current : 80 Train Acc : 0.4582000017464161 Test Acc : 0.42737500091083347\n",
      "\n",
      "Current : 81 Train Acc : 0.45840000227093697 Test Acc : 0.4271250007022172\n",
      "\n",
      "Current : 82 Train Acc : 0.46040000185370444 Test Acc : 0.42950000063516197\n",
      "\n",
      "Current : 83 Train Acc : 0.4618000020682812 Test Acc : 0.43137500105425713\n",
      "\n",
      "Current : 84 Train Acc : 0.4636000010073185 Test Acc : 0.431875000949949\n",
      "\n",
      "Current : 85 Train Acc : 0.4642000012099743 Test Acc : 0.4331250008288771\n",
      "\n",
      "Current : 86 Train Acc : 0.4660000011026859 Test Acc : 0.4328750012908131\n",
      "\n",
      "Current : 87 Train Acc : 0.46740000137686727 Test Acc : 0.43700000130571426\n",
      "\n",
      "Current : 88 Train Acc : 0.4688000008761883 Test Acc : 0.437500001238659\n",
      "\n",
      "Current : 89 Train Acc : 0.4710000011026859 Test Acc : 0.43637500071898105\n",
      "\n",
      "Current : 90 Train Acc : 0.4706000012457371 Test Acc : 0.43662500085309147\n",
      "\n",
      "Current : 91 Train Acc : 0.473000000923872 Test Acc : 0.43737500127404927\n",
      "\n",
      "Current : 92 Train Acc : 0.4748000014722347 Test Acc : 0.4366250007972121\n",
      "\n",
      "Current : 93 Train Acc : 0.4764000016748905 Test Acc : 0.4365000009536743\n",
      "\n",
      "Current : 94 Train Acc : 0.477000001758337 Test Acc : 0.43562500085681677\n",
      "\n",
      "Current : 95 Train Acc : 0.4772000019848347 Test Acc : 0.4366250004991889\n",
      "\n",
      "Current : 96 Train Acc : 0.47860000148415566 Test Acc : 0.4367500006407499\n",
      "\n",
      "Current : 97 Train Acc : 0.48080000206828116 Test Acc : 0.4371250008046627\n",
      "\n",
      "Current : 98 Train Acc : 0.4826000025570393 Test Acc : 0.43550000078976153\n",
      "\n",
      "Current : 99 Train Acc : 0.4848000026643276 Test Acc : 0.43637500148266556\n",
      "\n",
      "Current : 100 Train Acc : 0.4858000031411648 Test Acc : 0.43625000115484\n",
      "\n",
      "Current : 101 Train Acc : 0.48760000213980675 Test Acc : 0.43662500064820053\n",
      "\n",
      "Current : 102 Train Acc : 0.49000000187754633 Test Acc : 0.43675000082701443\n",
      "\n",
      "Current : 103 Train Acc : 0.4918000020682812 Test Acc : 0.4362500000372529\n",
      "\n",
      "Current : 104 Train Acc : 0.49560000148415567 Test Acc : 0.4358749998733401\n",
      "\n",
      "Current : 105 Train Acc : 0.4960000014603138 Test Acc : 0.43674999989569185\n",
      "\n",
      "Current : 106 Train Acc : 0.4970000008642673 Test Acc : 0.4376249999180436\n",
      "\n",
      "Current : 107 Train Acc : 0.4982000007927418 Test Acc : 0.437625000923872\n",
      "\n",
      "Current : 108 Train Acc : 0.4978000010550022 Test Acc : 0.4386250010877848\n",
      "\n",
      "Current : 109 Train Acc : 0.4986000011265278 Test Acc : 0.4393750009313226\n",
      "\n",
      "Current : 110 Train Acc : 0.4994000017940998 Test Acc : 0.43825000043958423\n",
      "\n",
      "Current : 111 Train Acc : 0.5016000012457371 Test Acc : 0.4368750011920929\n",
      "\n",
      "Current : 112 Train Acc : 0.5004000017642974 Test Acc : 0.43812500085681677\n",
      "\n",
      "Current : 113 Train Acc : 0.5010000017881393 Test Acc : 0.43925000112503765\n",
      "\n",
      "Current : 114 Train Acc : 0.5022000014781952 Test Acc : 0.4401250010356307\n",
      "\n",
      "Current : 115 Train Acc : 0.5026000015735627 Test Acc : 0.43962500054389236\n",
      "\n",
      "Current : 116 Train Acc : 0.5064000015258789 Test Acc : 0.4398750004172325\n",
      "\n",
      "Current : 117 Train Acc : 0.5066000014543534 Test Acc : 0.440875000692904\n",
      "\n",
      "Current : 118 Train Acc : 0.506400000989437 Test Acc : 0.4413750009983778\n",
      "\n",
      "Current : 119 Train Acc : 0.5078000018000602 Test Acc : 0.44200000070035456\n",
      "\n",
      "Current : 120 Train Acc : 0.5096000017523765 Test Acc : 0.4428750006482005\n",
      "\n",
      "Current : 121 Train Acc : 0.5122000017762184 Test Acc : 0.44462500102818014\n",
      "\n",
      "Current : 122 Train Acc : 0.5142000015377999 Test Acc : 0.4463750009611249\n",
      "\n",
      "Current : 123 Train Acc : 0.5130000010132789 Test Acc : 0.44675000105053186\n",
      "\n",
      "Current : 124 Train Acc : 0.5150000013709068 Test Acc : 0.4466250009834766\n",
      "\n",
      "Current : 125 Train Acc : 0.5152000009417533 Test Acc : 0.4481250014528632\n",
      "\n",
      "Current : 126 Train Acc : 0.5172000009417533 Test Acc : 0.44650000128895045\n",
      "\n",
      "Current : 127 Train Acc : 0.5192000009417533 Test Acc : 0.44837500143796205\n",
      "\n",
      "Current : 128 Train Acc : 0.5186000000834465 Test Acc : 0.44787500109523537\n",
      "\n",
      "Current : 129 Train Acc : 0.5195999997258186 Test Acc : 0.44787500109523537\n",
      "\n",
      "Current : 130 Train Acc : 0.5208000003099441 Test Acc : 0.4501250010728836\n",
      "\n",
      "Current : 131 Train Acc : 0.5216000001430512 Test Acc : 0.4512500008940697\n",
      "\n",
      "Current : 132 Train Acc : 0.5220000004768371 Test Acc : 0.4525000007450581\n",
      "\n",
      "Current : 133 Train Acc : 0.5228000003099441 Test Acc : 0.452125001065433\n",
      "\n",
      "Current : 134 Train Acc : 0.522600000500679 Test Acc : 0.4522500007227063\n",
      "\n",
      "Current : 135 Train Acc : 0.523600000500679 Test Acc : 0.4522500003874302\n",
      "\n",
      "Current : 136 Train Acc : 0.5248000011444092 Test Acc : 0.4518750005215406\n",
      "\n",
      "Current : 137 Train Acc : 0.5266000014543534 Test Acc : 0.4527500008791685\n",
      "\n",
      "Current : 138 Train Acc : 0.5282000017166137 Test Acc : 0.45162500023841856\n",
      "\n",
      "Current : 139 Train Acc : 0.5270000017881393 Test Acc : 0.45187500040978196\n",
      "\n",
      "Current : 140 Train Acc : 0.5294000017642975 Test Acc : 0.4501250004768372\n",
      "\n",
      "Current : 141 Train Acc : 0.5282000014781952 Test Acc : 0.4505000005662441\n",
      "\n",
      "Current : 142 Train Acc : 0.529200002193451 Test Acc : 0.4517500004917383\n",
      "\n",
      "Current : 143 Train Acc : 0.5300000022649765 Test Acc : 0.45250000063329937\n",
      "\n",
      "Current : 144 Train Acc : 0.5298000024557113 Test Acc : 0.4535000006482005\n",
      "\n",
      "Current : 145 Train Acc : 0.5318000019788742 Test Acc : 0.45375000063329934\n",
      "\n",
      "Current : 146 Train Acc : 0.53200000166893 Test Acc : 0.45512500062584876\n",
      "\n",
      "Current : 147 Train Acc : 0.5322000020742417 Test Acc : 0.45625000074505806\n",
      "\n",
      "Current : 148 Train Acc : 0.53180000269413 Test Acc : 0.4556250011175871\n",
      "\n",
      "Current : 149 Train Acc : 0.5340000025033951 Test Acc : 0.45637500058859587\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# F\n",
    "current_exp_name = 'F';\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# create layers\n",
    "l1 = CNN(3,3, 16,which_reg=current_exp_name); \n",
    "l2 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "l3 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "\n",
    "l4 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "l5 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "l6 = CNN(3,16,10,which_reg=current_exp_name); \n",
    "\n",
    "# 2. graph \n",
    "x = tf.placeholder(tf.float32,(batch_size,96,96,3))\n",
    "y = tf.placeholder(tf.float32,(batch_size,10))\n",
    "\n",
    "layer1, layer1a = l1. feedforward(x,stride=2)\n",
    "layer2, layer2a = l2. feedforward(layer1a,stride=2)\n",
    "layer3, layer3a = l3. feedforward(layer2a,stride=2)\n",
    "layer4, layer4a = l4. feedforward(layer3a,stride=2)\n",
    "layer5, layer5a = l5. feedforward(layer4a)\n",
    "layer6, layer6a = l6. feedforward(layer5a)\n",
    "\n",
    "final_layer   = tf.reduce_mean(layer6a,(1,2))\n",
    "final_softmax = tf_softmax(final_layer)\n",
    "cost          = -tf.reduce_mean(y * tf.log(final_softmax + 1e-8))\n",
    "correct_prediction = tf.equal(tf.argmax(final_softmax, 1), tf.argmax(y, 1))\n",
    "accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "gradient = tf.tile((final_softmax-y)[:,None,None,:],[1,6,6,1])/batch_size\n",
    "grad6p,grad6w,grad6_up = l6.backprop(gradient)\n",
    "grad5p,grad5w,grad5_up = l5.backprop(grad6p)\n",
    "grad4p,grad4w,grad4_up = l4.backprop(grad5p,stride=2)\n",
    "grad3p,grad3w,grad3_up = l3.backprop(grad4p,stride=2)\n",
    "grad2p,grad2w,grad2_up = l2.backprop(grad3p,stride=2)\n",
    "grad1p,grad1w,grad1_up = l1.backprop(grad2p,stride=2)\n",
    "\n",
    "gradient_update = grad6_up + grad5_up + grad4_up + grad3_up + grad2_up + grad1_up \n",
    "\n",
    "# train\n",
    "sess.run(tf.global_variables_initializer())\n",
    "avg_acc_train = 0; avg_acc_test  = 0; train_acc = [];test_acc = []\n",
    "\n",
    "# mean std skew kurt non-zero\n",
    "llayer1 = [[],[],[],[],[]]; llayer2 = [[],[],[],[],[]]; llayer3 = [[],[],[],[],[]]\n",
    "llayer4 = [[],[],[],[],[]]; llayer5 = [[],[],[],[],[]]; llayer6 = [[],[],[],[],[]]\n",
    "\n",
    "llayer1a = [[],[],[],[],[]]; llayer2a = [[],[],[],[],[]]; llayer3a = [[],[],[],[],[]]\n",
    "llayer4a = [[],[],[],[],[]]; llayer5a = [[],[],[],[],[]]; llayer6a = [[],[],[],[],[]]\n",
    "\n",
    "weight1 = [[],[],[],[],[]]; weight2 = [[],[],[],[],[]]; weight3 = [[],[],[],[],[]];\n",
    "weight4 = [[],[],[],[],[]]; weight5 = [[],[],[],[],[]]; weight6 = [[],[],[],[],[]];\n",
    "\n",
    "gradw1  = [[],[],[],[],[]]; gradw2  = [[],[],[],[],[]]; gradw3  = [[],[],[],[],[]];\n",
    "gradw4  = [[],[],[],[],[]]; gradw5  = [[],[],[],[],[]]; gradw6  = [[],[],[],[],[]];\n",
    "\n",
    "gradp1  = [[],[],[],[],[]]; gradp2  = [[],[],[],[],[]]; gradp3  = [[],[],[],[],[]];\n",
    "gradp4  = [[],[],[],[],[]]; gradp5  = [[],[],[],[],[]]; gradp6  = [[],[],[],[],[]];\n",
    "\n",
    "gradup1  = [[],[],[],[],[]]; gradup2  = [[],[],[],[],[]]; gradup3  = [[],[],[],[],[]];\n",
    "gradup4  = [[],[],[],[],[]]; gradup5  = [[],[],[],[],[]]; gradup6  = [[],[],[],[],[]];\n",
    "\n",
    "list_of_outputs = [\n",
    "    layer1,layer2,layer3,layer4,layer5,layer6,\n",
    "    layer1a,layer2a,layer3a,layer4a,layer5a,layer6a,\n",
    "    l1.getw(),l2.getw(),l3.getw(),l4.getw(),l5.getw(),l6.getw(),\n",
    "    grad1w,grad2w,grad3w,grad4w,grad5w,grad6w,\n",
    "    grad1p,grad2p,grad3p,grad4p,grad5p,grad6p,\n",
    "    grad1_up[0],grad2_up[0],grad3_up[0],grad4_up[0],grad5_up[0],grad6_up[0]\n",
    "]\n",
    "\n",
    "for iter in range(num_epoch):\n",
    "\n",
    "    # Training Accuracy    \n",
    "    for current_batch_index in range(0,len(train_images),batch_size):\n",
    "        current_data  = train_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = train_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy,gradient_update],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(train_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_train = avg_acc_train + sess_results[0]\n",
    "        \n",
    "    # get the results\n",
    "    mid_stat = sess.run(list_of_outputs,feed_dict={x:current_data,y:current_label})\n",
    "    \n",
    "    # Test Accuracy    \n",
    "    for current_batch_index in range(0,len(test_images), batch_size):\n",
    "        current_data  = test_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = test_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(test_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_test = avg_acc_test + sess_results[0]   \n",
    "        \n",
    "    # ======================== extract stats ========================\n",
    "    llayer1 = append_stat(llayer1,mid_stat,0);  llayer2 = append_stat(llayer2,mid_stat,1);  llayer3 = append_stat(llayer3,mid_stat,2);\n",
    "    llayer4 = append_stat(llayer4,mid_stat,3);  llayer5 = append_stat(llayer5,mid_stat,4);  llayer6 = append_stat(llayer6,mid_stat,5);\n",
    "\n",
    "    llayer1a = append_stat(llayer1a,mid_stat,6);  llayer2a = append_stat(llayer2a,mid_stat,7);  llayer3a = append_stat(llayer3a,mid_stat,8);\n",
    "    llayer4a = append_stat(llayer4a,mid_stat,9);  llayer5a = append_stat(llayer5a,mid_stat,10); llayer6a = append_stat(llayer6a,mid_stat,11);\n",
    "    \n",
    "    weight1 = append_stat(weight1,mid_stat,12);  weight2 = append_stat(weight2,mid_stat,13);  weight3 = append_stat(weight3,mid_stat,14);\n",
    "    weight4 = append_stat(weight4,mid_stat,15);  weight5 = append_stat(weight5,mid_stat,16);  weight6 = append_stat(weight6,mid_stat,17);\n",
    "    \n",
    "    gradw1 = append_stat(gradw1,mid_stat,18); gradw2 = append_stat(gradw2,mid_stat,19); gradw3 = append_stat(gradw3,mid_stat,20);\n",
    "    gradw4 = append_stat(gradw4,mid_stat,21); gradw5 = append_stat(gradw5,mid_stat,22); gradw6 = append_stat(gradw6,mid_stat,23);\n",
    "    \n",
    "    gradp1 = append_stat(gradp1,mid_stat,24); gradp2 = append_stat(gradp2,mid_stat,25); gradp3 = append_stat(gradp3,mid_stat,26);\n",
    "    gradp4 = append_stat(gradp4,mid_stat,27); gradp5 = append_stat(gradp5,mid_stat,28); gradp6 = append_stat(gradp6,mid_stat,29);\n",
    "\n",
    "    gradup1 = append_stat(gradup1,mid_stat,30); gradup2 = append_stat(gradup2,mid_stat,31); gradup3 = append_stat(gradup3,mid_stat,32);\n",
    "    gradup4 = append_stat(gradup4,mid_stat,33); gradup5 = append_stat(gradup5,mid_stat,34); gradup6 = append_stat(gradup6,mid_stat,35);\n",
    "\n",
    "    train_acc.append(avg_acc_train/(len(train_images)/batch_size))\n",
    "    test_acc .append(avg_acc_test / (len(test_images)/batch_size))\n",
    "    # ======================== extract stats ========================\n",
    "    \n",
    "    # ======================== save to image ========================\n",
    "    save_to_image(mid_stat[0:6]   ,llayer1,llayer2,llayer3,llayer4,llayer5,llayer6,\"layer\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[6:12]  ,llayer1a,llayer2a,llayer3a,llayer4a,llayer5a,llayer6a,\"layera\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[12:18] ,weight1,weight2,weight3,weight4,weight5,weight6,\"weights\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[18:24] ,gradw1,gradw2,gradw3,gradw4,gradw5,gradw6,\"gradientw\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[24:30] ,gradp1,gradp2,gradp3,gradp4,gradp5,gradp6,\"gradientp\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[30:36] ,gradup1,gradup2,gradup3,gradup4,gradup5,gradup6,\"moment\",train_acc,test_acc,current_exp_name,iter)\n",
    "    # ======================== save to image ========================\n",
    "        \n",
    "    # ======================== print reset ========================\n",
    "    print(\"Current : \"+ str(iter) + \" Train Acc : \" + str(avg_acc_train/(len(train_images)/batch_size)) + \" Test Acc : \" + str(avg_acc_test/(len(test_images)/batch_size)) + '\\n')\n",
    "    avg_acc_train = 0 ; avg_acc_test  = 0\n",
    "    # ======================== print reset ========================\n",
    "\n",
    "np.save(current_exp_name+'/train_acc.npy',train_acc); np.save(current_exp_name+'/test_acc.npy', test_acc)    \n",
    "np.save(current_exp_name+'/llayer1.npy', llayer1);  np.save(current_exp_name+'/llayer2.npy', llayer2);  np.save(current_exp_name+'/llayer3.npy', llayer3); \n",
    "np.save(current_exp_name+'/llayer4.npy', llayer4);  np.save(current_exp_name+'/llayer5.npy', llayer5);  np.save(current_exp_name+'/llayer6.npy', llayer6); \n",
    "\n",
    "np.save(current_exp_name+'/llayer1a.npy', llayer1a);  np.save(current_exp_name+'/llayer2a.npy', llayer2a);  np.save(current_exp_name+'/llayer3a.npy', llayer3a); \n",
    "np.save(current_exp_name+'/llayer4a.npy', llayer4a);  np.save(current_exp_name+'/llayer5a.npy', llayer5a);  np.save(current_exp_name+'/llayer6a.npy', llayer6a); \n",
    "\n",
    "np.save(current_exp_name+'/weight1.npy', weight1);  np.save(current_exp_name+'/weight2.npy', weight2);  np.save(current_exp_name+'/weight3.npy', weight3);  \n",
    "np.save(current_exp_name+'/weight4.npy', weight4);  np.save(current_exp_name+'/weight5.npy', weight5);  np.save(current_exp_name+'/weight6.npy', weight6);  \n",
    "\n",
    "np.save(current_exp_name+'/gradw1.npy', gradw1); np.save(current_exp_name+'/gradw2.npy', gradw2); np.save(current_exp_name+'/gradw3.npy', gradw3);\n",
    "np.save(current_exp_name+'/gradw4.npy', gradw4); np.save(current_exp_name+'/gradw5.npy', gradw5); np.save(current_exp_name+'/gradw6.npy', gradw6);\n",
    "\n",
    "np.save(current_exp_name+'/gradp1.npy', gradp1); np.save(current_exp_name+'/gradp2.npy', gradp2); np.save(current_exp_name+'/gradp3.npy', gradp3);\n",
    "np.save(current_exp_name+'/gradp4.npy', gradp4); np.save(current_exp_name+'/gradp5.npy', gradp5); np.save(current_exp_name+'/gradp6.npy', gradp6);\n",
    "\n",
    "np.save(current_exp_name+'/gradup1.npy', gradup1); np.save(current_exp_name+'/gradup2.npy', gradup2); np.save(current_exp_name+'/gradup3.npy', gradup3);\n",
    "np.save(current_exp_name+'/gradup4.npy', gradup4); np.save(current_exp_name+'/gradup5.npy', gradup5); np.save(current_exp_name+'/gradup6.npy', gradup6);\n",
    "\n",
    "sess.close(); tf.reset_default_graph();\n",
    "\n",
    "%reset_selective -f l1,l2,l3,l4,l5,l6\n",
    "%reset_selective -f layer1,layer2,layer3,layer4,layer5,layer6\n",
    "%reset_selective -f layer1a,layer2a,layer3a,layer4a,layer5a,layer6a\n",
    "%reset_selective -f train_acc,test_acc,llayer1,llayer2,llayer3,llayer4,llayer5,llayer6,llayer1a,llayer2a,llayer3a,llayer4a,llayer5a,llayer6a\n",
    "%reset_selective -f weight1,weight2,weight3,weight4,weight5,weight6\n",
    "%reset_selective -f gradw1,gradw2,gradw3,gradw4,gradw5,gradw6\n",
    "%reset_selective -f gradp1,gradp2,gradp3,gradp4,gradp5,gradp6\n",
    "%reset_selective -f gradup1,gradup2,gradup3,gradup4,gradup5,gradup6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-05T08:03:07.384001Z",
     "start_time": "2019-01-05T07:13:34.160509Z"
    },
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current : 0 Train Acc : 0.12100000256299973 Test Acc : 0.12725000255741178\n",
      "\n",
      "Current : 1 Train Acc : 0.17580000293254852 Test Acc : 0.17625000304542482\n",
      "\n",
      "Current : 2 Train Acc : 0.20820000354945659 Test Acc : 0.23425000340677798\n",
      "\n",
      "Current : 3 Train Acc : 0.26400000271201135 Test Acc : 0.27900000267662106\n",
      "\n",
      "Current : 4 Train Acc : 0.29400000210106375 Test Acc : 0.2908750024344772\n",
      "\n",
      "Current : 5 Train Acc : 0.3216000020503998 Test Acc : 0.334125001905486\n",
      "\n",
      "Current : 6 Train Acc : 0.338800002142787 Test Acc : 0.35450000101700424\n",
      "\n",
      "Current : 7 Train Acc : 0.3538000019490719 Test Acc : 0.359625001186505\n",
      "\n",
      "Current : 8 Train Acc : 0.367600001513958 Test Acc : 0.3646250015683472\n",
      "\n",
      "Current : 9 Train Acc : 0.37560000106692315 Test Acc : 0.3672500008903444\n",
      "\n",
      "Current : 10 Train Acc : 0.38960000106692316 Test Acc : 0.37550000080838797\n",
      "\n",
      "Current : 11 Train Acc : 0.3996000011563301 Test Acc : 0.37537500135600566\n",
      "\n",
      "Current : 12 Train Acc : 0.40080000001192095 Test Acc : 0.38412500109523534\n",
      "\n",
      "Current : 13 Train Acc : 0.4062000007033348 Test Acc : 0.3875000009313226\n",
      "\n",
      "Current : 14 Train Acc : 0.4114000008702278 Test Acc : 0.3930000007338822\n",
      "\n",
      "Current : 15 Train Acc : 0.41600000074505805 Test Acc : 0.3948750008456409\n",
      "\n",
      "Current : 16 Train Acc : 0.4162000009119511 Test Acc : 0.40212500086054204\n",
      "\n",
      "Current : 17 Train Acc : 0.4240000011920929 Test Acc : 0.40025000175461173\n",
      "\n",
      "Current : 18 Train Acc : 0.4254000018835068 Test Acc : 0.38887500077486037\n",
      "\n",
      "Current : 19 Train Acc : 0.4308000018000603 Test Acc : 0.39287500059232117\n",
      "\n",
      "Current : 20 Train Acc : 0.43640000200271606 Test Acc : 0.39775000093504786\n",
      "\n",
      "Current : 21 Train Acc : 0.43940000212192537 Test Acc : 0.4007500009611249\n",
      "\n",
      "Current : 22 Train Acc : 0.44400000125169753 Test Acc : 0.4062500012665987\n",
      "\n",
      "Current : 23 Train Acc : 0.4450000002980232 Test Acc : 0.4106250012293458\n",
      "\n",
      "Current : 24 Train Acc : 0.44620000064373017 Test Acc : 0.4135000010952353\n",
      "\n",
      "Current : 25 Train Acc : 0.4496000007390976 Test Acc : 0.41787500098347663\n",
      "\n",
      "Current : 26 Train Acc : 0.4520000009536743 Test Acc : 0.42312500175088646\n",
      "\n",
      "Current : 27 Train Acc : 0.45240000092983246 Test Acc : 0.42625000182539224\n",
      "\n",
      "Current : 28 Train Acc : 0.4568000013232231 Test Acc : 0.4293750014528632\n",
      "\n",
      "Current : 29 Train Acc : 0.4598000013828278 Test Acc : 0.4318750016018748\n",
      "\n",
      "Current : 30 Train Acc : 0.46100000125169754 Test Acc : 0.43500000175088643\n",
      "\n",
      "Current : 31 Train Acc : 0.4656000015735626 Test Acc : 0.43825000170618295\n",
      "\n",
      "Current : 32 Train Acc : 0.46760000109672545 Test Acc : 0.43925000190734864\n",
      "\n",
      "Current : 33 Train Acc : 0.47000000125169755 Test Acc : 0.44150000140070916\n",
      "\n",
      "Current : 34 Train Acc : 0.47120000183582306 Test Acc : 0.4423750012740493\n",
      "\n",
      "Current : 35 Train Acc : 0.4752000017762184 Test Acc : 0.4435000015050173\n",
      "\n",
      "Current : 36 Train Acc : 0.4764000017046928 Test Acc : 0.4452500017732382\n",
      "\n",
      "Current : 37 Train Acc : 0.4780000013709068 Test Acc : 0.44612500227987767\n",
      "\n",
      "Current : 38 Train Acc : 0.480600001513958 Test Acc : 0.4482500018551946\n",
      "\n",
      "Current : 39 Train Acc : 0.48220000088214876 Test Acc : 0.4493750012293458\n",
      "\n",
      "Current : 40 Train Acc : 0.48320000094175336 Test Acc : 0.45012500137090683\n",
      "\n",
      "Current : 41 Train Acc : 0.48520000153779985 Test Acc : 0.4531250014528632\n",
      "\n",
      "Current : 42 Train Acc : 0.486000001847744 Test Acc : 0.4538750013709068\n",
      "\n",
      "Current : 43 Train Acc : 0.4880000016093254 Test Acc : 0.45775000110268593\n",
      "\n",
      "Current : 44 Train Acc : 0.4898000016212463 Test Acc : 0.4573750011622906\n",
      "\n",
      "Current : 45 Train Acc : 0.4924000014066696 Test Acc : 0.45612500090152025\n",
      "\n",
      "Current : 46 Train Acc : 0.4956000010967255 Test Acc : 0.4571250012516975\n",
      "\n",
      "Current : 47 Train Acc : 0.5002000010013581 Test Acc : 0.4578750004991889\n",
      "\n",
      "Current : 48 Train Acc : 0.5034000010490417 Test Acc : 0.4583750007674098\n",
      "\n",
      "Current : 49 Train Acc : 0.5070000013709068 Test Acc : 0.4605000004544854\n",
      "\n",
      "Current : 50 Train Acc : 0.509600000679493 Test Acc : 0.462250000461936\n",
      "\n",
      "Current : 51 Train Acc : 0.5122000009417534 Test Acc : 0.46137500043958424\n",
      "\n",
      "Current : 52 Train Acc : 0.513800000846386 Test Acc : 0.46300000060349705\n",
      "\n",
      "Current : 53 Train Acc : 0.5168000006079674 Test Acc : 0.4632500010356307\n",
      "\n",
      "Current : 54 Train Acc : 0.5204000006318092 Test Acc : 0.46562500096857545\n",
      "\n",
      "Current : 55 Train Acc : 0.5204000001549721 Test Acc : 0.46437500074505805\n",
      "\n",
      "Current : 56 Train Acc : 0.522400000333786 Test Acc : 0.46562500022351744\n",
      "\n",
      "Current : 57 Train Acc : 0.5231999999880791 Test Acc : 0.4655000005289912\n",
      "\n",
      "Current : 58 Train Acc : 0.5246000002622604 Test Acc : 0.4663749999180436\n",
      "\n",
      "Current : 59 Train Acc : 0.5248000012040138 Test Acc : 0.466500000320375\n",
      "\n",
      "Current : 60 Train Acc : 0.5256000007390976 Test Acc : 0.4665000002458692\n",
      "\n",
      "Current : 61 Train Acc : 0.5272000011205673 Test Acc : 0.465875000320375\n",
      "\n",
      "Current : 62 Train Acc : 0.5294000006914139 Test Acc : 0.4660000007972121\n",
      "\n",
      "Current : 63 Train Acc : 0.5304000016450882 Test Acc : 0.4672500009462237\n",
      "\n",
      "Current : 64 Train Acc : 0.5328000017404556 Test Acc : 0.46600000113248824\n",
      "\n",
      "Current : 65 Train Acc : 0.5354000021219254 Test Acc : 0.4646250007674098\n",
      "\n",
      "Current : 66 Train Acc : 0.534800002336502 Test Acc : 0.46650000113993884\n",
      "\n",
      "Current : 67 Train Acc : 0.5366000026464463 Test Acc : 0.46725000116974114\n",
      "\n",
      "Current : 68 Train Acc : 0.5374000027179718 Test Acc : 0.46775000136345624\n",
      "\n",
      "Current : 69 Train Acc : 0.5388000029325485 Test Acc : 0.4698750013485551\n",
      "\n",
      "Current : 70 Train Acc : 0.5414000036716461 Test Acc : 0.4711250014230609\n",
      "\n",
      "Current : 71 Train Acc : 0.5442000029087066 Test Acc : 0.4720000014826655\n",
      "\n",
      "Current : 72 Train Acc : 0.5466000028848648 Test Acc : 0.47250000182539226\n",
      "\n",
      "Current : 73 Train Acc : 0.5496000028848648 Test Acc : 0.47287500187754633\n",
      "\n",
      "Current : 74 Train Acc : 0.5508000024557114 Test Acc : 0.47300000209361315\n",
      "\n",
      "Current : 75 Train Acc : 0.5526000020503998 Test Acc : 0.4726250021532178\n",
      "\n",
      "Current : 76 Train Acc : 0.5528000017404556 Test Acc : 0.47112500175833705\n",
      "\n",
      "Current : 77 Train Acc : 0.5566000016927719 Test Acc : 0.47312500201165675\n",
      "\n",
      "Current : 78 Train Acc : 0.5578000016212463 Test Acc : 0.47300000179558993\n",
      "\n",
      "Current : 79 Train Acc : 0.5588000023365021 Test Acc : 0.4722500021010637\n",
      "\n",
      "Current : 80 Train Acc : 0.5604000014066696 Test Acc : 0.4718750024959445\n",
      "\n",
      "Current : 81 Train Acc : 0.5622000018358231 Test Acc : 0.4720000025257468\n",
      "\n",
      "Current : 82 Train Acc : 0.5650000010728836 Test Acc : 0.47175000235438347\n",
      "\n",
      "Current : 83 Train Acc : 0.566600001335144 Test Acc : 0.47237500190734866\n",
      "\n",
      "Current : 84 Train Acc : 0.5688000017404556 Test Acc : 0.47200000174343587\n",
      "\n",
      "Current : 85 Train Acc : 0.5698000020980835 Test Acc : 0.47112500190734863\n",
      "\n",
      "Current : 86 Train Acc : 0.5718000022172928 Test Acc : 0.47137500159442425\n",
      "\n",
      "Current : 87 Train Acc : 0.5732000021934509 Test Acc : 0.47262500151991843\n",
      "\n",
      "Current : 88 Train Acc : 0.5756000015735626 Test Acc : 0.4721250016987324\n",
      "\n",
      "Current : 89 Train Acc : 0.5772000024318695 Test Acc : 0.47287500198930504\n",
      "\n",
      "Current : 90 Train Acc : 0.5800000026226043 Test Acc : 0.47312500156462195\n",
      "\n",
      "Current : 91 Train Acc : 0.581400002360344 Test Acc : 0.47262500137090685\n",
      "\n",
      "Current : 92 Train Acc : 0.5832000020742416 Test Acc : 0.47300000179558993\n",
      "\n",
      "Current : 93 Train Acc : 0.5840000015497208 Test Acc : 0.4751250018924475\n",
      "\n",
      "Current : 94 Train Acc : 0.5866000016927719 Test Acc : 0.4752500024810433\n",
      "\n",
      "Current : 95 Train Acc : 0.588800001859665 Test Acc : 0.4750000026449561\n",
      "\n",
      "Current : 96 Train Acc : 0.5914000010490418 Test Acc : 0.4752500020712614\n",
      "\n",
      "Current : 97 Train Acc : 0.591800001859665 Test Acc : 0.4751250015944242\n",
      "\n",
      "Current : 98 Train Acc : 0.5942000018358231 Test Acc : 0.4747500019147992\n",
      "\n",
      "Current : 99 Train Acc : 0.5942000018358231 Test Acc : 0.47425000194460154\n",
      "\n",
      "Current : 100 Train Acc : 0.5948000010251999 Test Acc : 0.4733750021085143\n",
      "\n",
      "Current : 101 Train Acc : 0.5984000009298325 Test Acc : 0.4746250022575259\n",
      "\n",
      "Current : 102 Train Acc : 0.598800000667572 Test Acc : 0.4716250021755695\n",
      "\n",
      "Current : 103 Train Acc : 0.6020000007152557 Test Acc : 0.47175000198185446\n",
      "\n",
      "Current : 104 Train Acc : 0.601800001502037 Test Acc : 0.4713750015571713\n",
      "\n",
      "Current : 105 Train Acc : 0.6044000010490418 Test Acc : 0.4723750017210841\n",
      "\n",
      "Current : 106 Train Acc : 0.6068000000715256 Test Acc : 0.4727500019967556\n",
      "\n",
      "Current : 107 Train Acc : 0.6091999998092651 Test Acc : 0.4723750012740493\n",
      "\n",
      "Current : 108 Train Acc : 0.6105999995470047 Test Acc : 0.47262500148266556\n",
      "\n",
      "Current : 109 Train Acc : 0.6118000004291534 Test Acc : 0.47362500075250863\n",
      "\n",
      "Current : 110 Train Acc : 0.6149999995231629 Test Acc : 0.4737500013038516\n",
      "\n",
      "Current : 111 Train Acc : 0.6165999999046325 Test Acc : 0.4737500014528632\n",
      "\n",
      "Current : 112 Train Acc : 0.6153999993801117 Test Acc : 0.4738750013336539\n",
      "\n",
      "Current : 113 Train Acc : 0.616199999332428 Test Acc : 0.4737500016018748\n",
      "\n",
      "Current : 114 Train Acc : 0.6199999986886978 Test Acc : 0.4741250010207295\n",
      "\n",
      "Current : 115 Train Acc : 0.6185999981164932 Test Acc : 0.47462500128895047\n",
      "\n",
      "Current : 116 Train Acc : 0.6205999987125397 Test Acc : 0.4742500013113022\n",
      "\n",
      "Current : 117 Train Acc : 0.6243999996185303 Test Acc : 0.4735000010952353\n",
      "\n",
      "Current : 118 Train Acc : 0.625 Test Acc : 0.4735000010952353\n",
      "\n",
      "Current : 119 Train Acc : 0.6267999993562698 Test Acc : 0.4730000011622906\n",
      "\n",
      "Current : 120 Train Acc : 0.6279999989271164 Test Acc : 0.4732500009983778\n",
      "\n",
      "Current : 121 Train Acc : 0.6287999991178512 Test Acc : 0.473375001065433\n",
      "\n",
      "Current : 122 Train Acc : 0.6310000002384186 Test Acc : 0.47287500090897083\n",
      "\n",
      "Current : 123 Train Acc : 0.6308000009059906 Test Acc : 0.4716250010207295\n",
      "\n",
      "Current : 124 Train Acc : 0.6316000012159347 Test Acc : 0.47125000137835743\n",
      "\n",
      "Current : 125 Train Acc : 0.6336000008583069 Test Acc : 0.47125000149011614\n",
      "\n",
      "Current : 126 Train Acc : 0.6346000010967254 Test Acc : 0.46950000133365394\n",
      "\n",
      "Current : 127 Train Acc : 0.6358000007867813 Test Acc : 0.4701250020414591\n",
      "\n",
      "Current : 128 Train Acc : 0.6362000004053115 Test Acc : 0.4696250019222498\n",
      "\n",
      "Current : 129 Train Acc : 0.6364000014066696 Test Acc : 0.46925000127404926\n",
      "\n",
      "Current : 130 Train Acc : 0.6388000010251998 Test Acc : 0.4702500012516975\n",
      "\n",
      "Current : 131 Train Acc : 0.6390000008344651 Test Acc : 0.46912500116974115\n",
      "\n",
      "Current : 132 Train Acc : 0.6422000007629395 Test Acc : 0.4686250014975667\n",
      "\n",
      "Current : 133 Train Acc : 0.6454000000953675 Test Acc : 0.46975000195205213\n",
      "\n",
      "Current : 134 Train Acc : 0.6470000007152558 Test Acc : 0.4711250015348196\n",
      "\n",
      "Current : 135 Train Acc : 0.6478000004291534 Test Acc : 0.469875001385808\n",
      "\n",
      "Current : 136 Train Acc : 0.6502000005245209 Test Acc : 0.46850000105798245\n",
      "\n",
      "Current : 137 Train Acc : 0.6508000004291534 Test Acc : 0.46912500105798244\n",
      "\n",
      "Current : 138 Train Acc : 0.6532000000476837 Test Acc : 0.4686250014975667\n",
      "\n",
      "Current : 139 Train Acc : 0.6561999998092651 Test Acc : 0.46937500149011613\n",
      "\n",
      "Current : 140 Train Acc : 0.6550000004768372 Test Acc : 0.46812500085681674\n",
      "\n",
      "Current : 141 Train Acc : 0.6594000006914139 Test Acc : 0.46837500125169756\n",
      "\n",
      "Current : 142 Train Acc : 0.6599999998807907 Test Acc : 0.46775000154972074\n",
      "\n",
      "Current : 143 Train Acc : 0.6610000002384185 Test Acc : 0.4680000015720725\n",
      "\n",
      "Current : 144 Train Acc : 0.6628000001907348 Test Acc : 0.4670000008493662\n",
      "\n",
      "Current : 145 Train Acc : 0.6635999997854233 Test Acc : 0.4681250012293458\n",
      "\n",
      "Current : 146 Train Acc : 0.6669999995231628 Test Acc : 0.46937500085681677\n",
      "\n",
      "Current : 147 Train Acc : 0.6677999992370606 Test Acc : 0.46837500084191563\n",
      "\n",
      "Current : 148 Train Acc : 0.6693999994993209 Test Acc : 0.46725000079721213\n",
      "\n",
      "Current : 149 Train Acc : 0.6705999995470047 Test Acc : 0.46775000128895045\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# G\n",
    "current_exp_name = 'G';\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# create layers\n",
    "l1 = CNN(3,3, 16,which_reg=current_exp_name); \n",
    "l2 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "l3 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "\n",
    "l4 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "l5 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "l6 = CNN(3,16,10,which_reg=current_exp_name); \n",
    "\n",
    "# 2. graph \n",
    "x = tf.placeholder(tf.float32,(batch_size,96,96,3))\n",
    "y = tf.placeholder(tf.float32,(batch_size,10))\n",
    "\n",
    "layer1, layer1a = l1. feedforward(x,stride=2)\n",
    "layer2, layer2a = l2. feedforward(layer1a,stride=2)\n",
    "layer3, layer3a = l3. feedforward(layer2a,stride=2)\n",
    "layer4, layer4a = l4. feedforward(layer3a,stride=2)\n",
    "layer5, layer5a = l5. feedforward(layer4a)\n",
    "layer6, layer6a = l6. feedforward(layer5a)\n",
    "\n",
    "final_layer   = tf.reduce_mean(layer6a,(1,2))\n",
    "final_softmax = tf_softmax(final_layer)\n",
    "cost          = -tf.reduce_mean(y * tf.log(final_softmax + 1e-8))\n",
    "correct_prediction = tf.equal(tf.argmax(final_softmax, 1), tf.argmax(y, 1))\n",
    "accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "gradient = tf.tile((final_softmax-y)[:,None,None,:],[1,6,6,1])/batch_size\n",
    "grad6p,grad6w,grad6_up = l6.backprop(gradient)\n",
    "grad5p,grad5w,grad5_up = l5.backprop(grad6p)\n",
    "grad4p,grad4w,grad4_up = l4.backprop(grad5p,stride=2)\n",
    "grad3p,grad3w,grad3_up = l3.backprop(grad4p,stride=2)\n",
    "grad2p,grad2w,grad2_up = l2.backprop(grad3p,stride=2)\n",
    "grad1p,grad1w,grad1_up = l1.backprop(grad2p,stride=2)\n",
    "\n",
    "gradient_update = grad6_up + grad5_up + grad4_up + grad3_up + grad2_up + grad1_up \n",
    "\n",
    "# train\n",
    "sess.run(tf.global_variables_initializer())\n",
    "avg_acc_train = 0; avg_acc_test  = 0; train_acc = [];test_acc = []\n",
    "\n",
    "# mean std skew kurt non-zero\n",
    "llayer1 = [[],[],[],[],[]]; llayer2 = [[],[],[],[],[]]; llayer3 = [[],[],[],[],[]]\n",
    "llayer4 = [[],[],[],[],[]]; llayer5 = [[],[],[],[],[]]; llayer6 = [[],[],[],[],[]]\n",
    "\n",
    "llayer1a = [[],[],[],[],[]]; llayer2a = [[],[],[],[],[]]; llayer3a = [[],[],[],[],[]]\n",
    "llayer4a = [[],[],[],[],[]]; llayer5a = [[],[],[],[],[]]; llayer6a = [[],[],[],[],[]]\n",
    "\n",
    "weight1 = [[],[],[],[],[]]; weight2 = [[],[],[],[],[]]; weight3 = [[],[],[],[],[]];\n",
    "weight4 = [[],[],[],[],[]]; weight5 = [[],[],[],[],[]]; weight6 = [[],[],[],[],[]];\n",
    "\n",
    "gradw1  = [[],[],[],[],[]]; gradw2  = [[],[],[],[],[]]; gradw3  = [[],[],[],[],[]];\n",
    "gradw4  = [[],[],[],[],[]]; gradw5  = [[],[],[],[],[]]; gradw6  = [[],[],[],[],[]];\n",
    "\n",
    "gradp1  = [[],[],[],[],[]]; gradp2  = [[],[],[],[],[]]; gradp3  = [[],[],[],[],[]];\n",
    "gradp4  = [[],[],[],[],[]]; gradp5  = [[],[],[],[],[]]; gradp6  = [[],[],[],[],[]];\n",
    "\n",
    "gradup1  = [[],[],[],[],[]]; gradup2  = [[],[],[],[],[]]; gradup3  = [[],[],[],[],[]];\n",
    "gradup4  = [[],[],[],[],[]]; gradup5  = [[],[],[],[],[]]; gradup6  = [[],[],[],[],[]];\n",
    "\n",
    "list_of_outputs = [\n",
    "    layer1,layer2,layer3,layer4,layer5,layer6,\n",
    "    layer1a,layer2a,layer3a,layer4a,layer5a,layer6a,\n",
    "    l1.getw(),l2.getw(),l3.getw(),l4.getw(),l5.getw(),l6.getw(),\n",
    "    grad1w,grad2w,grad3w,grad4w,grad5w,grad6w,\n",
    "    grad1p,grad2p,grad3p,grad4p,grad5p,grad6p,\n",
    "    grad1_up[0],grad2_up[0],grad3_up[0],grad4_up[0],grad5_up[0],grad6_up[0]\n",
    "]\n",
    "\n",
    "for iter in range(num_epoch):\n",
    "\n",
    "    # Training Accuracy    \n",
    "    for current_batch_index in range(0,len(train_images),batch_size):\n",
    "        current_data  = train_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = train_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy,gradient_update],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(train_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_train = avg_acc_train + sess_results[0]\n",
    "        \n",
    "    # get the results\n",
    "    mid_stat = sess.run(list_of_outputs,feed_dict={x:current_data,y:current_label})\n",
    "    \n",
    "    # Test Accuracy    \n",
    "    for current_batch_index in range(0,len(test_images), batch_size):\n",
    "        current_data  = test_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = test_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(test_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_test = avg_acc_test + sess_results[0]   \n",
    "        \n",
    "    # ======================== extract stats ========================\n",
    "    llayer1 = append_stat(llayer1,mid_stat,0);  llayer2 = append_stat(llayer2,mid_stat,1);  llayer3 = append_stat(llayer3,mid_stat,2);\n",
    "    llayer4 = append_stat(llayer4,mid_stat,3);  llayer5 = append_stat(llayer5,mid_stat,4);  llayer6 = append_stat(llayer6,mid_stat,5);\n",
    "\n",
    "    llayer1a = append_stat(llayer1a,mid_stat,6);  llayer2a = append_stat(llayer2a,mid_stat,7);  llayer3a = append_stat(llayer3a,mid_stat,8);\n",
    "    llayer4a = append_stat(llayer4a,mid_stat,9);  llayer5a = append_stat(llayer5a,mid_stat,10); llayer6a = append_stat(llayer6a,mid_stat,11);\n",
    "    \n",
    "    weight1 = append_stat(weight1,mid_stat,12);  weight2 = append_stat(weight2,mid_stat,13);  weight3 = append_stat(weight3,mid_stat,14);\n",
    "    weight4 = append_stat(weight4,mid_stat,15);  weight5 = append_stat(weight5,mid_stat,16);  weight6 = append_stat(weight6,mid_stat,17);\n",
    "    \n",
    "    gradw1 = append_stat(gradw1,mid_stat,18); gradw2 = append_stat(gradw2,mid_stat,19); gradw3 = append_stat(gradw3,mid_stat,20);\n",
    "    gradw4 = append_stat(gradw4,mid_stat,21); gradw5 = append_stat(gradw5,mid_stat,22); gradw6 = append_stat(gradw6,mid_stat,23);\n",
    "    \n",
    "    gradp1 = append_stat(gradp1,mid_stat,24); gradp2 = append_stat(gradp2,mid_stat,25); gradp3 = append_stat(gradp3,mid_stat,26);\n",
    "    gradp4 = append_stat(gradp4,mid_stat,27); gradp5 = append_stat(gradp5,mid_stat,28); gradp6 = append_stat(gradp6,mid_stat,29);\n",
    "\n",
    "    gradup1 = append_stat(gradup1,mid_stat,30); gradup2 = append_stat(gradup2,mid_stat,31); gradup3 = append_stat(gradup3,mid_stat,32);\n",
    "    gradup4 = append_stat(gradup4,mid_stat,33); gradup5 = append_stat(gradup5,mid_stat,34); gradup6 = append_stat(gradup6,mid_stat,35);\n",
    "\n",
    "    train_acc.append(avg_acc_train/(len(train_images)/batch_size))\n",
    "    test_acc .append(avg_acc_test / (len(test_images)/batch_size))\n",
    "    # ======================== extract stats ========================\n",
    "    \n",
    "    # ======================== save to image ========================\n",
    "    save_to_image(mid_stat[0:6]   ,llayer1,llayer2,llayer3,llayer4,llayer5,llayer6,\"layer\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[6:12]  ,llayer1a,llayer2a,llayer3a,llayer4a,llayer5a,llayer6a,\"layera\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[12:18] ,weight1,weight2,weight3,weight4,weight5,weight6,\"weights\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[18:24] ,gradw1,gradw2,gradw3,gradw4,gradw5,gradw6,\"gradientw\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[24:30] ,gradp1,gradp2,gradp3,gradp4,gradp5,gradp6,\"gradientp\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[30:36] ,gradup1,gradup2,gradup3,gradup4,gradup5,gradup6,\"moment\",train_acc,test_acc,current_exp_name,iter)\n",
    "    # ======================== save to image ========================\n",
    "        \n",
    "    # ======================== print reset ========================\n",
    "    print(\"Current : \"+ str(iter) + \" Train Acc : \" + str(avg_acc_train/(len(train_images)/batch_size)) + \" Test Acc : \" + str(avg_acc_test/(len(test_images)/batch_size)) + '\\n')\n",
    "    avg_acc_train = 0 ; avg_acc_test  = 0\n",
    "    # ======================== print reset ========================\n",
    "\n",
    "np.save(current_exp_name+'/train_acc.npy',train_acc); np.save(current_exp_name+'/test_acc.npy', test_acc)    \n",
    "np.save(current_exp_name+'/llayer1.npy', llayer1);  np.save(current_exp_name+'/llayer2.npy', llayer2);  np.save(current_exp_name+'/llayer3.npy', llayer3); \n",
    "np.save(current_exp_name+'/llayer4.npy', llayer4);  np.save(current_exp_name+'/llayer5.npy', llayer5);  np.save(current_exp_name+'/llayer6.npy', llayer6); \n",
    "\n",
    "np.save(current_exp_name+'/llayer1a.npy', llayer1a);  np.save(current_exp_name+'/llayer2a.npy', llayer2a);  np.save(current_exp_name+'/llayer3a.npy', llayer3a); \n",
    "np.save(current_exp_name+'/llayer4a.npy', llayer4a);  np.save(current_exp_name+'/llayer5a.npy', llayer5a);  np.save(current_exp_name+'/llayer6a.npy', llayer6a); \n",
    "\n",
    "np.save(current_exp_name+'/weight1.npy', weight1);  np.save(current_exp_name+'/weight2.npy', weight2);  np.save(current_exp_name+'/weight3.npy', weight3);  \n",
    "np.save(current_exp_name+'/weight4.npy', weight4);  np.save(current_exp_name+'/weight5.npy', weight5);  np.save(current_exp_name+'/weight6.npy', weight6);  \n",
    "\n",
    "np.save(current_exp_name+'/gradw1.npy', gradw1); np.save(current_exp_name+'/gradw2.npy', gradw2); np.save(current_exp_name+'/gradw3.npy', gradw3);\n",
    "np.save(current_exp_name+'/gradw4.npy', gradw4); np.save(current_exp_name+'/gradw5.npy', gradw5); np.save(current_exp_name+'/gradw6.npy', gradw6);\n",
    "\n",
    "np.save(current_exp_name+'/gradp1.npy', gradp1); np.save(current_exp_name+'/gradp2.npy', gradp2); np.save(current_exp_name+'/gradp3.npy', gradp3);\n",
    "np.save(current_exp_name+'/gradp4.npy', gradp4); np.save(current_exp_name+'/gradp5.npy', gradp5); np.save(current_exp_name+'/gradp6.npy', gradp6);\n",
    "\n",
    "np.save(current_exp_name+'/gradup1.npy', gradup1); np.save(current_exp_name+'/gradup2.npy', gradup2); np.save(current_exp_name+'/gradup3.npy', gradup3);\n",
    "np.save(current_exp_name+'/gradup4.npy', gradup4); np.save(current_exp_name+'/gradup5.npy', gradup5); np.save(current_exp_name+'/gradup6.npy', gradup6);\n",
    "\n",
    "sess.close(); tf.reset_default_graph();\n",
    "\n",
    "%reset_selective -f train_acc,test_acc,llayer1,llayer2,llayer3,llayer4,llayer5,llayer6,llayer1a,llayer2a,llayer3a,llayer4a,llayer5a,llayer6a\n",
    "%reset_selective -f weight1,weight2,weight3,weight4,weight5,weight6\n",
    "%reset_selective -f gradw1,gradw2,gradw3,gradw4,gradw5,gradw6\n",
    "%reset_selective -f gradp1,gradp2,gradp3,gradp4,gradp5,gradp6\n",
    "%reset_selective -f gradup1,gradup2,gradup3,gradup4,gradup5,gradup6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-05T08:53:17.654001Z",
     "start_time": "2019-01-05T08:03:07.417716Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current : 0 Train Acc : 0.11660000205039978 Test Acc : 0.15762500289827586\n",
      "\n",
      "Current : 1 Train Acc : 0.162000003144145 Test Acc : 0.19175000336952508\n",
      "\n",
      "Current : 2 Train Acc : 0.2112000034302473 Test Acc : 0.21912500316277148\n",
      "\n",
      "Current : 3 Train Acc : 0.2452000031620264 Test Acc : 0.26387500318698587\n",
      "\n",
      "Current : 4 Train Acc : 0.2856000025719404 Test Acc : 0.29600000239908697\n",
      "\n",
      "Current : 5 Train Acc : 0.3072000023275614 Test Acc : 0.3161250020284206\n",
      "\n",
      "Current : 6 Train Acc : 0.31620000217854977 Test Acc : 0.3295000014267862\n",
      "\n",
      "Current : 7 Train Acc : 0.3226000022441149 Test Acc : 0.3398750021122396\n",
      "\n",
      "Current : 8 Train Acc : 0.33020000158250334 Test Acc : 0.34825000225566327\n",
      "\n",
      "Current : 9 Train Acc : 0.3402000017911196 Test Acc : 0.35137500138953326\n",
      "\n",
      "Current : 10 Train Acc : 0.34520000125467776 Test Acc : 0.3661250007338822\n",
      "\n",
      "Current : 11 Train Acc : 0.35660000146925447 Test Acc : 0.37237500129267576\n",
      "\n",
      "Current : 12 Train Acc : 0.36620000141859055 Test Acc : 0.3760000010579824\n",
      "\n",
      "Current : 13 Train Acc : 0.3744000009000301 Test Acc : 0.38062500055879356\n",
      "\n",
      "Current : 14 Train Acc : 0.38280000138282777 Test Acc : 0.3803750010021031\n",
      "\n",
      "Current : 15 Train Acc : 0.3878000011742115 Test Acc : 0.3842500011995435\n",
      "\n",
      "Current : 16 Train Acc : 0.39140000110864637 Test Acc : 0.38600000116974115\n",
      "\n",
      "Current : 17 Train Acc : 0.3948000023365021 Test Acc : 0.3906250014528632\n",
      "\n",
      "Current : 18 Train Acc : 0.39780000180006025 Test Acc : 0.3927500019595027\n",
      "\n",
      "Current : 19 Train Acc : 0.4002000015974045 Test Acc : 0.39525000121444465\n",
      "\n",
      "Current : 20 Train Acc : 0.4040000017285347 Test Acc : 0.39750000186264517\n",
      "\n",
      "Current : 21 Train Acc : 0.4096000009775162 Test Acc : 0.40150000154972076\n",
      "\n",
      "Current : 22 Train Acc : 0.4108000014424324 Test Acc : 0.40087500125169756\n",
      "\n",
      "Current : 23 Train Acc : 0.41320000144839286 Test Acc : 0.4051250009983778\n",
      "\n",
      "Current : 24 Train Acc : 0.4166000013947487 Test Acc : 0.4081250014901161\n",
      "\n",
      "Current : 25 Train Acc : 0.42040000146627426 Test Acc : 0.412375001385808\n",
      "\n",
      "Current : 26 Train Acc : 0.42180000185966493 Test Acc : 0.4135000017285347\n",
      "\n",
      "Current : 27 Train Acc : 0.42360000163316724 Test Acc : 0.4161250017955899\n",
      "\n",
      "Current : 28 Train Acc : 0.42620000159740445 Test Acc : 0.4157500010356307\n",
      "\n",
      "Current : 29 Train Acc : 0.42820000159740446 Test Acc : 0.41850000064820053\n",
      "\n",
      "Current : 30 Train Acc : 0.42840000170469283 Test Acc : 0.41912500094622374\n",
      "\n",
      "Current : 31 Train Acc : 0.43300000154972074 Test Acc : 0.4222500008530915\n",
      "\n",
      "Current : 32 Train Acc : 0.43540000200271606 Test Acc : 0.4230000007711351\n",
      "\n",
      "Current : 33 Train Acc : 0.4372000018954277 Test Acc : 0.4245000006072223\n",
      "\n",
      "Current : 34 Train Acc : 0.43820000141859056 Test Acc : 0.4290000004135072\n",
      "\n",
      "Current : 35 Train Acc : 0.43920000225305555 Test Acc : 0.4313750004954636\n",
      "\n",
      "Current : 36 Train Acc : 0.4420000025629997 Test Acc : 0.43262500027194617\n",
      "\n",
      "Current : 37 Train Acc : 0.44420000237226487 Test Acc : 0.43487500058487055\n",
      "\n",
      "Current : 38 Train Acc : 0.44840000253915785 Test Acc : 0.43837500067427754\n",
      "\n",
      "Current : 39 Train Acc : 0.4504000021219254 Test Acc : 0.44112500013783573\n",
      "\n",
      "Current : 40 Train Acc : 0.4522000026702881 Test Acc : 0.442249999884516\n",
      "\n",
      "Current : 41 Train Acc : 0.45480000245571134 Test Acc : 0.44262499975040553\n",
      "\n",
      "Current : 42 Train Acc : 0.45660000228881836 Test Acc : 0.44424999989569186\n",
      "\n",
      "Current : 43 Train Acc : 0.4602000021934509 Test Acc : 0.44337500013411046\n",
      "\n",
      "Current : 44 Train Acc : 0.46120000183582305 Test Acc : 0.4431250002235174\n",
      "\n",
      "Current : 45 Train Acc : 0.46340000194311143 Test Acc : 0.4432500003278255\n",
      "\n",
      "Current : 46 Train Acc : 0.46540000146627425 Test Acc : 0.4446250008791685\n",
      "\n",
      "Current : 47 Train Acc : 0.46620000153779984 Test Acc : 0.4462500007636845\n",
      "\n",
      "Current : 48 Train Acc : 0.46680000096559526 Test Acc : 0.44600000070407986\n",
      "\n",
      "Current : 49 Train Acc : 0.46920000118017197 Test Acc : 0.44862500043585896\n",
      "\n",
      "Current : 50 Train Acc : 0.4710000014901161 Test Acc : 0.45075000112876296\n",
      "\n",
      "Current : 51 Train Acc : 0.47200000125169755 Test Acc : 0.4531250008381903\n",
      "\n",
      "Current : 52 Train Acc : 0.4736000005602837 Test Acc : 0.45312500135973094\n",
      "\n",
      "Current : 53 Train Acc : 0.47340000069141386 Test Acc : 0.45362500129267574\n",
      "\n",
      "Current : 54 Train Acc : 0.4778000007867813 Test Acc : 0.45462500108405945\n",
      "\n",
      "Current : 55 Train Acc : 0.47920000088214876 Test Acc : 0.4550000013969839\n",
      "\n",
      "Current : 56 Train Acc : 0.47900000095367434 Test Acc : 0.4552500015310943\n",
      "\n",
      "Current : 57 Train Acc : 0.48120000088214876 Test Acc : 0.45500000124797224\n",
      "\n",
      "Current : 58 Train Acc : 0.4820000007152557 Test Acc : 0.4551250007562339\n",
      "\n",
      "Current : 59 Train Acc : 0.48340000051259996 Test Acc : 0.45687500117346647\n",
      "\n",
      "Current : 60 Train Acc : 0.4858000004887581 Test Acc : 0.4583750014938414\n",
      "\n",
      "Current : 61 Train Acc : 0.4846000004410744 Test Acc : 0.4590000009350479\n",
      "\n",
      "Current : 62 Train Acc : 0.4866000004410744 Test Acc : 0.45862500118091704\n",
      "\n",
      "Current : 63 Train Acc : 0.4874000009894371 Test Acc : 0.45962500086054203\n",
      "\n",
      "Current : 64 Train Acc : 0.48980000108480454 Test Acc : 0.46062500121071936\n",
      "\n",
      "Current : 65 Train Acc : 0.4894000011086464 Test Acc : 0.4632500007562339\n",
      "\n",
      "Current : 66 Train Acc : 0.4912000008225441 Test Acc : 0.4641250011138618\n",
      "\n",
      "Current : 67 Train Acc : 0.4916000008583069 Test Acc : 0.4635000012628734\n",
      "\n",
      "Current : 68 Train Acc : 0.4920000012516975 Test Acc : 0.4635000007040799\n",
      "\n",
      "Current : 69 Train Acc : 0.4932000016570091 Test Acc : 0.46500000115483997\n",
      "\n",
      "Current : 70 Train Acc : 0.49540000092983244 Test Acc : 0.4662500013783574\n",
      "\n",
      "Current : 71 Train Acc : 0.49620000171661377 Test Acc : 0.46650000136345626\n",
      "\n",
      "Current : 72 Train Acc : 0.49820000195503233 Test Acc : 0.467875001616776\n",
      "\n",
      "Current : 73 Train Acc : 0.5012000011205673 Test Acc : 0.46837500188499687\n",
      "\n",
      "Current : 74 Train Acc : 0.5002000012397766 Test Acc : 0.46950000151991844\n",
      "\n",
      "Current : 75 Train Acc : 0.5018000016212464 Test Acc : 0.47000000182539226\n",
      "\n",
      "Current : 76 Train Acc : 0.5020000016689301 Test Acc : 0.4705000011995435\n",
      "\n",
      "Current : 77 Train Acc : 0.5044000015258789 Test Acc : 0.4692500014230609\n",
      "\n",
      "Current : 78 Train Acc : 0.5052000011205673 Test Acc : 0.47037500090897083\n",
      "\n",
      "Current : 79 Train Acc : 0.5068000017404556 Test Acc : 0.4720000011101365\n",
      "\n",
      "Current : 80 Train Acc : 0.5072000019550323 Test Acc : 0.4752500011399388\n",
      "\n",
      "Current : 81 Train Acc : 0.5072000020742417 Test Acc : 0.4745000010728836\n",
      "\n",
      "Current : 82 Train Acc : 0.5086000024080276 Test Acc : 0.47512500077486036\n",
      "\n",
      "Current : 83 Train Acc : 0.5100000021457672 Test Acc : 0.4745000011101365\n",
      "\n",
      "Current : 84 Train Acc : 0.5090000023841857 Test Acc : 0.4767500010505319\n",
      "\n",
      "Current : 85 Train Acc : 0.5104000021219254 Test Acc : 0.4772500005736947\n",
      "\n",
      "Current : 86 Train Acc : 0.5108000016212464 Test Acc : 0.47912500105798245\n",
      "\n",
      "Current : 87 Train Acc : 0.5114000014662743 Test Acc : 0.48112500112503764\n",
      "\n",
      "Current : 88 Train Acc : 0.5150000019073486 Test Acc : 0.47925000108778476\n",
      "\n",
      "Current : 89 Train Acc : 0.5148000013828278 Test Acc : 0.4806250013783574\n",
      "\n",
      "Current : 90 Train Acc : 0.5160000011920929 Test Acc : 0.48175000105053184\n",
      "\n",
      "Current : 91 Train Acc : 0.5168000016808509 Test Acc : 0.48250000111758706\n",
      "\n",
      "Current : 92 Train Acc : 0.5216000024080276 Test Acc : 0.48075000155717135\n",
      "\n",
      "Current : 93 Train Acc : 0.5200000010728836 Test Acc : 0.48475000128149986\n",
      "\n",
      "Current : 94 Train Acc : 0.5204000011086464 Test Acc : 0.4855000012740493\n",
      "\n",
      "Current : 95 Train Acc : 0.5210000013709069 Test Acc : 0.48637500129640104\n",
      "\n",
      "Current : 96 Train Acc : 0.5196000013947487 Test Acc : 0.486250001527369\n",
      "\n",
      "Current : 97 Train Acc : 0.5218000018000603 Test Acc : 0.48750000160187484\n",
      "\n",
      "Current : 98 Train Acc : 0.5234000015854835 Test Acc : 0.48775000136345625\n",
      "\n",
      "Current : 99 Train Acc : 0.5264000018239021 Test Acc : 0.48737500097602604\n",
      "\n",
      "Current : 100 Train Acc : 0.528000001847744 Test Acc : 0.4887500012665987\n",
      "\n",
      "Current : 101 Train Acc : 0.5274000023007392 Test Acc : 0.49000000149011613\n",
      "\n",
      "Current : 102 Train Acc : 0.5266000019907952 Test Acc : 0.4910000006109476\n",
      "\n",
      "Current : 103 Train Acc : 0.5292000021338463 Test Acc : 0.4923750005662441\n",
      "\n",
      "Current : 104 Train Acc : 0.5314000019431114 Test Acc : 0.49312500074505805\n",
      "\n",
      "Current : 105 Train Acc : 0.5340000010132789 Test Acc : 0.4946250007301569\n",
      "\n",
      "Current : 106 Train Acc : 0.5350000010132789 Test Acc : 0.4970000007748604\n",
      "\n",
      "Current : 107 Train Acc : 0.5356000004410744 Test Acc : 0.49512500055134295\n",
      "\n",
      "Current : 108 Train Acc : 0.5376000007987023 Test Acc : 0.4956250012665987\n",
      "\n",
      "Current : 109 Train Acc : 0.5394000008106231 Test Acc : 0.4980000010505319\n",
      "\n",
      "Current : 110 Train Acc : 0.5400000009536743 Test Acc : 0.49850000072270634\n",
      "\n",
      "Current : 111 Train Acc : 0.5432000004649162 Test Acc : 0.49912500036880375\n",
      "\n",
      "Current : 112 Train Acc : 0.5438000002503395 Test Acc : 0.49937500098720194\n",
      "\n",
      "Current : 113 Train Acc : 0.5454000002741813 Test Acc : 0.5015000008791685\n",
      "\n",
      "Current : 114 Train Acc : 0.5476000003218651 Test Acc : 0.5007500011846423\n",
      "\n",
      "Current : 115 Train Acc : 0.5473999998569489 Test Acc : 0.5006250010058284\n",
      "\n",
      "Current : 116 Train Acc : 0.547800000011921 Test Acc : 0.5012500009685755\n",
      "\n",
      "Current : 117 Train Acc : 0.5493999999761582 Test Acc : 0.5011250009760261\n",
      "\n",
      "Current : 118 Train Acc : 0.5508000009059906 Test Acc : 0.5017500009015202\n",
      "\n",
      "Current : 119 Train Acc : 0.5514000002145767 Test Acc : 0.5015000007860363\n",
      "\n",
      "Current : 120 Train Acc : 0.553200001358986 Test Acc : 0.5028750011511147\n",
      "\n",
      "Current : 121 Train Acc : 0.5548000006675721 Test Acc : 0.5040000018104911\n",
      "\n",
      "Current : 122 Train Acc : 0.556400000333786 Test Acc : 0.5036250016279519\n",
      "\n",
      "Current : 123 Train Acc : 0.5578000001907348 Test Acc : 0.5077500014007091\n",
      "\n",
      "Current : 124 Train Acc : 0.559200000166893 Test Acc : 0.507500001359731\n",
      "\n",
      "Current : 125 Train Acc : 0.5599999998807907 Test Acc : 0.5081250014156103\n",
      "\n",
      "Current : 126 Train Acc : 0.5624000005722046 Test Acc : 0.5086250013113022\n",
      "\n",
      "Current : 127 Train Acc : 0.5648000007867813 Test Acc : 0.5081250016391278\n",
      "\n",
      "Current : 128 Train Acc : 0.5646000015735626 Test Acc : 0.5086250010877847\n",
      "\n",
      "Current : 129 Train Acc : 0.5682000014781952 Test Acc : 0.5095000015199185\n",
      "\n",
      "Current : 130 Train Acc : 0.568600001335144 Test Acc : 0.5100000014901162\n",
      "\n",
      "Current : 131 Train Acc : 0.5692000017166138 Test Acc : 0.5107500014454126\n",
      "\n",
      "Current : 132 Train Acc : 0.5706000014543533 Test Acc : 0.5110000012814999\n",
      "\n",
      "Current : 133 Train Acc : 0.5700000016689301 Test Acc : 0.5123750014603138\n",
      "\n",
      "Current : 134 Train Acc : 0.5714000014066696 Test Acc : 0.5108750015497208\n",
      "\n",
      "Current : 135 Train Acc : 0.5734000011682511 Test Acc : 0.5121250016987324\n",
      "\n",
      "Current : 136 Train Acc : 0.5768000009059906 Test Acc : 0.5126250015944243\n",
      "\n",
      "Current : 137 Train Acc : 0.5782000004053116 Test Acc : 0.5130000013113022\n",
      "\n",
      "Current : 138 Train Acc : 0.5782000006437301 Test Acc : 0.5130000013113022\n",
      "\n",
      "Current : 139 Train Acc : 0.5796000001430511 Test Acc : 0.5127500014007091\n",
      "\n",
      "Current : 140 Train Acc : 0.5802000007629394 Test Acc : 0.5153750013560057\n",
      "\n",
      "Current : 141 Train Acc : 0.5812000006437301 Test Acc : 0.5146250013262034\n",
      "\n",
      "Current : 142 Train Acc : 0.5820000011920929 Test Acc : 0.5150000008195639\n",
      "\n",
      "Current : 143 Train Acc : 0.583800000667572 Test Acc : 0.514125000834465\n",
      "\n",
      "Current : 144 Train Acc : 0.5842000008821487 Test Acc : 0.5151250004768372\n",
      "\n",
      "Current : 145 Train Acc : 0.5850000009536743 Test Acc : 0.5152500012516975\n",
      "\n",
      "Current : 146 Train Acc : 0.5856000009775162 Test Acc : 0.5161250009387731\n",
      "\n",
      "Current : 147 Train Acc : 0.5868000013828277 Test Acc : 0.5168750010617077\n",
      "\n",
      "Current : 148 Train Acc : 0.587000001192093 Test Acc : 0.5183750009723007\n",
      "\n",
      "Current : 149 Train Acc : 0.5884000008106232 Test Acc : 0.5178750013746322\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# H\n",
    "current_exp_name = 'H';\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# create layers\n",
    "l1 = CNN(3,3, 16,which_reg=current_exp_name); \n",
    "l2 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "l3 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "\n",
    "l4 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "l5 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "l6 = CNN(3,16,10,which_reg=current_exp_name); \n",
    "\n",
    "# 2. graph \n",
    "x = tf.placeholder(tf.float32,(batch_size,96,96,3))\n",
    "y = tf.placeholder(tf.float32,(batch_size,10))\n",
    "\n",
    "layer1, layer1a = l1. feedforward(x,stride=2)\n",
    "layer2, layer2a = l2. feedforward(layer1a,stride=2)\n",
    "layer3, layer3a = l3. feedforward(layer2a,stride=2)\n",
    "layer4, layer4a = l4. feedforward(layer3a,stride=2)\n",
    "layer5, layer5a = l5. feedforward(layer4a)\n",
    "layer6, layer6a = l6. feedforward(layer5a)\n",
    "\n",
    "final_layer   = tf.reduce_mean(layer6a,(1,2))\n",
    "final_softmax = tf_softmax(final_layer)\n",
    "cost          = -tf.reduce_mean(y * tf.log(final_softmax + 1e-8))\n",
    "correct_prediction = tf.equal(tf.argmax(final_softmax, 1), tf.argmax(y, 1))\n",
    "accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "gradient = tf.tile((final_softmax-y)[:,None,None,:],[1,6,6,1])/batch_size\n",
    "grad6p,grad6w,grad6_up = l6.backprop(gradient)\n",
    "grad5p,grad5w,grad5_up = l5.backprop(grad6p)\n",
    "grad4p,grad4w,grad4_up = l4.backprop(grad5p,stride=2)\n",
    "grad3p,grad3w,grad3_up = l3.backprop(grad4p,stride=2)\n",
    "grad2p,grad2w,grad2_up = l2.backprop(grad3p,stride=2)\n",
    "grad1p,grad1w,grad1_up = l1.backprop(grad2p,stride=2)\n",
    "\n",
    "gradient_update = grad6_up + grad5_up + grad4_up + grad3_up + grad2_up + grad1_up \n",
    "\n",
    "# train\n",
    "sess.run(tf.global_variables_initializer())\n",
    "avg_acc_train = 0; avg_acc_test  = 0; train_acc = [];test_acc = []\n",
    "\n",
    "# mean std skew kurt non-zero\n",
    "llayer1 = [[],[],[],[],[]]; llayer2 = [[],[],[],[],[]]; llayer3 = [[],[],[],[],[]]\n",
    "llayer4 = [[],[],[],[],[]]; llayer5 = [[],[],[],[],[]]; llayer6 = [[],[],[],[],[]]\n",
    "\n",
    "llayer1a = [[],[],[],[],[]]; llayer2a = [[],[],[],[],[]]; llayer3a = [[],[],[],[],[]]\n",
    "llayer4a = [[],[],[],[],[]]; llayer5a = [[],[],[],[],[]]; llayer6a = [[],[],[],[],[]]\n",
    "\n",
    "weight1 = [[],[],[],[],[]]; weight2 = [[],[],[],[],[]]; weight3 = [[],[],[],[],[]];\n",
    "weight4 = [[],[],[],[],[]]; weight5 = [[],[],[],[],[]]; weight6 = [[],[],[],[],[]];\n",
    "\n",
    "gradw1  = [[],[],[],[],[]]; gradw2  = [[],[],[],[],[]]; gradw3  = [[],[],[],[],[]];\n",
    "gradw4  = [[],[],[],[],[]]; gradw5  = [[],[],[],[],[]]; gradw6  = [[],[],[],[],[]];\n",
    "\n",
    "gradp1  = [[],[],[],[],[]]; gradp2  = [[],[],[],[],[]]; gradp3  = [[],[],[],[],[]];\n",
    "gradp4  = [[],[],[],[],[]]; gradp5  = [[],[],[],[],[]]; gradp6  = [[],[],[],[],[]];\n",
    "\n",
    "gradup1  = [[],[],[],[],[]]; gradup2  = [[],[],[],[],[]]; gradup3  = [[],[],[],[],[]];\n",
    "gradup4  = [[],[],[],[],[]]; gradup5  = [[],[],[],[],[]]; gradup6  = [[],[],[],[],[]];\n",
    "\n",
    "list_of_outputs = [\n",
    "    layer1,layer2,layer3,layer4,layer5,layer6,\n",
    "    layer1a,layer2a,layer3a,layer4a,layer5a,layer6a,\n",
    "    l1.getw(),l2.getw(),l3.getw(),l4.getw(),l5.getw(),l6.getw(),\n",
    "    grad1w,grad2w,grad3w,grad4w,grad5w,grad6w,\n",
    "    grad1p,grad2p,grad3p,grad4p,grad5p,grad6p,\n",
    "    grad1_up[0],grad2_up[0],grad3_up[0],grad4_up[0],grad5_up[0],grad6_up[0]\n",
    "]\n",
    "\n",
    "for iter in range(num_epoch):\n",
    "\n",
    "    # Training Accuracy    \n",
    "    for current_batch_index in range(0,len(train_images),batch_size):\n",
    "        current_data  = train_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = train_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy,gradient_update],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(train_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_train = avg_acc_train + sess_results[0]\n",
    "        \n",
    "    # get the results\n",
    "    mid_stat = sess.run(list_of_outputs,feed_dict={x:current_data,y:current_label})\n",
    "    \n",
    "    # Test Accuracy    \n",
    "    for current_batch_index in range(0,len(test_images), batch_size):\n",
    "        current_data  = test_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = test_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(test_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_test = avg_acc_test + sess_results[0]   \n",
    "        \n",
    "    # ======================== extract stats ========================\n",
    "    llayer1 = append_stat(llayer1,mid_stat,0);  llayer2 = append_stat(llayer2,mid_stat,1);  llayer3 = append_stat(llayer3,mid_stat,2);\n",
    "    llayer4 = append_stat(llayer4,mid_stat,3);  llayer5 = append_stat(llayer5,mid_stat,4);  llayer6 = append_stat(llayer6,mid_stat,5);\n",
    "\n",
    "    llayer1a = append_stat(llayer1a,mid_stat,6);  llayer2a = append_stat(llayer2a,mid_stat,7);  llayer3a = append_stat(llayer3a,mid_stat,8);\n",
    "    llayer4a = append_stat(llayer4a,mid_stat,9);  llayer5a = append_stat(llayer5a,mid_stat,10); llayer6a = append_stat(llayer6a,mid_stat,11);\n",
    "    \n",
    "    weight1 = append_stat(weight1,mid_stat,12);  weight2 = append_stat(weight2,mid_stat,13);  weight3 = append_stat(weight3,mid_stat,14);\n",
    "    weight4 = append_stat(weight4,mid_stat,15);  weight5 = append_stat(weight5,mid_stat,16);  weight6 = append_stat(weight6,mid_stat,17);\n",
    "    \n",
    "    gradw1 = append_stat(gradw1,mid_stat,18); gradw2 = append_stat(gradw2,mid_stat,19); gradw3 = append_stat(gradw3,mid_stat,20);\n",
    "    gradw4 = append_stat(gradw4,mid_stat,21); gradw5 = append_stat(gradw5,mid_stat,22); gradw6 = append_stat(gradw6,mid_stat,23);\n",
    "    \n",
    "    gradp1 = append_stat(gradp1,mid_stat,24); gradp2 = append_stat(gradp2,mid_stat,25); gradp3 = append_stat(gradp3,mid_stat,26);\n",
    "    gradp4 = append_stat(gradp4,mid_stat,27); gradp5 = append_stat(gradp5,mid_stat,28); gradp6 = append_stat(gradp6,mid_stat,29);\n",
    "\n",
    "    gradup1 = append_stat(gradup1,mid_stat,30); gradup2 = append_stat(gradup2,mid_stat,31); gradup3 = append_stat(gradup3,mid_stat,32);\n",
    "    gradup4 = append_stat(gradup4,mid_stat,33); gradup5 = append_stat(gradup5,mid_stat,34); gradup6 = append_stat(gradup6,mid_stat,35);\n",
    "\n",
    "    train_acc.append(avg_acc_train/(len(train_images)/batch_size))\n",
    "    test_acc .append(avg_acc_test / (len(test_images)/batch_size))\n",
    "    # ======================== extract stats ========================\n",
    "    \n",
    "    # ======================== save to image ========================\n",
    "    save_to_image(mid_stat[0:6]   ,llayer1,llayer2,llayer3,llayer4,llayer5,llayer6,\"layer\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[6:12]  ,llayer1a,llayer2a,llayer3a,llayer4a,llayer5a,llayer6a,\"layera\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[12:18] ,weight1,weight2,weight3,weight4,weight5,weight6,\"weights\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[18:24] ,gradw1,gradw2,gradw3,gradw4,gradw5,gradw6,\"gradientw\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[24:30] ,gradp1,gradp2,gradp3,gradp4,gradp5,gradp6,\"gradientp\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[30:36] ,gradup1,gradup2,gradup3,gradup4,gradup5,gradup6,\"moment\",train_acc,test_acc,current_exp_name,iter)\n",
    "    # ======================== save to image ========================\n",
    "        \n",
    "    # ======================== print reset ========================\n",
    "    print(\"Current : \"+ str(iter) + \" Train Acc : \" + str(avg_acc_train/(len(train_images)/batch_size)) + \" Test Acc : \" + str(avg_acc_test/(len(test_images)/batch_size)) + '\\n')\n",
    "    avg_acc_train = 0 ; avg_acc_test  = 0\n",
    "    # ======================== print reset ========================\n",
    "\n",
    "np.save(current_exp_name+'/train_acc.npy',train_acc); np.save(current_exp_name+'/test_acc.npy', test_acc)    \n",
    "np.save(current_exp_name+'/llayer1.npy', llayer1);  np.save(current_exp_name+'/llayer2.npy', llayer2);  np.save(current_exp_name+'/llayer3.npy', llayer3); \n",
    "np.save(current_exp_name+'/llayer4.npy', llayer4);  np.save(current_exp_name+'/llayer5.npy', llayer5);  np.save(current_exp_name+'/llayer6.npy', llayer6); \n",
    "\n",
    "np.save(current_exp_name+'/llayer1a.npy', llayer1a);  np.save(current_exp_name+'/llayer2a.npy', llayer2a);  np.save(current_exp_name+'/llayer3a.npy', llayer3a); \n",
    "np.save(current_exp_name+'/llayer4a.npy', llayer4a);  np.save(current_exp_name+'/llayer5a.npy', llayer5a);  np.save(current_exp_name+'/llayer6a.npy', llayer6a); \n",
    "\n",
    "np.save(current_exp_name+'/weight1.npy', weight1);  np.save(current_exp_name+'/weight2.npy', weight2);  np.save(current_exp_name+'/weight3.npy', weight3);  \n",
    "np.save(current_exp_name+'/weight4.npy', weight4);  np.save(current_exp_name+'/weight5.npy', weight5);  np.save(current_exp_name+'/weight6.npy', weight6);  \n",
    "\n",
    "np.save(current_exp_name+'/gradw1.npy', gradw1); np.save(current_exp_name+'/gradw2.npy', gradw2); np.save(current_exp_name+'/gradw3.npy', gradw3);\n",
    "np.save(current_exp_name+'/gradw4.npy', gradw4); np.save(current_exp_name+'/gradw5.npy', gradw5); np.save(current_exp_name+'/gradw6.npy', gradw6);\n",
    "\n",
    "np.save(current_exp_name+'/gradp1.npy', gradp1); np.save(current_exp_name+'/gradp2.npy', gradp2); np.save(current_exp_name+'/gradp3.npy', gradp3);\n",
    "np.save(current_exp_name+'/gradp4.npy', gradp4); np.save(current_exp_name+'/gradp5.npy', gradp5); np.save(current_exp_name+'/gradp6.npy', gradp6);\n",
    "\n",
    "np.save(current_exp_name+'/gradup1.npy', gradup1); np.save(current_exp_name+'/gradup2.npy', gradup2); np.save(current_exp_name+'/gradup3.npy', gradup3);\n",
    "np.save(current_exp_name+'/gradup4.npy', gradup4); np.save(current_exp_name+'/gradup5.npy', gradup5); np.save(current_exp_name+'/gradup6.npy', gradup6);\n",
    "\n",
    "sess.close(); tf.reset_default_graph();\n",
    "\n",
    "%reset_selective -f l1,l2,l3,l4,l5,l6\n",
    "%reset_selective -f layer1,layer2,layer3,layer4,layer5,layer6\n",
    "%reset_selective -f layer1a,layer2a,layer3a,layer4a,layer5a,layer6a\n",
    "%reset_selective -f train_acc,test_acc,llayer1,llayer2,llayer3,llayer4,llayer5,llayer6,llayer1a,llayer2a,llayer3a,llayer4a,llayer5a,llayer6a\n",
    "%reset_selective -f weight1,weight2,weight3,weight4,weight5,weight6\n",
    "%reset_selective -f gradw1,gradw2,gradw3,gradw4,gradw5,gradw6\n",
    "%reset_selective -f gradp1,gradp2,gradp3,gradp4,gradp5,gradp6\n",
    "%reset_selective -f gradup1,gradup2,gradup3,gradup4,gradup5,gradup6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-05T09:43:30.495501Z",
     "start_time": "2019-01-05T08:53:17.688614Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current : 0 Train Acc : 0.1282000027000904 Test Acc : 0.16050000282935797\n",
      "\n",
      "Current : 1 Train Acc : 0.20300000336766244 Test Acc : 0.25075000267475844\n",
      "\n",
      "Current : 2 Train Acc : 0.28500000208616255 Test Acc : 0.3080000021215528\n",
      "\n",
      "Current : 3 Train Acc : 0.2996000024974346 Test Acc : 0.3130000018142164\n",
      "\n",
      "Current : 4 Train Acc : 0.3142000027894974 Test Acc : 0.3287500016577542\n",
      "\n",
      "Current : 5 Train Acc : 0.32520000368356705 Test Acc : 0.3353750020172447\n",
      "\n",
      "Current : 6 Train Acc : 0.334200002014637 Test Acc : 0.34000000222586096\n",
      "\n",
      "Current : 7 Train Acc : 0.3384000027179718 Test Acc : 0.345125001585111\n",
      "\n",
      "Current : 8 Train Acc : 0.34240000146627425 Test Acc : 0.34775000139139595\n",
      "\n",
      "Current : 9 Train Acc : 0.34820000201463697 Test Acc : 0.3542500012461096\n",
      "\n",
      "Current : 10 Train Acc : 0.3538000017106533 Test Acc : 0.3602500017080456\n",
      "\n",
      "Current : 11 Train Acc : 0.3592000021338463 Test Acc : 0.36525000097230076\n",
      "\n",
      "Current : 12 Train Acc : 0.3654000018835068 Test Acc : 0.37137500088661907\n",
      "\n",
      "Current : 13 Train Acc : 0.37200000193715094 Test Acc : 0.3752500010281801\n",
      "\n",
      "Current : 14 Train Acc : 0.37860000267624855 Test Acc : 0.37825000047683716\n",
      "\n",
      "Current : 15 Train Acc : 0.38680000278353693 Test Acc : 0.38100000098347664\n",
      "\n",
      "Current : 16 Train Acc : 0.3896000021994114 Test Acc : 0.384500000551343\n",
      "\n",
      "Current : 17 Train Acc : 0.3930000023543835 Test Acc : 0.38825000045821073\n",
      "\n",
      "Current : 18 Train Acc : 0.39660000228881837 Test Acc : 0.3918750003539026\n",
      "\n",
      "Current : 19 Train Acc : 0.4038000023365021 Test Acc : 0.3962500004284084\n",
      "\n",
      "Current : 20 Train Acc : 0.40820000278949736 Test Acc : 0.3997499999962747\n",
      "\n",
      "Current : 21 Train Acc : 0.4108000022172928 Test Acc : 0.40037500045262275\n",
      "\n",
      "Current : 22 Train Acc : 0.41560000240802764 Test Acc : 0.401375000430271\n",
      "\n",
      "Current : 23 Train Acc : 0.41920000249147416 Test Acc : 0.4040000009443611\n",
      "\n",
      "Current : 24 Train Acc : 0.4198000022172928 Test Acc : 0.4063750008121133\n",
      "\n",
      "Current : 25 Train Acc : 0.423600002348423 Test Acc : 0.4081250006146729\n",
      "\n",
      "Current : 26 Train Acc : 0.42340000182390214 Test Acc : 0.4116250006109476\n",
      "\n",
      "Current : 27 Train Acc : 0.4258000025749207 Test Acc : 0.41312500094994903\n",
      "\n",
      "Current : 28 Train Acc : 0.42820000195503233 Test Acc : 0.41312500048428774\n",
      "\n",
      "Current : 29 Train Acc : 0.429200002014637 Test Acc : 0.41612500077113507\n",
      "\n",
      "Current : 30 Train Acc : 0.43460000115633013 Test Acc : 0.41962500104680656\n",
      "\n",
      "Current : 31 Train Acc : 0.43700000113248827 Test Acc : 0.4201250016503036\n",
      "\n",
      "Current : 32 Train Acc : 0.4400000006556511 Test Acc : 0.420750002451241\n",
      "\n",
      "Current : 33 Train Acc : 0.44100000005960466 Test Acc : 0.42312500208616255\n",
      "\n",
      "Current : 34 Train Acc : 0.4453999997675419 Test Acc : 0.4273750020936131\n",
      "\n",
      "Current : 35 Train Acc : 0.4495999999344349 Test Acc : 0.42537500167265535\n",
      "\n",
      "Current : 36 Train Acc : 0.4536000013649464 Test Acc : 0.4276250017248094\n",
      "\n",
      "Current : 37 Train Acc : 0.4596000020205975 Test Acc : 0.42812500139698384\n",
      "\n",
      "Current : 38 Train Acc : 0.4608000014722347 Test Acc : 0.42962500095367434\n",
      "\n",
      "Current : 39 Train Acc : 0.4640000012814999 Test Acc : 0.42937500173226\n",
      "\n",
      "Current : 40 Train Acc : 0.4654000011086464 Test Acc : 0.4308750009536743\n",
      "\n",
      "Current : 41 Train Acc : 0.46680000084638595 Test Acc : 0.43400000100955366\n",
      "\n",
      "Current : 42 Train Acc : 0.46800000077486037 Test Acc : 0.4366250016912818\n",
      "\n",
      "Current : 43 Train Acc : 0.46980000084638596 Test Acc : 0.4386250014975667\n",
      "\n",
      "Current : 44 Train Acc : 0.4694000008702278 Test Acc : 0.43950000155717134\n",
      "\n",
      "Current : 45 Train Acc : 0.4712000007033348 Test Acc : 0.4395000014454126\n",
      "\n",
      "Current : 46 Train Acc : 0.47160000014305115 Test Acc : 0.44150000080466273\n",
      "\n",
      "Current : 47 Train Acc : 0.47380000019073487 Test Acc : 0.4437500013783574\n",
      "\n",
      "Current : 48 Train Acc : 0.47520000088214875 Test Acc : 0.44462500136345623\n",
      "\n",
      "Current : 49 Train Acc : 0.47960000014305115 Test Acc : 0.4450000016018748\n",
      "\n",
      "Current : 50 Train Acc : 0.4808000010251999 Test Acc : 0.4443750019744039\n",
      "\n",
      "Current : 51 Train Acc : 0.48400000083446504 Test Acc : 0.4458750016614795\n",
      "\n",
      "Current : 52 Train Acc : 0.48820000094175336 Test Acc : 0.4455000013485551\n",
      "\n",
      "Current : 53 Train Acc : 0.4880000009536743 Test Acc : 0.44662500116974113\n",
      "\n",
      "Current : 54 Train Acc : 0.4920000007748604 Test Acc : 0.4467500016465783\n",
      "\n",
      "Current : 55 Train Acc : 0.494200001180172 Test Acc : 0.4458750006183982\n",
      "\n",
      "Current : 56 Train Acc : 0.4948000013232231 Test Acc : 0.44437500040978195\n",
      "\n",
      "Current : 57 Train Acc : 0.49600000113248827 Test Acc : 0.44562500044703485\n",
      "\n",
      "Current : 58 Train Acc : 0.4978000014424324 Test Acc : 0.4458750006183982\n",
      "\n",
      "Current : 59 Train Acc : 0.49840000075101853 Test Acc : 0.44675000142306087\n",
      "\n",
      "Current : 60 Train Acc : 0.49800000113248827 Test Acc : 0.44825000159442424\n",
      "\n",
      "Current : 61 Train Acc : 0.5010000012516975 Test Acc : 0.448875001296401\n",
      "\n",
      "Current : 62 Train Acc : 0.5010000017881393 Test Acc : 0.4503750016540289\n",
      "\n",
      "Current : 63 Train Acc : 0.5034000012874603 Test Acc : 0.45200000125914813\n",
      "\n",
      "Current : 64 Train Acc : 0.506400001168251 Test Acc : 0.4527500009164214\n",
      "\n",
      "Current : 65 Train Acc : 0.5058000016212464 Test Acc : 0.4556250012665987\n",
      "\n",
      "Current : 66 Train Acc : 0.5096000018119812 Test Acc : 0.4561250014603138\n",
      "\n",
      "Current : 67 Train Acc : 0.5120000015497208 Test Acc : 0.4568750010058284\n",
      "\n",
      "Current : 68 Train Acc : 0.510200001835823 Test Acc : 0.4571250010281801\n",
      "\n",
      "Current : 69 Train Acc : 0.5126000018119812 Test Acc : 0.4560000010207295\n",
      "\n",
      "Current : 70 Train Acc : 0.5126000012159347 Test Acc : 0.4570000009611249\n",
      "\n",
      "Current : 71 Train Acc : 0.5142000017166137 Test Acc : 0.4537500012665987\n",
      "\n",
      "Current : 72 Train Acc : 0.5158000013828278 Test Acc : 0.45237500097602606\n",
      "\n",
      "Current : 73 Train Acc : 0.5138000011444092 Test Acc : 0.4518750009313226\n",
      "\n",
      "Current : 74 Train Acc : 0.5166000009775161 Test Acc : 0.45312500059604643\n",
      "\n",
      "Current : 75 Train Acc : 0.5208000015020371 Test Acc : 0.4545000002533197\n",
      "\n",
      "Current : 76 Train Acc : 0.5220000019073486 Test Acc : 0.46075000062584875\n",
      "\n",
      "Current : 77 Train Acc : 0.5220000014305115 Test Acc : 0.4572500008717179\n",
      "\n",
      "Current : 78 Train Acc : 0.5262000010609627 Test Acc : 0.45687500115484\n",
      "\n",
      "Current : 79 Train Acc : 0.527200001180172 Test Acc : 0.459500001296401\n",
      "\n",
      "Current : 80 Train Acc : 0.5300000016093254 Test Acc : 0.4583750008419156\n",
      "\n",
      "Current : 81 Train Acc : 0.5310000011324882 Test Acc : 0.4582500009983778\n",
      "\n",
      "Current : 82 Train Acc : 0.5308000009655952 Test Acc : 0.4583750008791685\n",
      "\n",
      "Current : 83 Train Acc : 0.5340000002980232 Test Acc : 0.4575000010430813\n",
      "\n",
      "Current : 84 Train Acc : 0.5328000006079674 Test Acc : 0.4545000006631017\n",
      "\n",
      "Current : 85 Train Acc : 0.5346000004410744 Test Acc : 0.45400000032037496\n",
      "\n",
      "Current : 86 Train Acc : 0.5364000020623207 Test Acc : 0.4532500005885959\n",
      "\n",
      "Current : 87 Train Acc : 0.5382000018954277 Test Acc : 0.45300000071525576\n",
      "\n",
      "Current : 88 Train Acc : 0.5402000016570091 Test Acc : 0.4532500011101365\n",
      "\n",
      "Current : 89 Train Acc : 0.5416000017523765 Test Acc : 0.45250000115484\n",
      "\n",
      "Current : 90 Train Acc : 0.5440000010132789 Test Acc : 0.452125001065433\n",
      "\n",
      "Current : 91 Train Acc : 0.5452000003457069 Test Acc : 0.4537500012293458\n",
      "\n",
      "Current : 92 Train Acc : 0.5476000010371208 Test Acc : 0.4531250006705523\n",
      "\n",
      "Current : 93 Train Acc : 0.5492000005841255 Test Acc : 0.4517500002682209\n",
      "\n",
      "Current : 94 Train Acc : 0.549200000822544 Test Acc : 0.45275000046938657\n",
      "\n",
      "Current : 95 Train Acc : 0.5496000004410744 Test Acc : 0.4525000010430813\n",
      "\n",
      "Current : 96 Train Acc : 0.5514000000357628 Test Acc : 0.452125000692904\n",
      "\n",
      "Current : 97 Train Acc : 0.5504000012278557 Test Acc : 0.45112500082701446\n",
      "\n",
      "Current : 98 Train Acc : 0.5528000006079674 Test Acc : 0.45125000074505806\n",
      "\n",
      "Current : 99 Train Acc : 0.5535999994874 Test Acc : 0.45200000070035457\n",
      "\n",
      "Current : 100 Train Acc : 0.5554000003933907 Test Acc : 0.4520000009983778\n",
      "\n",
      "Current : 101 Train Acc : 0.5563999995589256 Test Acc : 0.4500000007450581\n",
      "\n",
      "Current : 102 Train Acc : 0.557599999845028 Test Acc : 0.4490000004321337\n",
      "\n",
      "Current : 103 Train Acc : 0.5584000008702278 Test Acc : 0.4491250003874302\n",
      "\n",
      "Current : 104 Train Acc : 0.5588000007271766 Test Acc : 0.4520000000670552\n",
      "\n",
      "Current : 105 Train Acc : 0.5600000006556511 Test Acc : 0.45262500062584876\n",
      "\n",
      "Current : 106 Train Acc : 0.5602000004649162 Test Acc : 0.45450000047683714\n",
      "\n",
      "Current : 107 Train Acc : 0.5621999993920326 Test Acc : 0.4562500002980232\n",
      "\n",
      "Current : 108 Train Acc : 0.5629999998211861 Test Acc : 0.45650000005960467\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current : 109 Train Acc : 0.5633999999165535 Test Acc : 0.4596250006556511\n",
      "\n",
      "Current : 110 Train Acc : 0.5656000003218651 Test Acc : 0.45937500029802325\n",
      "\n",
      "Current : 111 Train Acc : 0.5654000009894371 Test Acc : 0.4588750005140901\n",
      "\n",
      "Current : 112 Train Acc : 0.5666000007987022 Test Acc : 0.46012500070035456\n",
      "\n",
      "Current : 113 Train Acc : 0.5666000002026558 Test Acc : 0.4597500006854534\n",
      "\n",
      "Current : 114 Train Acc : 0.5688000006079674 Test Acc : 0.4597500008717179\n",
      "\n",
      "Current : 115 Train Acc : 0.5696000002026558 Test Acc : 0.4577500012516975\n",
      "\n",
      "Current : 116 Train Acc : 0.5691999997496605 Test Acc : 0.45862500064074996\n",
      "\n",
      "Current : 117 Train Acc : 0.5686000004410744 Test Acc : 0.4572500007599592\n",
      "\n",
      "Current : 118 Train Acc : 0.5689999997019768 Test Acc : 0.45500000048428774\n",
      "\n",
      "Current : 119 Train Acc : 0.5694000007510185 Test Acc : 0.4537500003352761\n",
      "\n",
      "Current : 120 Train Acc : 0.5712000005841256 Test Acc : 0.4532500002160668\n",
      "\n",
      "Current : 121 Train Acc : 0.5743999996781349 Test Acc : 0.4572500005364418\n",
      "\n",
      "Current : 122 Train Acc : 0.5747999998927117 Test Acc : 0.4555000000447035\n",
      "\n",
      "Current : 123 Train Acc : 0.5754000000357627 Test Acc : 0.4567500001192093\n",
      "\n",
      "Current : 124 Train Acc : 0.5766000003218651 Test Acc : 0.4595000006258488\n",
      "\n",
      "Current : 125 Train Acc : 0.5786000005602837 Test Acc : 0.4628750007972121\n",
      "\n",
      "Current : 126 Train Acc : 0.5792000007033348 Test Acc : 0.4628750005364418\n",
      "\n",
      "Current : 127 Train Acc : 0.5778000003695488 Test Acc : 0.46187500044703483\n",
      "\n",
      "Current : 128 Train Acc : 0.5816000007987022 Test Acc : 0.464750000461936\n",
      "\n",
      "Current : 129 Train Acc : 0.5824000001549721 Test Acc : 0.46475000083446505\n",
      "\n",
      "Current : 130 Train Acc : 0.583400000333786 Test Acc : 0.46575000032782554\n",
      "\n",
      "Current : 131 Train Acc : 0.5822000002861023 Test Acc : 0.4641250003874302\n",
      "\n",
      "Current : 132 Train Acc : 0.5853999997377396 Test Acc : 0.465125000551343\n",
      "\n",
      "Current : 133 Train Acc : 0.5868000000715256 Test Acc : 0.46662500090897086\n",
      "\n",
      "Current : 134 Train Acc : 0.5895999997854233 Test Acc : 0.46712500110268595\n",
      "\n",
      "Current : 135 Train Acc : 0.591400000333786 Test Acc : 0.4666250007599592\n",
      "\n",
      "Current : 136 Train Acc : 0.5930000005960464 Test Acc : 0.46662500105798244\n",
      "\n",
      "Current : 137 Train Acc : 0.5960000009536743 Test Acc : 0.4683750006556511\n",
      "\n",
      "Current : 138 Train Acc : 0.5963999999761581 Test Acc : 0.46687500081956385\n",
      "\n",
      "Current : 139 Train Acc : 0.597 Test Acc : 0.46925000108778475\n",
      "\n",
      "Current : 140 Train Acc : 0.5981999995708466 Test Acc : 0.46800000078976156\n",
      "\n",
      "Current : 141 Train Acc : 0.6001999992132186 Test Acc : 0.4687500013038516\n",
      "\n",
      "Current : 142 Train Acc : 0.6033999993801117 Test Acc : 0.46862500116229056\n",
      "\n",
      "Current : 143 Train Acc : 0.6035999993085861 Test Acc : 0.46800000093877314\n",
      "\n",
      "Current : 144 Train Acc : 0.6031999998092651 Test Acc : 0.46737500086426736\n",
      "\n",
      "Current : 145 Train Acc : 0.6045999999046325 Test Acc : 0.4686250015348196\n",
      "\n",
      "Current : 146 Train Acc : 0.6045999993085861 Test Acc : 0.4662500020116568\n",
      "\n",
      "Current : 147 Train Acc : 0.6047999994754791 Test Acc : 0.4645000016689301\n",
      "\n",
      "Current : 148 Train Acc : 0.6059999994039535 Test Acc : 0.46587500169873236\n",
      "\n",
      "Current : 149 Train Acc : 0.6063999992609024 Test Acc : 0.4660000016540289\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# I\n",
    "current_exp_name = 'I';\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# create layers\n",
    "l1 = CNN(3,3, 16,which_reg=current_exp_name); \n",
    "l2 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "l3 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "\n",
    "l4 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "l5 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "l6 = CNN(3,16,10,which_reg=current_exp_name); \n",
    "\n",
    "# 2. graph \n",
    "x = tf.placeholder(tf.float32,(batch_size,96,96,3))\n",
    "y = tf.placeholder(tf.float32,(batch_size,10))\n",
    "\n",
    "layer1, layer1a = l1. feedforward(x,stride=2)\n",
    "layer2, layer2a = l2. feedforward(layer1a,stride=2)\n",
    "layer3, layer3a = l3. feedforward(layer2a,stride=2)\n",
    "layer4, layer4a = l4. feedforward(layer3a,stride=2)\n",
    "layer5, layer5a = l5. feedforward(layer4a)\n",
    "layer6, layer6a = l6. feedforward(layer5a)\n",
    "\n",
    "final_layer   = tf.reduce_mean(layer6a,(1,2))\n",
    "final_softmax = tf_softmax(final_layer)\n",
    "cost          = -tf.reduce_mean(y * tf.log(final_softmax + 1e-8))\n",
    "correct_prediction = tf.equal(tf.argmax(final_softmax, 1), tf.argmax(y, 1))\n",
    "accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "gradient = tf.tile((final_softmax-y)[:,None,None,:],[1,6,6,1])/batch_size\n",
    "grad6p,grad6w,grad6_up = l6.backprop(gradient)\n",
    "grad5p,grad5w,grad5_up = l5.backprop(grad6p)\n",
    "grad4p,grad4w,grad4_up = l4.backprop(grad5p,stride=2)\n",
    "grad3p,grad3w,grad3_up = l3.backprop(grad4p,stride=2)\n",
    "grad2p,grad2w,grad2_up = l2.backprop(grad3p,stride=2)\n",
    "grad1p,grad1w,grad1_up = l1.backprop(grad2p,stride=2)\n",
    "\n",
    "gradient_update = grad6_up + grad5_up + grad4_up + grad3_up + grad2_up + grad1_up \n",
    "\n",
    "# train\n",
    "sess.run(tf.global_variables_initializer())\n",
    "avg_acc_train = 0; avg_acc_test  = 0; train_acc = [];test_acc = []\n",
    "\n",
    "# mean std skew kurt non-zero\n",
    "llayer1 = [[],[],[],[],[]]; llayer2 = [[],[],[],[],[]]; llayer3 = [[],[],[],[],[]]\n",
    "llayer4 = [[],[],[],[],[]]; llayer5 = [[],[],[],[],[]]; llayer6 = [[],[],[],[],[]]\n",
    "\n",
    "llayer1a = [[],[],[],[],[]]; llayer2a = [[],[],[],[],[]]; llayer3a = [[],[],[],[],[]]\n",
    "llayer4a = [[],[],[],[],[]]; llayer5a = [[],[],[],[],[]]; llayer6a = [[],[],[],[],[]]\n",
    "\n",
    "weight1 = [[],[],[],[],[]]; weight2 = [[],[],[],[],[]]; weight3 = [[],[],[],[],[]];\n",
    "weight4 = [[],[],[],[],[]]; weight5 = [[],[],[],[],[]]; weight6 = [[],[],[],[],[]];\n",
    "\n",
    "gradw1  = [[],[],[],[],[]]; gradw2  = [[],[],[],[],[]]; gradw3  = [[],[],[],[],[]];\n",
    "gradw4  = [[],[],[],[],[]]; gradw5  = [[],[],[],[],[]]; gradw6  = [[],[],[],[],[]];\n",
    "\n",
    "gradp1  = [[],[],[],[],[]]; gradp2  = [[],[],[],[],[]]; gradp3  = [[],[],[],[],[]];\n",
    "gradp4  = [[],[],[],[],[]]; gradp5  = [[],[],[],[],[]]; gradp6  = [[],[],[],[],[]];\n",
    "\n",
    "gradup1  = [[],[],[],[],[]]; gradup2  = [[],[],[],[],[]]; gradup3  = [[],[],[],[],[]];\n",
    "gradup4  = [[],[],[],[],[]]; gradup5  = [[],[],[],[],[]]; gradup6  = [[],[],[],[],[]];\n",
    "\n",
    "list_of_outputs = [\n",
    "    layer1,layer2,layer3,layer4,layer5,layer6,\n",
    "    layer1a,layer2a,layer3a,layer4a,layer5a,layer6a,\n",
    "    l1.getw(),l2.getw(),l3.getw(),l4.getw(),l5.getw(),l6.getw(),\n",
    "    grad1w,grad2w,grad3w,grad4w,grad5w,grad6w,\n",
    "    grad1p,grad2p,grad3p,grad4p,grad5p,grad6p,\n",
    "    grad1_up[0],grad2_up[0],grad3_up[0],grad4_up[0],grad5_up[0],grad6_up[0]\n",
    "]\n",
    "\n",
    "for iter in range(num_epoch):\n",
    "\n",
    "    # Training Accuracy    \n",
    "    for current_batch_index in range(0,len(train_images),batch_size):\n",
    "        current_data  = train_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = train_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy,gradient_update],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(train_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_train = avg_acc_train + sess_results[0]\n",
    "        \n",
    "    # get the results\n",
    "    mid_stat = sess.run(list_of_outputs,feed_dict={x:current_data,y:current_label})\n",
    "    \n",
    "    # Test Accuracy    \n",
    "    for current_batch_index in range(0,len(test_images), batch_size):\n",
    "        current_data  = test_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = test_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(test_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_test = avg_acc_test + sess_results[0]   \n",
    "        \n",
    "    # ======================== extract stats ========================\n",
    "    llayer1 = append_stat(llayer1,mid_stat,0);  llayer2 = append_stat(llayer2,mid_stat,1);  llayer3 = append_stat(llayer3,mid_stat,2);\n",
    "    llayer4 = append_stat(llayer4,mid_stat,3);  llayer5 = append_stat(llayer5,mid_stat,4);  llayer6 = append_stat(llayer6,mid_stat,5);\n",
    "\n",
    "    llayer1a = append_stat(llayer1a,mid_stat,6);  llayer2a = append_stat(llayer2a,mid_stat,7);  llayer3a = append_stat(llayer3a,mid_stat,8);\n",
    "    llayer4a = append_stat(llayer4a,mid_stat,9);  llayer5a = append_stat(llayer5a,mid_stat,10); llayer6a = append_stat(llayer6a,mid_stat,11);\n",
    "    \n",
    "    weight1 = append_stat(weight1,mid_stat,12);  weight2 = append_stat(weight2,mid_stat,13);  weight3 = append_stat(weight3,mid_stat,14);\n",
    "    weight4 = append_stat(weight4,mid_stat,15);  weight5 = append_stat(weight5,mid_stat,16);  weight6 = append_stat(weight6,mid_stat,17);\n",
    "    \n",
    "    gradw1 = append_stat(gradw1,mid_stat,18); gradw2 = append_stat(gradw2,mid_stat,19); gradw3 = append_stat(gradw3,mid_stat,20);\n",
    "    gradw4 = append_stat(gradw4,mid_stat,21); gradw5 = append_stat(gradw5,mid_stat,22); gradw6 = append_stat(gradw6,mid_stat,23);\n",
    "    \n",
    "    gradp1 = append_stat(gradp1,mid_stat,24); gradp2 = append_stat(gradp2,mid_stat,25); gradp3 = append_stat(gradp3,mid_stat,26);\n",
    "    gradp4 = append_stat(gradp4,mid_stat,27); gradp5 = append_stat(gradp5,mid_stat,28); gradp6 = append_stat(gradp6,mid_stat,29);\n",
    "\n",
    "    gradup1 = append_stat(gradup1,mid_stat,30); gradup2 = append_stat(gradup2,mid_stat,31); gradup3 = append_stat(gradup3,mid_stat,32);\n",
    "    gradup4 = append_stat(gradup4,mid_stat,33); gradup5 = append_stat(gradup5,mid_stat,34); gradup6 = append_stat(gradup6,mid_stat,35);\n",
    "\n",
    "    train_acc.append(avg_acc_train/(len(train_images)/batch_size))\n",
    "    test_acc .append(avg_acc_test / (len(test_images)/batch_size))\n",
    "    # ======================== extract stats ========================\n",
    "    \n",
    "    # ======================== save to image ========================\n",
    "    save_to_image(mid_stat[0:6]   ,llayer1,llayer2,llayer3,llayer4,llayer5,llayer6,\"layer\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[6:12]  ,llayer1a,llayer2a,llayer3a,llayer4a,llayer5a,llayer6a,\"layera\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[12:18] ,weight1,weight2,weight3,weight4,weight5,weight6,\"weights\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[18:24] ,gradw1,gradw2,gradw3,gradw4,gradw5,gradw6,\"gradientw\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[24:30] ,gradp1,gradp2,gradp3,gradp4,gradp5,gradp6,\"gradientp\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[30:36] ,gradup1,gradup2,gradup3,gradup4,gradup5,gradup6,\"moment\",train_acc,test_acc,current_exp_name,iter)\n",
    "    # ======================== save to image ========================\n",
    "        \n",
    "    # ======================== print reset ========================\n",
    "    print(\"Current : \"+ str(iter) + \" Train Acc : \" + str(avg_acc_train/(len(train_images)/batch_size)) + \" Test Acc : \" + str(avg_acc_test/(len(test_images)/batch_size)) + '\\n')\n",
    "    avg_acc_train = 0 ; avg_acc_test  = 0\n",
    "    # ======================== print reset ========================\n",
    "\n",
    "np.save(current_exp_name+'/train_acc.npy',train_acc); np.save(current_exp_name+'/test_acc.npy', test_acc)    \n",
    "np.save(current_exp_name+'/llayer1.npy', llayer1);  np.save(current_exp_name+'/llayer2.npy', llayer2);  np.save(current_exp_name+'/llayer3.npy', llayer3); \n",
    "np.save(current_exp_name+'/llayer4.npy', llayer4);  np.save(current_exp_name+'/llayer5.npy', llayer5);  np.save(current_exp_name+'/llayer6.npy', llayer6); \n",
    "\n",
    "np.save(current_exp_name+'/llayer1a.npy', llayer1a);  np.save(current_exp_name+'/llayer2a.npy', llayer2a);  np.save(current_exp_name+'/llayer3a.npy', llayer3a); \n",
    "np.save(current_exp_name+'/llayer4a.npy', llayer4a);  np.save(current_exp_name+'/llayer5a.npy', llayer5a);  np.save(current_exp_name+'/llayer6a.npy', llayer6a); \n",
    "\n",
    "np.save(current_exp_name+'/weight1.npy', weight1);  np.save(current_exp_name+'/weight2.npy', weight2);  np.save(current_exp_name+'/weight3.npy', weight3);  \n",
    "np.save(current_exp_name+'/weight4.npy', weight4);  np.save(current_exp_name+'/weight5.npy', weight5);  np.save(current_exp_name+'/weight6.npy', weight6);  \n",
    "\n",
    "np.save(current_exp_name+'/gradw1.npy', gradw1); np.save(current_exp_name+'/gradw2.npy', gradw2); np.save(current_exp_name+'/gradw3.npy', gradw3);\n",
    "np.save(current_exp_name+'/gradw4.npy', gradw4); np.save(current_exp_name+'/gradw5.npy', gradw5); np.save(current_exp_name+'/gradw6.npy', gradw6);\n",
    "\n",
    "np.save(current_exp_name+'/gradp1.npy', gradp1); np.save(current_exp_name+'/gradp2.npy', gradp2); np.save(current_exp_name+'/gradp3.npy', gradp3);\n",
    "np.save(current_exp_name+'/gradp4.npy', gradp4); np.save(current_exp_name+'/gradp5.npy', gradp5); np.save(current_exp_name+'/gradp6.npy', gradp6);\n",
    "\n",
    "np.save(current_exp_name+'/gradup1.npy', gradup1); np.save(current_exp_name+'/gradup2.npy', gradup2); np.save(current_exp_name+'/gradup3.npy', gradup3);\n",
    "np.save(current_exp_name+'/gradup4.npy', gradup4); np.save(current_exp_name+'/gradup5.npy', gradup5); np.save(current_exp_name+'/gradup6.npy', gradup6);\n",
    "\n",
    "sess.close(); tf.reset_default_graph();\n",
    "\n",
    "%reset_selective -f l1,l2,l3,l4,l5,l6\n",
    "%reset_selective -f layer1,layer2,layer3,layer4,layer5,layer6\n",
    "%reset_selective -f layer1a,layer2a,layer3a,layer4a,layer5a,layer6a\n",
    "%reset_selective -f train_acc,test_acc,llayer1,llayer2,llayer3,llayer4,llayer5,llayer6,llayer1a,llayer2a,llayer3a,llayer4a,llayer5a,llayer6a\n",
    "%reset_selective -f weight1,weight2,weight3,weight4,weight5,weight6\n",
    "%reset_selective -f gradw1,gradw2,gradw3,gradw4,gradw5,gradw6\n",
    "%reset_selective -f gradp1,gradp2,gradp3,gradp4,gradp5,gradp6\n",
    "%reset_selective -f gradup1,gradup2,gradup3,gradup4,gradup5,gradup6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-05T10:32:12.592282Z",
     "start_time": "2019-01-05T09:43:30.532375Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current : 0 Train Acc : 0.13660000264644623 Test Acc : 0.18250000298954547\n",
      "\n",
      "Current : 1 Train Acc : 0.1886000033020973 Test Acc : 0.24550000268034636\n",
      "\n",
      "Current : 2 Train Acc : 0.26500000309944155 Test Acc : 0.28837500270456073\n",
      "\n",
      "Current : 3 Train Acc : 0.3016000020503998 Test Acc : 0.30175000315532086\n",
      "\n",
      "Current : 4 Train Acc : 0.31720000216364863 Test Acc : 0.3260000018123537\n",
      "\n",
      "Current : 5 Train Acc : 0.33960000149905684 Test Acc : 0.33725000120699405\n",
      "\n",
      "Current : 6 Train Acc : 0.35220000141859054 Test Acc : 0.3482500013336539\n",
      "\n",
      "Current : 7 Train Acc : 0.35900000178813934 Test Acc : 0.350250001354143\n",
      "\n",
      "Current : 8 Train Acc : 0.36860000175237656 Test Acc : 0.3653750009648502\n",
      "\n",
      "Current : 9 Train Acc : 0.3840000012218952 Test Acc : 0.3838750014733523\n",
      "\n",
      "Current : 10 Train Acc : 0.3950000014901161 Test Acc : 0.39362500051036475\n",
      "\n",
      "Current : 11 Train Acc : 0.40060000133514406 Test Acc : 0.40037500105798246\n",
      "\n",
      "Current : 12 Train Acc : 0.405400001347065 Test Acc : 0.40200000124052165\n",
      "\n",
      "Current : 13 Train Acc : 0.40860000199079516 Test Acc : 0.40675000151619317\n",
      "\n",
      "Current : 14 Train Acc : 0.4096000016927719 Test Acc : 0.4077500013448298\n",
      "\n",
      "Current : 15 Train Acc : 0.41220000201463697 Test Acc : 0.41175000118091704\n",
      "\n",
      "Current : 16 Train Acc : 0.41620000123977663 Test Acc : 0.41300000140443444\n",
      "\n",
      "Current : 17 Train Acc : 0.41980000185966493 Test Acc : 0.4147500008530915\n",
      "\n",
      "Current : 18 Train Acc : 0.4226000012159348 Test Acc : 0.41612500106915834\n",
      "\n",
      "Current : 19 Train Acc : 0.42500000101327895 Test Acc : 0.4166250010207295\n",
      "\n",
      "Current : 20 Train Acc : 0.43160000002384186 Test Acc : 0.4195000008493662\n",
      "\n",
      "Current : 21 Train Acc : 0.43480000057816504 Test Acc : 0.4201250014081597\n",
      "\n",
      "Current : 22 Train Acc : 0.4309999998509884 Test Acc : 0.4203750006482005\n",
      "\n",
      "Current : 23 Train Acc : 0.4392000005841255 Test Acc : 0.42387500137090683\n",
      "\n",
      "Current : 24 Train Acc : 0.4402000004053116 Test Acc : 0.42300000101327895\n",
      "\n",
      "Current : 25 Train Acc : 0.442000000834465 Test Acc : 0.4245000015571713\n",
      "\n",
      "Current : 26 Train Acc : 0.4428000010251999 Test Acc : 0.4271250019595027\n",
      "\n",
      "Current : 27 Train Acc : 0.4462000013589859 Test Acc : 0.42675000220537185\n",
      "\n",
      "Current : 28 Train Acc : 0.45040000200271607 Test Acc : 0.4296250017359853\n",
      "\n",
      "Current : 29 Train Acc : 0.4514000011086464 Test Acc : 0.4320000013336539\n",
      "\n",
      "Current : 30 Train Acc : 0.4548000004291534 Test Acc : 0.43375000104308126\n",
      "\n",
      "Current : 31 Train Acc : 0.45780000030994417 Test Acc : 0.43412500075995925\n",
      "\n",
      "Current : 32 Train Acc : 0.4592000004649162 Test Acc : 0.43537500109523536\n",
      "\n",
      "Current : 33 Train Acc : 0.46039999967813494 Test Acc : 0.435625000782311\n",
      "\n",
      "Current : 34 Train Acc : 0.4636000003814697 Test Acc : 0.43562500055879355\n",
      "\n",
      "Current : 35 Train Acc : 0.46459999918937683 Test Acc : 0.43862500093877316\n",
      "\n",
      "Current : 36 Train Acc : 0.4646000012755394 Test Acc : 0.4405000009387732\n",
      "\n",
      "Current : 37 Train Acc : 0.4668000006079674 Test Acc : 0.4417500004917383\n",
      "\n",
      "Current : 38 Train Acc : 0.46760000079870223 Test Acc : 0.4432500006258488\n",
      "\n",
      "Current : 39 Train Acc : 0.46920000106096266 Test Acc : 0.4447500009834766\n",
      "\n",
      "Current : 40 Train Acc : 0.4720000011920929 Test Acc : 0.4445000008866191\n",
      "\n",
      "Current : 41 Train Acc : 0.47600000101327894 Test Acc : 0.4445000008866191\n",
      "\n",
      "Current : 42 Train Acc : 0.47820000106096267 Test Acc : 0.44562500074505806\n",
      "\n",
      "Current : 43 Train Acc : 0.47980000108480453 Test Acc : 0.4481250012665987\n",
      "\n",
      "Current : 44 Train Acc : 0.48280000168085097 Test Acc : 0.44862500138580796\n",
      "\n",
      "Current : 45 Train Acc : 0.4832000021338463 Test Acc : 0.4487500013783574\n",
      "\n",
      "Current : 46 Train Acc : 0.4838000001311302 Test Acc : 0.4471250021457672\n",
      "\n",
      "Current : 47 Train Acc : 0.4820000010728836 Test Acc : 0.44100000120699406\n",
      "\n",
      "Current : 48 Train Acc : 0.47920000141859054 Test Acc : 0.44450000174343585\n",
      "\n",
      "Current : 49 Train Acc : 0.4854000012278557 Test Acc : 0.45250000193715095\n",
      "\n",
      "Current : 50 Train Acc : 0.4926000006198883 Test Acc : 0.45712500117719174\n",
      "\n",
      "Current : 51 Train Acc : 0.4954000017642975 Test Acc : 0.459375\n",
      "\n",
      "Current : 52 Train Acc : 0.4964000019431114 Test Acc : 0.4601250009983778\n",
      "\n",
      "Current : 53 Train Acc : 0.4956000012159347 Test Acc : 0.46100000068545344\n",
      "\n",
      "Current : 54 Train Acc : 0.5028000012040138 Test Acc : 0.4611250007525086\n",
      "\n",
      "Current : 55 Train Acc : 0.4966000004410744 Test Acc : 0.46350000075995923\n",
      "\n",
      "Current : 56 Train Acc : 0.5023999999165535 Test Acc : 0.466875000782311\n",
      "\n",
      "Current : 57 Train Acc : 0.5034000005125999 Test Acc : 0.46575000040233133\n",
      "\n",
      "Current : 58 Train Acc : 0.5088000009059906 Test Acc : 0.4665000012889504\n",
      "\n",
      "Current : 59 Train Acc : 0.5126000009775161 Test Acc : 0.4665000012889504\n",
      "\n",
      "Current : 60 Train Acc : 0.5166000013947487 Test Acc : 0.47075000062584876\n",
      "\n",
      "Current : 61 Train Acc : 0.5188000002503395 Test Acc : 0.47250000067055226\n",
      "\n",
      "Current : 62 Train Acc : 0.5216000011563301 Test Acc : 0.4700000008940697\n",
      "\n",
      "Current : 63 Train Acc : 0.5198000001907349 Test Acc : 0.46412500116974115\n",
      "\n",
      "Current : 64 Train Acc : 0.5192000009417533 Test Acc : 0.46850000057369473\n",
      "\n",
      "Current : 65 Train Acc : 0.523000001847744 Test Acc : 0.4708750009909272\n",
      "\n",
      "Current : 66 Train Acc : 0.5229999993443489 Test Acc : 0.46525000102818015\n",
      "\n",
      "Current : 67 Train Acc : 0.5218000002503395 Test Acc : 0.46062500067055223\n",
      "\n",
      "Current : 68 Train Acc : 0.5242000002264976 Test Acc : 0.4581250011175871\n",
      "\n",
      "Current : 69 Train Acc : 0.5265999996066093 Test Acc : 0.4562500009685755\n",
      "\n",
      "Current : 70 Train Acc : 0.5291999997496605 Test Acc : 0.45350000116974115\n",
      "\n",
      "Current : 71 Train Acc : 0.5333999997973442 Test Acc : 0.45637500084936616\n",
      "\n",
      "Current : 72 Train Acc : 0.537800000667572 Test Acc : 0.4556250009685755\n",
      "\n",
      "Current : 73 Train Acc : 0.539800000667572 Test Acc : 0.4571250006556511\n",
      "\n",
      "Current : 74 Train Acc : 0.5426000000238419 Test Acc : 0.4588750006258488\n",
      "\n",
      "Current : 75 Train Acc : 0.5454000000953674 Test Acc : 0.45575000140815974\n",
      "\n",
      "Current : 76 Train Acc : 0.5458000001907348 Test Acc : 0.4547500008717179\n",
      "\n",
      "Current : 77 Train Acc : 0.5474000002145767 Test Acc : 0.4587500008568168\n",
      "\n",
      "Current : 78 Train Acc : 0.5487999999523163 Test Acc : 0.4557500012591481\n",
      "\n",
      "Current : 79 Train Acc : 0.5524000005722046 Test Acc : 0.45325000148266553\n",
      "\n",
      "Current : 80 Train Acc : 0.552600000500679 Test Acc : 0.4580000011250377\n",
      "\n",
      "Current : 81 Train Acc : 0.5561999989748001 Test Acc : 0.4562500010803342\n",
      "\n",
      "Current : 82 Train Acc : 0.5580000004768372 Test Acc : 0.45850000079721215\n",
      "\n",
      "Current : 83 Train Acc : 0.5592000004053116 Test Acc : 0.45412500113248827\n",
      "\n",
      "Current : 84 Train Acc : 0.5608000010251999 Test Acc : 0.4590000014007092\n",
      "\n",
      "Current : 85 Train Acc : 0.5614000011682511 Test Acc : 0.4582500016316772\n",
      "\n",
      "Current : 86 Train Acc : 0.5640000009536743 Test Acc : 0.4593750014528632\n",
      "\n",
      "Current : 87 Train Acc : 0.5642000012397766 Test Acc : 0.45950000181794165\n",
      "\n",
      "Current : 88 Train Acc : 0.5688000005483628 Test Acc : 0.46650000136345626\n",
      "\n",
      "Current : 89 Train Acc : 0.5692000013589859 Test Acc : 0.46925000127404926\n",
      "\n",
      "Current : 90 Train Acc : 0.5716000019311905 Test Acc : 0.4695000011101365\n",
      "\n",
      "Current : 91 Train Acc : 0.5742000012397767 Test Acc : 0.47200000151991844\n",
      "\n",
      "Current : 92 Train Acc : 0.5750000020265579 Test Acc : 0.46937500208616256\n",
      "\n",
      "Current : 93 Train Acc : 0.5786000009775162 Test Acc : 0.46550000216811893\n",
      "\n",
      "Current : 94 Train Acc : 0.5782000012397767 Test Acc : 0.46412500225007536\n",
      "\n",
      "Current : 95 Train Acc : 0.5786000015735626 Test Acc : 0.4632500021904707\n",
      "\n",
      "Current : 96 Train Acc : 0.5812000013589859 Test Acc : 0.46850000232458117\n",
      "\n",
      "Current : 97 Train Acc : 0.5838000013828277 Test Acc : 0.46875000204890965\n",
      "\n",
      "Current : 98 Train Acc : 0.5848000017404557 Test Acc : 0.4678750017285347\n",
      "\n",
      "Current : 99 Train Acc : 0.5864000006914138 Test Acc : 0.4701250020414591\n",
      "\n",
      "Current : 100 Train Acc : 0.5874000015258789 Test Acc : 0.47025000259280203\n",
      "\n",
      "Current : 101 Train Acc : 0.5874000017642975 Test Acc : 0.4688750024139881\n",
      "\n",
      "Current : 102 Train Acc : 0.5896000012159347 Test Acc : 0.47237500254064796\n",
      "\n",
      "Current : 103 Train Acc : 0.5910000010728836 Test Acc : 0.47362500201910734\n",
      "\n",
      "Current : 104 Train Acc : 0.5932000005245208 Test Acc : 0.4768750014156103\n",
      "\n",
      "Current : 105 Train Acc : 0.5929999996423722 Test Acc : 0.476125001385808\n",
      "\n",
      "Current : 106 Train Acc : 0.5931999996900559 Test Acc : 0.478750001527369\n",
      "\n",
      "Current : 107 Train Acc : 0.593399999499321 Test Acc : 0.47587500113993886\n",
      "\n",
      "Current : 108 Train Acc : 0.5951999990940094 Test Acc : 0.47712500117719175\n",
      "\n",
      "Current : 109 Train Acc : 0.5949999994039535 Test Acc : 0.4768750010803342\n",
      "\n",
      "Current : 110 Train Acc : 0.598799998998642 Test Acc : 0.47712500136345626\n",
      "\n",
      "Current : 111 Train Acc : 0.5996000003814698 Test Acc : 0.4783750009909272\n",
      "\n",
      "Current : 112 Train Acc : 0.6013999998569488 Test Acc : 0.4791250006482005\n",
      "\n",
      "Current : 113 Train Acc : 0.6023999993801117 Test Acc : 0.48037500116974113\n",
      "\n",
      "Current : 114 Train Acc : 0.6053999999761581 Test Acc : 0.4810000007227063\n",
      "\n",
      "Current : 115 Train Acc : 0.6056000000238418 Test Acc : 0.4816250003501773\n",
      "\n",
      "Current : 116 Train Acc : 0.607 Test Acc : 0.4832500009611249\n",
      "\n",
      "Current : 117 Train Acc : 0.608800000667572 Test Acc : 0.4823749998956919\n",
      "\n",
      "Current : 118 Train Acc : 0.6116000007390976 Test Acc : 0.48474999975413086\n",
      "\n",
      "Current : 119 Train Acc : 0.6124000014066696 Test Acc : 0.48712500046938656\n",
      "\n",
      "Current : 120 Train Acc : 0.6146000007390976 Test Acc : 0.4878750003129244\n",
      "\n",
      "Current : 121 Train Acc : 0.6156000008583069 Test Acc : 0.49162500087171795\n",
      "\n",
      "Current : 122 Train Acc : 0.616000000834465 Test Acc : 0.4931250007078052\n",
      "\n",
      "Current : 123 Train Acc : 0.6184000006914139 Test Acc : 0.49150000091642143\n",
      "\n",
      "Current : 124 Train Acc : 0.6218000005483627 Test Acc : 0.4907500008493662\n",
      "\n",
      "Current : 125 Train Acc : 0.6210000005960464 Test Acc : 0.4868750006705522\n",
      "\n",
      "Current : 126 Train Acc : 0.6188000018596649 Test Acc : 0.48400000024586914\n",
      "\n",
      "Current : 127 Train Acc : 0.6224000002145768 Test Acc : 0.4862500005587935\n",
      "\n",
      "Current : 128 Train Acc : 0.6228000007867813 Test Acc : 0.48700000032782553\n",
      "\n",
      "Current : 129 Train Acc : 0.6240000013113022 Test Acc : 0.4895000008866191\n",
      "\n",
      "Current : 130 Train Acc : 0.6260000009536744 Test Acc : 0.49150000073015687\n",
      "\n",
      "Current : 131 Train Acc : 0.6288000000715256 Test Acc : 0.4900000008568168\n",
      "\n",
      "Current : 132 Train Acc : 0.6276000009775162 Test Acc : 0.4911250004544854\n",
      "\n",
      "Current : 133 Train Acc : 0.6290000005960464 Test Acc : 0.49025000039488076\n",
      "\n",
      "Current : 134 Train Acc : 0.630400000333786 Test Acc : 0.4917500003427267\n",
      "\n",
      "Current : 135 Train Acc : 0.6360000003576278 Test Acc : 0.49362500112503765\n",
      "\n",
      "Current : 136 Train Acc : 0.6367999993562699 Test Acc : 0.4922500010207295\n",
      "\n",
      "Current : 137 Train Acc : 0.6366000002622605 Test Acc : 0.49475000094622373\n",
      "\n",
      "Current : 138 Train Acc : 0.6362000008821488 Test Acc : 0.5012500010430813\n",
      "\n",
      "Current : 139 Train Acc : 0.6388000010251998 Test Acc : 0.5032500016689301\n",
      "\n",
      "Current : 140 Train Acc : 0.639600000500679 Test Acc : 0.5033750013262034\n",
      "\n",
      "Current : 141 Train Acc : 0.6410000003576278 Test Acc : 0.504000001847744\n",
      "\n",
      "Current : 142 Train Acc : 0.6426000010967254 Test Acc : 0.5057500015199184\n",
      "\n",
      "Current : 143 Train Acc : 0.6456000007390976 Test Acc : 0.5045000021159649\n",
      "\n",
      "Current : 144 Train Acc : 0.6486000013351441 Test Acc : 0.5035000019520521\n",
      "\n",
      "Current : 145 Train Acc : 0.6496000003814697 Test Acc : 0.5035000019520521\n",
      "\n",
      "Current : 146 Train Acc : 0.6510000004768371 Test Acc : 0.5041250017285347\n",
      "\n",
      "Current : 147 Train Acc : 0.653400000333786 Test Acc : 0.5048750015348196\n",
      "\n",
      "Current : 148 Train Acc : 0.6538000013828278 Test Acc : 0.504000001475215\n",
      "\n",
      "Current : 149 Train Acc : 0.6552000005245209 Test Acc : 0.5041250016540288\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# J\n",
    "current_exp_name = 'J';\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# create layers\n",
    "l1 = CNN(3,3, 16,which_reg=current_exp_name); \n",
    "l2 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "l3 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "\n",
    "l4 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "l5 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "l6 = CNN(3,16,10,which_reg=current_exp_name); \n",
    "\n",
    "# 2. graph \n",
    "x = tf.placeholder(tf.float32,(batch_size,96,96,3))\n",
    "y = tf.placeholder(tf.float32,(batch_size,10))\n",
    "\n",
    "layer1, layer1a = l1. feedforward(x,stride=2)\n",
    "layer2, layer2a = l2. feedforward(layer1a,stride=2)\n",
    "layer3, layer3a = l3. feedforward(layer2a,stride=2)\n",
    "layer4, layer4a = l4. feedforward(layer3a,stride=2)\n",
    "layer5, layer5a = l5. feedforward(layer4a)\n",
    "layer6, layer6a = l6. feedforward(layer5a)\n",
    "\n",
    "final_layer   = tf.reduce_mean(layer6a,(1,2))\n",
    "final_softmax = tf_softmax(final_layer)\n",
    "cost          = -tf.reduce_mean(y * tf.log(final_softmax + 1e-8))\n",
    "correct_prediction = tf.equal(tf.argmax(final_softmax, 1), tf.argmax(y, 1))\n",
    "accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "gradient = tf.tile((final_softmax-y)[:,None,None,:],[1,6,6,1])/batch_size\n",
    "grad6p,grad6w,grad6_up = l6.backprop(gradient)\n",
    "grad5p,grad5w,grad5_up = l5.backprop(grad6p)\n",
    "grad4p,grad4w,grad4_up = l4.backprop(grad5p,stride=2)\n",
    "grad3p,grad3w,grad3_up = l3.backprop(grad4p,stride=2)\n",
    "grad2p,grad2w,grad2_up = l2.backprop(grad3p,stride=2)\n",
    "grad1p,grad1w,grad1_up = l1.backprop(grad2p,stride=2)\n",
    "\n",
    "gradient_update = grad6_up + grad5_up + grad4_up + grad3_up + grad2_up + grad1_up \n",
    "\n",
    "# train\n",
    "sess.run(tf.global_variables_initializer())\n",
    "avg_acc_train = 0; avg_acc_test  = 0; train_acc = [];test_acc = []\n",
    "\n",
    "# mean std skew kurt non-zero\n",
    "llayer1 = [[],[],[],[],[]]; llayer2 = [[],[],[],[],[]]; llayer3 = [[],[],[],[],[]]\n",
    "llayer4 = [[],[],[],[],[]]; llayer5 = [[],[],[],[],[]]; llayer6 = [[],[],[],[],[]]\n",
    "\n",
    "llayer1a = [[],[],[],[],[]]; llayer2a = [[],[],[],[],[]]; llayer3a = [[],[],[],[],[]]\n",
    "llayer4a = [[],[],[],[],[]]; llayer5a = [[],[],[],[],[]]; llayer6a = [[],[],[],[],[]]\n",
    "\n",
    "weight1 = [[],[],[],[],[]]; weight2 = [[],[],[],[],[]]; weight3 = [[],[],[],[],[]];\n",
    "weight4 = [[],[],[],[],[]]; weight5 = [[],[],[],[],[]]; weight6 = [[],[],[],[],[]];\n",
    "\n",
    "gradw1  = [[],[],[],[],[]]; gradw2  = [[],[],[],[],[]]; gradw3  = [[],[],[],[],[]];\n",
    "gradw4  = [[],[],[],[],[]]; gradw5  = [[],[],[],[],[]]; gradw6  = [[],[],[],[],[]];\n",
    "\n",
    "gradp1  = [[],[],[],[],[]]; gradp2  = [[],[],[],[],[]]; gradp3  = [[],[],[],[],[]];\n",
    "gradp4  = [[],[],[],[],[]]; gradp5  = [[],[],[],[],[]]; gradp6  = [[],[],[],[],[]];\n",
    "\n",
    "gradup1  = [[],[],[],[],[]]; gradup2  = [[],[],[],[],[]]; gradup3  = [[],[],[],[],[]];\n",
    "gradup4  = [[],[],[],[],[]]; gradup5  = [[],[],[],[],[]]; gradup6  = [[],[],[],[],[]];\n",
    "\n",
    "list_of_outputs = [\n",
    "    layer1,layer2,layer3,layer4,layer5,layer6,\n",
    "    layer1a,layer2a,layer3a,layer4a,layer5a,layer6a,\n",
    "    l1.getw(),l2.getw(),l3.getw(),l4.getw(),l5.getw(),l6.getw(),\n",
    "    grad1w,grad2w,grad3w,grad4w,grad5w,grad6w,\n",
    "    grad1p,grad2p,grad3p,grad4p,grad5p,grad6p,\n",
    "    grad1_up[0],grad2_up[0],grad3_up[0],grad4_up[0],grad5_up[0],grad6_up[0]\n",
    "]\n",
    "\n",
    "for iter in range(num_epoch):\n",
    "\n",
    "    # Training Accuracy    \n",
    "    for current_batch_index in range(0,len(train_images),batch_size):\n",
    "        current_data  = train_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = train_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy,gradient_update],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(train_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_train = avg_acc_train + sess_results[0]\n",
    "        \n",
    "    # get the results\n",
    "    mid_stat = sess.run(list_of_outputs,feed_dict={x:current_data,y:current_label})\n",
    "    \n",
    "    # Test Accuracy    \n",
    "    for current_batch_index in range(0,len(test_images), batch_size):\n",
    "        current_data  = test_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = test_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(test_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_test = avg_acc_test + sess_results[0]   \n",
    "        \n",
    "    # ======================== extract stats ========================\n",
    "    llayer1 = append_stat(llayer1,mid_stat,0);  llayer2 = append_stat(llayer2,mid_stat,1);  llayer3 = append_stat(llayer3,mid_stat,2);\n",
    "    llayer4 = append_stat(llayer4,mid_stat,3);  llayer5 = append_stat(llayer5,mid_stat,4);  llayer6 = append_stat(llayer6,mid_stat,5);\n",
    "\n",
    "    llayer1a = append_stat(llayer1a,mid_stat,6);  llayer2a = append_stat(llayer2a,mid_stat,7);  llayer3a = append_stat(llayer3a,mid_stat,8);\n",
    "    llayer4a = append_stat(llayer4a,mid_stat,9);  llayer5a = append_stat(llayer5a,mid_stat,10); llayer6a = append_stat(llayer6a,mid_stat,11);\n",
    "    \n",
    "    weight1 = append_stat(weight1,mid_stat,12);  weight2 = append_stat(weight2,mid_stat,13);  weight3 = append_stat(weight3,mid_stat,14);\n",
    "    weight4 = append_stat(weight4,mid_stat,15);  weight5 = append_stat(weight5,mid_stat,16);  weight6 = append_stat(weight6,mid_stat,17);\n",
    "    \n",
    "    gradw1 = append_stat(gradw1,mid_stat,18); gradw2 = append_stat(gradw2,mid_stat,19); gradw3 = append_stat(gradw3,mid_stat,20);\n",
    "    gradw4 = append_stat(gradw4,mid_stat,21); gradw5 = append_stat(gradw5,mid_stat,22); gradw6 = append_stat(gradw6,mid_stat,23);\n",
    "    \n",
    "    gradp1 = append_stat(gradp1,mid_stat,24); gradp2 = append_stat(gradp2,mid_stat,25); gradp3 = append_stat(gradp3,mid_stat,26);\n",
    "    gradp4 = append_stat(gradp4,mid_stat,27); gradp5 = append_stat(gradp5,mid_stat,28); gradp6 = append_stat(gradp6,mid_stat,29);\n",
    "\n",
    "    gradup1 = append_stat(gradup1,mid_stat,30); gradup2 = append_stat(gradup2,mid_stat,31); gradup3 = append_stat(gradup3,mid_stat,32);\n",
    "    gradup4 = append_stat(gradup4,mid_stat,33); gradup5 = append_stat(gradup5,mid_stat,34); gradup6 = append_stat(gradup6,mid_stat,35);\n",
    "\n",
    "    train_acc.append(avg_acc_train/(len(train_images)/batch_size))\n",
    "    test_acc .append(avg_acc_test / (len(test_images)/batch_size))\n",
    "    # ======================== extract stats ========================\n",
    "    \n",
    "    # ======================== save to image ========================\n",
    "    save_to_image(mid_stat[0:6]   ,llayer1,llayer2,llayer3,llayer4,llayer5,llayer6,\"layer\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[6:12]  ,llayer1a,llayer2a,llayer3a,llayer4a,llayer5a,llayer6a,\"layera\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[12:18] ,weight1,weight2,weight3,weight4,weight5,weight6,\"weights\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[18:24] ,gradw1,gradw2,gradw3,gradw4,gradw5,gradw6,\"gradientw\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[24:30] ,gradp1,gradp2,gradp3,gradp4,gradp5,gradp6,\"gradientp\",train_acc,test_acc,current_exp_name,iter)\n",
    "    save_to_image(mid_stat[30:36] ,gradup1,gradup2,gradup3,gradup4,gradup5,gradup6,\"moment\",train_acc,test_acc,current_exp_name,iter)\n",
    "    # ======================== save to image ========================\n",
    "        \n",
    "    # ======================== print reset ========================\n",
    "    print(\"Current : \"+ str(iter) + \" Train Acc : \" + str(avg_acc_train/(len(train_images)/batch_size)) + \" Test Acc : \" + str(avg_acc_test/(len(test_images)/batch_size)) + '\\n')\n",
    "    avg_acc_train = 0 ; avg_acc_test  = 0\n",
    "    # ======================== print reset ========================\n",
    "\n",
    "np.save(current_exp_name+'/train_acc.npy',train_acc); np.save(current_exp_name+'/test_acc.npy', test_acc)    \n",
    "np.save(current_exp_name+'/llayer1.npy', llayer1);  np.save(current_exp_name+'/llayer2.npy', llayer2);  np.save(current_exp_name+'/llayer3.npy', llayer3); \n",
    "np.save(current_exp_name+'/llayer4.npy', llayer4);  np.save(current_exp_name+'/llayer5.npy', llayer5);  np.save(current_exp_name+'/llayer6.npy', llayer6); \n",
    "\n",
    "np.save(current_exp_name+'/llayer1a.npy', llayer1a);  np.save(current_exp_name+'/llayer2a.npy', llayer2a);  np.save(current_exp_name+'/llayer3a.npy', llayer3a); \n",
    "np.save(current_exp_name+'/llayer4a.npy', llayer4a);  np.save(current_exp_name+'/llayer5a.npy', llayer5a);  np.save(current_exp_name+'/llayer6a.npy', llayer6a); \n",
    "\n",
    "np.save(current_exp_name+'/weight1.npy', weight1);  np.save(current_exp_name+'/weight2.npy', weight2);  np.save(current_exp_name+'/weight3.npy', weight3);  \n",
    "np.save(current_exp_name+'/weight4.npy', weight4);  np.save(current_exp_name+'/weight5.npy', weight5);  np.save(current_exp_name+'/weight6.npy', weight6);  \n",
    "\n",
    "np.save(current_exp_name+'/gradw1.npy', gradw1); np.save(current_exp_name+'/gradw2.npy', gradw2); np.save(current_exp_name+'/gradw3.npy', gradw3);\n",
    "np.save(current_exp_name+'/gradw4.npy', gradw4); np.save(current_exp_name+'/gradw5.npy', gradw5); np.save(current_exp_name+'/gradw6.npy', gradw6);\n",
    "\n",
    "np.save(current_exp_name+'/gradp1.npy', gradp1); np.save(current_exp_name+'/gradp2.npy', gradp2); np.save(current_exp_name+'/gradp3.npy', gradp3);\n",
    "np.save(current_exp_name+'/gradp4.npy', gradp4); np.save(current_exp_name+'/gradp5.npy', gradp5); np.save(current_exp_name+'/gradp6.npy', gradp6);\n",
    "\n",
    "np.save(current_exp_name+'/gradup1.npy', gradup1); np.save(current_exp_name+'/gradup2.npy', gradup2); np.save(current_exp_name+'/gradup3.npy', gradup3);\n",
    "np.save(current_exp_name+'/gradup4.npy', gradup4); np.save(current_exp_name+'/gradup5.npy', gradup5); np.save(current_exp_name+'/gradup6.npy', gradup6);\n",
    "\n",
    "sess.close(); tf.reset_default_graph();\n",
    "\n",
    "%reset_selective -f l1,l2,l3,l4,l5,l6\n",
    "%reset_selective -f layer1,layer2,layer3,layer4,layer5,layer6\n",
    "%reset_selective -f layer1a,layer2a,layer3a,layer4a,layer5a,layer6a\n",
    "%reset_selective -f train_acc,test_acc,llayer1,llayer2,llayer3,llayer4,llayer5,llayer6,llayer1a,llayer2a,llayer3a,llayer4a,llayer5a,llayer6a\n",
    "%reset_selective -f weight1,weight2,weight3,weight4,weight5,weight6\n",
    "%reset_selective -f gradw1,gradw2,gradw3,gradw4,gradw5,gradw6\n",
    "%reset_selective -f gradp1,gradp2,gradp3,gradp4,gradp5,gradp6\n",
    "%reset_selective -f gradup1,gradup2,gradup3,gradup4,gradup5,gradup6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-05T10:32:13.164960Z",
     "start_time": "2019-01-05T10:32:12.622203Z"
    }
   },
   "outputs": [],
   "source": [
    "! start . "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
