{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T08:32:44.880984Z",
     "start_time": "2019-02-03T08:32:28.718871Z"
    },
    "code_folding": [
     0,
     34,
     61,
     69,
     76
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 96, 96, 3) 1.0 0.0\n",
      "(5000, 10) 1.0 0.0\n",
      "(8000, 96, 96, 3) 1.0 0.0\n",
      "(8000, 10) 1.0 0.0\n"
     ]
    }
   ],
   "source": [
    "# import Library and some random image data set\n",
    "import tensorflow as tf\n",
    "import numpy      as np\n",
    "import seaborn    as sns \n",
    "import pandas     as pd\n",
    "import os,sys\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "np.random.seed(78); tf.set_random_seed(78)\n",
    "\n",
    "# get some of the STL data set\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from skimage import util \n",
    "from skimage.transform import resize\n",
    "from skimage.io import imread\n",
    "import warnings\n",
    "from numpy import inf\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from scipy.stats import kurtosis,skew\n",
    "\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import gc\n",
    "from IPython.display import display, clear_output\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from matplotlib import animation\n",
    "# %load_ext jupyternotify\n",
    "\n",
    "# read all of the data (STL 10) https://github.com/mttk/STL10\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "def read_all_images(path_to_data):\n",
    "    \"\"\"\n",
    "    :param path_to_data: the file containing the binary images from the STL-10 dataset\n",
    "    :return: an array containing all the images\n",
    "    \"\"\"\n",
    "\n",
    "    with open(path_to_data, 'rb') as f:\n",
    "        # read whole file in uint8 chunks\n",
    "        everything = np.fromfile(f, dtype=np.uint8)\n",
    "\n",
    "        # We force the data into 3x96x96 chunks, since the\n",
    "        # images are stored in \"column-major order\", meaning\n",
    "        # that \"the first 96*96 values are the red channel,\n",
    "        # the next 96*96 are green, and the last are blue.\"\n",
    "        # The -1 is since the size of the pictures depends\n",
    "        # on the input file, and this way numpy determines\n",
    "        # the size on its own.\n",
    "\n",
    "        images = np.reshape(everything, (-1, 3, 96, 96))\n",
    "\n",
    "        # Now transpose the images into a standard image format\n",
    "        # readable by, for example, matplotlib.imshow\n",
    "        # You might want to comment this line or reverse the shuffle\n",
    "        # if you will use a learning algorithm like CNN, since they like\n",
    "        # their channels separated.\n",
    "        images = np.transpose(images, (0, 3, 2, 1))\n",
    "        return images\n",
    "def read_labels(path_to_labels):\n",
    "    \"\"\"\n",
    "    :param path_to_labels: path to the binary file containing labels from the STL-10 dataset\n",
    "    :return: an array containing the labels\n",
    "    \"\"\"\n",
    "    with open(path_to_labels, 'rb') as f:\n",
    "        labels = np.fromfile(f, dtype=np.uint8)\n",
    "        return labels\n",
    "def show_images(data,row=1,col=1):\n",
    "    fig=plt.figure(figsize=(10,10))\n",
    "    columns = col; rows = row\n",
    "    for i in range(1, columns*rows +1):\n",
    "        fig.add_subplot(rows, columns, i)\n",
    "        plt.imshow(data[i-1])\n",
    "    plt.show()\n",
    "def send_notification_email(letter,episode):\n",
    "    import smtplib, ssl\n",
    "\n",
    "    port = 587  # For starttls\n",
    "    smtp_server = \"smtp.gmail.com\"\n",
    "    sender_email = \"sendresultsforme@gmail.com\"\n",
    "    receiver_email = \"jae.duk.seo@gmail.com\"\n",
    "    password = \"Password123*\"\n",
    "    message = \"Subject: \" + str(letter) + \" : \"+str(episode)+\" is done!\"\n",
    "\n",
    "    context = ssl.create_default_context()\n",
    "    with smtplib.SMTP(smtp_server, port) as server:\n",
    "        server.ehlo()  # Can be omitted\n",
    "        server.starttls(context=context)\n",
    "        server.ehlo()  # Can be omitted\n",
    "        server.login(sender_email, password)\n",
    "        server.sendmail(sender_email, receiver_email, message)\n",
    "train_images = read_all_images(\"../../DataSet/STL10/stl10_binary/train_X.bin\") / 255.0\n",
    "train_labels = read_labels    (\"../../DataSet/STL10/stl10_binary/train_Y.bin\")\n",
    "test_images  = read_all_images(\"../../DataSet/STL10/stl10_binary/test_X.bin\")  / 255.0\n",
    "test_labels  = read_labels    (\"../../DataSet/STL10/stl10_binary/test_y.bin\")\n",
    "\n",
    "label_encoder= OneHotEncoder(sparse=False,categories='auto')\n",
    "train_labels = label_encoder.fit_transform(train_labels.reshape((-1,1)))\n",
    "test_labels  = label_encoder.fit_transform(test_labels.reshape((-1,1)))\n",
    "\n",
    "print(train_images.shape,train_images.max(),train_images.min())\n",
    "print(train_labels.shape,train_labels.max(),train_labels.min())\n",
    "print(test_images.shape,test_images.max(),test_images.min())\n",
    "print(test_labels.shape,test_labels.max(),test_labels.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T08:32:44.930904Z",
     "start_time": "2019-02-03T08:32:44.883976Z"
    },
    "code_folding": [
     0,
     7,
     9,
     16,
     57,
     99
    ]
   },
   "outputs": [],
   "source": [
    "# create the layers and the needed functions\n",
    "def tf_softmax(x): return tf.nn.softmax(x)\n",
    "def tf_relu(x):   return tf.nn.relu(x)\n",
    "def d_tf_relu(x): return tf.cast(tf.greater(x,0),tf.float32)\n",
    "def tf_iden(x): return x\n",
    "def d_tf_iden(x): return tf.ones_like(x)\n",
    "\n",
    "class CNN():\n",
    "\n",
    "    def __init__(self,k,inc,out, stddev=0.05,which_reg='A',act=tf_iden,d_act=d_tf_iden):\n",
    "        self.w              = tf.Variable(tf.random_normal([k,k,inc,out],stddev=stddev,seed=2,dtype=tf.float32))\n",
    "        self.m,self.v       = tf.Variable(tf.zeros_like(self.w)),tf.Variable(tf.zeros_like(self.w))\n",
    "        self.act,self.d_act = act,d_act\n",
    "        self.current_case   = which_reg\n",
    "        \n",
    "    def getw(self): return self.w\n",
    "    def feedforward(self,input,stride=1,padding='SAME',training_phase=True,std_value=0.0005):\n",
    "        self.input  = input\n",
    "        \n",
    "        if self.current_case == 'B':\n",
    "            def training_fn():  return tf.nn.dropout(tf.nn.conv2d(input,self.w,strides=[1,stride,stride,1],padding=padding),0.8)\n",
    "            def  testing_fn():  return tf.nn.conv2d(input,self.w,strides=[1,stride,stride,1],padding=padding) \n",
    "            self.layer  = tf.cond(training_phase,true_fn=training_fn,false_fn=testing_fn)\n",
    "            \n",
    "        elif self.current_case == 'E':\n",
    "            def training_fn():  return tf.nn.conv2d(input,self.w,strides=[1,stride,stride,1],padding=padding) \n",
    "            def  testing_fn():\n",
    "                sampled_weight = tf.squeeze(tf.distributions.Normal(loc=self.w, scale=std_value).sample(1))\n",
    "                return tf.nn.conv2d(input,sampled_weight,strides=[1,stride,stride,1],padding=padding) \n",
    "            self.layer  = tf.cond(training_phase,true_fn=training_fn,false_fn=testing_fn)\n",
    "            \n",
    "        else: self.layer = tf.nn.conv2d(input,self.w,strides=[1,stride,stride,1],padding=padding) \n",
    "        \n",
    "        self.layerA = self.act(self.layer)\n",
    "        return self.layer, self.layerA\n",
    "    \n",
    "    def backprop(self,gradient,std_value,stride=1,padding='SAME'):\n",
    "        grad_part_1 = gradient\n",
    "        grad_part_2 = self.d_act(self.layer)\n",
    "        grad_part_3 = self.input\n",
    "\n",
    "        grad_middle = grad_part_1 * grad_part_2\n",
    "        grad        = tf.nn.conv2d_backprop_filter(input = grad_part_3,filter_sizes = tf.shape(self.w),  out_backprop = grad_middle,strides=[1,stride,stride,1],padding=padding) \n",
    "        grad_pass   = tf.nn.conv2d_backprop_input (input_sizes = tf.shape(self.input),filter= self.w,out_backprop = grad_middle,strides=[1,stride,stride,1],padding=padding)\n",
    "        \n",
    "        if self.current_case == 'D' or self.current_case == 'E': grad = tf.squeeze(tf.distributions.Normal(loc=grad, scale=std_value).sample(1))\n",
    "        \n",
    "        update_w = []\n",
    "        update_w.append(tf.assign( self.m,self.m*beta1 + (1-beta1) * (grad)   ))\n",
    "        update_w.append(tf.assign( self.v,self.v*beta2 + (1-beta2) * (grad ** 2)   ))\n",
    "        m_hat = self.m / (1-beta1) ; v_hat = self.v / (1-beta2)\n",
    "        adam_middle = m_hat * learning_rate/(tf.sqrt(v_hat) + adam_e)\n",
    "        \n",
    "        if self.current_case == 'C' or self.current_case == 'D' or self.current_case == 'E': adam_middle = tf.squeeze(tf.distributions.Normal(loc=adam_middle, scale=std_value).sample(1))\n",
    "        \n",
    "        update_w.append(tf.assign(self.w,tf.subtract(self.w,adam_middle  )))\n",
    "        return grad_pass,grad,update_w\n",
    "class RELU_as_Reg():\n",
    "    \n",
    "    def __init__(self,batch,width,channel,regularizer):\n",
    "        self.w = tf.Variable(tf.ones([batch,width,width,channel],tf.float32) )\n",
    "        self.m,self.v       = tf.Variable(tf.zeros_like(self.w)),tf.Variable(tf.zeros_like(self.w))\n",
    "        self.regularizer = regularizer\n",
    "        self.lamda = 0.0001\n",
    "    \n",
    "    def feedforward(self,input):\n",
    "        self.input  = input\n",
    "        self.layerA = self.w * input\n",
    "        return self.layerA\n",
    "    \n",
    "    def backprop(self,gradient):\n",
    "        grad = gradient * self.input\n",
    "        gradient_p = self.w * gradient\n",
    "        \n",
    "        # add reg here\n",
    "        if self.regularizer == 'A': grad = grad + self.lamda * tf.sign(self.w)\n",
    "        if self.regularizer == 'B': grad = grad + self.lamda * 2.0 * self.w\n",
    "        if self.regularizer == 'C': grad = grad + self.lamda * (1.0/tf.sqrt(tf.square(self.w)+ 1e-5)) * self.w\n",
    "        if self.regularizer == 'D': grad = grad + self.lamda * -(2*self.w)/(1 + self.w**2)\n",
    "        if self.regularizer == 'E': grad = grad + self.lamda * -(1-tf.tanh(self.w) ** 2)\n",
    "        if self.regularizer == 'F': grad = grad + self.lamda * -(1-tf.tanh(self.w** 2) ** 2) * 2.0 * self.w \n",
    "        if self.regularizer == 'G': grad = grad + self.lamda * -(1-tf.tanh(tf.abs(self.w)) ** 2) * tf.sign(self.w)\n",
    "        if self.regularizer == 'H': grad = grad + self.lamda * -(1-tf.tanh(tf.abs(self.w)** 2) ** 2) * 2.0 * tf.abs(self.w) *  tf.sign(self.w)\n",
    "        if self.regularizer == 'I': grad = grad + self.lamda * tf.cos(self.w)\n",
    "        if self.regularizer == 'J': grad = grad + self.lamda * tf.sign(tf.sin(self.w)) * tf.cos(self.w)\n",
    "        if self.regularizer == 'K': grad = grad + self.lamda * (2)/(self.w + 1e-5)\n",
    "        if self.regularizer == 'L': grad = grad + self.lamda * (tf.log(self.w**2 + 1e-5) + 2.0)\n",
    "        # add reg here\n",
    "\n",
    "        update_w = []\n",
    "        update_w.append(tf.assign( self.m,self.m*beta1 + (1-beta1) * (grad)   ))\n",
    "        update_w.append(tf.assign( self.v,self.v*beta2 + (1-beta2) * (grad ** 2)   ))\n",
    "        m_hat = self.m / (1-beta1) ; v_hat = self.v / (1-beta2)\n",
    "        adam_middle = m_hat * learning_rate/(tf.sqrt(v_hat) + adam_e)\n",
    "        \n",
    "        gradient_temp = tf.clip_by_value(self.w - adam_middle,clip_value_min=0.0,clip_value_max=1.0)\n",
    "        update_w.append(self.w.assign(gradient_temp))\n",
    "        \n",
    "        return gradient_p,update_w\n",
    "class tf_batch_norm_layer():\n",
    "    \n",
    "    def __init__(self,vector_shape,axis):\n",
    "        self.moving_mean = tf.Variable(tf.zeros(shape=[1,1,1,vector_shape],dtype=tf.float32))\n",
    "        self.moving_vari = tf.Variable(tf.zeros(shape=[1,1,1,vector_shape],dtype=tf.float32))\n",
    "        self.axis        = axis\n",
    "    def feedforward(self,input,training_phase=True,eps = 1e-8):\n",
    "        self.input = input\n",
    "        self.input_size          = self.input.shape\n",
    "        self.batch,self.h,self.w,self.c = self.input_size[0].value,self.input_size[1].value,self.input_size[2].value,self.input_size[3].value\n",
    "\n",
    "        # Training Moving Average Mean         \n",
    "        def training_fn():\n",
    "            self.mean    = tf.reduce_mean(self.input,axis=self.axis ,keepdims=True)\n",
    "            self.var     = tf.reduce_mean(tf.square(self.input-self.mean),axis=self.axis,keepdims=True)\n",
    "            centered_data= (self.input - self.mean)/tf.sqrt(self.var + eps)\n",
    "            \n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean*0.9 + 0.1 * self.mean ))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari*0.9 + 0.1 * self.var  ))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        # Testing Moving Average Mean        \n",
    "        def  testing_fn():\n",
    "            centered_data   = (self.input - self.moving_mean)/tf.sqrt(self.moving_vari + eps)\n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        self.output,update_variable = tf.cond(training_phase,true_fn=training_fn,false_fn=testing_fn)\n",
    "        return self.output,update_variable\n",
    "    def backprop(self,grad,eps = 1e-8):\n",
    "        change_parts = 1.0 /(self.batch * self.h * self.w)\n",
    "        grad_sigma   = tf.reduce_sum( grad *  (self.input-self.mean)     ,axis=self.axis,keepdims=True) * -0.5 * (self.var+eps) ** -1.5\n",
    "        grad_mean    = tf.reduce_sum( grad *  (-1./tf.sqrt(self.var+eps)),axis=self.axis,keepdims=True) + grad_sigma * change_parts * 2.0 * tf.reduce_sum((self.input-self.mean),axis=self.axis,keepdims=True) * -1\n",
    "        grad_x       = grad * 1/(tf.sqrt(self.var+eps)) + grad_sigma * change_parts * 2.0 * (self.input-self.mean) + grad_mean * change_parts\n",
    "        return grad_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T08:32:44.945055Z",
     "start_time": "2019-02-03T08:32:44.931850Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# set hyper parameter\n",
    "plt.rcParams.update({'font.size': 25})\n",
    "num_eps   = 5; num_epoch = 200; learning_rate = 0.0008; batch_size = 20; \n",
    "beta1,beta2,adam_e = 0.9,0.999,1e-9; \n",
    "print_iter = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-03T08:32:28.721Z"
    },
    "code_folding": [
     0,
     13,
     91,
     103,
     117
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        ================================================\n",
      "                    Starting Episode: 0 for A\n",
      "        ================================================\n",
      "\n",
      "Current : 0\t Train Acc : 0.144\t Test Acc : 0.163\t STD : 0.0005\t50005\n",
      "Current : 10\t Train Acc : 0.313\t Test Acc : 0.303\t STD : 0.0005\t5\n",
      "Current : 20\t Train Acc : 0.35\t Test Acc : 0.33\t STD : 0.0005\t555\n",
      "Current : 30\t Train Acc : 0.368\t Test Acc : 0.35\t STD : 0.0005\t55\n",
      "Current : 40\t Train Acc : 0.377\t Test Acc : 0.358\t STD : 0.0005\t5\n",
      "Current : 50\t Train Acc : 0.399\t Test Acc : 0.361\t STD : 0.0005\t5\n",
      "Current : 60\t Train Acc : 0.409\t Test Acc : 0.386\t STD : 0.0005\t5\n",
      "Current : 70\t Train Acc : 0.425\t Test Acc : 0.39\t STD : 0.0005\t55\n",
      "Current : 80\t Train Acc : 0.434\t Test Acc : 0.401\t STD : 0.0005\t5\n",
      "Current : 90\t Train Acc : 0.442\t Test Acc : 0.399\t STD : 0.0005\t5\n",
      "Current : 100\t Train Acc : 0.452\t Test Acc : 0.408\t STD : 0.0005\t5\n",
      "Current : 110\t Train Acc : 0.466\t Test Acc : 0.414\t STD : 0.0005\t5\n",
      "Current : 120\t Train Acc : 0.467\t Test Acc : 0.417\t STD : 0.0005\t5\n",
      "Current : 130\t Train Acc : 0.476\t Test Acc : 0.402\t STD : 0.0005\t5\n",
      "Current : 140\t Train Acc : 0.481\t Test Acc : 0.419\t STD : 0.0005\t5\n",
      "Current : 150\t Train Acc : 0.488\t Test Acc : 0.423\t STD : 0.0005\t5\n",
      "Current : 160\t Train Acc : 0.499\t Test Acc : 0.422\t STD : 0.0005\t5\n",
      "Current : 170\t Train Acc : 0.502\t Test Acc : 0.422\t STD : 0.0005\t5\n",
      "Current : 180\t Train Acc : 0.507\t Test Acc : 0.414\t STD : 0.0005\t5\n",
      "Current : 190\t Train Acc : 0.52\t Test Acc : 0.414\t STD : 0.0005\t55\n",
      "Current : 199\t Train Acc : 0.521\t Test Acc : 0.419\t STD : 0.0005\t5\n",
      "\n",
      "        ================================================\n",
      "                    Starting Episode: 1 for A\n",
      "        ================================================\n",
      "\n",
      "Current : 0\t Train Acc : 0.141\t Test Acc : 0.169\t STD : 0.0005\t5.0005\n",
      "Current : 10\t Train Acc : 0.3\t Test Acc : 0.282\t STD : 0.0005\t555\n",
      "Current : 20\t Train Acc : 0.334\t Test Acc : 0.34\t STD : 0.0005\t55\n",
      "Current : 30\t Train Acc : 0.365\t Test Acc : 0.355\t STD : 0.0005\t5\n",
      "Current : 40\t Train Acc : 0.376\t Test Acc : 0.354\t STD : 0.0005\t5\n",
      "Current : 50\t Train Acc : 0.385\t Test Acc : 0.378\t STD : 0.0005\t5\n",
      "Current : 60\t Train Acc : 0.401\t Test Acc : 0.372\t STD : 0.0005\t5\n",
      "Current : 70\t Train Acc : 0.413\t Test Acc : 0.37\t STD : 0.0005\t55\n",
      "Current : 80\t Train Acc : 0.418\t Test Acc : 0.379\t STD : 0.0005\t5\n",
      "Current : 90\t Train Acc : 0.422\t Test Acc : 0.393\t STD : 0.0005\t5\n",
      "Current : 100\t Train Acc : 0.44\t Test Acc : 0.393\t STD : 0.0005\t55\n",
      "Current : 110\t Train Acc : 0.444\t Test Acc : 0.4\t STD : 0.0005\t555\n",
      "Current : 120\t Train Acc : 0.455\t Test Acc : 0.389\t STD : 0.0005\t5\n",
      "Current : 130\t Train Acc : 0.463\t Test Acc : 0.399\t STD : 0.0005\t5\n",
      "Current : 140\t Train Acc : 0.465\t Test Acc : 0.399\t STD : 0.0005\t5\n",
      "Current : 150\t Train Acc : 0.483\t Test Acc : 0.404\t STD : 0.0005\t5\n",
      "Current : 160\t Train Acc : 0.491\t Test Acc : 0.401\t STD : 0.0005\t5\n",
      "Current : 170\t Train Acc : 0.495\t Test Acc : 0.406\t STD : 0.0005\t5\n",
      "Current : 180\t Train Acc : 0.504\t Test Acc : 0.404\t STD : 0.0005\t5\n",
      "Current : 190\t Train Acc : 0.509\t Test Acc : 0.412\t STD : 0.0005\t5\n",
      "Current : 199\t Train Acc : 0.523\t Test Acc : 0.398\t STD : 0.0005\t5\n",
      "\n",
      "        ================================================\n",
      "                    Starting Episode: 2 for A\n",
      "        ================================================\n",
      "\n",
      "Current : 0\t Train Acc : 0.141\t Test Acc : 0.203\t STD : 0.0005\t5.0005\n",
      "Current : 10\t Train Acc : 0.31\t Test Acc : 0.317\t STD : 0.0005\t55\n",
      "Current : 20\t Train Acc : 0.359\t Test Acc : 0.346\t STD : 0.0005\t5\n",
      "Current : 30\t Train Acc : 0.385\t Test Acc : 0.366\t STD : 0.0005\t5\n",
      "Current : 40\t Train Acc : 0.4\t Test Acc : 0.37\t STD : 0.0005\t0555\n",
      "Current : 50\t Train Acc : 0.408\t Test Acc : 0.38\t STD : 0.0005\t55\n",
      "Current : 60\t Train Acc : 0.419\t Test Acc : 0.388\t STD : 0.0005\t5\n",
      "Current : 70\t Train Acc : 0.437\t Test Acc : 0.386\t STD : 0.0005\t5\n",
      "Current : 80\t Train Acc : 0.431\t Test Acc : 0.395\t STD : 0.0005\t5\n",
      "Current : 90\t Train Acc : 0.453\t Test Acc : 0.405\t STD : 0.0005\t5\n",
      "Current : 100\t Train Acc : 0.457\t Test Acc : 0.399\t STD : 0.0005\t5\n",
      "Current : 110\t Train Acc : 0.469\t Test Acc : 0.396\t STD : 0.0005\t5\n",
      "Current : 120\t Train Acc : 0.479\t Test Acc : 0.406\t STD : 0.0005\t5\n",
      "Current : 130\t Train Acc : 0.484\t Test Acc : 0.41\t STD : 0.0005\t55\n",
      "Current : 140\t Train Acc : 0.489\t Test Acc : 0.414\t STD : 0.0005\t5\n",
      "Current : 150\t Train Acc : 0.505\t Test Acc : 0.41\t STD : 0.0005\t55\n",
      "Current : 160\t Train Acc : 0.511\t Test Acc : 0.414\t STD : 0.0005\t5\n",
      "Current : 170\t Train Acc : 0.516\t Test Acc : 0.41\t STD : 0.0005\t55\n",
      "Current : 180\t Train Acc : 0.527\t Test Acc : 0.416\t STD : 0.0005\t5\n",
      "Current : 190\t Train Acc : 0.534\t Test Acc : 0.416\t STD : 0.0005\t5\n",
      "Current : 199\t Train Acc : 0.55\t Test Acc : 0.412\t STD : 0.0005\t55\n",
      "\n",
      "        ================================================\n",
      "                    Starting Episode: 3 for A\n",
      "        ================================================\n",
      "\n",
      "Current : 0\t Train Acc : 0.133\t Test Acc : 0.172\t STD : 0.0005\t50005\n",
      "Current : 10\t Train Acc : 0.267\t Test Acc : 0.271\t STD : 0.0005\t5\n",
      "Current : 20\t Train Acc : 0.33\t Test Acc : 0.334\t STD : 0.0005\t55\n",
      "Current : 30\t Train Acc : 0.358\t Test Acc : 0.352\t STD : 0.0005\t5\n",
      "Current : 40\t Train Acc : 0.372\t Test Acc : 0.355\t STD : 0.0005\t5\n",
      "Current : 50\t Train Acc : 0.386\t Test Acc : 0.358\t STD : 0.0005\t5\n",
      "Current : 60\t Train Acc : 0.395\t Test Acc : 0.361\t STD : 0.0005\t5\n",
      "Current : 70\t Train Acc : 0.404\t Test Acc : 0.379\t STD : 0.0005\t5\n",
      "Current : 80\t Train Acc : 0.418\t Test Acc : 0.379\t STD : 0.0005\t5\n",
      "Current : 90\t Train Acc : 0.415\t Test Acc : 0.384\t STD : 0.0005\t5\n",
      "Current : 100\t Train Acc : 0.43\t Test Acc : 0.389\t STD : 0.0005\t55\n",
      "Current : 110\t Train Acc : 0.437\t Test Acc : 0.385\t STD : 0.0005\t5\n",
      "Current : 120\t Train Acc : 0.452\t Test Acc : 0.393\t STD : 0.0005\t5\n",
      "Current : 130\t Train Acc : 0.456\t Test Acc : 0.392\t STD : 0.0005\t5\n",
      "Current : 140\t Train Acc : 0.47\t Test Acc : 0.399\t STD : 0.0005\t55\n",
      "Current : 150\t Train Acc : 0.463\t Test Acc : 0.39\t STD : 0.0005\t55\n",
      "Current : 160\t Train Acc : 0.476\t Test Acc : 0.392\t STD : 0.0005\t5\n",
      "Current : 170\t Train Acc : 0.486\t Test Acc : 0.398\t STD : 0.0005\t5\n",
      "Current : 180\t Train Acc : 0.496\t Test Acc : 0.396\t STD : 0.0005\t5\n",
      "Current : 190\t Train Acc : 0.498\t Test Acc : 0.407\t STD : 0.0005\t5\n",
      "Current : 199\t Train Acc : 0.513\t Test Acc : 0.398\t STD : 0.0005\t5\n",
      "\n",
      "        ================================================\n",
      "                    Starting Episode: 4 for A\n",
      "        ================================================\n",
      "\n",
      "Current : 0\t Train Acc : 0.127\t Test Acc : 0.167\t STD : 0.0005\t50005\n",
      "Current : 10\t Train Acc : 0.295\t Test Acc : 0.289\t STD : 0.0005\t5\n",
      "Current : 20\t Train Acc : 0.348\t Test Acc : 0.334\t STD : 0.0005\t5\n",
      "Current : 30\t Train Acc : 0.364\t Test Acc : 0.35\t STD : 0.0005\t55\n",
      "Current : 40\t Train Acc : 0.383\t Test Acc : 0.369\t STD : 0.0005\t5\n",
      "Current : 50\t Train Acc : 0.387\t Test Acc : 0.384\t STD : 0.0005\t5\n",
      "Current : 60\t Train Acc : 0.41\t Test Acc : 0.377\t STD : 0.0005\t55\n",
      "Current : 70\t Train Acc : 0.416\t Test Acc : 0.392\t STD : 0.0005\t5\n",
      "Current : 80\t Train Acc : 0.433\t Test Acc : 0.397\t STD : 0.0005\t5\n",
      "Current : 90\t Train Acc : 0.43\t Test Acc : 0.406\t STD : 0.0005\t55\n",
      "Current : 100\t Train Acc : 0.451\t Test Acc : 0.403\t STD : 0.0005\t5\n",
      "Current : 110\t Train Acc : 0.455\t Test Acc : 0.405\t STD : 0.0005\t5\n",
      "Current : 120\t Train Acc : 0.468\t Test Acc : 0.41\t STD : 0.0005\t55\n",
      "Current : 130\t Train Acc : 0.475\t Test Acc : 0.413\t STD : 0.0005\t5\n",
      "Current : 140\t Train Acc : 0.489\t Test Acc : 0.408\t STD : 0.0005\t5\n",
      "Current : 150\t Train Acc : 0.497\t Test Acc : 0.42\t STD : 0.0005\t55\n",
      "Current : 160\t Train Acc : 0.502\t Test Acc : 0.419\t STD : 0.0005\t5\n",
      "Current : 170\t Train Acc : 0.514\t Test Acc : 0.428\t STD : 0.0005\t5\n",
      "Current : 180\t Train Acc : 0.516\t Test Acc : 0.417\t STD : 0.0005\t5\n",
      " Current Iter : 182/200\tbatch : 4380/5000\tacc : 0.4\tstd  : 0.00055\r"
     ]
    }
   ],
   "source": [
    "# no batch\n",
    "current_batch_norm_type = 'act_as_reg'\n",
    "all_the_exp = ['A','B','C','D','E','F','G','H','I','J','K','L']\n",
    "\n",
    "for letter in all_the_exp:\n",
    "    current_exp_name = letter\n",
    "    sess = tf.InteractiveSession()\n",
    "    current_exp_train_accuracy = np.zeros((num_eps,num_epoch))\n",
    "    current_exp_test_accuracy  = np.zeros((num_eps,num_epoch))\n",
    "    MAX_STD_VALUE = 0.0005\n",
    "\n",
    "    for episode in range(num_eps):\n",
    "        sys.stdout.write(\"\"\"\n",
    "        ================================================\n",
    "                    Starting Episode: \"\"\" + str(episode) + \" for \" + str(letter) + \"\"\"\n",
    "        ================================================\\n\n",
    "        \"\"\");sys.stdout.flush();\n",
    "\n",
    "        # create layers\n",
    "        l1 = CNN(3,3, 16,which_reg='D'); \n",
    "        l1_act = RELU_as_Reg(batch_size,48,16,regularizer=letter)\n",
    "        l2 = CNN(3,16,16,which_reg='D'); \n",
    "        l2_act = RELU_as_Reg(batch_size,24,16,regularizer=letter)\n",
    "        l3 = CNN(3,16,16,which_reg='D'); \n",
    "        l3_act = RELU_as_Reg(batch_size,12,16,regularizer=letter)\n",
    "        l4 = CNN(3,16,16,which_reg='D'); \n",
    "        l4_act = RELU_as_Reg(batch_size,6 ,16,regularizer=letter)\n",
    "        l5 = CNN(3,16,16,which_reg='D'); \n",
    "        l5_act = RELU_as_Reg(batch_size,6 ,16,regularizer=letter)\n",
    "        l6 = CNN(3,16,10,which_reg='D',act=tf_relu,d_act=d_tf_relu); \n",
    "\n",
    "        # 2. graph \n",
    "        x = tf.placeholder(tf.float32,(batch_size,96,96,3))\n",
    "        y = tf.placeholder(tf.float32,(batch_size,10))\n",
    "        is_train  = tf.placeholder_with_default(True,())\n",
    "        std_value = tf.placeholder(tf.float32)\n",
    "\n",
    "        layer1, layer1a = l1. feedforward(x,stride=2,training_phase=is_train,std_value=std_value)\n",
    "        layer1ar = l1_act.feedforward(layer1a)\n",
    "        layer2, layer2a = l2. feedforward(layer1ar,stride=2,training_phase=is_train,std_value=std_value)\n",
    "        layer2ar = l2_act.feedforward(layer2a)\n",
    "        layer3, layer3a = l3. feedforward(layer2ar,stride=2,training_phase=is_train,std_value=std_value)\n",
    "        layer3ar = l3_act.feedforward(layer3a)\n",
    "        layer4, layer4a = l4. feedforward(layer3ar,stride=2,training_phase=is_train,std_value=std_value)\n",
    "        layer4ar = l4_act.feedforward(layer4a)\n",
    "        layer5, layer5a = l5. feedforward(layer4ar,training_phase=is_train,std_value=std_value)\n",
    "        layer5ar = l5_act.feedforward(layer5a)\n",
    "        layer6, layer6a = l6. feedforward(layer5ar,training_phase=is_train,std_value=std_value)\n",
    "        \n",
    "        final_layer   = tf.reduce_mean(layer6a,(1,2))\n",
    "        final_softmax = tf_softmax(final_layer)\n",
    "        cost          = -tf.reduce_mean(y * tf.log(final_softmax + 1e-8))\n",
    "        correct_prediction = tf.equal(tf.argmax(final_softmax, 1), tf.argmax(y, 1))\n",
    "        accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "        gradient = tf.tile((final_softmax-y)[:,None,None,:],[1,6,6,1])/batch_size\n",
    "        grad6p,grad6w,grad6_up = l6.backprop(gradient,std_value=std_value)\n",
    "        \n",
    "        grad5ap,grad5ap_up     = l5_act.backprop(grad6p)\n",
    "        grad5p,grad5w,grad5_up = l5.backprop(grad5ap,std_value=std_value)\n",
    "        \n",
    "        grad4ap,grad4ap_up     = l4_act.backprop(grad5p)\n",
    "        grad4p,grad4w,grad4_up = l4.backprop(grad4ap,stride=2,std_value=std_value)\n",
    "        \n",
    "        grad3ap,grad3ap_up     = l3_act.backprop(grad4p)\n",
    "        grad3p,grad3w,grad3_up = l3.backprop(grad3ap,stride=2,std_value=std_value)\n",
    "\n",
    "        grad2ap,grad2ap_up     = l2_act.backprop(grad3p)\n",
    "        grad2p,grad2w,grad2_up = l2.backprop(grad2ap,stride=2,std_value=std_value)\n",
    "\n",
    "        grad1ap,grad1ap_up     = l1_act.backprop(grad2p)\n",
    "        grad1p,grad1w,grad1_up = l1.backprop(grad1ap,stride=2,std_value=std_value)\n",
    "\n",
    "        gradient_update = grad6_up + \\\n",
    "                          grad5ap_up + grad5_up + \\\n",
    "                          grad4ap_up + grad4_up + \\\n",
    "                          grad3ap_up + grad3_up + \\\n",
    "                          grad2ap_up + grad2_up + \\\n",
    "                          grad1ap_up + grad1_up \n",
    "\n",
    "        # train\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        avg_acc_train = 0; avg_acc_test  = 0; train_acc = [];test_acc = []\n",
    "\n",
    "        for iter in range(num_epoch):\n",
    "\n",
    "            # NOW WE ARE ADDING SHUFFLE!\n",
    "            current_std_value = MAX_STD_VALUE\n",
    "            train_images_shuffled,train_labels_shuffled = shuffle(train_images,train_labels)\n",
    "\n",
    "            # Training Accuracy    \n",
    "            for current_batch_index in range(0,len(train_images),batch_size):\n",
    "                current_data  = train_images_shuffled[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "                current_label = train_labels_shuffled[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "                sess_results  = sess.run([accuracy,gradient_update],feed_dict={x:current_data,y:current_label,std_value:current_std_value})\n",
    "                sys.stdout.write(' Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + \n",
    "                                 '\\tbatch : ' + str(current_batch_index) + '/'+ str(len(train_images)) + \n",
    "                                 '\\tacc : ' + str(sess_results[0]) +\n",
    "                                 '\\tstd  : ' + str(current_std_value) +\n",
    "                                 '\\r')\n",
    "                sys.stdout.flush(); avg_acc_train = avg_acc_train + sess_results[0]\n",
    "\n",
    "            # Test Accuracy    \n",
    "            for current_batch_index in range(0,len(test_images), batch_size):\n",
    "                current_data  = test_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "                current_label = test_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "                sess_results  = sess.run([accuracy],feed_dict={x:current_data,y:current_label,is_train:False,std_value:current_std_value})\n",
    "                sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + \n",
    "                                 '\\tbatch : ' + str(current_batch_index) + '/'+ str(len(test_images)) + \n",
    "                                 '\\tacc : ' + str(sess_results[0]) + \n",
    "                                 '\\tstd  : ' + str(current_std_value) +\n",
    "                                 '\\r')\n",
    "                sys.stdout.flush(); avg_acc_test = avg_acc_test + sess_results[0]   \n",
    "\n",
    "            # ======================== print reset ========================\n",
    "            train_acc.append(avg_acc_train/(len(train_images)/batch_size))\n",
    "            test_acc .append(avg_acc_test / (len(test_images)/batch_size))\n",
    "            if iter%print_iter == 0 or iter==num_epoch-1 :\n",
    "                sys.stdout.write(\"Current : \"+ str(iter) + \"\\t\" +\n",
    "                      \" Train Acc : \" + str(np.around(avg_acc_train/(len(train_images)/batch_size),3)) + \"\\t\" +\n",
    "                      \" Test Acc : \"  + str(np.around(avg_acc_test/(len(test_images)/batch_size),3)) +  \"\\t\" +\n",
    "                      \" STD : \"  + str(current_std_value) + \n",
    "                      \"\\t\\n\")\n",
    "                sys.stdout.flush();\n",
    "            avg_acc_train = 0 ; avg_acc_test  = 0\n",
    "            # ======================== print reset ========================\n",
    "            \n",
    "        # save the file\n",
    "        current_exp_train_accuracy[episode,:] = train_acc\n",
    "        current_exp_test_accuracy [episode,:] = test_acc\n",
    "        send_notification_email(letter,episode)\n",
    "\n",
    "    # close the session and save\n",
    "    sess.close()\n",
    "    # save to the file\n",
    "    np.save(str(current_batch_norm_type)+'/'+str(letter)+'/train.npy', current_exp_train_accuracy)\n",
    "    np.save(str(current_batch_norm_type)+'/'+str(letter)+'/test.npy', current_exp_test_accuracy)\n",
    "    print(current_exp_train_accuracy.shape,current_exp_train_accuracy.mean())\n",
    "    print(current_exp_test_accuracy.shape,current_exp_test_accuracy.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
