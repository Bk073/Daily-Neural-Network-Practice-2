{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T19:13:15.343501Z",
     "start_time": "2018-12-16T19:13:15.339487Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# import Library and some random image data set\n",
    "import tensorflow as tf\n",
    "import numpy      as np\n",
    "import seaborn    as sns \n",
    "import pandas     as pd\n",
    "import os,sys\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(78); tf.set_random_seed(78)\n",
    "\n",
    "# get some of the STL data set\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from skimage import util \n",
    "from skimage.transform import resize\n",
    "from skimage.io import imread\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from numpy import inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T19:08:58.432582Z",
     "start_time": "2018-12-16T19:08:56.425795Z"
    },
    "code_folding": [
     0,
     1,
     28
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 96, 96, 3) 1.0 0.0\n",
      "(5000, 10) 1.0 0.0\n",
      "(8000, 96, 96, 3) 1.0 0.0\n",
      "(8000, 10) 1.0 0.0\n"
     ]
    }
   ],
   "source": [
    "# read all of the data\n",
    "def read_all_images(path_to_data):\n",
    "    \"\"\"\n",
    "    :param path_to_data: the file containing the binary images from the STL-10 dataset\n",
    "    :return: an array containing all the images\n",
    "    \"\"\"\n",
    "\n",
    "    with open(path_to_data, 'rb') as f:\n",
    "        # read whole file in uint8 chunks\n",
    "        everything = np.fromfile(f, dtype=np.uint8)\n",
    "\n",
    "        # We force the data into 3x96x96 chunks, since the\n",
    "        # images are stored in \"column-major order\", meaning\n",
    "        # that \"the first 96*96 values are the red channel,\n",
    "        # the next 96*96 are green, and the last are blue.\"\n",
    "        # The -1 is since the size of the pictures depends\n",
    "        # on the input file, and this way numpy determines\n",
    "        # the size on its own.\n",
    "\n",
    "        images = np.reshape(everything, (-1, 3, 96, 96))\n",
    "\n",
    "        # Now transpose the images into a standard image format\n",
    "        # readable by, for example, matplotlib.imshow\n",
    "        # You might want to comment this line or reverse the shuffle\n",
    "        # if you will use a learning algorithm like CNN, since they like\n",
    "        # their channels separated.\n",
    "        images = np.transpose(images, (0, 3, 2, 1))\n",
    "        return images\n",
    "def read_labels(path_to_labels):\n",
    "    \"\"\"\n",
    "    :param path_to_labels: path to the binary file containing labels from the STL-10 dataset\n",
    "    :return: an array containing the labels\n",
    "    \"\"\"\n",
    "    with open(path_to_labels, 'rb') as f:\n",
    "        labels = np.fromfile(f, dtype=np.uint8)\n",
    "        return labels\n",
    "    \n",
    "train_images = read_all_images(\"../../../DataSet/STL10/stl10_binary/train_X.bin\") / 255.0\n",
    "train_labels = read_labels    (\"../../../DataSet/STL10/stl10_binary/train_Y.bin\")\n",
    "test_images  = read_all_images(\"../../../DataSet/STL10/stl10_binary/test_X.bin\")  / 255.0\n",
    "test_labels  = read_labels    (\"../../../DataSet/STL10/stl10_binary/test_y.bin\")\n",
    "\n",
    "label_encoder= OneHotEncoder(sparse=False,categories='auto')\n",
    "train_labels = label_encoder.fit_transform(train_labels.reshape((-1,1)))\n",
    "test_labels  = label_encoder.fit_transform(test_labels.reshape((-1,1)))\n",
    "\n",
    "print(train_images.shape,train_images.max(),train_images.min())\n",
    "print(train_labels.shape,train_labels.max(),train_labels.min())\n",
    "print(test_images.shape,test_images.max(),test_images.min())\n",
    "print(test_labels.shape,test_labels.max(),test_labels.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T19:08:58.565592Z",
     "start_time": "2018-12-16T19:08:58.483812Z"
    },
    "code_folding": [
     15,
     42,
     83,
     90,
     124,
     165,
     185,
     241,
     272
    ]
   },
   "outputs": [],
   "source": [
    "# create the layers\n",
    "def tf_softmax(x): return tf.nn.softmax(x)\n",
    "\n",
    "def tf_elu(x):   return tf.nn.elu(x)\n",
    "def d_tf_elu(x): return tf.cast(tf.greater(x,0),tf.float32)  + (tf_elu(tf.cast(tf.less_equal(x,0),tf.float32) * x) + 1.0)\n",
    "\n",
    "def tf_relu(x):   return tf.nn.relu(x)\n",
    "def d_tf_relu(x): return tf.cast(tf.greater(x,0),tf.float32)\n",
    "\n",
    "def tf_tanh(x):   return tf.nn.tanh(x)\n",
    "def d_tf_tanh(x): return 1 - tf_tanh(x) ** 2\n",
    "\n",
    "def tf_sigmoid(x):   return tf.nn.sigmoid(x)\n",
    "def d_tf_sigmoid(x): return tf_sigmoid(x) * (1.0-tf_sigmoid(x))\n",
    "\n",
    "class CNN():\n",
    "\n",
    "    def __init__(self,k,inc,out, stddev=0.05,which_reg=0,act=tf_relu,d_act=d_tf_relu):\n",
    "        self.w          = tf.Variable(tf.random_normal([k,k,inc,out],stddev=stddev,seed=2,dtype=tf.float32))\n",
    "        self.m,self.v   = tf.Variable(tf.zeros_like(self.w)),tf.Variable(tf.zeros_like(self.w))\n",
    "        self.act,self.d_act = act,d_act\n",
    "        self.which_reg  = which_reg\n",
    "        \n",
    "    def getw(self): return self.w\n",
    "\n",
    "    def feedforward(self,input,stride=1,padding='SAME'):\n",
    "        self.input  = input\n",
    "        self.layer  = tf.nn.conv2d(input,self.w,strides=[1,stride,stride,1],padding=padding) \n",
    "        self.layerA = self.act(self.layer)\n",
    "        return [self.layer,self.layerA]\n",
    "\n",
    "    def backprop(self,gradient,stride=1,padding='SAME'):\n",
    "        grad_part_1 = gradient\n",
    "        grad_part_2 = self.d_act(self.layer)\n",
    "        grad_part_3 = self.input\n",
    "\n",
    "        grad_middle = grad_part_1 * grad_part_2\n",
    "        grad        = tf.nn.conv2d_backprop_filter(input = grad_part_3,filter_sizes = tf.shape(self.w),  out_backprop = grad_middle,strides=[1,stride,stride,1],padding=padding) / batch_size\n",
    "        grad_pass   = tf.nn.conv2d_backprop_input (input_sizes = tf.shape(self.input),filter= self.w,out_backprop = grad_middle,strides=[1,stride,stride,1],padding=padding)\n",
    "        \n",
    "        return [grad,grad_pass]\n",
    "    \n",
    "class tf_batch_norm_layer():\n",
    "    \n",
    "    def __init__(self,vector_shape,axis):\n",
    "        self.moving_mean = tf.Variable(tf.zeros(shape=[1,1,1,vector_shape],dtype=tf.float32))\n",
    "        self.moving_vari = tf.Variable(tf.zeros(shape=[1,1,1,vector_shape],dtype=tf.float32))\n",
    "        self.axis        = axis\n",
    "        \n",
    "    def feedforward(self,input,training_phase=True,eps = 1e-8):\n",
    "        self.input = input\n",
    "        self.input_size          = self.input.shape\n",
    "        self.batch,self.h,self.w,self.c = self.input_size[0].value,self.input_size[1].value,self.input_size[2].value,self.input_size[3].value\n",
    "\n",
    "        # Training Moving Average Mean         \n",
    "        def training_fn():\n",
    "            self.mean    = tf.reduce_mean(self.input,axis=self.axis ,keepdims=True)\n",
    "            self.var     = tf.reduce_mean(tf.square(self.input-self.mean),axis=self.axis,keepdims=True)\n",
    "            centered_data= (self.input - self.mean)/tf.sqrt(self.var + eps)\n",
    "            \n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean*0.9 + 0.1 * self.mean ))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari*0.9 + 0.1 * self.var  ))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        # Testing Moving Average Mean        \n",
    "        def  testing_fn():\n",
    "            centered_data   = (self.input - self.moving_mean)/tf.sqrt(self.moving_vari + eps)\n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        self.output,update_variable = tf.cond(training_phase,true_fn=training_fn,false_fn=testing_fn)\n",
    "        return self.output,update_variable\n",
    "    \n",
    "    def backprop(self,grad,eps = 1e-8):\n",
    "        change_parts = 1.0 /(self.batch * self.h * self.w)\n",
    "        grad_sigma   = tf.reduce_sum( grad *  (self.input-self.mean)     ,axis=self.axis,keepdims=True) * -0.5 * (self.var+eps) ** -1.5\n",
    "        grad_mean    = tf.reduce_sum( grad *  (-1./tf.sqrt(self.var+eps)),axis=self.axis,keepdims=True) + grad_sigma * change_parts * 2.0 * tf.reduce_sum((self.input-self.mean),axis=self.axis,keepdims=True) * -1\n",
    "        grad_x       = grad * 1/(tf.sqrt(self.var+eps)) + grad_sigma * change_parts * 2.0 * (self.input-self.mean) + grad_mean * change_parts\n",
    "        return grad_x\n",
    "\n",
    "class tf_layer_norm_layer():\n",
    "    \n",
    "    def __init__(self,vector_shape,axis):\n",
    "        self.moving_mean = tf.Variable(tf.zeros(shape=[vector_shape,1,1,1],dtype=tf.float32))\n",
    "        self.moving_vari = tf.Variable(tf.zeros(shape=[vector_shape,1,1,1],dtype=tf.float32))\n",
    "        self.axis        = axis\n",
    "        \n",
    "    def feedforward(self,input,training_phase=True,eps = 1e-8):\n",
    "        self.input = input\n",
    "        self.input_size          = self.input.shape\n",
    "        self.batch,self.h,self.w,self.c = self.input_size[0].value,self.input_size[1].value,self.input_size[2].value,self.input_size[3].value\n",
    "\n",
    "        # Training Moving Average Mean         \n",
    "        def training_fn():\n",
    "            self.mean    = tf.reduce_mean(self.input,axis=self.axis ,keepdims=True)\n",
    "            self.var     = tf.reduce_mean(tf.square(self.input-self.mean),axis=self.axis,keepdims=True)\n",
    "            centered_data= (self.input - self.mean)/tf.sqrt(self.var + eps)\n",
    "            \n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean*0.9 + 0.1 * self.mean ))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari*0.9 + 0.1 * self.var  ))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        # Testing Moving Average Mean        \n",
    "        def  testing_fn():\n",
    "            centered_data   = (self.input - self.moving_mean)/tf.sqrt(self.moving_vari + eps)\n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        self.output,update_variable = tf.cond(training_phase,true_fn=training_fn,false_fn=testing_fn)\n",
    "        return self.output,update_variable\n",
    "    \n",
    "    def backprop(self,grad,eps = 1e-8):\n",
    "        change_parts = 1.0 /(self.h * self.w * self.c)\n",
    "        grad_sigma   = tf.reduce_sum( grad *  (self.input-self.mean)     ,axis=self.axis,keepdims=True) * -0.5 * (self.var+eps) ** -1.5\n",
    "        grad_mean    = tf.reduce_sum( grad *  (-1./tf.sqrt(self.var+eps)),axis=self.axis,keepdims=True) + grad_sigma * change_parts * 2.0 * tf.reduce_sum((self.input-self.mean),axis=self.axis,keepdims=True) * -1\n",
    "        grad_x       = grad * 1/(tf.sqrt(self.var+eps)) + grad_sigma * change_parts * 2.0 * (self.input-self.mean) + grad_mean * change_parts\n",
    "        return grad_x\n",
    "    \n",
    "class tf_instance_norm_layer():\n",
    "    \n",
    "    def __init__(self,batch_size,vector_shape,axis):\n",
    "        self.moving_mean = tf.Variable(tf.zeros(shape=[batch_size,1,1,vector_shape],dtype=tf.float32))\n",
    "        self.moving_vari = tf.Variable(tf.zeros(shape=[batch_size,1,1,vector_shape],dtype=tf.float32))\n",
    "        self.axis        = axis\n",
    "        \n",
    "    def feedforward(self,input,training_phase=True,eps = 1e-8):\n",
    "        self.input = input\n",
    "        self.input_size          = self.input.shape\n",
    "        self.batch,self.h,self.w,self.c = self.input_size[0].value,self.input_size[1].value,self.input_size[2].value,self.input_size[3].value\n",
    "\n",
    "        # Training Moving Average Mean         \n",
    "        def training_fn():\n",
    "            self.mean    = tf.reduce_mean(self.input,axis=self.axis ,keepdims=True)\n",
    "            self.var     = tf.reduce_mean(tf.square(self.input-self.mean),axis=self.axis,keepdims=True)\n",
    "            centered_data= (self.input - self.mean)/tf.sqrt(self.var + eps)\n",
    "            \n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean*0.9 + 0.1 * self.mean ))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari*0.9 + 0.1 * self.var  ))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        # Testing Moving Average Mean        \n",
    "        def  testing_fn():\n",
    "            centered_data   = (self.input - self.moving_mean)/tf.sqrt(self.moving_vari + eps)\n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        self.output,update_variable = tf.cond(training_phase,true_fn=training_fn,false_fn=testing_fn)\n",
    "        return self.output,update_variable\n",
    "    \n",
    "    def backprop(self,grad,eps = 1e-8):\n",
    "        change_parts = 1.0 /(self.h * self.w)\n",
    "        grad_sigma   = tf.reduce_sum( grad *  (self.input-self.mean)     ,axis=self.axis,keepdims=True) * -0.5 * (self.var+eps) ** -1.5\n",
    "        grad_mean    = tf.reduce_sum( grad *  (-1./tf.sqrt(self.var+eps)),axis=self.axis,keepdims=True) + grad_sigma * change_parts * 2.0 * tf.reduce_sum((self.input-self.mean),axis=self.axis,keepdims=True) * -1\n",
    "        grad_x       = grad * 1/(tf.sqrt(self.var+eps)) + grad_sigma * change_parts * 2.0 * (self.input-self.mean) + grad_mean * change_parts\n",
    "        return grad_x\n",
    "  \n",
    "class tf_box_cox():\n",
    "    \n",
    "    def __init__(self,lmbda=2.0):\n",
    "        self.lmbda = lmbda\n",
    "    \n",
    "    def feedforward(self,data):\n",
    "        self.input = data\n",
    "        self.layer = (tf.pow((data + 1.0),self.lmbda) - 1.0)/self.lmbda\n",
    "        return self.layer \n",
    "    \n",
    "    def backprop(self,grad):\n",
    "        grad_input = tf.pow((self.input + 1),self.lmbda-1.0)\n",
    "        \n",
    "        # Grad respect to the lmbda value (not tested!)\n",
    "        grad_lmbda = tf.pow((self.input+1),self.lmbda) * \\\n",
    "        (tf.log(self.input+1)*self.lmbda -1) + 1\n",
    "        grad_lmbda = grad_lmbda / (self.lmbda ** 2)\n",
    "        \n",
    "        return grad_input * grad\n",
    "\n",
    "class tf_min_max_layer():\n",
    "    \n",
    "    def __init__(self,vector_shape,user_max=1.0,user_min=0.0):\n",
    "        self.moving_min = tf.Variable(tf.zeros(shape=(vector_shape,1),dtype=tf.float32))\n",
    "        self.moving_max = tf.Variable(tf.zeros(shape=(vector_shape,1),dtype=tf.float32))\n",
    "        self.user_min   = tf.Variable(user_min,dtype=tf.float32); \n",
    "        self.user_max   = tf.Variable(user_max,dtype=tf.float32); \n",
    "        \n",
    "    def feedforward(self,input,training_phase):\n",
    "        self.input    = input\n",
    "        self.min_vec  = tf.reduce_min(input,-1)[:,None]\n",
    "        self.min_index= tf.argmin(input,-1)\n",
    "        self.max_vec  = tf.reduce_max(input,-1)[:,None]\n",
    "        self.max_index= tf.argmax(input,-1)\n",
    "        \n",
    "        def training_fn():\n",
    "            normalized_data = (self.user_max-self.user_min)  * \\\n",
    "            ((self.input - self.min_vec)/(self.max_vec - self.min_vec))          + self.user_min\n",
    "            \n",
    "            update_min_max = []\n",
    "            update_min_max.append(tf.assign(self.moving_min,self.moving_min * 0.9 + 0.1 * self.min_vec))\n",
    "            update_min_max.append(tf.assign(self.moving_max,self.moving_max * 0.9 + 0.1 * self.max_vec))\n",
    "            return normalized_data,update_min_max\n",
    "        \n",
    "        # Testing Moving Average Mean        \n",
    "        def  testing_fn():\n",
    "            normalized_data = (self.user_max-self.user_min) * \\\n",
    "            ((self.input - self.moving_min)/(self.moving_max - self.moving_min)) + self.user_min\n",
    "            \n",
    "            update_min_max = []\n",
    "            update_min_max.append(tf.assign(self.moving_min,self.moving_min))\n",
    "            update_min_max.append(tf.assign(self.moving_max,self.moving_max))\n",
    "            return normalized_data,update_min_max\n",
    "        \n",
    "        self.output,update_min_max = tf.cond(training_phase,true_fn=training_fn,false_fn=testing_fn)\n",
    "        return self.output,update_min_max\n",
    "    \n",
    "    def backprop(self,grad):\n",
    "        grad1   = grad\n",
    "        \n",
    "        # Create Mask for min / max value for row\n",
    "        indices = tf.range(0, self.input.shape[0].value,dtype=tf.int64)\n",
    "        min_indices = tf.stack([indices, self.min_index], axis=1)\n",
    "        max_indices = tf.stack([indices, self.max_index], axis=1)\n",
    "        grad_min = tf.cast(tf.sparse_to_dense(min_indices, self.input.shape, sparse_values=1, default_value=0),dtype=tf.float32)\n",
    "        grad_max = tf.cast(tf.sparse_to_dense(max_indices, self.input.shape, sparse_values=1, default_value=0),dtype=tf.float32)\n",
    "        \n",
    "        grad_max_min = 1.0/(self.max_vec-self.min_vec)\n",
    "        grad_pass    = grad1 * (self.user_max-self.user_min) * (\n",
    "            grad_max_min + \\\n",
    "            (self.input - self.max_vec)/tf.square(grad_max_min) * grad_min + \\\n",
    "            (self.min_vec - self.input)/tf.square(grad_max_min) * grad_max\n",
    "        )\n",
    "        \n",
    "        return grad_pass\n",
    "\n",
    "class tf_svd_layer():\n",
    "    \n",
    "    def __init__(self,batch_size,channel_size):\n",
    "        self.moving_s = tf.Variable(tf.zeros((batch_size,channel_size),dtype=tf.float32))\n",
    "    \n",
    "    def feedforward(self,data,training_phase):\n",
    "        \n",
    "        with tf.device('/cpu:0'):\n",
    "            s,U,V = tf.svd(data)\n",
    "\n",
    "        smean,sstd = tf.nn.moments(s, axes=(-1))\n",
    "\n",
    "        def training_fn():\n",
    "            snorm  = (s-smean[:,None])/sstd[:,None] + smean[:,None]\n",
    "            data   = U @ tf.matrix_diag(snorm) @ tf.transpose(V,(0,2,1))\n",
    "            update = []\n",
    "            update.append(tf.assign(self.moving_s,self.moving_s*0.9 + 0.1 * snorm))\n",
    "            return data,update\n",
    "            \n",
    "        def testing_fn():\n",
    "            data   = U @ tf.matrix_diag(self.moving_s) @ tf.transpose(V,(0,2,1))\n",
    "            update = []\n",
    "            update.append(tf.assign(self.moving_s,self.moving_s))\n",
    "            return data,update\n",
    "        \n",
    "        data,update  = tf.cond(training_phase,true_fn=training_fn,false_fn=testing_fn)\n",
    "        return data,update\n",
    "    \n",
    "    def backprop(self,grad):\n",
    "        raise NotImplemented('Do not rely on Auto Differentiation')\n",
    "    \n",
    "def show_histogram(layer1,layer1a,grad1w,grad1p):\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(141); plt.hist(layer1. ravel(),batch_size); plt.title('layer')\n",
    "    plt.subplot(142); plt.hist(layer1a.ravel(),batch_size); plt.title('layer a')\n",
    "    plt.subplot(143); plt.hist(grad1w.ravel(),batch_size); plt.title('grad w')\n",
    "    plt.subplot(144); plt.hist(grad1p.ravel(),batch_size); plt.title('grad p')\n",
    "    plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T19:09:21.725727Z",
     "start_time": "2018-12-16T19:09:21.721736Z"
    }
   },
   "outputs": [],
   "source": [
    "# hyper paraneter for all \n",
    "num_epoch = 100; batch_size = 20 ; learning_rate = 0.0008; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-16T19:13:20.399Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Iter : 0/100 batch : 7980/8000 acc : 0.25\n",
      " Current : 0 Acc : 0.2666000024229288 Test Acc : 0.27862500298768283\n",
      "\n",
      "Current Iter : 1/100 batch : 7980/8000 acc : 0.45\n",
      " Current : 1 Acc : 0.33420000079274176 Test Acc : 0.3586250013113022\n",
      "\n",
      "Current Iter : 2/100 batch : 7980/8000 acc : 0.35\n",
      " Current : 2 Acc : 0.3684000014066696 Test Acc : 0.35950000129640103\n",
      "\n",
      "Current Iter : 3/100 batch : 7980/8000 acc : 0.45\n",
      " Current : 3 Acc : 0.3998000012934208 Test Acc : 0.3796250008419156\n",
      "\n",
      "Current Iter : 4/100 batch : 7980/8000 acc : 0.35\n",
      " Current : 4 Acc : 0.4292000005245209 Test Acc : 0.4127500000037253\n",
      "\n",
      "Current Iter : 5/100 batch : 7980/8000 acc : 0.45\n",
      " Current : 5 Acc : 0.4654000018239021 Test Acc : 0.42812500067055226\n",
      "\n",
      "Current Iter : 6/100 batch : 7980/8000 acc : 0.45\n",
      " Current : 6 Acc : 0.49500000244379044 Test Acc : 0.45775000216439365\n",
      "\n",
      "Current Iter : 7/100 batch : 7980/8000 acc : 0.55\n",
      " Current : 7 Acc : 0.5186000024676323 Test Acc : 0.46287500094622375\n",
      "\n",
      "Current Iter : 8/100 batch : 7980/8000 acc : 0.55\n",
      " Current : 8 Acc : 0.5444000024795532 Test Acc : 0.4785000007227063\n",
      "\n",
      "Current Iter : 9/100 batch : 7980/8000 acc : 0.55\n",
      " Current : 9 Acc : 0.5552000019550324 Test Acc : 0.4746250016614795\n",
      "\n",
      "Current Iter : 10/100 batch : 7980/8000 acc : 0.55\n",
      " Current : 10 Acc : 0.5752000024318695 Test Acc : 0.47300000054761765\n",
      "\n",
      "Current Iter : 11/100 batch : 7980/8000 acc : 0.55\n",
      " Current : 11 Acc : 0.5979999995231629 Test Acc : 0.4746250012144446\n",
      "\n",
      "Current Iter : 12/100 batch : 7980/8000 acc : 0.65\n",
      " Current : 12 Acc : 0.6111999969482422 Test Acc : 0.48225000165402887\n",
      "\n",
      "Current Iter : 13/100 batch : 7980/8000 acc : 0.55\n",
      " Current : 13 Acc : 0.6327999985218048 Test Acc : 0.4695000018551946\n",
      "\n",
      "Current Iter : 14/100 batch : 7980/8000 acc : 0.65\n",
      " Current : 14 Acc : 0.6504000004529953 Test Acc : 0.4692500024661422\n",
      "\n",
      "Current Iter : 15/100 batch : 7980/8000 acc : 0.65\n",
      " Current : 15 Acc : 0.6615999982357025 Test Acc : 0.4716250017657876\n",
      "\n",
      "Current Iter : 16/100 batch : 7980/8000 acc : 0.55\n",
      " Current : 16 Acc : 0.6735999993085862 Test Acc : 0.4641250018775463\n",
      "\n",
      "Current Iter : 17/100 batch : 7980/8000 acc : 0.55\n",
      " Current : 17 Acc : 0.681999999165535 Test Acc : 0.4736250016838312\n",
      "\n",
      "Current Iter : 18/100 batch : 7980/8000 acc : 0.55\n",
      " Current : 18 Acc : 0.6993999975919724 Test Acc : 0.4588750019297004\n",
      "\n",
      "Current Iter : 19/100 batch : 7980/8000 acc : 0.55\n",
      " Current : 19 Acc : 0.7032000000476837 Test Acc : 0.4741250018402934\n",
      "\n",
      "Current Iter : 20/100 batch : 7980/8000 acc : 0.45\n",
      " Current : 20 Acc : 0.7157999981641769 Test Acc : 0.47525000121444466\n",
      "\n",
      "Current Iter : 21/100 batch : 7980/8000 acc : 0.45\n",
      " Current : 21 Acc : 0.7201999992132186 Test Acc : 0.4677500018849969\n",
      "\n",
      "Current Iter : 22/100 batch : 7980/8000 acc : 0.45\n",
      " Current : 22 Acc : 0.7295999994277954 Test Acc : 0.472625001296401\n",
      "\n",
      "Current Iter : 23/100 batch : 7980/8000 acc : 0.55\n",
      " Current : 23 Acc : 0.7372000000476837 Test Acc : 0.4671250009536743\n",
      "\n",
      "Current Iter : 24/100 batch : 7980/8000 acc : 0.45\n",
      " Current : 24 Acc : 0.7489999990463256 Test Acc : 0.4655000015720725\n",
      "\n",
      "Current Iter : 25/100 batch : 7980/8000 acc : 0.45\n",
      " Current : 25 Acc : 0.7587999999523163 Test Acc : 0.47350000143051146\n",
      "\n",
      "Current Iter : 26/100 batch : 7980/8000 acc : 0.55\n",
      " Current : 26 Acc : 0.7608000004291534 Test Acc : 0.47425000086426733\n",
      "\n",
      "Current Iter : 27/100 batch : 7980/8000 acc : 0.45\n",
      " Current : 27 Acc : 0.7666000015735627 Test Acc : 0.4707500014454126\n",
      "\n",
      "Current Iter : 28/100 batch : 7980/8000 acc : 0.55\n",
      " Current : 28 Acc : 0.7756000013351441 Test Acc : 0.4701250014081597\n",
      "\n",
      "Current Iter : 29/100 batch : 7980/8000 acc : 0.55\n",
      " Current : 29 Acc : 0.779000002861023 Test Acc : 0.46687500111758706\n",
      "\n",
      "Current Iter : 30/100 batch : 7980/8000 acc : 0.55\n",
      " Current : 30 Acc : 0.7894000015258789 Test Acc : 0.46475000120699406\n",
      "\n",
      "Current Iter : 31/100 batch : 4240/5000 acc : 0.85\r"
     ]
    }
   ],
   "source": [
    "# frist batch normalization\n",
    "# sess = tf.InteractiveSession()\n",
    "\n",
    "# 1. layers\n",
    "l1 = CNN(3,3, 16); l1n = tf_batch_norm_layer(16,(0,1,2))\n",
    "l2 = CNN(3,16,16); l2n = tf_batch_norm_layer(16,(0,1,2))\n",
    "l3 = CNN(3,16,16); l3n = tf_batch_norm_layer(16,(0,1,2))\n",
    "l4 = CNN(3,16,16); l4n = tf_batch_norm_layer(16,(0,1,2))\n",
    "l5 = CNN(3,16,16); l5n = tf_batch_norm_layer(16,(0,1,2))\n",
    "l6 = CNN(3,16,10); \n",
    "\n",
    "# 2. graph \n",
    "x = tf.placeholder(tf.float32,(batch_size,96,96,3))\n",
    "y = tf.placeholder(tf.float32,(batch_size,10))\n",
    "is_train = tf.placeholder_with_default(True,())\n",
    "\n",
    "layer1, layer1a = l1. feedforward(x,stride=2)\n",
    "layer1b,update1 = l1n.feedforward(layer1a,is_train)\n",
    "layer2, layer2a = l2. feedforward(layer1b,stride=2)\n",
    "layer2b,update2 = l2n.feedforward(layer2a,is_train)\n",
    "layer3, layer3a = l3. feedforward(layer2b,stride=2)\n",
    "layer3b,update3 = l3n.feedforward(layer3a,is_train)\n",
    "layer4, layer4a = l4. feedforward(layer3b,stride=2)\n",
    "layer4b,update4 = l4n.feedforward(layer4a,is_train)\n",
    "\n",
    "layer5, layer5a = l5. feedforward(layer4b)\n",
    "layer5b,update5 = l5n.feedforward(layer5a,is_train)\n",
    "layer6, layer6a = l6. feedforward(layer5b)\n",
    "\n",
    "final_layer = tf.reduce_mean(layer6a,(1,2))\n",
    "cost        = tf.nn.softmax_cross_entropy_with_logits_v2(logits=final_layer,labels=y)\n",
    "auto_train  = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "update_ops  = update1 + update2 + update3 + update4 + update5\n",
    "final_softmax      = tf_softmax(final_layer)\n",
    "correct_prediction = tf.equal(tf.argmax(final_softmax, 1), tf.argmax(y, 1))\n",
    "accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# 3. train\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "avg_acc_train = 0; avg_acc_test  = 0; \n",
    "\n",
    "for iter in range(num_epoch):\n",
    "\n",
    "    for current_batch_index in range(0,len(train_images),batch_size):\n",
    "        current_data  = train_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = train_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy,auto_train,update_ops],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(train_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_train = avg_acc_train + sess_results[0]\n",
    "        \n",
    "    for current_batch_index in range(0,len(test_images), batch_size):\n",
    "        current_data  = test_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = test_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy],feed_dict={x:current_data,y:current_label,is_train:False})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(test_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_test = avg_acc_test + sess_results[0]        \n",
    "        \n",
    "    print(\"\\n Current : \"+ str(iter) + \" Acc : \" + str(avg_acc_train/(len(train_images)/batch_size)) + \" Test Acc : \" + str(avg_acc_test/(len(test_images)/batch_size)) + '\\n')\n",
    "    avg_acc_train = 0 ; avg_acc_test  = 0\n",
    "\n",
    "# 4. reset \n",
    "tf.reset_default_graph()\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T04:50:59.316183Z",
     "start_time": "2018-12-16T04:50:59.300228Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T05:41:32.276917Z",
     "start_time": "2018-12-16T05:41:32.269932Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
