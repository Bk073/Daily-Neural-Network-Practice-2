{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-17T00:51:52.878708Z",
     "start_time": "2018-12-17T00:51:52.871752Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# import Library and some random image data set\n",
    "import tensorflow as tf\n",
    "import numpy      as np\n",
    "import seaborn    as sns \n",
    "import pandas     as pd\n",
    "import os,sys\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(78); tf.set_random_seed(78)\n",
    "\n",
    "# get some of the STL data set\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from skimage import util \n",
    "from skimage.transform import resize\n",
    "from skimage.io import imread\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from numpy import inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-17T00:51:58.002489Z",
     "start_time": "2018-12-17T00:51:55.141495Z"
    },
    "code_folding": [
     0,
     1,
     28
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 96, 96, 3) 1.0 0.0\n",
      "(5000, 10) 1.0 0.0\n",
      "(8000, 96, 96, 3) 1.0 0.0\n",
      "(8000, 10) 1.0 0.0\n"
     ]
    }
   ],
   "source": [
    "# read all of the data\n",
    "def read_all_images(path_to_data):\n",
    "    \"\"\"\n",
    "    :param path_to_data: the file containing the binary images from the STL-10 dataset\n",
    "    :return: an array containing all the images\n",
    "    \"\"\"\n",
    "\n",
    "    with open(path_to_data, 'rb') as f:\n",
    "        # read whole file in uint8 chunks\n",
    "        everything = np.fromfile(f, dtype=np.uint8)\n",
    "\n",
    "        # We force the data into 3x96x96 chunks, since the\n",
    "        # images are stored in \"column-major order\", meaning\n",
    "        # that \"the first 96*96 values are the red channel,\n",
    "        # the next 96*96 are green, and the last are blue.\"\n",
    "        # The -1 is since the size of the pictures depends\n",
    "        # on the input file, and this way numpy determines\n",
    "        # the size on its own.\n",
    "\n",
    "        images = np.reshape(everything, (-1, 3, 96, 96))\n",
    "\n",
    "        # Now transpose the images into a standard image format\n",
    "        # readable by, for example, matplotlib.imshow\n",
    "        # You might want to comment this line or reverse the shuffle\n",
    "        # if you will use a learning algorithm like CNN, since they like\n",
    "        # their channels separated.\n",
    "        images = np.transpose(images, (0, 3, 2, 1))\n",
    "        return images\n",
    "def read_labels(path_to_labels):\n",
    "    \"\"\"\n",
    "    :param path_to_labels: path to the binary file containing labels from the STL-10 dataset\n",
    "    :return: an array containing the labels\n",
    "    \"\"\"\n",
    "    with open(path_to_labels, 'rb') as f:\n",
    "        labels = np.fromfile(f, dtype=np.uint8)\n",
    "        return labels\n",
    "    \n",
    "train_images = read_all_images(\"../../../DataSet/STL10/stl10_binary/train_X.bin\") / 255.0\n",
    "train_labels = read_labels    (\"../../../DataSet/STL10/stl10_binary/train_Y.bin\")\n",
    "test_images  = read_all_images(\"../../../DataSet/STL10/stl10_binary/test_X.bin\")  / 255.0\n",
    "test_labels  = read_labels    (\"../../../DataSet/STL10/stl10_binary/test_y.bin\")\n",
    "\n",
    "label_encoder= OneHotEncoder(sparse=False,categories='auto')\n",
    "train_labels = label_encoder.fit_transform(train_labels.reshape((-1,1)))\n",
    "test_labels  = label_encoder.fit_transform(test_labels.reshape((-1,1)))\n",
    "\n",
    "print(train_images.shape,train_images.max(),train_images.min())\n",
    "print(train_labels.shape,train_labels.max(),train_labels.min())\n",
    "print(test_images.shape,test_images.max(),test_images.min())\n",
    "print(test_labels.shape,test_labels.max(),test_labels.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-17T02:47:39.330531Z",
     "start_time": "2018-12-17T02:47:39.245548Z"
    },
    "code_folding": [
     0,
     42,
     83,
     90,
     124,
     165,
     185,
     241,
     307
    ]
   },
   "outputs": [],
   "source": [
    "# create the layers\n",
    "def tf_softmax(x): return tf.nn.softmax(x)\n",
    "\n",
    "def tf_elu(x):   return tf.nn.elu(x)\n",
    "def d_tf_elu(x): return tf.cast(tf.greater(x,0),tf.float32)  + (tf_elu(tf.cast(tf.less_equal(x,0),tf.float32) * x) + 1.0)\n",
    "\n",
    "def tf_relu(x):   return tf.nn.relu(x)\n",
    "def d_tf_relu(x): return tf.cast(tf.greater(x,0),tf.float32)\n",
    "\n",
    "def tf_tanh(x):   return tf.nn.tanh(x)\n",
    "def d_tf_tanh(x): return 1 - tf_tanh(x) ** 2\n",
    "\n",
    "def tf_sigmoid(x):   return tf.nn.sigmoid(x)\n",
    "def d_tf_sigmoid(x): return tf_sigmoid(x) * (1.0-tf_sigmoid(x))\n",
    "\n",
    "class CNN():\n",
    "\n",
    "    def __init__(self,k,inc,out, stddev=0.05,which_reg=0,act=tf_relu,d_act=d_tf_relu):\n",
    "        self.w          = tf.Variable(tf.random_normal([k,k,inc,out],stddev=stddev,seed=2,dtype=tf.float32))\n",
    "        self.m,self.v   = tf.Variable(tf.zeros_like(self.w)),tf.Variable(tf.zeros_like(self.w))\n",
    "        self.act,self.d_act = act,d_act\n",
    "        self.which_reg  = which_reg\n",
    "        \n",
    "    def getw(self): return self.w\n",
    "\n",
    "    def feedforward(self,input,stride=1,padding='SAME'):\n",
    "        self.input  = input\n",
    "        self.layer  = tf.nn.conv2d(input,self.w,strides=[1,stride,stride,1],padding=padding) \n",
    "        self.layerA = self.act(self.layer)\n",
    "        return [self.layer,self.layerA]\n",
    "\n",
    "    def backprop(self,gradient,stride=1,padding='SAME'):\n",
    "        grad_part_1 = gradient\n",
    "        grad_part_2 = self.d_act(self.layer)\n",
    "        grad_part_3 = self.input\n",
    "\n",
    "        grad_middle = grad_part_1 * grad_part_2\n",
    "        grad        = tf.nn.conv2d_backprop_filter(input = grad_part_3,filter_sizes = tf.shape(self.w),  out_backprop = grad_middle,strides=[1,stride,stride,1],padding=padding) / batch_size\n",
    "        grad_pass   = tf.nn.conv2d_backprop_input (input_sizes = tf.shape(self.input),filter= self.w,out_backprop = grad_middle,strides=[1,stride,stride,1],padding=padding)\n",
    "        \n",
    "        return [grad,grad_pass]\n",
    "    \n",
    "class tf_batch_norm_layer():\n",
    "    \n",
    "    def __init__(self,vector_shape,axis):\n",
    "        self.moving_mean = tf.Variable(tf.zeros(shape=[1,1,1,vector_shape],dtype=tf.float32))\n",
    "        self.moving_vari = tf.Variable(tf.zeros(shape=[1,1,1,vector_shape],dtype=tf.float32))\n",
    "        self.axis        = axis\n",
    "        \n",
    "    def feedforward(self,input,training_phase=True,eps = 1e-8):\n",
    "        self.input = input\n",
    "        self.input_size          = self.input.shape\n",
    "        self.batch,self.h,self.w,self.c = self.input_size[0].value,self.input_size[1].value,self.input_size[2].value,self.input_size[3].value\n",
    "\n",
    "        # Training Moving Average Mean         \n",
    "        def training_fn():\n",
    "            self.mean    = tf.reduce_mean(self.input,axis=self.axis ,keepdims=True)\n",
    "            self.var     = tf.reduce_mean(tf.square(self.input-self.mean),axis=self.axis,keepdims=True)\n",
    "            centered_data= (self.input - self.mean)/tf.sqrt(self.var + eps)\n",
    "            \n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean*0.9 + 0.1 * self.mean ))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari*0.9 + 0.1 * self.var  ))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        # Testing Moving Average Mean        \n",
    "        def  testing_fn():\n",
    "            centered_data   = (self.input - self.moving_mean)/tf.sqrt(self.moving_vari + eps)\n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        self.output,update_variable = tf.cond(training_phase,true_fn=training_fn,false_fn=testing_fn)\n",
    "        return self.output,update_variable\n",
    "    \n",
    "    def backprop(self,grad,eps = 1e-8):\n",
    "        change_parts = 1.0 /(self.batch * self.h * self.w)\n",
    "        grad_sigma   = tf.reduce_sum( grad *  (self.input-self.mean)     ,axis=self.axis,keepdims=True) * -0.5 * (self.var+eps) ** -1.5\n",
    "        grad_mean    = tf.reduce_sum( grad *  (-1./tf.sqrt(self.var+eps)),axis=self.axis,keepdims=True) + grad_sigma * change_parts * 2.0 * tf.reduce_sum((self.input-self.mean),axis=self.axis,keepdims=True) * -1\n",
    "        grad_x       = grad * 1/(tf.sqrt(self.var+eps)) + grad_sigma * change_parts * 2.0 * (self.input-self.mean) + grad_mean * change_parts\n",
    "        return grad_x\n",
    "\n",
    "class tf_layer_norm_layer():\n",
    "    \n",
    "    def __init__(self,vector_shape,axis):\n",
    "        self.moving_mean = tf.Variable(tf.zeros(shape=[vector_shape,1,1,1],dtype=tf.float32))\n",
    "        self.moving_vari = tf.Variable(tf.zeros(shape=[vector_shape,1,1,1],dtype=tf.float32))\n",
    "        self.axis        = axis\n",
    "        \n",
    "    def feedforward(self,input,training_phase=True,eps = 1e-8):\n",
    "        self.input = input\n",
    "        self.input_size          = self.input.shape\n",
    "        self.batch,self.h,self.w,self.c = self.input_size[0].value,self.input_size[1].value,self.input_size[2].value,self.input_size[3].value\n",
    "\n",
    "        # Training Moving Average Mean         \n",
    "        def training_fn():\n",
    "            self.mean    = tf.reduce_mean(self.input,axis=self.axis ,keepdims=True)\n",
    "            self.var     = tf.reduce_mean(tf.square(self.input-self.mean),axis=self.axis,keepdims=True)\n",
    "            centered_data= (self.input - self.mean)/tf.sqrt(self.var + eps)\n",
    "            \n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean*0.9 + 0.1 * self.mean ))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari*0.9 + 0.1 * self.var  ))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        # Testing Moving Average Mean        \n",
    "        def  testing_fn():\n",
    "            centered_data   = (self.input - self.moving_mean)/tf.sqrt(self.moving_vari + eps)\n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        self.output,update_variable = tf.cond(training_phase,true_fn=training_fn,false_fn=testing_fn)\n",
    "        return self.output,update_variable\n",
    "    \n",
    "    def backprop(self,grad,eps = 1e-8):\n",
    "        change_parts = 1.0 /(self.h * self.w * self.c)\n",
    "        grad_sigma   = tf.reduce_sum( grad *  (self.input-self.mean)     ,axis=self.axis,keepdims=True) * -0.5 * (self.var+eps) ** -1.5\n",
    "        grad_mean    = tf.reduce_sum( grad *  (-1./tf.sqrt(self.var+eps)),axis=self.axis,keepdims=True) + grad_sigma * change_parts * 2.0 * tf.reduce_sum((self.input-self.mean),axis=self.axis,keepdims=True) * -1\n",
    "        grad_x       = grad * 1/(tf.sqrt(self.var+eps)) + grad_sigma * change_parts * 2.0 * (self.input-self.mean) + grad_mean * change_parts\n",
    "        return grad_x\n",
    "    \n",
    "class tf_instance_norm_layer():\n",
    "    \n",
    "    def __init__(self,batch_size,vector_shape,axis):\n",
    "        self.moving_mean = tf.Variable(tf.zeros(shape=[batch_size,1,1,vector_shape],dtype=tf.float32))\n",
    "        self.moving_vari = tf.Variable(tf.zeros(shape=[batch_size,1,1,vector_shape],dtype=tf.float32))\n",
    "        self.axis        = axis\n",
    "        \n",
    "    def feedforward(self,input,training_phase=True,eps = 1e-8):\n",
    "        self.input = input\n",
    "        self.input_size          = self.input.shape\n",
    "        self.batch,self.h,self.w,self.c = self.input_size[0].value,self.input_size[1].value,self.input_size[2].value,self.input_size[3].value\n",
    "\n",
    "        # Training Moving Average Mean         \n",
    "        def training_fn():\n",
    "            self.mean    = tf.reduce_mean(self.input,axis=self.axis ,keepdims=True)\n",
    "            self.var     = tf.reduce_mean(tf.square(self.input-self.mean),axis=self.axis,keepdims=True)\n",
    "            centered_data= (self.input - self.mean)/tf.sqrt(self.var + eps)\n",
    "            \n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean*0.9 + 0.1 * self.mean ))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari*0.9 + 0.1 * self.var  ))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        # Testing Moving Average Mean        \n",
    "        def  testing_fn():\n",
    "            centered_data   = (self.input - self.moving_mean)/tf.sqrt(self.moving_vari + eps)\n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        self.output,update_variable = tf.cond(training_phase,true_fn=training_fn,false_fn=testing_fn)\n",
    "        return self.output,update_variable\n",
    "    \n",
    "    def backprop(self,grad,eps = 1e-8):\n",
    "        change_parts = 1.0 /(self.h * self.w)\n",
    "        grad_sigma   = tf.reduce_sum( grad *  (self.input-self.mean)     ,axis=self.axis,keepdims=True) * -0.5 * (self.var+eps) ** -1.5\n",
    "        grad_mean    = tf.reduce_sum( grad *  (-1./tf.sqrt(self.var+eps)),axis=self.axis,keepdims=True) + grad_sigma * change_parts * 2.0 * tf.reduce_sum((self.input-self.mean),axis=self.axis,keepdims=True) * -1\n",
    "        grad_x       = grad * 1/(tf.sqrt(self.var+eps)) + grad_sigma * change_parts * 2.0 * (self.input-self.mean) + grad_mean * change_parts\n",
    "        return grad_x\n",
    "  \n",
    "class tf_box_cox():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.lmbda = tf.Variable(2.0)\n",
    "    \n",
    "    def feedforward(self,data):\n",
    "        self.input = data\n",
    "        self.layer = (tf.pow((self.input + 1.0),self.lmbda) - 1.0)/self.lmbda\n",
    "        return self.layer \n",
    "    \n",
    "    def backprop(self,grad):\n",
    "        grad_input = tf.pow((self.input + 1),self.lmbda-1.0)\n",
    "        \n",
    "        # Grad respect to the lmbda value (not tested!)\n",
    "        grad_lmbda = tf.pow((self.input+1),self.lmbda) * \\\n",
    "        (tf.log(self.input+1)*self.lmbda -1) + 1\n",
    "        grad_lmbda = grad_lmbda / (self.lmbda ** 2)\n",
    "        \n",
    "        return grad_input * grad\n",
    "\n",
    "class tf_min_max_layer():\n",
    "    \n",
    "    def __init__(self,vector_shape,user_max=1.0,user_min=0.0):\n",
    "        self.moving_min = tf.Variable(tf.zeros(shape=(vector_shape,1),dtype=tf.float32))\n",
    "        self.moving_max = tf.Variable(tf.zeros(shape=(vector_shape,1),dtype=tf.float32))\n",
    "        self.user_min   = tf.Variable(user_min,dtype=tf.float32); \n",
    "        self.user_max   = tf.Variable(user_max,dtype=tf.float32); \n",
    "        \n",
    "    def feedforward(self,input,training_phase):\n",
    "        self.input    = input\n",
    "        self.min_vec  = tf.reduce_min(input,-1)[:,None]\n",
    "        self.min_index= tf.argmin(input,-1)\n",
    "        self.max_vec  = tf.reduce_max(input,-1)[:,None]\n",
    "        self.max_index= tf.argmax(input,-1)\n",
    "        \n",
    "        def training_fn():\n",
    "            normalized_data = (self.user_max-self.user_min)  * \\\n",
    "            ((self.input - self.min_vec)/(self.max_vec - self.min_vec))          + self.user_min\n",
    "            \n",
    "            update_min_max = []\n",
    "            update_min_max.append(tf.assign(self.moving_min,self.moving_min * 0.9 + 0.1 * self.min_vec))\n",
    "            update_min_max.append(tf.assign(self.moving_max,self.moving_max * 0.9 + 0.1 * self.max_vec))\n",
    "            return normalized_data,update_min_max\n",
    "        \n",
    "        # Testing Moving Average Mean        \n",
    "        def  testing_fn():\n",
    "            normalized_data = (self.user_max-self.user_min) * \\\n",
    "            ((self.input - self.moving_min)/(self.moving_max - self.moving_min)) + self.user_min\n",
    "            \n",
    "            update_min_max = []\n",
    "            update_min_max.append(tf.assign(self.moving_min,self.moving_min))\n",
    "            update_min_max.append(tf.assign(self.moving_max,self.moving_max))\n",
    "            return normalized_data,update_min_max\n",
    "        \n",
    "        self.output,update_min_max = tf.cond(training_phase,true_fn=training_fn,false_fn=testing_fn)\n",
    "        return self.output,update_min_max\n",
    "    \n",
    "    def backprop(self,grad):\n",
    "        grad1   = grad\n",
    "        \n",
    "        # Create Mask for min / max value for row\n",
    "        indices = tf.range(0, self.input.shape[0].value,dtype=tf.int64)\n",
    "        min_indices = tf.stack([indices, self.min_index], axis=1)\n",
    "        max_indices = tf.stack([indices, self.max_index], axis=1)\n",
    "        grad_min = tf.cast(tf.sparse_to_dense(min_indices, self.input.shape, sparse_values=1, default_value=0),dtype=tf.float32)\n",
    "        grad_max = tf.cast(tf.sparse_to_dense(max_indices, self.input.shape, sparse_values=1, default_value=0),dtype=tf.float32)\n",
    "        \n",
    "        grad_max_min = 1.0/(self.max_vec-self.min_vec)\n",
    "        grad_pass    = grad1 * (self.user_max-self.user_min) * (\n",
    "            grad_max_min + \\\n",
    "            (self.input - self.max_vec)/tf.square(grad_max_min) * grad_min + \\\n",
    "            (self.min_vec - self.input)/tf.square(grad_max_min) * grad_max\n",
    "        )\n",
    "        \n",
    "        return grad_pass\n",
    "\n",
    "class tf_svd_layer():\n",
    "    \n",
    "    def __init__(self,batch_size,size,width):\n",
    "        self.n = size\n",
    "        self.moving_s = tf.Variable(tf.zeros((batch_size,size),dtype=tf.float32))\n",
    "        self.moving_u = tf.Variable(tf.zeros((batch_size,width**2,size),dtype=tf.float32))\n",
    "        self.moving_v = tf.Variable(tf.zeros((batch_size,size,size),dtype=tf.float32))\n",
    "    \n",
    "    def feedforward(self,data,training_phase):\n",
    "        \n",
    "        with tf.device('/cpu:0'):\n",
    "            s,U,V = tf.svd(data)\n",
    "        smin = tf.reduce_min(s,1,keepdims=True)\n",
    "        smax = tf.reduce_max(s,1,keepdims=True)\n",
    "        scaleds = (s - smin)/(smax-smin + 1e-8)\n",
    "        def training_fn():\n",
    "            data      = U[:,:,:] @ tf.matrix_diag(s)[:,:,:] @ tf.transpose(V,(0,2,1))[:,:,:]\n",
    "            data = data  * tf.reduce_mean(tf.transpose(tf.abs(V),(0,2,1)) * scaleds[:,:,None] ,(1),keepdims=True)\n",
    "            # data      = data * tf.reduce_mean(tf.transpose(tf.abs(V),(0,2,1)) * scaleds[:,:,None] ,(1),keepdims=True)\n",
    "            update = []\n",
    "            update.append(tf.assign(self.moving_u,self.moving_u*0.9 + 0.1 * U))\n",
    "            update.append(tf.assign(self.moving_v,self.moving_v*0.9 + 0.1 * V))\n",
    "            return data,update\n",
    "            \n",
    "        def testing_fn():\n",
    "            data      = U[:,:,:] @ tf.matrix_diag(s)[:,:,:] @ tf.transpose(V,(0,2,1))[:,:,:]\n",
    "            # data      = data * tf.reduce_mean(tf.transpose(tf.abs(V),(0,2,1)) * scaleds[:,:,None] ,(1),keepdims=True)\n",
    "            update = []\n",
    "            update.append(tf.assign(self.moving_u,self.moving_u))\n",
    "            update.append(tf.assign(self.moving_v,self.moving_v))\n",
    "            return data,update\n",
    "        \n",
    "        data,update  = tf.cond(training_phase,true_fn=training_fn,false_fn=testing_fn)\n",
    "        return data,update\n",
    "    \n",
    "class tf_svd_layer_std():\n",
    "    \n",
    "    def __init__(self,batch_size,channel_size):\n",
    "        self.moving_s = tf.Variable(tf.zeros((batch_size,channel_size),dtype=tf.float32))\n",
    "    \n",
    "    def feedforward(self,data,training_phase):\n",
    "        \n",
    "        with tf.device('/cpu:0'):\n",
    "            s,U,V = tf.svd(data)\n",
    "\n",
    "        smean,sstd = tf.nn.moments(s, axes=(-1))\n",
    "\n",
    "        def training_fn():\n",
    "            snorm  = (s-smean[:,None])/sstd[:,None] + smean[:,None]\n",
    "            data   = U @ tf.matrix_diag(snorm) @ tf.transpose(V,(0,2,1))\n",
    "            update = []\n",
    "            update.append(tf.assign(self.moving_s,self.moving_s*0.9 + 0.1 * snorm))\n",
    "            return data,update\n",
    "            \n",
    "        def testing_fn():\n",
    "            data   = U @ tf.matrix_diag(self.moving_s) @ tf.transpose(V,(0,2,1))\n",
    "            update = []\n",
    "            update.append(tf.assign(self.moving_s,self.moving_s))\n",
    "            return data,update\n",
    "        \n",
    "        data,update  = tf.cond(training_phase,true_fn=training_fn,false_fn=testing_fn)\n",
    "        return data,update\n",
    "    \n",
    "    def backprop(self,grad):\n",
    "        raise NotImplemented('Do not rely on Auto Differentiation')\n",
    "    \n",
    "def show_histogram(layer1,layer1a,grad1w,grad1p):\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(141); plt.hist(layer1. ravel(),batch_size); plt.title('layer')\n",
    "    plt.subplot(142); plt.hist(layer1a.ravel(),batch_size); plt.title('layer a')\n",
    "    plt.subplot(143); plt.hist(grad1w.ravel(),batch_size); plt.title('grad w')\n",
    "    plt.subplot(144); plt.hist(grad1p.ravel(),batch_size); plt.title('grad p')\n",
    "    plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-17T00:52:11.927661Z",
     "start_time": "2018-12-17T00:52:11.924666Z"
    }
   },
   "outputs": [],
   "source": [
    "# hyper paraneter for all \n",
    "num_epoch = 50; batch_size = 20 ; learning_rate = 0.0008; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-17T01:12:39.559754Z",
     "start_time": "2018-12-17T01:06:42.526020Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Iter : 0/50 batch : 7980/8000 acc : 0.35\n",
      " Current : 0 Acc : 0.2598000031709671 Test Acc : 0.31537500210106373\n",
      "\n",
      "Current Iter : 1/50 batch : 7980/8000 acc : 0.35\n",
      " Current : 1 Acc : 0.32500000163912773 Test Acc : 0.3465000016335398\n",
      "\n",
      "Current Iter : 2/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 2 Acc : 0.35980000177025795 Test Acc : 0.36875000143423675\n",
      "\n",
      "Current Iter : 3/50 batch : 7980/8000 acc : 0.55\n",
      " Current : 3 Acc : 0.39720000022649765 Test Acc : 0.3572500013373792\n",
      "\n",
      "Current Iter : 4/50 batch : 7980/8000 acc : 0.55\n",
      " Current : 4 Acc : 0.42340000092983243 Test Acc : 0.38600000108592214\n",
      "\n",
      "Current Iter : 5/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 5 Acc : 0.45360000151395796 Test Acc : 0.43537500105798244\n",
      "\n",
      "Current Iter : 6/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 6 Acc : 0.4786000017821789 Test Acc : 0.42825000051409007\n",
      "\n",
      "Current Iter : 7/50 batch : 7980/8000 acc : 0.55\n",
      " Current : 7 Acc : 0.5080000010132789 Test Acc : 0.4410000009275973\n",
      "\n",
      "Current Iter : 8/50 batch : 7980/8000 acc : 0.55\n",
      " Current : 8 Acc : 0.5292000022530555 Test Acc : 0.4587500004842877\n",
      "\n",
      "Current Iter : 9/50 batch : 7980/8000 acc : 0.55\n",
      " Current : 9 Acc : 0.5581999986767768 Test Acc : 0.46912500225007536\n",
      "\n",
      "Current Iter : 10/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 10 Acc : 0.5764000000953674 Test Acc : 0.47300000209361315\n",
      "\n",
      "Current Iter : 11/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 11 Acc : 0.5918000006079673 Test Acc : 0.475500002540648\n",
      "\n",
      "Current Iter : 12/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 12 Acc : 0.6106000010967254 Test Acc : 0.48312500059604646\n",
      "\n",
      "Current Iter : 13/50 batch : 7980/8000 acc : 0.35\n",
      " Current : 13 Acc : 0.6240000011920929 Test Acc : 0.48287500072270634\n",
      "\n",
      "Current Iter : 14/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 14 Acc : 0.6372000000476837 Test Acc : 0.4772500010021031\n",
      "\n",
      "Current Iter : 15/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 15 Acc : 0.6449999992847443 Test Acc : 0.4710000004339963\n",
      "\n",
      "Current Iter : 16/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 16 Acc : 0.6612000004053116 Test Acc : 0.4658750013075769\n",
      "\n",
      "Current Iter : 17/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 17 Acc : 0.6728000000715256 Test Acc : 0.4722500008717179\n",
      "\n",
      "Current Iter : 18/50 batch : 7980/8000 acc : 0.55\n",
      " Current : 18 Acc : 0.6808000000715255 Test Acc : 0.4627500014379621\n",
      "\n",
      "Current Iter : 19/50 batch : 7980/8000 acc : 0.55\n",
      " Current : 19 Acc : 0.6942000000476837 Test Acc : 0.4728750008530915\n",
      "\n",
      "Current Iter : 20/50 batch : 7980/8000 acc : 0.55\n",
      " Current : 20 Acc : 0.7001999998092652 Test Acc : 0.47187500070780514\n",
      "\n",
      "Current Iter : 21/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 21 Acc : 0.7108000017404557 Test Acc : 0.4642500012554228\n",
      "\n",
      "Current Iter : 22/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 22 Acc : 0.7260000004768371 Test Acc : 0.46000000154599546\n",
      "\n",
      "Current Iter : 23/50 batch : 7980/8000 acc : 0.55\n",
      " Current : 23 Acc : 0.7298000009059906 Test Acc : 0.45175000205636023\n",
      "\n",
      "Current Iter : 24/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 24 Acc : 0.7394000003337861 Test Acc : 0.45212500140070916\n",
      "\n",
      "Current Iter : 25/50 batch : 7980/8000 acc : 0.55\n",
      " Current : 25 Acc : 0.748599999666214 Test Acc : 0.44737500216811893\n",
      "\n",
      "Current Iter : 26/50 batch : 7980/8000 acc : 0.55\n",
      " Current : 26 Acc : 0.7596000003814697 Test Acc : 0.4496250026114285\n",
      "\n",
      "Current Iter : 27/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 27 Acc : 0.7588000018596649 Test Acc : 0.4542500002123415\n",
      "\n",
      "Current Iter : 28/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 28 Acc : 0.7692000011205673 Test Acc : 0.4466250011883676\n",
      "\n",
      "Current Iter : 29/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 29 Acc : 0.7756000027656555 Test Acc : 0.44537500133737923\n",
      "\n",
      "Current Iter : 30/50 batch : 7980/8000 acc : 0.55\n",
      " Current : 30 Acc : 0.7796000025272369 Test Acc : 0.4525000016205013\n",
      "\n",
      "Current Iter : 31/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 31 Acc : 0.7920000011920929 Test Acc : 0.4501250013336539\n",
      "\n",
      "Current Iter : 32/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 32 Acc : 0.7994000008106231 Test Acc : 0.4563750012591481\n",
      "\n",
      "Current Iter : 33/50 batch : 7980/8000 acc : 0.55\n",
      " Current : 33 Acc : 0.8040000011920929 Test Acc : 0.4505000009387732\n",
      "\n",
      "Current Iter : 34/50 batch : 7980/8000 acc : 0.55\n",
      " Current : 34 Acc : 0.8026000037193298 Test Acc : 0.44937500104308126\n",
      "\n",
      "Current Iter : 35/50 batch : 7980/8000 acc : 0.55\n",
      " Current : 35 Acc : 0.81900000166893 Test Acc : 0.4487500014528632\n",
      "\n",
      "Current Iter : 36/50 batch : 7980/8000 acc : 0.55\n",
      " Current : 36 Acc : 0.8178000032901764 Test Acc : 0.4410000018961728\n",
      "\n",
      "Current Iter : 37/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 37 Acc : 0.8118000004291535 Test Acc : 0.4406250015925616\n",
      "\n",
      "Current Iter : 38/50 batch : 7980/8000 acc : 0.55\n",
      " Current : 38 Acc : 0.8192000012397767 Test Acc : 0.44400000139139595\n",
      "\n",
      "Current Iter : 39/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 39 Acc : 0.8292000021934509 Test Acc : 0.4473750009946525\n",
      "\n",
      "Current Iter : 40/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 40 Acc : 0.8346000005006791 Test Acc : 0.44225000144913795\n",
      "\n",
      "Current Iter : 41/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 41 Acc : 0.8466000006198883 Test Acc : 0.44037500163540244\n",
      "\n",
      "Current Iter : 42/50 batch : 7980/8000 acc : 0.55\n",
      " Current : 42 Acc : 0.852400000333786 Test Acc : 0.4377500008419156\n",
      "\n",
      "Current Iter : 43/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 43 Acc : 0.8583999996185303 Test Acc : 0.4401250008773059\n",
      "\n",
      "Current Iter : 44/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 44 Acc : 0.8625999991893768 Test Acc : 0.43425000173971057\n",
      "\n",
      "Current Iter : 45/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 45 Acc : 0.8769999983310699 Test Acc : 0.43350000215694307\n",
      "\n",
      "Current Iter : 46/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 46 Acc : 0.8819999959468842 Test Acc : 0.43225000109523537\n",
      "\n",
      "Current Iter : 47/50 batch : 7980/8000 acc : 0.55\n",
      " Current : 47 Acc : 0.8835999965667725 Test Acc : 0.43412500122562053\n",
      "\n",
      "Current Iter : 48/50 batch : 7980/8000 acc : 0.55\n",
      " Current : 48 Acc : 0.881599997997284 Test Acc : 0.4275000008754432\n",
      "\n",
      "Current Iter : 49/50 batch : 7980/8000 acc : 0.55\n",
      " Current : 49 Acc : 0.885399995803833 Test Acc : 0.43487500175833704\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# frist batch normalization\n",
    "# sess = tf.InteractiveSession()\n",
    "\n",
    "# 1. layers\n",
    "l1 = CNN(3,3, 16); l1n = tf_batch_norm_layer(16,(0,1,2))\n",
    "l2 = CNN(3,16,16); l2n = tf_batch_norm_layer(16,(0,1,2))\n",
    "l3 = CNN(3,16,16); l3n = tf_batch_norm_layer(16,(0,1,2))\n",
    "l4 = CNN(3,16,16); l4n = tf_batch_norm_layer(16,(0,1,2))\n",
    "l5 = CNN(3,16,16); l5n = tf_batch_norm_layer(16,(0,1,2))\n",
    "l6 = CNN(3,16,10); \n",
    "\n",
    "# 2. graph \n",
    "x = tf.placeholder(tf.float32,(batch_size,96,96,3))\n",
    "y = tf.placeholder(tf.float32,(batch_size,10))\n",
    "is_train = tf.placeholder_with_default(True,())\n",
    "\n",
    "layer1, layer1a = l1. feedforward(x,stride=2)\n",
    "layer1b,update1 = l1n.feedforward(layer1a,is_train)\n",
    "layer2, layer2a = l2. feedforward(layer1b,stride=2)\n",
    "layer2b,update2 = l2n.feedforward(layer2a,is_train)\n",
    "layer3, layer3a = l3. feedforward(layer2b,stride=2)\n",
    "layer3b,update3 = l3n.feedforward(layer3a,is_train)\n",
    "layer4, layer4a = l4. feedforward(layer3b,stride=2)\n",
    "layer4b,update4 = l4n.feedforward(layer4a,is_train)\n",
    "\n",
    "layer5, layer5a = l5. feedforward(layer4b)\n",
    "layer5b,update5 = l5n.feedforward(layer5a,is_train)\n",
    "layer6, layer6a = l6. feedforward(layer5b)\n",
    "\n",
    "final_layer = tf.reduce_mean(layer6a,(1,2))\n",
    "cost        = tf.nn.softmax_cross_entropy_with_logits_v2(logits=final_layer,labels=y)\n",
    "auto_train  = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "update_ops  = update1 + update2 + update3 + update4 + update5\n",
    "final_softmax      = tf_softmax(final_layer)\n",
    "correct_prediction = tf.equal(tf.argmax(final_softmax, 1), tf.argmax(y, 1))\n",
    "accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# 3. train\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "avg_acc_train = 0; avg_acc_test  = 0; \n",
    "train_acc     = [];test_acc = []\n",
    "for iter in range(num_epoch):\n",
    "\n",
    "    for current_batch_index in range(0,len(train_images),batch_size):\n",
    "        current_data  = train_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = train_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy,auto_train,update_ops],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(train_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_train = avg_acc_train + sess_results[0]\n",
    "        \n",
    "    for current_batch_index in range(0,len(test_images), batch_size):\n",
    "        current_data  = test_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = test_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy],feed_dict={x:current_data,y:current_label,is_train:False})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(test_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_test = avg_acc_test + sess_results[0]        \n",
    "        \n",
    "    print(\"\\n Current : \"+ str(iter) + \" Acc : \" + str(avg_acc_train/(len(train_images)/batch_size)) + \" Test Acc : \" + str(avg_acc_test/(len(test_images)/batch_size)) + '\\n')\n",
    "    \n",
    "    # save the training\n",
    "    train_acc.append(avg_acc_train/(len(train_images)/batch_size))\n",
    "    test_acc .append(avg_acc_test/(len(test_images)/batch_size)  )\n",
    "    \n",
    "    # get weights\n",
    "    l1w,l2w,l3w,l4w,l5w,l6w = sess.run([l1.getw(),l2.getw(),l3.getw(),l4.getw(),l5.getw(),l6.getw()])\n",
    "    \n",
    "    plt.figure(figsize=(25,15))\n",
    "    plt.suptitle('Current Iter : ' + str(iter))\n",
    "    plt.subplot(231); plt.hist(l1w.ravel(),50); plt.title('layer 1')\n",
    "    plt.subplot(232); plt.hist(l2w.ravel(),50); plt.title('layer 2')\n",
    "    plt.subplot(233); plt.hist(l3w.ravel(),50); plt.title('layer 3')\n",
    "    plt.subplot(234); plt.hist(l4w.ravel(),50); plt.title('layer 4')\n",
    "    plt.subplot(235); plt.hist(l5w.ravel(),50); plt.title('layer 5')\n",
    "    plt.subplot(236); plt.hist(l6w.ravel(),50); plt.title('layer 6')\n",
    "    plt.savefig('figure/' + str(iter)+'.png')\n",
    "    plt.tight_layout()\n",
    "    plt.close('all')\n",
    "    \n",
    "    avg_acc_train = 0 ; avg_acc_test  = 0\n",
    "\n",
    "# 4. reset \n",
    "\n",
    "np.save('figure/train.npy',train_acc)\n",
    "np.save('figure/test.npy',test_acc)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-17T01:23:24.050558Z",
     "start_time": "2018-12-17T01:17:17.733907Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Iter : 0/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 0 Acc : 0.22460000264644622 Test Acc : 0.24662500339560212\n",
      "\n",
      "Current Iter : 1/50 batch : 7980/8000 acc : 0.25\n",
      " Current : 1 Acc : 0.3102000021636486 Test Acc : 0.25825000297278167\n",
      "\n",
      "Current Iter : 2/50 batch : 7980/8000 acc : 0.35\n",
      " Current : 2 Acc : 0.36360000070929527 Test Acc : 0.3091250027064234\n",
      "\n",
      "Current Iter : 3/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 3 Acc : 0.39660000130534173 Test Acc : 0.34225000088103114\n",
      "\n",
      "Current Iter : 4/50 batch : 7980/8000 acc : 0.35\n",
      " Current : 4 Acc : 0.43200000110268594 Test Acc : 0.3442500014416873\n",
      "\n",
      "Current Iter : 5/50 batch : 7980/8000 acc : 0.35\n",
      " Current : 5 Acc : 0.4632000015079975 Test Acc : 0.3567500010598451\n",
      "\n",
      "Current Iter : 6/50 batch : 7980/8000 acc : 0.35\n",
      " Current : 6 Acc : 0.4896000019609928 Test Acc : 0.3442500009201467\n",
      "\n",
      "Current Iter : 7/50 batch : 7980/8000 acc : 0.35\n",
      " Current : 7 Acc : 0.5116000009775161 Test Acc : 0.35700000163167717\n",
      "\n",
      "Current Iter : 8/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 8 Acc : 0.5318000007271767 Test Acc : 0.36275000104680655\n",
      "\n",
      "Current Iter : 9/50 batch : 7980/8000 acc : 0.35\n",
      " Current : 9 Acc : 0.5448000012636185 Test Acc : 0.36387500071898105\n",
      "\n",
      "Current Iter : 10/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 10 Acc : 0.5588000006079674 Test Acc : 0.36512500112876295\n",
      "\n",
      "Current Iter : 11/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 11 Acc : 0.5669999998211861 Test Acc : 0.3610000010021031\n",
      "\n",
      "Current Iter : 12/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 12 Acc : 0.5798000012636184 Test Acc : 0.3635000011138618\n",
      "\n",
      "Current Iter : 13/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 13 Acc : 0.5958000003695488 Test Acc : 0.36250000189058484\n",
      "\n",
      "Current Iter : 14/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 14 Acc : 0.6030000022053719 Test Acc : 0.35537500130012634\n",
      "\n",
      "Current Iter : 15/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 15 Acc : 0.6160000013113022 Test Acc : 0.3495000010728836\n",
      "\n",
      "Current Iter : 16/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 16 Acc : 0.6206000002622605 Test Acc : 0.35237500108778474\n",
      "\n",
      "Current Iter : 17/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 17 Acc : 0.6360000004768371 Test Acc : 0.360750000718981\n",
      "\n",
      "Current Iter : 18/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 18 Acc : 0.6460000021457672 Test Acc : 0.35900000121444464\n",
      "\n",
      "Current Iter : 19/50 batch : 7980/8000 acc : 0.55\n",
      " Current : 19 Acc : 0.6590000011920929 Test Acc : 0.35237500187940896\n",
      "\n",
      "Current Iter : 20/50 batch : 7980/8000 acc : 0.55\n",
      " Current : 20 Acc : 0.665000000834465 Test Acc : 0.34800000126473607\n",
      "\n",
      "Current Iter : 21/50 batch : 7980/8000 acc : 0.55\n",
      " Current : 21 Acc : 0.6751999999284745 Test Acc : 0.3521250016987324\n",
      "\n",
      "Current Iter : 22/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 22 Acc : 0.6774000012874604 Test Acc : 0.3446250015683472\n",
      "\n",
      "Current Iter : 23/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 23 Acc : 0.678 Test Acc : 0.33962500108405946\n",
      "\n",
      "Current Iter : 24/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 24 Acc : 0.6876000002622604 Test Acc : 0.32675000130198895\n",
      "\n",
      "Current Iter : 25/50 batch : 7980/8000 acc : 0.55\n",
      " Current : 25 Acc : 0.6984000022411346 Test Acc : 0.33212500100955367\n",
      "\n",
      "Current Iter : 26/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 26 Acc : 0.712200001001358 Test Acc : 0.3281250018067658\n",
      "\n",
      "Current Iter : 27/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 27 Acc : 0.7200000010728836 Test Acc : 0.32400000139139595\n",
      "\n",
      "Current Iter : 28/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 28 Acc : 0.7266000012159347 Test Acc : 0.3303750015702099\n",
      "\n",
      "Current Iter : 29/50 batch : 7980/8000 acc : 0.35\n",
      " Current : 29 Acc : 0.7310000002384186 Test Acc : 0.3438750012591481\n",
      "\n",
      "Current Iter : 30/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 30 Acc : 0.7356000008583069 Test Acc : 0.3526250010728836\n",
      "\n",
      "Current Iter : 31/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 31 Acc : 0.7368000011444091 Test Acc : 0.3430000012740493\n",
      "\n",
      "Current Iter : 32/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 32 Acc : 0.7483999998569488 Test Acc : 0.3440000014659017\n",
      "\n",
      "Current Iter : 33/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 33 Acc : 0.7392000013589859 Test Acc : 0.34712500172667204\n",
      "\n",
      "Current Iter : 34/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 34 Acc : 0.7460000011920929 Test Acc : 0.3576250018645078\n",
      "\n",
      "Current Iter : 35/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 35 Acc : 0.7470000009536744 Test Acc : 0.3643750017974526\n",
      "\n",
      "Current Iter : 36/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 36 Acc : 0.7442000026702881 Test Acc : 0.3623750018142164\n",
      "\n",
      "Current Iter : 37/50 batch : 7980/8000 acc : 0.55\n",
      " Current : 37 Acc : 0.7520000002384186 Test Acc : 0.35775000086054204\n",
      "\n",
      "Current Iter : 38/50 batch : 7980/8000 acc : 0.55\n",
      " Current : 38 Acc : 0.7568000020980835 Test Acc : 0.3535000014677644\n",
      "\n",
      "Current Iter : 39/50 batch : 7980/8000 acc : 0.55\n",
      " Current : 39 Acc : 0.764400001525879 Test Acc : 0.35537500160746277\n",
      "\n",
      "Current Iter : 40/50 batch : 7980/8000 acc : 0.55\n",
      " Current : 40 Acc : 0.77500000166893 Test Acc : 0.36100000121630726\n",
      "\n",
      "Current Iter : 41/50 batch : 7980/8000 acc : 0.55\n",
      " Current : 41 Acc : 0.7882000029087066 Test Acc : 0.3611250010598451\n",
      "\n",
      "Current Iter : 42/50 batch : 7980/8000 acc : 0.55\n",
      " Current : 42 Acc : 0.7992000024318695 Test Acc : 0.35975000173784794\n",
      "\n",
      "Current Iter : 43/50 batch : 7980/8000 acc : 0.55\n",
      " Current : 43 Acc : 0.8058000011444092 Test Acc : 0.35725000059232115\n",
      "\n",
      "Current Iter : 44/50 batch : 7980/8000 acc : 0.65\n",
      " Current : 44 Acc : 0.8106000020503997 Test Acc : 0.36012500178068874\n",
      "\n",
      "Current Iter : 45/50 batch : 7980/8000 acc : 0.55\n",
      " Current : 45 Acc : 0.8170000002384186 Test Acc : 0.3590000009536743\n",
      "\n",
      "Current Iter : 46/50 batch : 7980/8000 acc : 0.55\n",
      " Current : 46 Acc : 0.8260000004768372 Test Acc : 0.357875001328066\n",
      "\n",
      "Current Iter : 47/50 batch : 7980/8000 acc : 0.55\n",
      " Current : 47 Acc : 0.8265999999046326 Test Acc : 0.3545000011008233\n",
      "\n",
      "Current Iter : 48/50 batch : 7980/8000 acc : 0.55\n",
      " Current : 48 Acc : 0.8229999992847442 Test Acc : 0.3550000012945384\n",
      "\n",
      "Current Iter : 49/50 batch : 7980/8000 acc : 0.55\n",
      " Current : 49 Acc : 0.833 Test Acc : 0.35262500141747294\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# frist layer normalization\n",
    "\n",
    "# 1. layers\n",
    "l1 = CNN(3,3, 16); l1n = tf_layer_norm_layer(batch_size,(1,2,3))\n",
    "l2 = CNN(3,16,16); l2n = tf_layer_norm_layer(batch_size,(1,2,3))\n",
    "l3 = CNN(3,16,16); l3n = tf_layer_norm_layer(batch_size,(1,2,3))\n",
    "l4 = CNN(3,16,16); l4n = tf_layer_norm_layer(batch_size,(1,2,3))\n",
    "l5 = CNN(3,16,16); l5n = tf_layer_norm_layer(batch_size,(1,2,3))\n",
    "l6 = CNN(3,16,10); \n",
    "\n",
    "# 2. graph \n",
    "x = tf.placeholder(tf.float32,(batch_size,96,96,3))\n",
    "y = tf.placeholder(tf.float32,(batch_size,10))\n",
    "is_train = tf.placeholder_with_default(True,())\n",
    "\n",
    "layer1, layer1a = l1. feedforward(x,stride=2)\n",
    "layer1b,update1 = l1n.feedforward(layer1a,is_train)\n",
    "layer2, layer2a = l2. feedforward(layer1b,stride=2)\n",
    "layer2b,update2 = l2n.feedforward(layer2a,is_train)\n",
    "layer3, layer3a = l3. feedforward(layer2b,stride=2)\n",
    "layer3b,update3 = l3n.feedforward(layer3a,is_train)\n",
    "layer4, layer4a = l4. feedforward(layer3b,stride=2)\n",
    "layer4b,update4 = l4n.feedforward(layer4a,is_train)\n",
    "\n",
    "layer5, layer5a = l5. feedforward(layer4b)\n",
    "layer5b,update5 = l5n.feedforward(layer5a,is_train)\n",
    "layer6, layer6a = l6. feedforward(layer5b)\n",
    "\n",
    "final_layer = tf.reduce_mean(layer6a,(1,2))\n",
    "cost        = tf.nn.softmax_cross_entropy_with_logits_v2(logits=final_layer,labels=y)\n",
    "auto_train  = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "update_ops  = update1 + update2 + update3 + update4 + update5\n",
    "final_softmax      = tf_softmax(final_layer)\n",
    "correct_prediction = tf.equal(tf.argmax(final_softmax, 1), tf.argmax(y, 1))\n",
    "accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# 3. train\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "avg_acc_train = 0; avg_acc_test  = 0; \n",
    "train_acc     = [];test_acc = []\n",
    "for iter in range(num_epoch):\n",
    "\n",
    "    for current_batch_index in range(0,len(train_images),batch_size):\n",
    "        current_data  = train_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = train_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy,auto_train,update_ops],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(train_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_train = avg_acc_train + sess_results[0]\n",
    "        \n",
    "    for current_batch_index in range(0,len(test_images), batch_size):\n",
    "        current_data  = test_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = test_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy],feed_dict={x:current_data,y:current_label,is_train:False})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(test_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_test = avg_acc_test + sess_results[0]        \n",
    "        \n",
    "    print(\"\\n Current : \"+ str(iter) + \" Acc : \" + str(avg_acc_train/(len(train_images)/batch_size)) + \" Test Acc : \" + str(avg_acc_test/(len(test_images)/batch_size)) + '\\n')\n",
    "    \n",
    "    # save the training\n",
    "    train_acc.append(avg_acc_train/(len(train_images)/batch_size))\n",
    "    test_acc .append(avg_acc_test/(len(test_images)/batch_size)  )\n",
    "    \n",
    "    # get weights\n",
    "    l1w,l2w,l3w,l4w,l5w,l6w = sess.run([l1.getw(),l2.getw(),l3.getw(),l4.getw(),l5.getw(),l6.getw()])\n",
    "    \n",
    "    plt.figure(figsize=(25,15))\n",
    "    plt.suptitle('Current Iter : ' + str(iter))\n",
    "    plt.subplot(231); plt.hist(l1w.ravel(),50); plt.title('layer 1')\n",
    "    plt.subplot(232); plt.hist(l2w.ravel(),50); plt.title('layer 2')\n",
    "    plt.subplot(233); plt.hist(l3w.ravel(),50); plt.title('layer 3')\n",
    "    plt.subplot(234); plt.hist(l4w.ravel(),50); plt.title('layer 4')\n",
    "    plt.subplot(235); plt.hist(l5w.ravel(),50); plt.title('layer 5')\n",
    "    plt.subplot(236); plt.hist(l6w.ravel(),50); plt.title('layer 6')\n",
    "    plt.savefig('figure/' + str(iter)+'.png')\n",
    "    plt.tight_layout()\n",
    "    plt.close('all')\n",
    "    \n",
    "    avg_acc_train = 0 ; avg_acc_test  = 0\n",
    "\n",
    "# 4. reset \n",
    "\n",
    "np.save('figure/train.npy',train_acc)\n",
    "np.save('figure/test.npy',test_acc)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-17T01:30:18.290916Z",
     "start_time": "2018-12-17T01:24:15.501290Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Iter : 0/50 batch : 7980/8000 acc : 0.25\n",
      " Current : 0 Acc : 0.20480000300705434 Test Acc : 0.1883750031143427\n",
      "\n",
      "Current Iter : 1/50 batch : 7980/8000 acc : 0.25\n",
      " Current : 1 Acc : 0.3100000019222498 Test Acc : 0.2053750030975789\n",
      "\n",
      "Current Iter : 2/50 batch : 7980/8000 acc : 0.25\n",
      " Current : 2 Acc : 0.35400000017881394 Test Acc : 0.17637500318698585\n",
      "\n",
      "Current Iter : 3/50 batch : 7980/8000 acc : 0.25\n",
      " Current : 3 Acc : 0.3974000030755997 Test Acc : 0.20562500338070094\n",
      "\n",
      "Current Iter : 4/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 4 Acc : 0.43620000156760214 Test Acc : 0.2027500033657998\n",
      "\n",
      "Current Iter : 5/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 5 Acc : 0.46700000035762784 Test Acc : 0.2048750031646341\n",
      "\n",
      "Current Iter : 6/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 6 Acc : 0.4872000002264977 Test Acc : 0.19212500320747494\n",
      "\n",
      "Current Iter : 7/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 7 Acc : 0.5124000014662743 Test Acc : 0.18200000326149166\n",
      "\n",
      "Current Iter : 8/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 8 Acc : 0.5230000007748604 Test Acc : 0.18212500302121043\n",
      "\n",
      "Current Iter : 9/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 9 Acc : 0.5510000016093254 Test Acc : 0.18212500311434268\n",
      "\n",
      "Current Iter : 10/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 10 Acc : 0.5616000003814697 Test Acc : 0.17087500323541463\n",
      "\n",
      "Current Iter : 11/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 11 Acc : 0.576000000834465 Test Acc : 0.18725000361911953\n",
      "\n",
      "Current Iter : 12/50 batch : 7980/8000 acc : 0.25\n",
      " Current : 12 Acc : 0.5891999993920326 Test Acc : 0.18612500291317702\n",
      "\n",
      "Current Iter : 13/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 13 Acc : 0.6016000007390976 Test Acc : 0.18337500338442625\n",
      "\n",
      "Current Iter : 14/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 14 Acc : 0.6236000001430512 Test Acc : 0.18987500319257378\n",
      "\n",
      "Current Iter : 15/50 batch : 7980/8000 acc : 0.25\n",
      " Current : 15 Acc : 0.6381999989748001 Test Acc : 0.18500000327825547\n",
      "\n",
      "Current Iter : 16/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 16 Acc : 0.6469999992847443 Test Acc : 0.18500000303611158\n",
      "\n",
      "Current Iter : 17/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 17 Acc : 0.658799998998642 Test Acc : 0.17562500306405127\n",
      "\n",
      "Current Iter : 18/50 batch : 7980/8000 acc : 0.05\n",
      " Current : 18 Acc : 0.6661999995708465 Test Acc : 0.1732500030193478\n",
      "\n",
      "Current Iter : 19/50 batch : 7980/8000 acc : 0.25\n",
      " Current : 19 Acc : 0.6774000002145767 Test Acc : 0.18175000279210507\n",
      "\n",
      "Current Iter : 20/50 batch : 7980/8000 acc : 0.25\n",
      " Current : 20 Acc : 0.6901999998092652 Test Acc : 0.1823750026617199\n",
      "\n",
      "Current Iter : 21/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 21 Acc : 0.6975999997854233 Test Acc : 0.1861250030901283\n",
      "\n",
      "Current Iter : 22/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 22 Acc : 0.720400000333786 Test Acc : 0.18225000296719374\n",
      "\n",
      "Current Iter : 23/50 batch : 7980/8000 acc : 0.25\n",
      " Current : 23 Acc : 0.7161999995708466 Test Acc : 0.1805000028386712\n",
      "\n",
      "Current Iter : 24/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 24 Acc : 0.7260000001192093 Test Acc : 0.18587500301189722\n",
      "\n",
      "Current Iter : 25/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 25 Acc : 0.7412000014781952 Test Acc : 0.17975000340491534\n",
      "\n",
      "Current Iter : 26/50 batch : 7980/8000 acc : 0.05\n",
      " Current : 26 Acc : 0.7530000011920929 Test Acc : 0.18100000358186663\n",
      "\n",
      "Current Iter : 27/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 27 Acc : 0.7556000008583069 Test Acc : 0.18637500314041971\n",
      "\n",
      "Current Iter : 28/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 28 Acc : 0.7608000004291534 Test Acc : 0.17612500333227218\n",
      "\n",
      "Current Iter : 29/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 29 Acc : 0.7730000021457673 Test Acc : 0.17300000296905638\n",
      "\n",
      "Current Iter : 30/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 30 Acc : 0.7792000019550324 Test Acc : 0.17350000320002437\n",
      "\n",
      "Current Iter : 31/50 batch : 7980/8000 acc : 0.05\n",
      " Current : 31 Acc : 0.781200001001358 Test Acc : 0.17275000289082526\n",
      "\n",
      "Current Iter : 32/50 batch : 7980/8000 acc : 0.05\n",
      " Current : 32 Acc : 0.7868000018596649 Test Acc : 0.17837500310502946\n",
      "\n",
      "Current Iter : 33/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 33 Acc : 0.7990000009536743 Test Acc : 0.17512500296346845\n",
      "\n",
      "Current Iter : 34/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 34 Acc : 0.8096000015735626 Test Acc : 0.17212500278837978\n",
      "\n",
      "Current Iter : 35/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 35 Acc : 0.821200001001358 Test Acc : 0.17050000299699605\n",
      "\n",
      "Current Iter : 36/50 batch : 7980/8000 acc : 0.05\n",
      " Current : 36 Acc : 0.8262000014781952 Test Acc : 0.17212500312365592\n",
      "\n",
      "Current Iter : 37/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 37 Acc : 0.8300000019073487 Test Acc : 0.16925000292249023\n",
      "\n",
      "Current Iter : 38/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 38 Acc : 0.835400000333786 Test Acc : 0.16812500320374965\n",
      "\n",
      "Current Iter : 39/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 39 Acc : 0.8334000010490418 Test Acc : 0.1728750032093376\n",
      "\n",
      "Current Iter : 40/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 40 Acc : 0.8424000000953674 Test Acc : 0.17187500287778676\n",
      "\n",
      "Current Iter : 41/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 41 Acc : 0.8512000005245208 Test Acc : 0.17525000290945172\n",
      "\n",
      "Current Iter : 42/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 42 Acc : 0.8528000001907349 Test Acc : 0.16925000342540442\n",
      "\n",
      "Current Iter : 43/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 43 Acc : 0.8557999973297119 Test Acc : 0.1720000029820949\n",
      "\n",
      "Current Iter : 44/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 44 Acc : 0.859599998474121 Test Acc : 0.17037500300444663\n",
      "\n",
      "Current Iter : 45/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 45 Acc : 0.8579999980926514 Test Acc : 0.17137500299140812\n",
      "\n",
      "Current Iter : 46/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 46 Acc : 0.8631999995708466 Test Acc : 0.1683750028721988\n",
      "\n",
      "Current Iter : 47/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 47 Acc : 0.860599998474121 Test Acc : 0.16700000290758907\n",
      "\n",
      "Current Iter : 48/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 48 Acc : 0.8630000002384186 Test Acc : 0.15662500302307308\n",
      "\n",
      "Current Iter : 49/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 49 Acc : 0.8706000001430512 Test Acc : 0.1593750027474016\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# frist instance normalization\n",
    "\n",
    "# 1. layers\n",
    "l1 = CNN(3,3, 16); l1n = tf_instance_norm_layer(batch_size,16,(1,2))\n",
    "l2 = CNN(3,16,16); l2n = tf_instance_norm_layer(batch_size,16,(1,2))\n",
    "l3 = CNN(3,16,16); l3n = tf_instance_norm_layer(batch_size,16,(1,2))\n",
    "l4 = CNN(3,16,16); l4n = tf_instance_norm_layer(batch_size,16,(1,2))\n",
    "l5 = CNN(3,16,16); l5n = tf_instance_norm_layer(batch_size,16,(1,2))\n",
    "l6 = CNN(3,16,10); \n",
    "\n",
    "# 2. graph \n",
    "x = tf.placeholder(tf.float32,(batch_size,96,96,3))\n",
    "y = tf.placeholder(tf.float32,(batch_size,10))\n",
    "is_train = tf.placeholder_with_default(True,())\n",
    "\n",
    "layer1, layer1a = l1. feedforward(x,stride=2)\n",
    "layer1b,update1 = l1n.feedforward(layer1a,is_train)\n",
    "layer2, layer2a = l2. feedforward(layer1b,stride=2)\n",
    "layer2b,update2 = l2n.feedforward(layer2a,is_train)\n",
    "layer3, layer3a = l3. feedforward(layer2b,stride=2)\n",
    "layer3b,update3 = l3n.feedforward(layer3a,is_train)\n",
    "layer4, layer4a = l4. feedforward(layer3b,stride=2)\n",
    "layer4b,update4 = l4n.feedforward(layer4a,is_train)\n",
    "\n",
    "layer5, layer5a = l5. feedforward(layer4b)\n",
    "layer5b,update5 = l5n.feedforward(layer5a,is_train)\n",
    "layer6, layer6a = l6. feedforward(layer5b)\n",
    "\n",
    "final_layer = tf.reduce_mean(layer6a,(1,2))\n",
    "cost        = tf.nn.softmax_cross_entropy_with_logits_v2(logits=final_layer,labels=y)\n",
    "auto_train  = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "update_ops  = update1 + update2 + update3 + update4 + update5\n",
    "final_softmax      = tf_softmax(final_layer)\n",
    "correct_prediction = tf.equal(tf.argmax(final_softmax, 1), tf.argmax(y, 1))\n",
    "accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# 3. train\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "avg_acc_train = 0; avg_acc_test  = 0; \n",
    "train_acc     = [];test_acc = []\n",
    "for iter in range(num_epoch):\n",
    "\n",
    "    for current_batch_index in range(0,len(train_images),batch_size):\n",
    "        current_data  = train_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = train_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy,auto_train,update_ops],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(train_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_train = avg_acc_train + sess_results[0]\n",
    "        \n",
    "    for current_batch_index in range(0,len(test_images), batch_size):\n",
    "        current_data  = test_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = test_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy],feed_dict={x:current_data,y:current_label,is_train:False})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(test_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_test = avg_acc_test + sess_results[0]        \n",
    "        \n",
    "    print(\"\\n Current : \"+ str(iter) + \" Acc : \" + str(avg_acc_train/(len(train_images)/batch_size)) + \" Test Acc : \" + str(avg_acc_test/(len(test_images)/batch_size)) + '\\n')\n",
    "    \n",
    "    # save the training\n",
    "    train_acc.append(avg_acc_train/(len(train_images)/batch_size))\n",
    "    test_acc .append(avg_acc_test/(len(test_images)/batch_size)  )\n",
    "    \n",
    "    # get weights\n",
    "    l1w,l2w,l3w,l4w,l5w,l6w = sess.run([l1.getw(),l2.getw(),l3.getw(),l4.getw(),l5.getw(),l6.getw()])\n",
    "    \n",
    "    plt.figure(figsize=(25,15))\n",
    "    plt.suptitle('Current Iter : ' + str(iter))\n",
    "    plt.subplot(231); plt.hist(l1w.ravel(),50); plt.title('layer 1')\n",
    "    plt.subplot(232); plt.hist(l2w.ravel(),50); plt.title('layer 2')\n",
    "    plt.subplot(233); plt.hist(l3w.ravel(),50); plt.title('layer 3')\n",
    "    plt.subplot(234); plt.hist(l4w.ravel(),50); plt.title('layer 4')\n",
    "    plt.subplot(235); plt.hist(l5w.ravel(),50); plt.title('layer 5')\n",
    "    plt.subplot(236); plt.hist(l6w.ravel(),50); plt.title('layer 6')\n",
    "    plt.savefig('figure/' + str(iter)+'.png')\n",
    "    plt.tight_layout()\n",
    "    plt.close('all')\n",
    "    \n",
    "    avg_acc_train = 0 ; avg_acc_test  = 0\n",
    "\n",
    "# 4. reset \n",
    "\n",
    "np.save('figure/train.npy',train_acc)\n",
    "np.save('figure/test.npy',test_acc)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-17T01:56:07.289530Z",
     "start_time": "2018-12-17T01:50:42.836627Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Iter : 0/50 batch : 7980/8000 acc : 0.35\n",
      " Current : 0 Acc : 0.14240000252425672 Test Acc : 0.202750003086403\n",
      "\n",
      "Current Iter : 1/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 1 Acc : 0.23560000264644623 Test Acc : 0.27787500246427954\n",
      "\n",
      "Current Iter : 2/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 2 Acc : 0.3140000018775463 Test Acc : 0.31537500181235373\n",
      "\n",
      "Current Iter : 3/50 batch : 7980/8000 acc : 0.35\n",
      " Current : 3 Acc : 0.3474000013321638 Test Acc : 0.32575000253506003\n",
      "\n",
      "Current Iter : 4/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 4 Acc : 0.3592000016719103 Test Acc : 0.34637500180862846\n",
      "\n",
      "Current Iter : 5/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 5 Acc : 0.37120000152289867 Test Acc : 0.3552500022482127\n",
      "\n",
      "Current Iter : 6/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 6 Acc : 0.38020000176131724 Test Acc : 0.3673750019073486\n",
      "\n",
      "Current Iter : 7/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 7 Acc : 0.38540000165998933 Test Acc : 0.3717500017769635\n",
      "\n",
      "Current Iter : 8/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 8 Acc : 0.3964000015407801 Test Acc : 0.37987500164657834\n",
      "\n",
      "Current Iter : 9/50 batch : 7980/8000 acc : 0.55\n",
      " Current : 9 Acc : 0.4048000008612871 Test Acc : 0.38987500123679636\n",
      "\n",
      "Current Iter : 10/50 batch : 7980/8000 acc : 0.55\n",
      " Current : 10 Acc : 0.41840000177919867 Test Acc : 0.3970000012032688\n",
      "\n",
      "Current Iter : 11/50 batch : 7980/8000 acc : 0.55\n",
      " Current : 11 Acc : 0.4254000011831522 Test Acc : 0.40687500128522514\n",
      "\n",
      "Current Iter : 12/50 batch : 7980/8000 acc : 0.55\n",
      " Current : 12 Acc : 0.4322000018507242 Test Acc : 0.4130000012926757\n",
      "\n",
      "Current Iter : 13/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 13 Acc : 0.43480000087618825 Test Acc : 0.4231250006146729\n",
      "\n",
      "Current Iter : 14/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 14 Acc : 0.4414000017940998 Test Acc : 0.42950000073760747\n",
      "\n",
      "Current Iter : 15/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 15 Acc : 0.4480000003874302 Test Acc : 0.43450000178068876\n",
      "\n",
      "Current Iter : 16/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 16 Acc : 0.4558000003397465 Test Acc : 0.44312500111758707\n",
      "\n",
      "Current Iter : 17/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 17 Acc : 0.46640000128746034 Test Acc : 0.4456250017695129\n",
      "\n",
      "Current Iter : 18/50 batch : 7980/8000 acc : 0.35\n",
      " Current : 18 Acc : 0.47860000145435333 Test Acc : 0.44500000121071936\n",
      "\n",
      "Current Iter : 19/50 batch : 7980/8000 acc : 0.35\n",
      " Current : 19 Acc : 0.482800000756979 Test Acc : 0.4435000017285347\n",
      "\n",
      "Current Iter : 20/50 batch : 7980/8000 acc : 0.35\n",
      " Current : 20 Acc : 0.489600001513958 Test Acc : 0.44687500145286324\n",
      "\n",
      "Current Iter : 21/50 batch : 7980/8000 acc : 0.35\n",
      " Current : 21 Acc : 0.4940000014603138 Test Acc : 0.4517500012367964\n",
      "\n",
      "Current Iter : 22/50 batch : 7980/8000 acc : 0.35\n",
      " Current : 22 Acc : 0.5028000021278858 Test Acc : 0.4570000005979091\n",
      "\n",
      "Current Iter : 23/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 23 Acc : 0.506800002604723 Test Acc : 0.4607499993685633\n",
      "\n",
      "Current Iter : 24/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 24 Acc : 0.5086000018417836 Test Acc : 0.4601249993406236\n",
      "\n",
      "Current Iter : 25/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 25 Acc : 0.5162000023424625 Test Acc : 0.46625000063329936\n",
      "\n",
      "Current Iter : 26/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 26 Acc : 0.5244000016450882 Test Acc : 0.47000000106170775\n",
      "\n",
      "Current Iter : 27/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 27 Acc : 0.5272000026702881 Test Acc : 0.47050000101327893\n",
      "\n",
      "Current Iter : 28/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 28 Acc : 0.5354000023603439 Test Acc : 0.4733750008419156\n",
      "\n",
      "Current Iter : 29/50 batch : 7980/8000 acc : 0.35\n",
      " Current : 29 Acc : 0.5402000012695789 Test Acc : 0.4765000001341104\n",
      "\n",
      "Current Iter : 30/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 30 Acc : 0.5488000010251999 Test Acc : 0.4801250005140901\n",
      "\n",
      "Current Iter : 31/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 31 Acc : 0.5524000001698732 Test Acc : 0.4828750004991889\n",
      "\n",
      "Current Iter : 32/50 batch : 7980/8000 acc : 0.35\n",
      " Current : 32 Acc : 0.5602000000476837 Test Acc : 0.48550000131130217\n",
      "\n",
      "Current Iter : 33/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 33 Acc : 0.5653999999761582 Test Acc : 0.48862500075250864\n",
      "\n",
      "Current Iter : 34/50 batch : 7980/8000 acc : 0.35\n",
      " Current : 34 Acc : 0.5723999995589256 Test Acc : 0.48950000014156103\n",
      "\n",
      "Current Iter : 35/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 35 Acc : 0.5764000009298325 Test Acc : 0.4910000006854534\n",
      "\n",
      "Current Iter : 36/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 36 Acc : 0.5798000006079673 Test Acc : 0.49225000020116566\n",
      "\n",
      "Current Iter : 37/50 batch : 7980/8000 acc : 0.55\n",
      " Current : 37 Acc : 0.5863999999165534 Test Acc : 0.4905000005662441\n",
      "\n",
      "Current Iter : 38/50 batch : 7980/8000 acc : 0.55\n",
      " Current : 38 Acc : 0.592400000333786 Test Acc : 0.4932500010728836\n",
      "\n",
      "Current Iter : 39/50 batch : 7980/8000 acc : 0.55\n",
      " Current : 39 Acc : 0.5994000004529954 Test Acc : 0.4958749999850988\n",
      "\n",
      "Current Iter : 40/50 batch : 7980/8000 acc : 0.55\n",
      " Current : 40 Acc : 0.6028000000715256 Test Acc : 0.49625000048428775\n",
      "\n",
      "Current Iter : 41/50 batch : 7980/8000 acc : 0.55\n",
      " Current : 41 Acc : 0.6061999998092651 Test Acc : 0.4971250008046627\n",
      "\n",
      "Current Iter : 42/50 batch : 7980/8000 acc : 0.55\n",
      " Current : 42 Acc : 0.6148000006079674 Test Acc : 0.4993750016018748\n",
      "\n",
      "Current Iter : 43/50 batch : 7980/8000 acc : 0.55\n",
      " Current : 43 Acc : 0.6164000018835067 Test Acc : 0.4947500018402934\n",
      "\n",
      "Current Iter : 44/50 batch : 7980/8000 acc : 0.55\n",
      " Current : 44 Acc : 0.6188000022172928 Test Acc : 0.491125001758337\n",
      "\n",
      "Current Iter : 45/50 batch : 7980/8000 acc : 0.55\n",
      " Current : 45 Acc : 0.6274000017046928 Test Acc : 0.4923750010505319\n",
      "\n",
      "Current Iter : 46/50 batch : 7980/8000 acc : 0.55\n",
      " Current : 46 Acc : 0.6356000009775161 Test Acc : 0.4916250015422702\n",
      "\n",
      "Current Iter : 47/50 batch : 7980/8000 acc : 0.65\n",
      " Current : 47 Acc : 0.6382000016570091 Test Acc : 0.4965000022947788\n",
      "\n",
      "Current Iter : 48/50 batch : 7980/8000 acc : 0.65\n",
      " Current : 48 Acc : 0.6410000008344651 Test Acc : 0.49650000311434267\n",
      "\n",
      "Current Iter : 49/50 batch : 7980/8000 acc : 0.65\n",
      " Current : 49 Acc : 0.6500000013113022 Test Acc : 0.49650000311434267\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# frist box normalization\n",
    "\n",
    "# 1. layers\n",
    "l1 = CNN(3,3, 16); l1n = tf_box_cox()\n",
    "l2 = CNN(3,16,16); l2n = tf_box_cox()\n",
    "l3 = CNN(3,16,16); l3n = tf_box_cox()\n",
    "l4 = CNN(3,16,16); l4n = tf_box_cox()\n",
    "l5 = CNN(3,16,16); l5n = tf_box_cox()\n",
    "l6 = CNN(3,16,10); \n",
    "\n",
    "# 2. graph \n",
    "x = tf.placeholder(tf.float32,(batch_size,96,96,3))\n",
    "y = tf.placeholder(tf.float32,(batch_size,10))\n",
    "is_train = tf.placeholder_with_default(True,())\n",
    "\n",
    "layer1, layer1a = l1. feedforward(x,stride=2)\n",
    "layer1b = l1n.feedforward(layer1a)\n",
    "layer2, layer2a = l2. feedforward(layer1b,stride=2)\n",
    "layer2b = l2n.feedforward(layer2a)\n",
    "layer3, layer3a = l3. feedforward(layer2b,stride=2)\n",
    "layer3b = l3n.feedforward(layer3a)\n",
    "layer4, layer4a = l4. feedforward(layer3b,stride=2)\n",
    "layer4b = l4n.feedforward(layer4a)\n",
    "\n",
    "layer5, layer5a = l5. feedforward(layer4b)\n",
    "layer5b = l5n.feedforward(layer5a)\n",
    "layer6, layer6a = l6. feedforward(layer5b)\n",
    "\n",
    "final_layer = tf.reduce_mean(layer6a,(1,2))\n",
    "cost        = tf.nn.softmax_cross_entropy_with_logits_v2(logits=final_layer,labels=y)\n",
    "auto_train  = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "final_softmax      = tf_softmax(final_layer)\n",
    "correct_prediction = tf.equal(tf.argmax(final_softmax, 1), tf.argmax(y, 1))\n",
    "accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# 3. train\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "avg_acc_train = 0; avg_acc_test  = 0; \n",
    "train_acc     = [];test_acc = []\n",
    "for iter in range(num_epoch):\n",
    "\n",
    "    for current_batch_index in range(0,len(train_images),batch_size):\n",
    "        current_data  = train_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = train_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy,auto_train],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(train_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_train = avg_acc_train + sess_results[0]\n",
    "        \n",
    "    for current_batch_index in range(0,len(test_images), batch_size):\n",
    "        current_data  = test_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = test_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy],feed_dict={x:current_data,y:current_label,is_train:False})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(test_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_test = avg_acc_test + sess_results[0]        \n",
    "        \n",
    "    print(\"\\n Current : \"+ str(iter) + \" Acc : \" + str(avg_acc_train/(len(train_images)/batch_size)) + \" Test Acc : \" + str(avg_acc_test/(len(test_images)/batch_size)) + '\\n')\n",
    "    \n",
    "    # save the training\n",
    "    train_acc.append(avg_acc_train/(len(train_images)/batch_size))\n",
    "    test_acc .append(avg_acc_test/(len(test_images)/batch_size)  )\n",
    "    \n",
    "    # get weights\n",
    "    l1w,l2w,l3w,l4w,l5w,l6w = sess.run([l1.getw(),l2.getw(),l3.getw(),l4.getw(),l5.getw(),l6.getw()])\n",
    "    \n",
    "    plt.figure(figsize=(25,15))\n",
    "    plt.suptitle('Current Iter : ' + str(iter))\n",
    "    plt.subplot(231); plt.hist(l1w.ravel(),50); plt.title('layer 1')\n",
    "    plt.subplot(232); plt.hist(l2w.ravel(),50); plt.title('layer 2')\n",
    "    plt.subplot(233); plt.hist(l3w.ravel(),50); plt.title('layer 3')\n",
    "    plt.subplot(234); plt.hist(l4w.ravel(),50); plt.title('layer 4')\n",
    "    plt.subplot(235); plt.hist(l5w.ravel(),50); plt.title('layer 5')\n",
    "    plt.subplot(236); plt.hist(l6w.ravel(),50); plt.title('layer 6')\n",
    "    plt.savefig('figure/' + str(iter)+'.png')\n",
    "    plt.tight_layout()\n",
    "    plt.close('all')\n",
    "    \n",
    "    avg_acc_train = 0 ; avg_acc_test  = 0\n",
    "\n",
    "# 4. reset \n",
    "\n",
    "np.save('figure/train.npy',train_acc)\n",
    "np.save('figure/test.npy',test_acc)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-17T02:06:31.227624Z",
     "start_time": "2018-12-17T02:00:19.974816Z"
    },
    "code_folding": [
     0,
     55
    ],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Iter : 0/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 0 Acc : 0.09980000193417073 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current Iter : 1/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 1 Acc : 0.10020000211894513 Test Acc : 0.10425000239163637\n",
      "\n",
      "Current Iter : 2/50 batch : 7980/8000 acc : 0.35\n",
      " Current : 2 Acc : 0.11060000211000443 Test Acc : 0.16362500314600767\n",
      "\n",
      "Current Iter : 3/50 batch : 7980/8000 acc : 0.35\n",
      " Current : 3 Acc : 0.1760000031143427 Test Acc : 0.19425000322051347\n",
      "\n",
      "Current Iter : 4/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 4 Acc : 0.21280000306665897 Test Acc : 0.18400000302121045\n",
      "\n",
      "Current Iter : 5/50 batch : 7980/8000 acc : 0.35\n",
      " Current : 5 Acc : 0.22500000305473805 Test Acc : 0.20525000306777655\n",
      "\n",
      "Current Iter : 6/50 batch : 7980/8000 acc : 0.35\n",
      " Current : 6 Acc : 0.2576000024676323 Test Acc : 0.22537500319071113\n",
      "\n",
      "Current Iter : 7/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 7 Acc : 0.2980000024884939 Test Acc : 0.26000000272877516\n",
      "\n",
      "Current Iter : 8/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 8 Acc : 0.31940000227093696 Test Acc : 0.2615000031888485\n",
      "\n",
      "Current Iter : 9/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 9 Acc : 0.34080000196397303 Test Acc : 0.2738750022370368\n",
      "\n",
      "Current Iter : 10/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 10 Acc : 0.35680000129342077 Test Acc : 0.2848750028014183\n",
      "\n",
      "Current Iter : 11/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 11 Acc : 0.37000000151991846 Test Acc : 0.2752500026952475\n",
      "\n",
      "Current Iter : 12/50 batch : 7980/8000 acc : 0.25\n",
      " Current : 12 Acc : 0.37440000115334987 Test Acc : 0.2662500028964132\n",
      "\n",
      "Current Iter : 13/50 batch : 7980/8000 acc : 0.35\n",
      " Current : 13 Acc : 0.37860000163316726 Test Acc : 0.2868750024866313\n",
      "\n",
      "Current Iter : 14/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 14 Acc : 0.38680000057816505 Test Acc : 0.29812500265426933\n",
      "\n",
      "Current Iter : 15/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 15 Acc : 0.3934000009596348 Test Acc : 0.29600000265985726\n",
      "\n",
      "Current Iter : 16/50 batch : 7980/8000 acc : 0.55\n",
      " Current : 16 Acc : 0.39400000047683714 Test Acc : 0.3023750017862767\n",
      "\n",
      "Current Iter : 17/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 17 Acc : 0.399000000834465 Test Acc : 0.2990000020340085\n",
      "\n",
      "Current Iter : 18/50 batch : 7980/8000 acc : 0.55\n",
      " Current : 18 Acc : 0.4022000018060207 Test Acc : 0.2961250023730099\n",
      "\n",
      "Current Iter : 19/50 batch : 7980/8000 acc : 0.55\n",
      " Current : 19 Acc : 0.40240000155568123 Test Acc : 0.30562500244006513\n",
      "\n",
      "Current Iter : 20/50 batch : 7980/8000 acc : 0.55\n",
      " Current : 20 Acc : 0.41460000070929526 Test Acc : 0.3020000022090972\n",
      "\n",
      "Current Iter : 21/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 21 Acc : 0.4188000011742115 Test Acc : 0.3151250018645078\n",
      "\n",
      "Current Iter : 22/50 batch : 7980/8000 acc : 0.65\n",
      " Current : 22 Acc : 0.4224000016748905 Test Acc : 0.31300000187009575\n",
      "\n",
      "Current Iter : 23/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 23 Acc : 0.42840000170469283 Test Acc : 0.32037500160746274\n",
      "\n",
      "Current Iter : 24/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 24 Acc : 0.43060000193119047 Test Acc : 0.3158750016801059\n",
      "\n",
      "Current Iter : 25/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 25 Acc : 0.4384000017642975 Test Acc : 0.3110000025015324\n",
      "\n",
      "Current Iter : 26/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 26 Acc : 0.43840000134706497 Test Acc : 0.32137500259093943\n",
      "\n",
      "Current Iter : 27/50 batch : 7980/8000 acc : 0.55\n",
      " Current : 27 Acc : 0.44140000092983245 Test Acc : 0.30750000271946193\n",
      "\n",
      "Current Iter : 28/50 batch : 7980/8000 acc : 0.55\n",
      " Current : 28 Acc : 0.44480000096559524 Test Acc : 0.318750001359731\n",
      "\n",
      "Current Iter : 29/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 29 Acc : 0.4506000007987022 Test Acc : 0.32300000249408184\n",
      "\n",
      "Current Iter : 30/50 batch : 7980/8000 acc : 0.55\n",
      " Current : 30 Acc : 0.45720000183582304 Test Acc : 0.3226250025816262\n",
      "\n",
      "Current Iter : 31/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 31 Acc : 0.4588000017702579 Test Acc : 0.32337500256486235\n",
      "\n",
      "Current Iter : 32/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 32 Acc : 0.46100000193715096 Test Acc : 0.32950000195764007\n",
      "\n",
      "Current Iter : 33/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 33 Acc : 0.4636000023186207 Test Acc : 0.3300000018905848\n",
      "\n",
      "Current Iter : 34/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 34 Acc : 0.4686000009775162 Test Acc : 0.32675000188872216\n",
      "\n",
      "Current Iter : 35/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 35 Acc : 0.4718000006079674 Test Acc : 0.3296250015217811\n",
      "\n",
      "Current Iter : 36/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 36 Acc : 0.48260000109672546 Test Acc : 0.3218750019650906\n",
      "\n",
      "Current Iter : 37/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 37 Acc : 0.48000000125169756 Test Acc : 0.33275000168941915\n",
      "\n",
      "Current Iter : 38/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 38 Acc : 0.4828000013232231 Test Acc : 0.31800000158138575\n",
      "\n",
      "Current Iter : 39/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 39 Acc : 0.4848000009655952 Test Acc : 0.3245000016782433\n",
      "\n",
      "Current Iter : 40/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 40 Acc : 0.4912000017762184 Test Acc : 0.33412500177510085\n",
      "\n",
      "Current Iter : 41/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 41 Acc : 0.49420000171661377 Test Acc : 0.3323750021122396\n",
      "\n",
      "Current Iter : 42/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 42 Acc : 0.5004000016450882 Test Acc : 0.33725000185891985\n",
      "\n",
      "Current Iter : 43/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 43 Acc : 0.5020000013113022 Test Acc : 0.33300000205636027\n",
      "\n",
      "Current Iter : 44/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 44 Acc : 0.5052000015377999 Test Acc : 0.32362500198185445\n",
      "\n",
      "Current Iter : 45/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 45 Acc : 0.5084000024199485 Test Acc : 0.3240000022575259\n",
      "\n",
      "Current Iter : 46/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 46 Acc : 0.5122000029087067 Test Acc : 0.3235000018961728\n",
      "\n",
      "Current Iter : 47/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 47 Acc : 0.5160000023841858 Test Acc : 0.31700000235810877\n",
      "\n",
      "Current Iter : 48/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 48 Acc : 0.5186000009775161 Test Acc : 0.31862500155344603\n",
      "\n",
      "Current Iter : 49/50 batch : 7980/8000 acc : 0.45\n",
      " Current : 49 Acc : 0.5170000011920929 Test Acc : 0.32337500227615235\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# frist ranged normalization\n",
    "\n",
    "# 1. layers\n",
    "l1 = CNN(3,3, 16); l1n = tf_min_max_layer(batch_size,user_max=0.5,user_min=-0.5)\n",
    "l2 = CNN(3,16,16); l2n = tf_min_max_layer(batch_size,user_max=0.5,user_min=-0.5)\n",
    "l3 = CNN(3,16,16); l3n = tf_min_max_layer(batch_size,user_max=0.5,user_min=-0.5)\n",
    "l4 = CNN(3,16,16); l4n = tf_min_max_layer(batch_size,user_max=0.5,user_min=-0.5)\n",
    "l5 = CNN(3,16,16); l5n = tf_min_max_layer(batch_size,user_max=0.5,user_min=-0.5)\n",
    "l6 = CNN(3,16,10); \n",
    "\n",
    "# 2. graph \n",
    "x = tf.placeholder(tf.float32,(batch_size,96,96,3))\n",
    "y = tf.placeholder(tf.float32,(batch_size,10))\n",
    "is_train = tf.placeholder_with_default(True,())\n",
    "\n",
    "layer1, layer1a = l1. feedforward(x,stride=2)\n",
    "layer1a   = tf.reshape(layer1a,(batch_size,-1))\n",
    "layer1b,update1 = l1n.feedforward(layer1a,is_train)\n",
    "layer1b   = tf.reshape(layer1b,(batch_size,48,48,16))\n",
    "\n",
    "layer2, layer2a = l2. feedforward(layer1b,stride=2)\n",
    "layer2a   = tf.reshape(layer2a,(batch_size,-1))\n",
    "layer2b,update2 = l2n.feedforward(layer2a,is_train)\n",
    "layer2b   = tf.reshape(layer2b,(batch_size,24,24,16))\n",
    "\n",
    "layer3, layer3a = l3. feedforward(layer2b,stride=2)\n",
    "layer3a   = tf.reshape(layer3a,(batch_size,-1))\n",
    "layer3b,update3 = l3n.feedforward(layer3a,is_train)\n",
    "layer3b   = tf.reshape(layer3b,(batch_size,12,12,16))\n",
    "\n",
    "layer4, layer4a = l4. feedforward(layer3b,stride=2)\n",
    "layer4a   = tf.reshape(layer4a,(batch_size,-1))\n",
    "layer4b,update4 = l4n.feedforward(layer4a,is_train)\n",
    "layer4b   = tf.reshape(layer4b,(batch_size,6,6,16))\n",
    "\n",
    "layer5, layer5a = l5. feedforward(layer4b)\n",
    "layer5a   = tf.reshape(layer5a,(batch_size,-1))\n",
    "layer5b,update5 = l5n.feedforward(layer5a,is_train)\n",
    "layer5b   = tf.reshape(layer5b,(batch_size,6,6,16))\n",
    "layer6, layer6a = l6. feedforward(layer5b)\n",
    "\n",
    "final_layer = tf.reduce_mean(layer6a,(1,2))\n",
    "cost        = tf.nn.softmax_cross_entropy_with_logits_v2(logits=final_layer,labels=y)\n",
    "auto_train  = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "update_ops  = update1 + update2 + update3 + update4 + update5\n",
    "final_softmax      = tf_softmax(final_layer)\n",
    "correct_prediction = tf.equal(tf.argmax(final_softmax, 1), tf.argmax(y, 1))\n",
    "accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# 3. train\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "avg_acc_train = 0; avg_acc_test  = 0; \n",
    "train_acc     = [];test_acc = []\n",
    "for iter in range(num_epoch):\n",
    "\n",
    "    for current_batch_index in range(0,len(train_images),batch_size):\n",
    "        current_data  = train_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = train_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy,auto_train,update_ops],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(train_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_train = avg_acc_train + sess_results[0]\n",
    "        \n",
    "    for current_batch_index in range(0,len(test_images), batch_size):\n",
    "        current_data  = test_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = test_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy],feed_dict={x:current_data,y:current_label,is_train:False})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(test_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_test = avg_acc_test + sess_results[0]        \n",
    "        \n",
    "    print(\"\\n Current : \"+ str(iter) + \" Acc : \" + str(avg_acc_train/(len(train_images)/batch_size)) + \" Test Acc : \" + str(avg_acc_test/(len(test_images)/batch_size)) + '\\n')\n",
    "    \n",
    "    # save the training\n",
    "    train_acc.append(avg_acc_train/(len(train_images)/batch_size))\n",
    "    test_acc .append(avg_acc_test/(len(test_images)/batch_size)  )\n",
    "    \n",
    "    # get weights\n",
    "    l1w,l2w,l3w,l4w,l5w,l6w = sess.run([l1.getw(),l2.getw(),l3.getw(),l4.getw(),l5.getw(),l6.getw()])\n",
    "    \n",
    "    plt.figure(figsize=(25,15))\n",
    "    plt.suptitle('Current Iter : ' + str(iter))\n",
    "    plt.subplot(231); plt.hist(l1w.ravel(),50); plt.title('layer 1')\n",
    "    plt.subplot(232); plt.hist(l2w.ravel(),50); plt.title('layer 2')\n",
    "    plt.subplot(233); plt.hist(l3w.ravel(),50); plt.title('layer 3')\n",
    "    plt.subplot(234); plt.hist(l4w.ravel(),50); plt.title('layer 4')\n",
    "    plt.subplot(235); plt.hist(l5w.ravel(),50); plt.title('layer 5')\n",
    "    plt.subplot(236); plt.hist(l6w.ravel(),50); plt.title('layer 6')\n",
    "    plt.savefig('figure/' + str(iter)+'.png')\n",
    "    plt.tight_layout()\n",
    "    plt.close('all')\n",
    "    \n",
    "    avg_acc_train = 0 ; avg_acc_test  = 0\n",
    "\n",
    "# 4. reset \n",
    "\n",
    "np.save('figure/train.npy',train_acc)\n",
    "np.save('figure/test.npy',test_acc)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-17T02:57:44.137846Z",
     "start_time": "2018-12-17T02:48:31.557955Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Iter : 0/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 0 Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current Iter : 1/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 1 Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current Iter : 2/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 2 Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current Iter : 3/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 3 Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current Iter : 4/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 4 Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current Iter : 5/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 5 Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current Iter : 6/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 6 Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current Iter : 7/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 7 Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current Iter : 8/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 8 Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current Iter : 9/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 9 Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current Iter : 10/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 10 Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current Iter : 11/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 11 Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current Iter : 12/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 12 Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current Iter : 13/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 13 Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current Iter : 14/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 14 Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current Iter : 15/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 15 Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current Iter : 16/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 16 Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current Iter : 17/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 17 Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current Iter : 18/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 18 Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current Iter : 19/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 19 Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current Iter : 20/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 20 Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current Iter : 21/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 21 Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current Iter : 22/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 22 Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current Iter : 23/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 23 Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current Iter : 24/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 24 Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current Iter : 25/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 25 Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current Iter : 26/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 26 Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current Iter : 27/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 27 Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current Iter : 28/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 28 Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current Iter : 29/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 29 Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current Iter : 30/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 30 Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current Iter : 31/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 31 Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current Iter : 32/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 32 Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current Iter : 33/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 33 Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current Iter : 34/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 34 Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current Iter : 35/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 35 Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current Iter : 36/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 36 Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current Iter : 37/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 37 Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current Iter : 38/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 38 Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current Iter : 39/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 39 Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current Iter : 40/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 40 Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current Iter : 41/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 41 Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current Iter : 42/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 42 Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current Iter : 43/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 43 Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current Iter : 44/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 44 Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current Iter : 45/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 45 Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current Iter : 46/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 46 Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current Iter : 47/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 47 Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current Iter : 48/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 48 Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n",
      "Current Iter : 49/50 batch : 7980/8000 acc : 0.15\n",
      " Current : 49 Acc : 0.10000000190734863 Test Acc : 0.1000000020954758\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# first svd \n",
    "\n",
    "# 1. layers\n",
    "l1 = CNN(3,3, 16); l1bc = tf_svd_layer_std(batch_size,16)\n",
    "l2 = CNN(3,16,16); l2bc = tf_svd_layer_std(batch_size,16)\n",
    "l3 = CNN(3,16,16); l3bc = tf_svd_layer_std(batch_size,16)\n",
    "l4 = CNN(3,16,16); l4bc = tf_svd_layer_std(batch_size,16)\n",
    "l5 = CNN(3,16,16); l5bc = tf_svd_layer_std(batch_size,16)\n",
    "l6 = CNN(3,16,10); \n",
    "\n",
    "# 2. graph \n",
    "x = tf.placeholder(tf.float32,(batch_size,96,96,3))\n",
    "y = tf.placeholder(tf.float32,(batch_size,10))\n",
    "is_train = tf.placeholder_with_default(True,())\n",
    "\n",
    "layer1,layer1a = l1.feedforward(x, stride=2)\n",
    "layer1a = tf.reshape(layer1a,(batch_size,48*48,16))\n",
    "layer1b,update1       = l1bc.feedforward(layer1a,is_train)\n",
    "layer1b = tf.reshape(layer1b,(batch_size,48,48,16))\n",
    "\n",
    "layer2,layer2a = l2.feedforward(layer1b,stride=2)\n",
    "layer2a = tf.reshape(layer2a,(batch_size,24*24,16))\n",
    "layer2b,update2        = l2bc.feedforward(layer2a,is_train)\n",
    "layer2b = tf.reshape(layer2b,(batch_size,24,24,16))\n",
    "\n",
    "layer3,layer3a = l3.feedforward(layer2b,stride=2)\n",
    "layer3a = tf.reshape(layer3a,(batch_size,12*12,16))\n",
    "layer3b,update3        = l3bc.feedforward(layer3a,is_train)\n",
    "layer3b = tf.reshape(layer3b,(batch_size,12,12,16))\n",
    "\n",
    "layer4,layer4a = l4.feedforward(layer3b,stride=2)\n",
    "layer4a = tf.reshape(layer4a,(batch_size,6*6,16))\n",
    "layer4b,update4        = l4bc.feedforward(layer4a,is_train)\n",
    "layer4b = tf.reshape(layer4b,(batch_size,6,6,16))\n",
    "\n",
    "layer5,layer5a = l5.feedforward(layer4b)\n",
    "layer5a = tf.reshape(layer5a,(batch_size,6*6,16))\n",
    "layer5b,update5        = l5bc.feedforward(layer5a,is_train)\n",
    "layer5b = tf.reshape(layer5b,(batch_size,6,6,16))\n",
    "\n",
    "layer6, layer6a = l6. feedforward(layer5b)\n",
    "\n",
    "final_layer = tf.reduce_mean(layer6a,(1,2))\n",
    "cost        = tf.nn.softmax_cross_entropy_with_logits_v2(logits=final_layer,labels=y)\n",
    "auto_train  = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "update_ops  = update1 + update2 + update3 + update4 + update5\n",
    "final_softmax      = tf_softmax(final_layer)\n",
    "correct_prediction = tf.equal(tf.argmax(final_softmax, 1), tf.argmax(y, 1))\n",
    "accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# 3. train\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "avg_acc_train = 0; avg_acc_test  = 0; \n",
    "train_acc     = [];test_acc = []\n",
    "for iter in range(num_epoch):\n",
    "\n",
    "    for current_batch_index in range(0,len(train_images),batch_size):\n",
    "        current_data  = train_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = train_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy,auto_train,update_ops],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(train_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_train = avg_acc_train + sess_results[0]\n",
    "        \n",
    "    for current_batch_index in range(0,len(test_images), batch_size):\n",
    "        current_data  = test_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = test_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy],feed_dict={x:current_data,y:current_label,is_train:False})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(test_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_test = avg_acc_test + sess_results[0]        \n",
    "        \n",
    "    print(\"\\n Current : \"+ str(iter) + \" Acc : \" + str(avg_acc_train/(len(train_images)/batch_size)) + \" Test Acc : \" + str(avg_acc_test/(len(test_images)/batch_size)) + '\\n')\n",
    "    \n",
    "    # save the training\n",
    "    train_acc.append(avg_acc_train/(len(train_images)/batch_size))\n",
    "    test_acc .append(avg_acc_test/(len(test_images)/batch_size)  )\n",
    "    \n",
    "    # get weights\n",
    "#     l1w,l2w,l3w,l4w,l5w,l6w = sess.run([l1.getw(),l2.getw(),l3.getw(),l4.getw(),l5.getw(),l6.getw()])\n",
    "    \n",
    "#     plt.figure(figsize=(25,15))\n",
    "#     plt.suptitle('Current Iter : ' + str(iter))\n",
    "#     plt.subplot(231); plt.hist(l1w.ravel(),50); plt.title('layer 1')\n",
    "#     plt.subplot(232); plt.hist(l2w.ravel(),50); plt.title('layer 2')\n",
    "#     plt.subplot(233); plt.hist(l3w.ravel(),50); plt.title('layer 3')\n",
    "#     plt.subplot(234); plt.hist(l4w.ravel(),50); plt.title('layer 4')\n",
    "#     plt.subplot(235); plt.hist(l5w.ravel(),50); plt.title('layer 5')\n",
    "#     plt.subplot(236); plt.hist(l6w.ravel(),50); plt.title('layer 6')\n",
    "#     plt.savefig('figure/' + str(iter)+'.png')\n",
    "#     plt.tight_layout()\n",
    "#     plt.close('all')\n",
    "    \n",
    "    avg_acc_train = 0 ; avg_acc_test  = 0\n",
    "\n",
    "# 4. reset \n",
    "\n",
    "np.save('figure/train.npy',train_acc)\n",
    "np.save('figure/test.npy',test_acc)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-17T03:12:26.396244Z",
     "start_time": "2018-12-17T03:12:26.257612Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a2d8826080>]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADx0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wcmMyLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvMCCy2AAAIABJREFUeJzt3Xl4VPXd/vH3JytrCEvCFkIQwr6JEWypVRQV64Kt0rr9Wtv6qFW71xb7VNvSVVtba2sXbH1q66PWrUotLoC4tlXCDgmBsIdAErYEAtk/vz+S+qQxkAEyOZmZ+3VdXJlz5pvJfS4nd47fOYu5OyIiEl3igg4gIiLtT+UuIhKFVO4iIlFI5S4iEoVU7iIiUUjlLiIShVTuIiJRSOUuIhKFVO4iIlEoIagf3K9fP8/Kygrqx4uIRKTly5fvdfe0tsYFVu5ZWVnk5uYG9eNFRCKSmW0PZVxI0zJmNsvMCsys0MzmtvL8UDNbYmZrzOw1M8s40cAiItJ+2ix3M4sHHgQuBsYC15jZ2BbDfgr8yd0nAvOAH7V3UBERCV0oe+5TgUJ33+LuNcATwOwWY8YCS5oeL23leRER6UChlPtgYGez5aKmdc2tBq5sevxRoKeZ9T31eCIicjJCKXdrZV3Li8B/DTjHzFYC5wC7gLr3vZDZTWaWa2a5ZWVlJxxWRERCE0q5FwFDmi1nAMXNB7h7sbt/zN1PB/67aV15yxdy9/nunuPuOWlpbR7JIyIiJymUcl8GZJvZMDNLAq4GFjQfYGb9zOzfr3Un8HD7xhQRkRPRZrm7ex1wO/AykA886e7rzWyemV3eNOxcoMDMNgL9gR+EKa+ISMRxd/aUV7E4r4T7F29k3a73TWy0u5BOYnL3hcDCFuvubvb4aeDp9o0mIhK8/ZU1vLNlH1v2VrLvcA37K6vZf6S28evhGmrqG0jr2YX+Kcn0b/qantKF7snxbCw5zPriCtbvKmdfZQ0AZtCvRzLjB/cKa+7AzlAVEemMyo/U8s7Wffxj8z7+tWUfG/Yceu+57knx9OmRRJ/uyaT1SGZU/xSSEoyyQ9WUVFSTV1zB3sPVNDQdcpIYb2Sn9+T8MemMG9SL8YNTGD0ghe7J4a9elbuICLBj3xHmPruGf27ZhzskJ8RxZlYf7rhoEGed1pdxg1Lokhjf5uvU1Tewr7KGiqO1ZPbtRnJC298TDip3EYl5C1YX881n1xJn8Pnzspk+vC+TM1NPqpgT4uPon9KF/ildwpD0BHIE+tNFRAJ0pKaO7yxYz5O5RUzJTOWBa04no3e3oGO1C5W7iMSk/N0V3P7YCrbsreS2GcP50syRJMZHzy0uVO4iElPKDlWzYHUx97y0gV5dE3n0s9OYPqJf0LHancpdRKLW0Zp61hWXs2rHQVbtbPy36+BRAM4ZmcZ9H59Evx7JAacMD5W7iESVHfuOsCi/hMV5JSzbtp+6puMSB6d2ZXJmKjd8MIvTM1OZktmbuLjWLp0VHVTuIhLRGhqc1UUHWZxfwuK8UgpKGo9LH9m/BzeefRo5Q3szaUgqaT2jcw/9WFTuIhJxDlXV8tamvby6oZSlBWXsPVxNfJxxZlZv7rp0LDPHpDO0b/egYwZK5S4iEWF3+VEWrt3DqxtKeHfrfmrrnZQuCZwzKp3zRqcxY1Q6qd2Sgo7ZaajcRaRTq66rZ/7rW/jV0kKq6xrITu/BZz40jPNGpXPG0N4kRNHhi+1J5S4indZbm/Zy9/Pr2LK3kksmDOSOi0aR1S+2p1tCpXIXkU6ntKKK7/09n7+tLiarbzce+cxUzhmpG/ycCJW7iHQa2/ZW8uK6PTy4tJCa+ga+PHMkN59zWkgX7JL/pHIXkcBUVNXyj8J9vLmpjDc37WXH/iNA4wlG3718nKZgToHKXUQ6lLvzSl4Jf3hzK8t3HKC+wemeFM8HhvfjxrOHcXZ2GsNU6qdM5S4iHcLdeXVDKT9btJH1xRVk9e3GrecO5+zsNE7PTI2qi3Z1Bip3EQkrd+f1jWX8fNFGVheVk9mnG/fNmcTsyYN0GGMYqdxF5D/U1jfw2Ds7OFRVS0rXRHp2SSClS+J7j4f26U7XpLY/4Kyuq2dxXikPv72V5dsPMDi1K/dcOYGPTcnQXnoHULmLyHsOHqnh1v9dwT827zvmmOSEOM7O7sfMMf05b0w66T3/745D7s7aXeU8vbyI51cVU360lsGpXfn+FeP5eM4QkhJU6h1F5S4iABSWHubGR5ZRfLCK++ZM4tJJAzlUVUfF0drGr1W1lB+tJXfbARbllbA4vxSAyUNSuWBsfxLjjWeW76Kg5BDJCXFcNG4AV52RwfQR/YiP4qsvdlbm7oH84JycHM/NzQ3kZ4vIf3pjYxm3PbaCpPg45n/yDM4Y2ue4492dDXsOsTivhMX5JawuKgfg9MxUrjojg0snDqJX18SOiB5zzGy5u+e0NS6kPXczmwX8AogHfu/uP27xfCbwCJDaNGauuy884dQi0qHcnT/9czvzXsgjO70Hv/9UTkj3EDUzxgxMYczAFD5/fjalFVVU1TaQ2Tc67j8aDdosdzOLBx4ELgCKgGVmtsDd85oN+xbwpLv/xszGAguBrDDkFZEQuDubyw6zbNsBlm3bz459R0jpmkhq10R6dUukd7ckUrslsm5XOU/mFjFzTH/uv3oyPZJPbqY2PaVL24OkQ4XyX3IqUOjuWwDM7AlgNtC83B1IaXrcCyhuz5Ai0raCPYd4raCUZdsOsHz7fg4cqQWgX48khqf1oPRQFQV7DlF+tJbD1XXvfd/N55zG1y8arXnxKBNKuQ8GdjZbLgKmtRjzHeAVM/s80B2Y2doLmdlNwE0AmZmZJ5pVRFo4UFnDgtXFPLV8J+t2VQBwWr/uXDC2PzlZfTgzqw9Zfbth9p/FXVvfwMEjtbi79rqjVCjl3tqf85afwl4D/NHd7zOzDwB/NrPx7t7wH9/kPh+YD40fqJ5MYJFYV1ffwBubyngqt4jF+SXU1jvjBqXw7cvGcsnEgf9xaOKxJMbHxdxt52JNKOVeBAxptpzB+6ddPgvMAnD3f5pZF6AfUNoeIUWk0dKCUu5+fh079x+lb/ckPvmBLK6cksHYQSltf7PElFDKfRmQbWbDgF3A1cC1LcbsAM4H/mhmY4AuQFl7BhWJZWWHqpn3Qh5/W13MiPQe/Pb6MzhvdLpOCpJjarPc3b3OzG4HXqbxMMeH3X29mc0Dct19AfBV4CEz+zKNUzY3eFAH0ItEkYYG58ncnfxwYT5VtQ185YLG65snJ+j65nJ8IR331HTM+sIW6+5u9jgPmN6+0URiW2HpIb757Dre3bafacP68MOPTWB4Wo+gY0mE0OUHRDqBQ1W1rN1VztqictY0fd2x/wi9uiZy75UTmZOT8b4jXkSOR+UuEoCGBmflzgMsXLuH1wpK2VxW+d5zGb27MjGjF9dMzeSqMzJ0VIucFJW7SAepb3De3bqfF9ft5uX1eyipqCYpPo4PjujLFZMHMyGjFxMzUunTPSnoqBIFVO4iYVZRVctDb2zhsXd2sK+yhi6JcZw7Mp2LJwzgvNHp9OyiC2xJ+1O5i4RJVW09j/xjG795fTMHj9Ry0bj+zJ48mHNHpdEtSb96El56h4m0s9r6Bp7KLeKBJZvYU1HFOSPTuOOiUYwf3CvoaBJDVO4iJ2hTySF++Woh+yqr6ZqYQNekeLolxtM1KZ7kxDheWV/C1r2VTMlM5f6rJ3PWaX2DjiwxSOUuEqK9h6u5f/FGHn93J92S4hnZvyf7K49SVVvPkZo6jtbUc7S2nuFpPXjokznMHJOuwxclMCp3kTZU1dbz8Ntb+fXSzRytref6aZl8ceZIHdUinZrKXeQY6hucF9YUc+9LBew6eJSZY/pz50dG6yxRiQgqd5EWquvqeW7lLn73+ha27K1k3KAUfjJnIh8c3i/oaCIhU7mLNDlcXcdj72znD29tpaSimvGDU/jVtafzkfEDidNdiiTCqNwl5u07XM3/vL2NP/1zGxVVdUwf0Zf75kxm+oi++kBUIpbKXWJWaUUV89/Ywv++s4OqunpmjRvALecMZ9KQ1KCjiZwylbvEnOKDR/nd65t5fNlO6huc2ZMGceuMEYxI1welEj1U7hITKqvryN9dwTMrdvH08p24w5VTMrh1xnCG9u0edDyRdqdyl6hTWV3Hqp0HWbernPXFFawrLmfr3krcISk+jk+cOYRbzhlORu9uQUcVCRuVu0SVd7bs4/bHV1J2qBqAwaldGTcohdmTBjN+cAqTh6TSt4eujy7RT+UuUcHdeejNLdzzUgGZfbpx75UTmTwkld46i1RilMpdIl5FVS1ff2oNL63fw6xxA/jJnIm6RrrEPJW7RLQNeyr43KMr2LH/CP/9kTHcePYwHZsugspdIpS788yKXXzrubX07JLI4/91FlOH9Qk6lkinEVK5m9ks4BdAPPB7d/9xi+d/DsxoWuwGpLu7zgSRsHh3635+9GI+K3ccZOqwPvzq2tNJ79kl6FginUqb5W5m8cCDwAVAEbDMzBa4e96/x7j7l5uN/zxwehiySowr2HOIe1/awJINpfRPSebHH5vAVWdkkBAfF3Q0kU4nlD33qUChu28BMLMngNlA3jHGXwN8u33iiTSeUfrzRRt5ZkUR3ZMT+PqsUXz6g8PomhQfdDSRTiuUch8M7Gy2XARMa22gmQ0FhgGvnno0iWV7yqtYnF/C4vwS/lG4D4DPfmgYt547Qoc3ioQglHJv7dADP8bYq4Gn3b2+1Rcyuwm4CSAzMzOkgBIb3J383YdYlNdY6Gt3lQMwtG83PvmBodwwPUtnlIqcgFDKvQgY0mw5Ayg+xtirgduO9ULuPh+YD5CTk3OsPxASY0orqrjz2bUs2VCKGUzJ7M03Zo3mgrHpDE/roUMbRU5CKOW+DMg2s2HALhoL/NqWg8xsFNAb+Ge7JpSo9sKaYr713DqO1tQz9+LRXHVGBv10eQCRU9Zmubt7nZndDrxM46GQD7v7ejObB+S6+4KmodcAT7i79silTQcqa7jr+XW8sGY3k4akct+cSbrkrkg7Cuk4d3dfCCxsse7uFsvfab9YEs1e3VDCN55Zy8EjNXztwpHccs5wHc4o0s50hqp0iIYG5+3Ne/nTP7ezKK+E0QN68sdPn8m4Qb2CjiYSlVTuElb7K2t4Kncnj727g+37jtCnexJfPD+bW2cMJzlBx6mLhIvKXcJi1c6D/PHtrSxcu4ea+gamZvXhKxeMZNb4ASp1kQ6gcpd2VVpRxY9e3MBfV+6iZ3IC107L5NppmYzs3zPoaCIxReUu7aKmroH/eXsrDyzZRG29c9uM4dx67gi6J+stJhIE/ebJKXt9Yxnf/dt6tpRVcv7odO66dCxZ/XTTaZEgqdzlpJUfqeUbzzTeASmrbzf+54YzmTE6PehYIoLKXU5SYelhbnxkGcUHq7jjolHcePYwfVAq0omo3OWELS0o5QuPrSQ5MY7Hb5rGGUN1BySRzkblLiFzd37/5lZ+9GI+owek8NCnchic2jXoWCLSCpW7hKS6rp5vPruOZ1YU8ZEJA/jpnEl0S9LbR6Sz0m+nHJe7s2LHQX7w9zxW7DjIl2Zm84XzsomL02V4RTozlbu0qvxILc+uLOKJd3dSUHKIHskJ/Oa6KVw8YWDQ0UQkBCp3eY+7s2zbAR5/dwcL1+6muq6BSRm9+NHHJnDZpEH00AlJIhFDv60CwK6DR5n7zBre3LSXnskJfDxnCFdPHaKrNopEKJV7jHN3nszdyfdeyKfBnW9fNpZPnDlEH5aKRDj9Bsew3eVHmfvMWl7fWMYHTuvLvVdNZEgf3YRaJBqo3GOQu/P08iLmvZBHXb0zb/Y4rp82VEfAiEQRlXuMKa2o4s5n17JkQylTs/rwkzkTGdpXF/kSiTYq9xjyt9XF3PX8Oo7W1HPXpWP59AeztLcuEqVU7jHgQGUNdz2/jhfW7GbSkFTumzOJEek9go4lImGkco9yr24o4RvPrOXgkRruuGgUN3/4NBLi44KOJSJhpnKPUnX1Ddy9YD2PvbOD0QN68sinpzJ2UErQsUSkg4S0C2dms8yswMwKzWzuMcZ83MzyzGy9mT3WvjHlRFTX1XPbYyt47J0d3HzOaTx/+3QVu0iMaXPP3czigQeBC4AiYJmZLXD3vGZjsoE7genufsDMdDuegBytqeeWR5fz+sYyvnPZWG6YPizoSCISgFD23KcChe6+xd1rgCeA2S3G/BfwoLsfAHD30vaNKaE4XF3Hp//4Lm9sKuOeKyeo2EViWCjlPhjY2Wy5qGldcyOBkWb2tpn9y8xmtfZCZnaTmeWaWW5ZWdnJJZZWlR+p5f/94R2WbTvA/Z+YzCfOzAw6kogEKJRyb+1AaG+xnABkA+cC1wC/N7PU932T+3x3z3H3nLS0tBPNKsew73A11zz0L9bvquDX101h9uSWf3tFJNaEUu5FwJBmyxlAcStjnnf3WnffChTQWPYSZtv3VfKJ+f9ic9lhHvpUDheNGxB0JBHpBEIp92VAtpkNM7Mk4GpgQYsxzwEzAMysH43TNFvaM6i839INpVz2y7coO1TNI5+Zyjkj9X9DItKozaNl3L3OzG4HXgbigYfdfb2ZzQNy3X1B03MXmlkeUA/c4e77whk8ljU0OL9YsokHXt3EmAEp/Pb6M8jsq6s5isj/MfeW0+cdIycnx3NzcwP52ZHs4JEavvyXVSwtKOPKKRn84KPj6ZIYH3QsEekgZrbc3XPaGqczVCPI+uJybnl0OXvKq/j+FeO5blomZrrwl4i8n8o9Qry0bg9ffGIlvbsl8eTNH+D0zN5BRxKRTkzlHgGeXLaTuc+uYfKQVOZ/Mod+PZKDjiQinZzKvZOb/8ZmfrhwAx8emcZvr5+ie5uKSEjUFJ2Uu3PPSwX89vXNXDpxID/7+GSSEnSpXhEJjcq9E6pvcL713Foef3cn103LZN7s8cTrjkkicgJU7p1MdV09X/7LKhau3cPtM0bw1QtH6ogYETlhKvdO5r//uo6Fa/fwrUvGcOPZpwUdR0QilCZxO5FFeSU8vbyI22eMULGLyClRuXcS+ytruPPZtYwZmMIXztc110Tk1GhappO46/l1lB+t4c+fnaqjYkTklKlFOoG/rS7m72t286WZIxkzUPc6FZFTp3IPWOmhKu56fh2ThqRy84c1zy4i7UPlHiB355vPruNoTT33zZlEQrz+c4hI+1CbBOiZFbtYnF/CHReNYkR6j6DjiEgUUbkHpPjgUb67YD1Ts/rw6enDgo4jIlFG5R6Aqtp6vvSXVdQ1OD+ZM1GXFhCRdqdDITtYXX0Dn398Je9u3c8vrp7M0L7dg44kIlFIe+4dqKHBmfvsWhbllfCdy8Yye/LgoCOJSJRSuXcQd+cHC/N5enkRX5qZzQ2aZxeRMFK5d5AHlxbyh7e2csMHs/iiLi8gImGmcu8Af/7Xdn76ykY+evpg7r50rC7hKyJhF1K5m9ksMysws0Izm9vK8zeYWZmZrWr6d2P7R41MC1YXc/fz65g5Jp17r5pInI6MEZEO0ObRMmYWDzwIXAAUAcvMbIG757UY+hd3vz0MGSPWxpJDfO3J1ZyZ1YdfXTuFRJ2BKiIdJJS2mQoUuvsWd68BngBmhzdW5Kurb+BrT62mR5cEfn3dFLokxgcdSURiSCjlPhjY2Wy5qGldS1ea2Roze9rMhrRLugj2uze2sKaonO/NHk+/HslBxxGRGBNKubc2Sewtlv8GZLn7RGAx8EirL2R2k5nlmlluWVnZiSWNIAV7DnH/4o1cMmEgl0wcGHQcEYlBoZR7EdB8TzwDKG4+wN33uXt10+JDwBmtvZC7z3f3HHfPSUtLO5m8nV5t03RMSpdE5s0eF3QcEYlRoZT7MiDbzIaZWRJwNbCg+QAza757ejmQ334RI8tvX9vM2l3lfP+K8fTVdIyIBKTNo2Xcvc7MbgdeBuKBh919vZnNA3LdfQHwBTO7HKgD9gM3hDFzp5W/u4IHXt3EpRMHcvEETceISHDMveX0ecfIycnx3NzcQH52ONTWN3DFg29TUlHFK18+hz7dk4KOJCJRyMyWu3tOW+N0Vch28pvXNrO+uILfXj9FxS4igdNZNe1g9c6D/PLVTVw2aRCzxms6RkSCp3I/RfsOV/O5R5eT3rML8y7X0TEi0jloWuYU1NU38IUnVrK3soZnP/dBems6RkQ6Ce25n4KfvFLA24X7+MEV4xk/uFfQcURE3qNyP0kvrt3N717fwnXTMpmTE/NXWxCRTkblfhIKSw/xtadWM3lIKndfNjboOCIi76NyP0GHq+u4+c/L6ZoUz2+un0Jygq72KCKdjz5QPQHuzh1PrWbbviM8+tlpDOzVNehIIiKt0p77CXjkH9t4cd0e7rx4NB8Y3jfoOCIix6RyD9HO/Ue456UCzh2Vxmc/NCzoOCIix6VyD4G7c+eza4kz+OFHJ+gG1yLS6ancQ/BUbhFvFe5l7kfGMChV8+wi0vmp3NtQUlHF9/6ex9RhfbhuambQcUREQqJyPw53567n1lFT18A9V04kLk7TMSISGVTux/H3tbt5Ja+Er1wwkmH9ugcdR0QkZCr3Y9hfWcO3n1/PxIxeOjpGRCKOTmI6hu+9kEf50VoevXEaCfH6GygikUWt1YpXN5Tw15W7uHXGCMYMTAk6jojICVO5t1BSUcUdT61hVP+e3DZjeNBxREROisq9mbr6Br7w+EqO1NTzq2tP10XBRCRiac69mQeWbOKdrfu5b84ksvv3DDqOiMhJ0557kzc2lvHLpYV8PCeDK8/ICDqOiMgpCanczWyWmRWYWaGZzT3OuKvMzM0sp/0ihl9JRRVf/ssqstN78N3LxwcdR0TklLVZ7mYWDzwIXAyMBa4xs/fdfsjMegJfAN5p75Dh1Hye/dfXTaFrkubZRSTyhbLnPhUodPct7l4DPAHMbmXc94B7gap2zBd2v2iaZ//+FeMZka55dhGJDqGU+2BgZ7PloqZ17zGz04Eh7v5CO2YLuzc2lvErzbOLSBQKpdxbu1qWv/ekWRzwc+Crbb6Q2U1mlmtmuWVlZaGnDIOq2nq+8cwazbOLSFQKpdyLgCHNljOA4mbLPYHxwGtmtg04C1jQ2oeq7j7f3XPcPSctLe3kU7eDR/+1nd3lVcybPV7z7CISdUIp92VAtpkNM7Mk4Gpgwb+fdPdyd+/n7lnungX8C7jc3XPDkrgdHKqq5cGlhZyd3Y+zTtO9UEUk+rRZ7u5eB9wOvAzkA0+6+3ozm2dml4c7YDj84a2tHDhSy9cvGh10FBGRsAjpDFV3XwgsbLHu7mOMPffUY4XP/soafv/mVi4eP4AJGb2CjiMiEhYxd4bqb14r5EhNHV+9cGTQUUREwiamyn13+VEe+ed2PjYlQ8e0i0hUi6lyf2DJJtydL83MDjqKiEhYxUy5b91byZO5RVw3bSgZvbsFHUdEJKxiptx/tmgjSfFx3DZjRNBRRETCLibKPa+4gr+tLuYzH8oirWdy0HFERMIuJsr9p68UkNIlgZs+rNvmiUhsiPpy/8fmvby6oZRbzh1Or66JQccREekQUV3u9Q3O91/IZ3BqVz4zfVjQcUREOkxUl/szK4rI213B12eNokuiLg4mIrEjasu9srqOn7xcwOmZqVw+aVDQcUREOlTUlvvvXt9M2aFq7rp0LGatXZJeRCR6RWW5Fx88yvw3t3DZpEFMyewddBwRkQ4XleV+70sbcIdvzBoVdBQRkUBEXbmv2nmQ51YVc+PZw3SZARGJWVFV7u7O91/Io1+PZD53ri4zICKxK6rK/e9rd5O7/QBfu3AkPZJDug+JiEhUippyr6qt58cvbmD0gJ7MyRnS9jeIiESxqCn3RXklFB04ytyLRxMfp0MfRSS2RU25L8kvoU/3JM7OTgs6iohI4KKi3OvqG1haUMa5o9K01y4iQpSU+/LtByg/WsvMMf2DjiIi0ilERbkvzi8hMd44O7tf0FFERDqFkMrdzGaZWYGZFZrZ3Faev8XM1prZKjN7y8zGtn/UY1uSX8pZp/WlZxddr11EBEIodzOLBx4ELgbGAte0Ut6PufsEd58M3Av8rN2THsOWssNs2VupKRkRkWZC2XOfChS6+xZ3rwGeAGY3H+DuFc0WuwPefhGPb0l+KQDnj0nvqB8pItLphXIa52BgZ7PlImBay0FmdhvwFSAJOK+1FzKzm4CbADIzM080a6sW55cwekBPXUdGRKSZUPbcWzu28H175u7+oLsPB74BfKu1F3L3+e6e4+45aWmnfjz6wSM15G4/oCkZEZEWQin3IqD5+fwZQPFxxj8BXHEqoUL1WkEZ9Q2uKRkRkRZCKfdlQLaZDTOzJOBqYEHzAWaW3WzxEmBT+0U8tsX5JfTrkcykjNSO+HEiIhGjzTl3d68zs9uBl4F44GF3X29m84Bcd18A3G5mM4Fa4ADwqXCGBqitb+D1jWVcPH4AcTorVUTkP4R0XVx3XwgsbLHu7maPv9jOudq0bOt+DlXVcb7m20VE3idiz1BdnF9KUkKczkoVEWlFRJa7u7NkQwnTh/elW5JuyiEi0lJElvvmssNs33dEUzIiIscQkeW+KE9npYqIHE9ElvuS/BLGDUphYK+uQUcREemUIq7c91fWsGKHzkoVETmeiCv3pRtKaXBU7iIixxFx5Z7SNZELx/Zn/OCUoKOIiHRaEXcc4QVj+3PBWO21i4gcT8TtuYuISNtU7iIiUUjlLiIShVTuIiJRSOUuIhKFVO4iIlFI5S4iEoVU7iIiUcjcPZgfbFYGbD/Jb+8H7G3HOJEiVrcbYnfbtd2xJZTtHuruaW29UGDlfirMLNfdc4LO0dFidbshdrdd2x1b2nO7NS0jIhKFVO4iIlEoUst9ftABAhKr2w2xu+3a7tjSbtsdkXPuIiJyfJG65y4iIscRceVuZrPMrMDMCs1sbtB5wsXMHjazUjNb12xdHzNbZGabmr72DjJjOJjZEDNbamb5ZrbezL7YtD6qt93MupjZu2a2umm7v9u0fpiZvdO03X8xs6Sgs4aDmcWb2Uoze6FpOeq328y2mdlaM1tlZrlN69rtfR5R5W5m8cCDwMXT5PXBAAACw0lEQVTAWOAaMxsbbKqw+SMwq8W6ucASd88GljQtR5s64KvuPgY4C7it6b9xtG97NXCeu08CJgOzzOws4B7g503bfQD4bIAZw+mLQH6z5VjZ7hnuPrnZ4Y/t9j6PqHIHpgKF7r7F3WuAJ4DZAWcKC3d/A9jfYvVs4JGmx48AV3RoqA7g7rvdfUXT40M0/sIPJsq33RsdblpMbPrnwHnA003ro267AcwsA7gE+H3TshED230M7fY+j7RyHwzsbLZc1LQuVvR3993QWIJAesB5wsrMsoDTgXeIgW1vmppYBZQCi4DNwEF3r2saEq3v9/uBrwMNTct9iY3tduAVM1tuZjc1rWu393mk3UPVWlmnw32ikJn1AJ4BvuTuFY07c9HN3euByWaWCvwVGNPasI5NFV5mdilQ6u7Lzezcf69uZWhUbXeT6e5ebGbpwCIz29CeLx5pe+5FwJBmyxlAcUBZglBiZgMBmr6WBpwnLMwskcZi/193f7ZpdUxsO4C7HwReo/Ezh1Qz+/dOWDS+36cDl5vZNhqnWc+jcU8+2rcbdy9u+lpK4x/zqbTj+zzSyn0ZkN30SXoScDWwIOBMHWkB8Kmmx58Cng8wS1g0zbf+Ach39581eyqqt93M0pr22DGzrsBMGj9vWApc1TQs6rbb3e909wx3z6Lx9/lVd7+OKN9uM+tuZj3//Ri4EFhHO77PI+4kJjP7CI1/2eOBh939BwFHCgszexw4l8arxJUA3waeA54EMoEdwBx3b/mha0Qzsw8BbwJr+b852G/SOO8etdtuZhNp/AAtnsadrifdfZ6ZnUbjHm0fYCVwvbtXB5c0fJqmZb7m7pdG+3Y3bd9fmxYTgMfc/Qdm1pd2ep9HXLmLiEjbIm1aRkREQqByFxGJQip3EZEopHIXEYlCKncRkSikchcRiUIqdxGRKKRyFxGJQv8fqYN0UvYMvhkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# compare the train accuracy\n",
    "batch_norm    = np.load('1 batch_norm/train.npy')\n",
    "layer_norm    = np.load('2 layer_norm/train.npy')\n",
    "instance_norm = np.load('3 Instance Norm/train.npy')\n",
    "ranged_norm   = np.load('4 ranged normalization/train.npy')\n",
    "box_cox       = np.load('5 box cox/train.npy')\n",
    "svd           = np.load('6 SVD/train.npy')\n",
    "\n",
    "plt.plot(batch_norm)\n",
    "plt.plot(layer_norm)\n",
    "plt.plot(instance_norm)\n",
    "plt.plot(ranged_norm)\n",
    "plt.plot(batch_norm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-17T03:10:37.104231Z",
     "start_time": "2018-12-17T03:10:36.799402Z"
    }
   },
   "outputs": [],
   "source": [
    "! start ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
