{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-13T01:42:25.974148Z",
     "start_time": "2018-12-13T01:42:25.970688Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# import lib\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys, os,cv2\n",
    "from sklearn.utils import shuffle\n",
    "from scipy.misc import imread,imresize\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from skimage.transform import resize\n",
    "from imgaug import augmenters as iaa\n",
    "import imgaug as ia\n",
    "from scipy.ndimage import zoom\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data\n",
    "n_examples, hidden_layer_dim = 100, 100\n",
    "input_dim = 1000\n",
    "X         = np.random.randn(n_examples, input_dim) # 100 examples of 1000 points\n",
    "n_layers  = 20\n",
    "layer_dim = [hidden_layer_dim] * n_layers # each one has 100 neurons\n",
    "\n",
    "hs = [X]\n",
    "zs = [X]\n",
    "ws = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feed forward process \n",
    "for i in np.arange(n_layers):\n",
    "    h = hs[-1] # get the input into this hidden layer\n",
    "    W = np.random.normal(size = (layer_dim[i], h.shape[0])) * 0.01 # weight init: gaussian around 0\n",
    "    z = np.dot(W, h)\n",
    "    h_out = z * (z > 0)\n",
    "    ws.append(W)\n",
    "    zs.append(z)\n",
    "    hs.append(h_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLdh    = 100 * np.random.randn(hidden_layer_dim, input_dim) # random incoming grad into our last layer\n",
    "h_grads = [dLdh] # store the incoming grads into each layer\n",
    "w_grads = [] # store dL/dw for each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1000)\n",
      "(100, 1000)\n",
      "(100, 1000)\n",
      "(100, 1000)\n",
      "(100, 1000)\n",
      "(100, 1000)\n",
      "(100, 1000)\n",
      "(100, 1000)\n",
      "(100, 1000)\n",
      "(100, 1000)\n",
      "(100, 1000)\n",
      "(100, 1000)\n",
      "(100, 1000)\n",
      "(100, 1000)\n",
      "(100, 1000)\n",
      "(100, 1000)\n",
      "(100, 1000)\n",
      "(100, 1000)\n",
      "(100, 1000)\n"
     ]
    }
   ],
   "source": [
    "# the backwards pass\n",
    "for i in np.flip(np.arange(1, n_layers), axis = 0):\n",
    "\t# get the incoming gradient\n",
    "\tincoming_loss_grad = h_grads[-1]\n",
    "\t# backprop through the relu\n",
    "\tprint(incoming_loss_grad.shape)\n",
    "\tdLdz = incoming_loss_grad * (zs[i] > 0)\n",
    "\t# get the gradient dL/dh_{i-1}, this will be the incoming grad into the next layer\n",
    "\th_grad = ws[i-1].T.dot(dLdz)\n",
    "\t# get the gradient of the weights of this layer (dL/dw)\n",
    "\tweight_grad = dLdz.dot(hs[i-1].T)\n",
    "\th_grads.append(h_grad)\n",
    "\tw_grads.append(weight_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the resulting activatiosn\n",
    "for i, activation in enumerate(hs):\n",
    "\tfig = plt.figure()\n",
    "\tnum_bins = 50\n",
    "\tprint('variance of linear units before relu is {}'.format(np.var(zs[i].ravel())))\n",
    "\tprint('variance of activations is {}'.format(np.var(activation.ravel())))\n",
    "\tratio = np.var(zs[i].ravel()) / np.var(activation.ravel())\n",
    "\tprint('ratio is {}'.format(ratio))\n",
    "\tn, bins, patches = plt.hist(activation.ravel(), num_bins, normed=1, facecolor='green', alpha=0.5)\n",
    "\tplt.title('Activation {}, var: {}'.format(i, np.var(activation.ravel())))\n",
    "\tplt.xlabel('Activation Value')\n",
    "\tplt.ylabel('Number of Activations')\n",
    "\t# Tweak spacing to prevent clipping of ylabel\n",
    "\tplt.subplots_adjust(left=0.15)\n",
    "\tplt.savefig('activation-plots/act-{}.png'.format(i))\t\n",
    "\tplt.ticklabel_format(axis='x',style='sci',scilimits=(1,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch master\n",
      "Your branch is up to date with 'origin/master'.\n",
      "\n",
      "nothing to commit, working tree clean\n",
      "Everything up-to-date\n"
     ]
    }
   ],
   "source": [
    "! git add .\n",
    "! git commit -m \"from mac\"\n",
    "! git push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
