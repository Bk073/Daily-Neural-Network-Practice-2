{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-11T20:31:13.834409Z",
     "start_time": "2018-12-11T20:31:10.536400Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# import lib\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys, os,cv2\n",
    "from sklearn.utils import shuffle\n",
    "from scipy.misc import imread,imresize\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from skimage.transform import resize\n",
    "from imgaug import augmenters as iaa\n",
    "import imgaug as ia\n",
    "from scipy.ndimage import zoom\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-11T20:31:21.251700Z",
     "start_time": "2018-12-11T20:31:19.198087Z"
    },
    "code_folding": [
     1,
     28
    ]
   },
   "outputs": [],
   "source": [
    "# read all of the data\n",
    "def read_all_images(path_to_data):\n",
    "    \"\"\"\n",
    "    :param path_to_data: the file containing the binary images from the STL-10 dataset\n",
    "    :return: an array containing all the images\n",
    "    \"\"\"\n",
    "\n",
    "    with open(path_to_data, 'rb') as f:\n",
    "        # read whole file in uint8 chunks\n",
    "        everything = np.fromfile(f, dtype=np.uint8)\n",
    "\n",
    "        # We force the data into 3x96x96 chunks, since the\n",
    "        # images are stored in \"column-major order\", meaning\n",
    "        # that \"the first 96*96 values are the red channel,\n",
    "        # the next 96*96 are green, and the last are blue.\"\n",
    "        # The -1 is since the size of the pictures depends\n",
    "        # on the input file, and this way numpy determines\n",
    "        # the size on its own.\n",
    "\n",
    "        images = np.reshape(everything, (-1, 3, 96, 96))\n",
    "\n",
    "        # Now transpose the images into a standard image format\n",
    "        # readable by, for example, matplotlib.imshow\n",
    "        # You might want to comment this line or reverse the shuffle\n",
    "        # if you will use a learning algorithm like CNN, since they like\n",
    "        # their channels separated.\n",
    "        images = np.transpose(images, (0, 3, 2, 1))\n",
    "        return images\n",
    "def read_labels(path_to_labels):\n",
    "    \"\"\"\n",
    "    :param path_to_labels: path to the binary file containing labels from the STL-10 dataset\n",
    "    :return: an array containing the labels\n",
    "    \"\"\"\n",
    "    with open(path_to_labels, 'rb') as f:\n",
    "        labels = np.fromfile(f, dtype=np.uint8)\n",
    "        return labels\n",
    "    \n",
    "train_images = read_all_images(\"../../DataSet/STL10/stl10_binary/train_X.bin\") / 255.0\n",
    "train_labels = read_labels    (\"../../DataSet/STL10/stl10_binary/train_Y.bin\")\n",
    "test_images  = read_all_images(\"../../DataSet/STL10/stl10_binary/test_X.bin\")  / 255.0\n",
    "test_labels  = read_labels    (\"../../DataSet/STL10/stl10_binary/test_y.bin\")\n",
    "\n",
    "label_encoder= OneHotEncoder(sparse=False,categories='auto')\n",
    "train_labels = label_encoder.fit_transform(train_labels.reshape((-1,1)))\n",
    "test_labels  = label_encoder.fit_transform(test_labels.reshape((-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-11T20:31:29.419455Z",
     "start_time": "2018-12-11T20:31:25.705387Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1.] [0. 0. 0.] [0.44671062 0.43980984 0.40664645] [0.26034098 0.25657727 0.27126738]\n",
      "(5000, 96, 96, 3)\n",
      "(5000, 10)\n",
      "[1. 1. 1.] [0. 0. 0.] [0.44723063 0.43964247 0.40495725] [0.2605645  0.25666146 0.26997382]\n",
      "(8000, 96, 96, 3)\n",
      "(8000, 10)\n"
     ]
    }
   ],
   "source": [
    "# some basic statistic of train and test image // hyper\n",
    "print(train_images.max((0,1,2)),train_images.min((0,1,2)),train_images.mean((0,1,2)),train_images.std((0,1,2)) )\n",
    "print(train_images.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_images.max((0,1,2)),test_images.min((0,1,2)),test_images.mean((0,1,2)),test_images.std((0,1,2)) )\n",
    "print(test_images.shape)\n",
    "print(test_labels.shape)\n",
    "num_epoch = 50 ; learning_rate = 0.0008; batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-12T04:07:13.798729Z",
     "start_time": "2018-12-12T04:07:13.758835Z"
    },
    "code_folding": [
     25
    ]
   },
   "outputs": [],
   "source": [
    "# import layers\n",
    "def tf_softmax(x): return tf.nn.softmax(x)\n",
    "def tf_elu(x):     return tf.nn.elu(x)\n",
    "def d_tf_elu(x):   return tf.cast(tf.greater(x,0),tf.float32)  + (tf_elu(tf.cast(tf.less_equal(x,0),tf.float32) * x) + 1.0)\n",
    "def tf_relu(x):    return tf.nn.relu(x)\n",
    "def d_tf_relu(x):  return tf.cast(tf.greater(x,0),tf.float32)\n",
    "def tf_iden(x): return x\n",
    "\n",
    "class CNN():\n",
    "\n",
    "    def __init__(self,k,inc,out, stddev=0.05,which_reg=0,act=tf_elu,d_act=d_tf_elu):\n",
    "        self.w          = tf.Variable(tf.random_normal([k,k,inc,out],stddev=stddev,seed=2,dtype=tf.float32))\n",
    "        self.moving_w   = tf.Variable(tf.zeros_like(self.w))\n",
    "        self.b          = tf.Variable(tf.zeros(out,dtype=tf.float32))\n",
    "        self.m,self.v   = tf.Variable(tf.zeros_like(self.w)),tf.Variable(tf.zeros_like(self.w))\n",
    "        self.mb,self.vb = tf.Variable(tf.zeros_like(self.b)),tf.Variable(tf.zeros_like(self.b))\n",
    "        self.act,self.d_act = act,d_act\n",
    "        self.which_reg  = which_reg\n",
    "        \n",
    "    def getw(self): return [self.w,self.b]\n",
    "    def feedforward(self,input,stride=1,padding='VALID'):\n",
    "        self.input  = input\n",
    "        self.layer  = tf.nn.conv2d(input,self.w,strides=[1,stride,stride,1],padding=padding)\n",
    "        self.layerA = self.act(self.layer)\n",
    "        return self.layerA\n",
    "\n",
    "    def backprop(self,gradient,stride=1,padding='VALID'):\n",
    "        grad_part_1 = gradient\n",
    "        grad_part_2 = self.d_act(self.layer)\n",
    "        grad_part_3 = self.input\n",
    "\n",
    "        grad_middle = grad_part_1 * grad_part_2\n",
    "        grad_b      = tf.reduce_mean(grad_middle,(0,1,2))/batch_size\n",
    "        grad        = tf.nn.conv2d_backprop_filter(input = grad_part_3,filter_sizes = tf.shape(self.w),  out_backprop = grad_middle,strides=[1,stride,stride,1],padding=padding) / batch_size\n",
    "        grad_pass   = tf.nn.conv2d_backprop_input (input_sizes = tf.shape(self.input),filter= self.w,out_backprop = grad_middle,strides=[1,stride,stride,1],padding=padding)\n",
    "\n",
    "        if self.which_reg == 0:   grad = grad\n",
    "        if self.which_reg == 0.5: grad = grad + lamda * (tf.sqrt(tf.abs(self.w))) * (1.0/tf.sqrt(tf.abs(self.w)+ 10e-5)) * tf.sign(self.w)\n",
    "        if self.which_reg == 1:   grad = grad + lamda * tf.sign(self.w)\n",
    "        if self.which_reg == 1.5: grad = grad + lamda * 1.0/(tf.sqrt(tf.square(self.w) + 10e-5)) * self.w\n",
    "        if self.which_reg == 2:   grad = grad + lamda * (1.0/tf.sqrt(tf.square(tf.abs(self.w))+ 10e-5)) * tf.abs(self.w) * tf.sign(self.w)\n",
    "        if self.which_reg == 2.5: grad = grad + lamda * 2.0 * self.w\n",
    "        if self.which_reg == 3:   grad = grad + lamda * tf.pow(tf.pow(tf.abs(self.w),3)+ 10e-5,-0.66) * tf.pow(tf.abs(self.w),2) * tf.sign(self.w)\n",
    "        if self.which_reg == 4:   grad = grad + lamda * tf.pow(tf.pow(tf.abs(self.w),4)+ 10e-5,-0.75) * tf.pow(tf.abs(self.w),3) * tf.sign(self.w)\n",
    "\n",
    "        update_w = []\n",
    "        \n",
    "        update_w.append(tf.assign( self.m,self.m*beta1 + (1-beta1) * (grad)   ))\n",
    "        update_w.append(tf.assign( self.v,self.v*beta2 + (1-beta2) * (grad ** 2)   ))\n",
    "        m_hat = self.m / (1-beta1) ; v_hat = self.v / (1-beta2)\n",
    "        adam_middle = m_hat * learning_rate/(tf.sqrt(v_hat) + adam_e)\n",
    "        update_w.append(tf.assign(self.w,tf.subtract(self.w,adam_middle  )))\n",
    "        \n",
    "        update_w.append(tf.assign( self.mb,self.mb*beta1 + (1-beta1) * (grad_b)   ))\n",
    "        update_w.append(tf.assign( self.vb,self.vb*beta2 + (1-beta2) * (grad_b ** 2)   ))\n",
    "        m_hatb = self.mb / (1-beta1) ; v_hatb = self.vb / (1-beta2)\n",
    "        adam_middleb = m_hatb * learning_rate/(tf.sqrt(v_hatb) + adam_e)\n",
    "        update_w.append(tf.assign(self.b,tf.subtract(self.b,adam_middleb  )))\n",
    "        \n",
    "        return grad_pass,update_w\n",
    "    \n",
    "    def updatew(self):\n",
    "        w,h,inc,outc = self.w.shape\n",
    "        \n",
    "        tempw = self.w\n",
    "#         tempw = tf.reshape(self.w,(w*h,inc,outc))\n",
    "#         tempw = tf.transpose(tempw,(2,1,0))\n",
    "        s,U,V = tf.svd(tempw)\n",
    "        print(s)\n",
    "        print(tf.reduce_min(s,1,True))\n",
    "        scaleds = (s-tf.reduce_min(s,1,True))/(tf.reduce_max(s,1,True)-tf.reduce_min(s,1,True)+1e-8)\n",
    "        _,_,n   = s.shape\n",
    "        n     = n//2\n",
    "        print(n)\n",
    "        neww  = U[:,:,:,:n] @ tf.matrix_diag(scaleds)[:,:n,:n] @ tf.transpose(V,(0,2,1))[:,:n,:]\n",
    "        neww  = tf.transpose(neww,(2,1,0))\n",
    "        neww  = tf.reshape(neww,(w,h,inc,outc))\n",
    "        \n",
    "        update = []\n",
    "#         update.append(tf.assign(self.moving_w,self.moving_w * 0.9 + 0.1 * neww))\n",
    "#         tempww = self.moving_w/(1-0.9)\n",
    "        update.append(tf.assign(self.w,neww))\n",
    "        return update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-12T04:07:14.158969Z",
     "start_time": "2018-12-12T04:07:14.146004Z"
    },
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# create the svd layer\n",
    "class svd_layer():\n",
    "    \n",
    "    def __init__(self,batch,size,width):\n",
    "        self.n = size\n",
    "        self.moving_s = tf.Variable(tf.zeros((batch_size,size),dtype=tf.float32))\n",
    "        self.moving_u = tf.Variable(tf.zeros((batch_size,width**2,size),dtype=tf.float32))\n",
    "        self.moving_v = tf.Variable(tf.zeros((batch_size,size,size),dtype=tf.float32))\n",
    "    \n",
    "    def feedforward(self,data,training_phase):\n",
    "        \n",
    "        with tf.device('/cpu:0'):\n",
    "            s,U,V = tf.svd(data)\n",
    "        print(s)\n",
    "        print(U)\n",
    "        print(V)\n",
    "        smin = tf.reduce_min(s,1,keepdims=True)\n",
    "        smax = tf.reduce_max(s,1,keepdims=True)\n",
    "        scaleds = (s - smin)/(smax-smin + 1e-8)\n",
    "        def training_fn():\n",
    "            data      = U[:,:,:] @ tf.matrix_diag(s)[:,:,:] @ tf.transpose(V,(0,2,1))[:,:,:]\n",
    "            data = data  * tf.reduce_mean(tf.transpose(tf.abs(V),(0,2,1)) * scaleds[:,:,None] ,(1),keepdims=True)\n",
    "            # data      = data * tf.reduce_mean(tf.transpose(tf.abs(V),(0,2,1)) * scaleds[:,:,None] ,(1),keepdims=True)\n",
    "            update = []\n",
    "            update.append(tf.assign(self.moving_u,self.moving_u*0.9 + 0.1 * U))\n",
    "            update.append(tf.assign(self.moving_v,self.moving_v*0.9 + 0.1 * V))\n",
    "            return data,update\n",
    "            \n",
    "        def testing_fn():\n",
    "            data      = U[:,:,:] @ tf.matrix_diag(s)[:,:,:] @ tf.transpose(V,(0,2,1))[:,:,:]\n",
    "            # data      = data * tf.reduce_mean(tf.transpose(tf.abs(V),(0,2,1)) * scaleds[:,:,None] ,(1),keepdims=True)\n",
    "            update = []\n",
    "            update.append(tf.assign(self.moving_u,self.moving_u))\n",
    "            update.append(tf.assign(self.moving_v,self.moving_v))\n",
    "            return data,update\n",
    "        \n",
    "        data,update  = tf.cond(training_phase,true_fn=training_fn,false_fn=testing_fn)\n",
    "        return data,update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-12T04:07:15.478431Z",
     "start_time": "2018-12-12T04:07:15.084447Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# restart the graph \n",
    "# sess.close()\n",
    "# tf.reset_default_graph()\n",
    "learning_rate = 0.0008; batch_size = 100\n",
    "\n",
    "l1 = CNN(3,3, 16)\n",
    "l2 = CNN(3,16,16)\n",
    "l3 = CNN(3,16,16)\n",
    "l3_svd = svd_layer(100,16,24)\n",
    "\n",
    "l4 = CNN(3,16,32)\n",
    "l5 = CNN(3,32,32)\n",
    "l6 = CNN(3,32,32)\n",
    "l6_svd = svd_layer(100,32,6)\n",
    "\n",
    "l7 = CNN(3,32,64)\n",
    "l8 = CNN(3,64,64)\n",
    "l9 = CNN(3,64,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-12T04:07:17.401364Z",
     "start_time": "2018-12-12T04:07:15.946215Z"
    },
    "code_folding": [
     1
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Svd_336:0\", shape=(16, 3), dtype=float32)\n",
      "Tensor(\"Min_250:0\", shape=(16, 1), dtype=float32)\n",
      "3\n",
      "Tensor(\"Svd_337:0\", shape=(16, 9), dtype=float32)\n",
      "Tensor(\"Min_253:0\", shape=(16, 1), dtype=float32)\n",
      "9\n",
      "Tensor(\"Svd_338:0\", shape=(16, 9), dtype=float32)\n",
      "Tensor(\"Min_256:0\", shape=(16, 1), dtype=float32)\n",
      "9\n",
      "Tensor(\"Svd_339:0\", shape=(32, 9), dtype=float32)\n",
      "Tensor(\"Min_259:0\", shape=(32, 1), dtype=float32)\n",
      "9\n",
      "Tensor(\"Svd_340:0\", shape=(32, 9), dtype=float32)\n",
      "Tensor(\"Min_262:0\", shape=(32, 1), dtype=float32)\n",
      "9\n",
      "Tensor(\"Svd_341:0\", shape=(32, 9), dtype=float32)\n",
      "Tensor(\"Min_265:0\", shape=(32, 1), dtype=float32)\n",
      "9\n",
      "Tensor(\"Svd_342:0\", shape=(64, 9), dtype=float32)\n",
      "Tensor(\"Min_268:0\", shape=(64, 1), dtype=float32)\n",
      "9\n",
      "Tensor(\"Svd_343:0\", shape=(64, 9), dtype=float32)\n",
      "Tensor(\"Min_271:0\", shape=(64, 1), dtype=float32)\n",
      "9\n",
      "Tensor(\"Svd_344:0\", shape=(10, 9), dtype=float32)\n",
      "Tensor(\"Min_274:0\", shape=(10, 1), dtype=float32)\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "# build graph \n",
    "x = tf.placeholder(shape=(batch_size,96,96,3),dtype=tf.float32)\n",
    "y = tf.placeholder(shape=(batch_size,10),dtype=tf.float32)\n",
    "is_train = tf.placeholder_with_default(False,())\n",
    "\n",
    "layer1 = l1.feedforward(x       ,padding='SAME',stride=2)\n",
    "layer2 = l2.feedforward(layer1  ,padding='SAME',stride=2) \n",
    "layer3 = l3.feedforward(layer2  ,padding='SAME',stride=1)\n",
    "\n",
    "layer4 = l4.feedforward(layer3  ,padding='SAME',stride=2) \n",
    "layer5 = l5.feedforward(layer4  ,padding='SAME',stride=2)\n",
    "layer6 = l6.feedforward(layer5  ,padding='SAME',stride=1)\n",
    "\n",
    "layer7 = l7.feedforward(layer6  ,padding='SAME',stride=1)\n",
    "layer8 = l8.feedforward(layer7  ,padding='SAME',stride=1) \n",
    "layer9 = l9.feedforward(layer8  ,padding='SAME',stride=1)\n",
    "\n",
    "all_update = l1.updatew() + l2.updatew() + l3.updatew() + \\\n",
    "             l4.updatew() + l5.updatew() + l6.updatew() + \\\n",
    "             l7.updatew() + l8.updatew() + l9.updatew() \n",
    "\n",
    "final_layer = tf.reduce_mean(layer9,axis=(1,2))\n",
    "cost        = tf.nn.softmax_cross_entropy_with_logits_v2(logits=final_layer,labels=y)\n",
    "auto_train  = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "final_softmax      = tf_softmax(final_layer)\n",
    "correct_prediction = tf.equal(tf.argmax(final_softmax, 1), tf.argmax(y, 1))\n",
    "accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-12T04:11:59.744459Z",
     "start_time": "2018-12-12T04:07:18.156874Z"
    },
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Iter : 0/300 batch : 7900/8000 acc : 0.2318\n",
      " Current : 0 Acc : 0.1492000003159046 Test Acc : 0.17325000008568167\n",
      "\n",
      "Current Iter : 1/300 batch : 7900/8000 acc : 0.2121\n",
      " Current : 1 Acc : 0.2012000000476837 Test Acc : 0.23362499978393317\n",
      "\n",
      "Current Iter : 2/300 batch : 7900/8000 acc : 0.2729\n",
      " Current : 2 Acc : 0.2600000014901161 Test Acc : 0.28175000157207253\n",
      "\n",
      "Current Iter : 3/300 batch : 7900/8000 acc : 0.2729\n",
      " Current : 3 Acc : 0.28419999837875365 Test Acc : 0.2668750012293458\n",
      "\n",
      "Current Iter : 4/300 batch : 7900/8000 acc : 0.3541\n",
      " Current : 4 Acc : 0.2933999982476234 Test Acc : 0.31325000170618295\n",
      "\n",
      "Current Iter : 5/300 batch : 7900/8000 acc : 0.3934\n",
      " Current : 5 Acc : 0.3178000009059906 Test Acc : 0.32600000165402887\n",
      "\n",
      "Current Iter : 6/300 batch : 7900/8000 acc : 0.3431\n",
      " Current : 6 Acc : 0.3516000032424927 Test Acc : 0.3391250004991889\n",
      "\n",
      "Current Iter : 7/300 batch : 7900/8000 acc : 0.3332\n",
      " Current : 7 Acc : 0.35920000076293945 Test Acc : 0.3516250018030405\n",
      "\n",
      "Current Iter : 8/300 batch : 7900/8000 acc : 0.4127\n",
      " Current : 8 Acc : 0.36899999976158143 Test Acc : 0.37950000129640105\n",
      "\n",
      "Current Iter : 9/300 batch : 7900/8000 acc : 0.3949\n",
      " Current : 9 Acc : 0.3908000004291534 Test Acc : 0.3858749996870756\n",
      "\n",
      "Current Iter : 10/300 batch : 7900/8000 acc : 0.4345\n",
      " Current : 10 Acc : 0.4139999973773956 Test Acc : 0.40975000001490114\n",
      "\n",
      "Current Iter : 11/300 batch : 7900/8000 acc : 0.3746\n",
      " Current : 11 Acc : 0.43019999682903287 Test Acc : 0.4018749989569187\n",
      "\n",
      "Current Iter : 12/300 batch : 7900/8000 acc : 0.4649\n",
      " Current : 12 Acc : 0.43760000050067904 Test Acc : 0.4136249989271164\n",
      "\n",
      "Current Iter : 13/300 batch : 7900/8000 acc : 0.4946\n",
      " Current : 13 Acc : 0.4559999978542328 Test Acc : 0.4188749991357327\n",
      "\n",
      "Current Iter : 14/300 batch : 7900/8000 acc : 0.3649\n",
      " Current : 14 Acc : 0.4638000017404556 Test Acc : 0.4078749969601631\n",
      "\n",
      "Current Iter : 15/300 batch : 7900/8000 acc : 0.4845\n",
      " Current : 15 Acc : 0.473799996972084 Test Acc : 0.4214999992400408\n",
      "\n",
      "Current Iter : 16/300 batch : 7900/8000 acc : 0.4358\n",
      " Current : 16 Acc : 0.4929999947547913 Test Acc : 0.44424999952316285\n",
      "\n",
      "Current Iter : 17/300 batch : 7900/8000 acc : 0.3752\n",
      " Current : 17 Acc : 0.49579999804496766 Test Acc : 0.4376249983906746\n",
      "\n",
      "Current Iter : 18/300 batch : 7900/8000 acc : 0.5448\n",
      " Current : 18 Acc : 0.5095999956130981 Test Acc : 0.4319999974220991\n",
      "\n",
      "Current Iter : 19/300 batch : 7900/8000 acc : 0.4953\n",
      " Current : 19 Acc : 0.5305999952554703 Test Acc : 0.45687499865889547\n",
      "\n",
      "Current Iter : 20/300 batch : 7900/8000 acc : 0.4753\n",
      " Current : 20 Acc : 0.5379999965429306 Test Acc : 0.4522499989718199\n",
      "\n",
      "Current Iter : 21/300 batch : 7900/8000 acc : 0.4453\n",
      " Current : 21 Acc : 0.5539999955892563 Test Acc : 0.44825000017881395\n",
      "\n",
      "Current Iter : 22/300 batch : 7900/8000 acc : 0.5854\n",
      " Current : 22 Acc : 0.5705999958515168 Test Acc : 0.45024999901652335\n",
      "\n",
      "Current Iter : 23/300 batch : 7900/8000 acc : 0.5467\n",
      " Current : 23 Acc : 0.5918000036478043 Test Acc : 0.4542499966919422\n",
      "\n",
      "Current Iter : 24/300 batch : 7900/8000 acc : 0.4256\n",
      " Current : 24 Acc : 0.5889999961853027 Test Acc : 0.46187499798834325\n",
      "\n",
      "Current Iter : 25/300 batch : 7900/8000 acc : 0.4267\n",
      " Current : 25 Acc : 0.607600000500679 Test Acc : 0.4507499970495701\n",
      "\n",
      "Current Iter : 26/300 batch : 7900/8000 acc : 0.5567\n",
      " Current : 26 Acc : 0.6151999998092651 Test Acc : 0.45224999710917474\n",
      "\n",
      "Current Iter : 27/300 batch : 7900/8000 acc : 0.4852\n",
      " Current : 27 Acc : 0.640200001001358 Test Acc : 0.4582499973475933\n",
      "\n",
      "Current Iter : 28/300 batch : 7900/8000 acc : 0.5559\n",
      " Current : 28 Acc : 0.6624000012874603 Test Acc : 0.4323749989271164\n",
      "\n",
      "Current Iter : 29/300 batch : 7900/8000 acc : 0.4159\n",
      " Current : 29 Acc : 0.6594000005722046 Test Acc : 0.4362499985843897\n",
      "\n",
      "Current Iter : 30/300 batch : 7900/8000 acc : 0.5168\n",
      " Current : 30 Acc : 0.6756000006198883 Test Acc : 0.45049999877810476\n",
      "\n",
      "Current Iter : 31/300 batch : 7900/8000 acc : 0.4872\n",
      " Current : 31 Acc : 0.7054000008106232 Test Acc : 0.44724999777972696\n",
      "\n",
      "Current Iter : 32/300 batch : 7900/8000 acc : 0.4376\n",
      " Current : 32 Acc : 0.7216000008583069 Test Acc : 0.45287499763071537\n",
      "\n",
      "Current Iter : 33/300 batch : 7900/8000 acc : 0.3965\n",
      " Current : 33 Acc : 0.7320000016689301 Test Acc : 0.44587499760091304\n",
      "\n",
      "Current Iter : 34/300 batch : 7900/8000 acc : 0.4379\n",
      " Current : 34 Acc : 0.7545999991893768 Test Acc : 0.4454999975860119\n",
      "\n",
      "Current Iter : 35/300 batch : 7900/8000 acc : 0.4875\n",
      " Current : 35 Acc : 0.7711999988555909 Test Acc : 0.441374995931983\n",
      "\n",
      "Current Iter : 36/300 batch : 7900/8000 acc : 0.3663\n",
      " Current : 36 Acc : 0.7737999939918518 Test Acc : 0.43862499929964543\n",
      "\n",
      "Current Iter : 37/300 batch : 7900/8000 acc : 0.4783\n",
      " Current : 37 Acc : 0.8005999958515168 Test Acc : 0.45174999758601186\n",
      "\n",
      "Current Iter : 38/300 batch : 7900/8000 acc : 0.3985\n",
      " Current : 38 Acc : 0.8370000004768372 Test Acc : 0.45174999833106994\n",
      "\n",
      "Current Iter : 39/300 batch : 7900/8000 acc : 0.5187\n",
      " Current : 39 Acc : 0.8426000010967255 Test Acc : 0.44562499709427356\n",
      "\n",
      "Current Iter : 40/300 batch : 7900/8000 acc : 0.5587\n",
      " Current : 40 Acc : 0.84799999833107 Test Acc : 0.4428749982267618\n",
      "\n",
      "Current Iter : 41/300 batch : 7900/8000 acc : 0.4884\n",
      " Current : 41 Acc : 0.8528000020980835 Test Acc : 0.44162499755620954\n",
      "\n",
      "Current Iter : 42/300 batch : 7900/8000 acc : 0.4684\n",
      " Current : 42 Acc : 0.8846000027656555 Test Acc : 0.4328749969601631\n",
      "\n",
      "Current Iter : 43/300 batch : 7900/8000 acc : 0.4485\n",
      " Current : 43 Acc : 0.8924000024795532 Test Acc : 0.4384999994188547\n",
      "\n",
      "Current Iter : 44/300 batch : 7900/8000 acc : 0.4195\n",
      " Current : 44 Acc : 0.8966000032424927 Test Acc : 0.43874999806284903\n",
      "\n",
      "Current Iter : 45/300 batch : 7900/8000 acc : 0.4784\n",
      "-----------reset\n",
      "\n",
      " Current : 45 Acc : 0.911800000667572 Test Acc : 0.4358749967068434\n",
      "\n",
      "Current Iter : 46/300 batch : 7900/8000 acc : 0.3742\n",
      " Current : 46 Acc : 0.30439999759197234 Test Acc : 0.370750000141561\n",
      "\n",
      "Current Iter : 47/300 batch : 7900/8000 acc : 0.3246\n",
      " Current : 47 Acc : 0.471999996304512 Test Acc : 0.41674999967217446\n",
      "\n",
      "Current Iter : 48/300 batch : 7900/8000 acc : 0.3943\n",
      " Current : 48 Acc : 0.5376000010967255 Test Acc : 0.43337499871850016\n",
      "\n",
      "Current Iter : 49/300 batch : 7900/8000 acc : 0.4856\n",
      " Current : 49 Acc : 0.5991999959945679 Test Acc : 0.4443749971687794\n",
      "\n",
      "Current Iter : 50/300 batch : 7900/8000 acc : 0.4869\n",
      " Current : 50 Acc : 0.64200000166893 Test Acc : 0.43924999609589577\n",
      "\n",
      "Current Iter : 51/300 batch : 7900/8000 acc : 0.4577\n",
      " Current : 51 Acc : 0.6788000035285949 Test Acc : 0.45324999541044236\n",
      "\n",
      "Current Iter : 52/300 batch : 7900/8000 acc : 0.4584\n",
      " Current : 52 Acc : 0.7282000088691711 Test Acc : 0.45249999687075615\n",
      "\n",
      "Current Iter : 53/300 batch : 7900/8000 acc : 0.5373\n",
      " Current : 53 Acc : 0.765200002193451 Test Acc : 0.4479999981820583\n",
      "\n",
      "Current Iter : 54/300 batch : 7900/8000 acc : 0.4668\n",
      " Current : 54 Acc : 0.7947999954223632 Test Acc : 0.45812499783933164\n",
      "\n",
      "Current Iter : 55/300 batch : 7900/8000 acc : 0.4778\n",
      " Current : 55 Acc : 0.8255999994277954 Test Acc : 0.45062499716877935\n",
      "\n",
      "Current Iter : 56/300 batch : 7900/8000 acc : 0.3781\n",
      " Current : 56 Acc : 0.8512000000476837 Test Acc : 0.44687499664723873\n",
      "\n",
      "Current Iter : 57/300 batch : 7900/8000 acc : 0.4383\n",
      " Current : 57 Acc : 0.8739999961853028 Test Acc : 0.4493749964982271\n",
      "\n",
      "Current Iter : 58/300 batch : 7900/8000 acc : 0.4389\n",
      "-----------reset\n",
      "\n",
      " Current : 58 Acc : 0.900400003194809 Test Acc : 0.44287499636411665\n",
      "\n",
      "Current Iter : 59/300 batch : 7900/8000 acc : 0.3986\n",
      " Current : 59 Acc : 0.8751999962329865 Test Acc : 0.43499999679625034\n",
      "\n",
      "Current Iter : 60/300 batch : 7900/8000 acc : 0.3889\n",
      "-----------reset\n",
      "\n",
      " Current : 60 Acc : 0.9050000023841858 Test Acc : 0.43574999906122686\n",
      "\n",
      "Current Iter : 61/300 batch : 7900/8000 acc : 0.3992\n",
      "-----------reset\n",
      "\n",
      " Current : 61 Acc : 0.9168000042438507 Test Acc : 0.43824999742209914\n",
      "\n",
      "Current Iter : 62/300 batch : 7900/8000 acc : 0.4187\n",
      "-----------reset\n",
      "\n",
      " Current : 62 Acc : 0.9216000032424927 Test Acc : 0.4301249973475933\n",
      "\n",
      "Current Iter : 63/300 batch : 7900/8000 acc : 0.4184\n",
      "-----------reset\n",
      "\n",
      " Current : 63 Acc : 0.928400000333786 Test Acc : 0.44187499657273294\n",
      "\n",
      "Current Iter : 64/300 batch : 7900/8000 acc : 0.4592\n",
      "-----------reset\n",
      "\n",
      " Current : 64 Acc : 0.9338000023365021 Test Acc : 0.4284999955445528\n",
      "\n",
      "Current Iter : 65/300 batch : 7900/8000 acc : 0.3995\n",
      "-----------reset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Current : 65 Acc : 0.9351999974250793 Test Acc : 0.4282499957829714\n",
      "\n",
      "0 Current Iter : 66/300 batch : 3900/5000 acc : 0.96\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-333-0be228c2d0e3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[0mcurrent_data\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mtrain_images\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcurrent_batch_index\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mcurrent_batch_index\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[0mcurrent_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcurrent_batch_index\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mcurrent_batch_index\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m             \u001b[0msess_results\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mauto_train\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mcurrent_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mcurrent_label\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m             \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' Current Iter : '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/'\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_epoch\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;33m+\u001b[0m \u001b[1;34m' batch : '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_batch_index\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/'\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' acc : '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'\\r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0mavg_acc_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mavg_acc_train\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msess_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1332\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# start the training \n",
    "# sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "num_epoch = 300 ; avg_acc_train = 0; avg_acc_test  = 0; train_more = 1\n",
    "\n",
    "for iter in range(num_epoch):\n",
    "    \n",
    "    test_images,test_labels   = shuffle(test_images,test_labels);\n",
    "\n",
    "    for i in range(train_more):\n",
    "        train_images,train_labels = shuffle(train_images,train_labels);\n",
    "        for current_batch_index in range(0,len(train_images),batch_size):\n",
    "            current_data  = train_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "            current_label = train_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "            sess_results  = sess.run([accuracy,auto_train],feed_dict={x:current_data,y:current_label})\n",
    "            sys.stdout.write(str(i) + ' Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(train_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "            sys.stdout.flush(); avg_acc_train = avg_acc_train + sess_results[0]\n",
    "        \n",
    "    for current_batch_index in range(0,len(test_images), batch_size):\n",
    "        current_data  = test_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = test_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(test_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_test = avg_acc_test + sess_results[0]        \n",
    "        \n",
    "\n",
    "    if avg_acc_train/(len(train_images)/batch_size)/train_more > 0.9 and avg_acc_test/(len(test_images)/batch_size) < 0.5 :  \n",
    "        print('\\n-----------reset')\n",
    "        sess.run(all_update)\n",
    "    print(\"\\n Current : \"+ str(iter) + \" Acc : \" + str(avg_acc_train/(len(train_images)/batch_size)/train_more) + \" Test Acc : \" + str(avg_acc_test/(len(test_images)/batch_size)) + '\\n')\n",
    "    avg_acc_train = 0 ; avg_acc_test  = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T21:03:17.629751Z",
     "start_time": "2018-12-10T21:03:16.795967Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference \n",
    "1. Brownlee, J. (2017). How to One Hot Encode Sequence Data in Python. Machine Learning Mastery. Retrieved 9 December 2018, from https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/\n",
    "2. tf.placeholder_with_default | TensorFlow. (2018). TensorFlow. Retrieved 9 December 2018, from https://www.tensorflow.org/api_docs/python/tf/placeholder_with_default\n",
    "3. tf.nn.softmax_cross_entropy_with_logits | TensorFlow. (2018). TensorFlow. Retrieved 9 December 2018, from https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits\n",
    "4. line, O. (2018). Output without new line. Stack Overflow. Retrieved 9 December 2018, from https://stackoverflow.com/questions/2623470/output-without-new-line\n",
    "5. shell?, H. (2018). How to tell if tensorflow is using gpu acceleration from inside python shell?. Stack Overflow. Retrieved 9 December 2018, from https://stackoverflow.com/questions/38009682/how-to-tell-if-tensorflow-is-using-gpu-acceleration-from-inside-python-shell\n",
    "6. GPU?, H. (2018). How to use TensorFlow GPU?. Stack Overflow. Retrieved 9 December 2018, from https://stackoverflow.com/questions/51306862/how-to-use-tensorflow-gpu\n",
    "7. Batch normalization: theory and how to use it with Tensorflow. (2018). Towards Data Science. Retrieved 9 December 2018, from https://towardsdatascience.com/batch-normalization-theory-and-how-to-use-it-with-tensorflow-1892ca0173ad\n",
    "8. Batch normalization: theory and how to use it with Tensorflow. (2018). Towards Data Science. Retrieved 9 December 2018, from https://towardsdatascience.com/batch-normalization-theory-and-how-to-use-it-with-tensorflow-1892ca0173ad\n",
    "9. tf.reset_default_graph | TensorFlow. (2018). TensorFlow. Retrieved 9 December 2018, from https://www.tensorflow.org/api_docs/python/tf/reset_default_graph\n",
    "10. tf.Session | TensorFlow. (2018). TensorFlow. Retrieved 9 December 2018, from https://www.tensorflow.org/api_docs/python/tf/Session\n",
    "11. tf.nn.moments | TensorFlow. (2018). TensorFlow. Retrieved 9 December 2018, from https://www.tensorflow.org/api_docs/python/tf/nn/moments\n",
    "12. CMD?, H. (2018). How do I run two commands in one line in Windows CMD?. Stack Overflow. Retrieved 9 December 2018, from https://stackoverflow.com/questions/8055371/how-do-i-run-two-commands-in-one-line-in-windows-cmd\n",
    "13. loop, B. (2018). Batch script loop. Stack Overflow. Retrieved 9 December 2018, from https://stackoverflow.com/questions/2591758/batch-script-loop\n",
    "14. tf.train.MomentumOptimizer | TensorFlow. (2018). TensorFlow. Retrieved 9 December 2018, from https://www.tensorflow.org/api_docs/python/tf/train/MomentumOptimizer\n",
    "15. Test if two numpy arrays are (close to) equal, i. (2018). Test if two numpy arrays are (close to) equal, including shape. Stack Overflow. Retrieved 9 December 2018, from https://stackoverflow.com/questions/32874840/test-if-two-numpy-arrays-are-close-to-equal-including-shape\n",
    "16. tf.linalg.diag | TensorFlow. (2018). TensorFlow. Retrieved 9 December 2018, from https://www.tensorflow.org/api_docs/python/tf/linalg/diag\n",
    "17. tf.layers.batch_normalization | TensorFlow. (2018). TensorFlow. Retrieved 10 December 2018, from https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization\n",
    "18. tf.layers.batch_normalization | TensorFlow. (2018). TensorFlow. Retrieved 10 December 2018, from https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization\n",
    "19. error, t. (2018). tf.layers.batch_normalization large test error. Stack Overflow. Retrieved 10 December 2018, from https://stackoverflow.com/questions/43234667/tf-layers-batch-normalization-large-test-error\n",
    "20. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
