{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-12T17:30:31.213021Z",
     "start_time": "2018-12-12T17:30:16.905306Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# import lib\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys, os,cv2\n",
    "from sklearn.utils import shuffle\n",
    "from scipy.misc import imread,imresize\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from skimage.transform import resize\n",
    "from imgaug import augmenters as iaa\n",
    "import imgaug as ia\n",
    "from scipy.ndimage import zoom\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-12T17:30:33.123538Z",
     "start_time": "2018-12-12T17:30:31.289821Z"
    },
    "code_folding": [
     0,
     1,
     28
    ]
   },
   "outputs": [],
   "source": [
    "# read all of the data\n",
    "def read_all_images(path_to_data):\n",
    "    \"\"\"\n",
    "    :param path_to_data: the file containing the binary images from the STL-10 dataset\n",
    "    :return: an array containing all the images\n",
    "    \"\"\"\n",
    "\n",
    "    with open(path_to_data, 'rb') as f:\n",
    "        # read whole file in uint8 chunks\n",
    "        everything = np.fromfile(f, dtype=np.uint8)\n",
    "\n",
    "        # We force the data into 3x96x96 chunks, since the\n",
    "        # images are stored in \"column-major order\", meaning\n",
    "        # that \"the first 96*96 values are the red channel,\n",
    "        # the next 96*96 are green, and the last are blue.\"\n",
    "        # The -1 is since the size of the pictures depends\n",
    "        # on the input file, and this way numpy determines\n",
    "        # the size on its own.\n",
    "\n",
    "        images = np.reshape(everything, (-1, 3, 96, 96))\n",
    "\n",
    "        # Now transpose the images into a standard image format\n",
    "        # readable by, for example, matplotlib.imshow\n",
    "        # You might want to comment this line or reverse the shuffle\n",
    "        # if you will use a learning algorithm like CNN, since they like\n",
    "        # their channels separated.\n",
    "        images = np.transpose(images, (0, 3, 2, 1))\n",
    "        return images\n",
    "def read_labels(path_to_labels):\n",
    "    \"\"\"\n",
    "    :param path_to_labels: path to the binary file containing labels from the STL-10 dataset\n",
    "    :return: an array containing the labels\n",
    "    \"\"\"\n",
    "    with open(path_to_labels, 'rb') as f:\n",
    "        labels = np.fromfile(f, dtype=np.uint8)\n",
    "        return labels\n",
    "    \n",
    "train_images = read_all_images(\"../../DataSet/STL10/stl10_binary/train_X.bin\") / 255.0\n",
    "train_labels = read_labels    (\"../../DataSet/STL10/stl10_binary/train_Y.bin\")\n",
    "test_images  = read_all_images(\"../../DataSet/STL10/stl10_binary/test_X.bin\")  / 255.0\n",
    "test_labels  = read_labels    (\"../../DataSet/STL10/stl10_binary/test_y.bin\")\n",
    "\n",
    "label_encoder= OneHotEncoder(sparse=False,categories='auto')\n",
    "train_labels = label_encoder.fit_transform(train_labels.reshape((-1,1)))\n",
    "test_labels  = label_encoder.fit_transform(test_labels.reshape((-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-12T17:30:37.024277Z",
     "start_time": "2018-12-12T17:30:33.380085Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1.] [0. 0. 0.] [0.44671062 0.43980984 0.40664645] [0.26034098 0.25657727 0.27126738]\n",
      "(5000, 96, 96, 3)\n",
      "(5000, 10)\n",
      "[1. 1. 1.] [0. 0. 0.] [0.44723063 0.43964247 0.40495725] [0.2605645  0.25666146 0.26997382]\n",
      "(8000, 96, 96, 3)\n",
      "(8000, 10)\n"
     ]
    }
   ],
   "source": [
    "# some basic statistic of train and test image // hyper\n",
    "print(train_images.max((0,1,2)),train_images.min((0,1,2)),train_images.mean((0,1,2)),train_images.std((0,1,2)) )\n",
    "print(train_images.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_images.max((0,1,2)),test_images.min((0,1,2)),test_images.mean((0,1,2)),test_images.std((0,1,2)) )\n",
    "print(test_images.shape)\n",
    "print(test_labels.shape)\n",
    "num_epoch = 50 ; learning_rate = 0.0008; batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-12T17:45:27.702385Z",
     "start_time": "2018-12-12T17:45:27.658458Z"
    },
    "code_folding": [
     26,
     82
    ]
   },
   "outputs": [],
   "source": [
    "# import layers\n",
    "def tf_softmax(x): return tf.nn.softmax(x)\n",
    "def tf_elu(x):     return tf.nn.elu(x)\n",
    "def d_tf_elu(x):   return tf.cast(tf.greater(x,0),tf.float32)  + (tf_elu(tf.cast(tf.less_equal(x,0),tf.float32) * x) + 1.0)\n",
    "def tf_relu(x):    return tf.nn.relu(x)\n",
    "def d_tf_relu(x):  return tf.cast(tf.greater(x,0),tf.float32)\n",
    "def tf_iden(x): return x\n",
    "\n",
    "class CNN():\n",
    "\n",
    "    def __init__(self,k,inc,out, stddev=0.05,which_reg=0,act=tf_elu,d_act=d_tf_elu):\n",
    "        self.w          = tf.Variable(tf.random_normal([k,k,inc,out],stddev=stddev,seed=2,dtype=tf.float32))\n",
    "        self.moving_w   = tf.Variable(tf.random_normal([k,k,inc,out],stddev=stddev,seed=2,dtype=tf.float32))\n",
    "        self.b          = tf.Variable(tf.zeros(out,dtype=tf.float32))\n",
    "        self.m,self.v   = tf.Variable(tf.zeros_like(self.w)),tf.Variable(tf.zeros_like(self.w))\n",
    "        self.mb,self.vb = tf.Variable(tf.zeros_like(self.b)),tf.Variable(tf.zeros_like(self.b))\n",
    "        self.act,self.d_act = act,d_act\n",
    "        self.which_reg  = which_reg\n",
    "        \n",
    "    def getw(self): return [self.w,self.b]\n",
    "    def feedforward(self,input,stride=1,padding='VALID'):\n",
    "        self.input  = input\n",
    "        self.layer  = tf.nn.conv2d(input,self.w,strides=[1,stride,stride,1],padding=padding)\n",
    "        self.layerA = self.act(self.layer)\n",
    "        return self.layerA\n",
    "\n",
    "    def backprop(self,gradient,stride=1,padding='VALID'):\n",
    "        grad_part_1 = gradient\n",
    "        grad_part_2 = self.d_act(self.layer)\n",
    "        grad_part_3 = self.input\n",
    "\n",
    "        grad_middle = grad_part_1 * grad_part_2\n",
    "        grad_b      = tf.reduce_mean(grad_middle,(0,1,2))/batch_size\n",
    "        grad        = tf.nn.conv2d_backprop_filter(input = grad_part_3,filter_sizes = tf.shape(self.w),  out_backprop = grad_middle,strides=[1,stride,stride,1],padding=padding) / batch_size\n",
    "        grad_pass   = tf.nn.conv2d_backprop_input (input_sizes = tf.shape(self.input),filter= self.w,out_backprop = grad_middle,strides=[1,stride,stride,1],padding=padding)\n",
    "\n",
    "        if self.which_reg == 0:   grad = grad\n",
    "        if self.which_reg == 0.5: grad = grad + lamda * (tf.sqrt(tf.abs(self.w))) * (1.0/tf.sqrt(tf.abs(self.w)+ 10e-5)) * tf.sign(self.w)\n",
    "        if self.which_reg == 1:   grad = grad + lamda * tf.sign(self.w)\n",
    "        if self.which_reg == 1.5: grad = grad + lamda * 1.0/(tf.sqrt(tf.square(self.w) + 10e-5)) * self.w\n",
    "        if self.which_reg == 2:   grad = grad + lamda * (1.0/tf.sqrt(tf.square(tf.abs(self.w))+ 10e-5)) * tf.abs(self.w) * tf.sign(self.w)\n",
    "        if self.which_reg == 2.5: grad = grad + lamda * 2.0 * self.w\n",
    "        if self.which_reg == 3:   grad = grad + lamda * tf.pow(tf.pow(tf.abs(self.w),3)+ 10e-5,-0.66) * tf.pow(tf.abs(self.w),2) * tf.sign(self.w)\n",
    "        if self.which_reg == 4:   grad = grad + lamda * tf.pow(tf.pow(tf.abs(self.w),4)+ 10e-5,-0.75) * tf.pow(tf.abs(self.w),3) * tf.sign(self.w)\n",
    "\n",
    "        update_w = []\n",
    "        \n",
    "        update_w.append(tf.assign( self.m,self.m*beta1 + (1-beta1) * (grad)   ))\n",
    "        update_w.append(tf.assign( self.v,self.v*beta2 + (1-beta2) * (grad ** 2)   ))\n",
    "        m_hat = self.m / (1-beta1) ; v_hat = self.v / (1-beta2)\n",
    "        adam_middle = m_hat * learning_rate/(tf.sqrt(v_hat) + adam_e)\n",
    "        update_w.append(tf.assign(self.w,tf.subtract(self.w,adam_middle  )))\n",
    "        \n",
    "        update_w.append(tf.assign( self.mb,self.mb*beta1 + (1-beta1) * (grad_b)   ))\n",
    "        update_w.append(tf.assign( self.vb,self.vb*beta2 + (1-beta2) * (grad_b ** 2)   ))\n",
    "        m_hatb = self.mb / (1-beta1) ; v_hatb = self.vb / (1-beta2)\n",
    "        adam_middleb = m_hatb * learning_rate/(tf.sqrt(v_hatb) + adam_e)\n",
    "        update_w.append(tf.assign(self.b,tf.subtract(self.b,adam_middleb  )))\n",
    "        \n",
    "        return grad_pass,update_w\n",
    "    \n",
    "    def updatew(self):\n",
    "        w,h,inc,outc = self.w.shape\n",
    "        \n",
    "        tempw = tf.reshape(self.w,(w*h,inc,outc))\n",
    "        tempw = tf.transpose(tempw,(1,2,0))\n",
    "        s,U,V = tf.svd(tempw)\n",
    "\n",
    "        scaleds = (s-tf.reduce_min(s,1,True))/(tf.reduce_max(s,1,True)-tf.reduce_min(s,1,True) + 1e-8 )\n",
    "        _,n     = s.shape\n",
    "        n       = 2\n",
    "\n",
    "        neww   = U[:,:,n:n+3] @ tf.matrix_diag(scaleds)[:,n:n+3,n:n+3] @ tf.transpose(V,(0,2,1))[:,n:n+3,:]\n",
    "        neww   = tf.transpose(neww,(2,0,1))\n",
    "        neww   = tf.reshape(neww,(w,h,inc,outc))\n",
    "        \n",
    "        update = []\n",
    "        # update.append(tf.assign(self.moving_w,self.moving_w * 0.5 + 0.5 * neww))\n",
    "        update.append(tf.assign(self.w,neww))\n",
    "        return update\n",
    "    \n",
    "# create the svd layer\n",
    "class svd_layer():\n",
    "    \n",
    "    def __init__(self,batch,size,width):\n",
    "        self.n = size\n",
    "        self.moving_s = tf.Variable(tf.zeros((batch_size,size),dtype=tf.float32))\n",
    "        self.moving_u = tf.Variable(tf.zeros((batch_size,width**2,size),dtype=tf.float32))\n",
    "        self.moving_v = tf.Variable(tf.zeros((batch_size,size,size),dtype=tf.float32))\n",
    "    \n",
    "    def feedforward(self,data,training_phase):\n",
    "        \n",
    "        with tf.device('/cpu:0'):\n",
    "            s,U,V = tf.svd(data)\n",
    "        print(s)\n",
    "        print(U)\n",
    "        print(V)\n",
    "        smin = tf.reduce_min(s,1,keepdims=True)\n",
    "        smax = tf.reduce_max(s,1,keepdims=True)\n",
    "        scaleds = (s - smin)/(smax-smin + 1e-8)\n",
    "        def training_fn():\n",
    "            data      = U[:,:,:] @ tf.matrix_diag(s)[:,:,:] @ tf.transpose(V,(0,2,1))[:,:,:]\n",
    "            data = data  * tf.reduce_mean(tf.transpose(tf.abs(V),(0,2,1)) * scaleds[:,:,None] ,(1),keepdims=True)\n",
    "            # data      = data * tf.reduce_mean(tf.transpose(tf.abs(V),(0,2,1)) * scaleds[:,:,None] ,(1),keepdims=True)\n",
    "            update = []\n",
    "            update.append(tf.assign(self.moving_u,self.moving_u*0.9 + 0.1 * U))\n",
    "            update.append(tf.assign(self.moving_v,self.moving_v*0.9 + 0.1 * V))\n",
    "            return data,update\n",
    "            \n",
    "        def testing_fn():\n",
    "            data      = U[:,:,:] @ tf.matrix_diag(s)[:,:,:] @ tf.transpose(V,(0,2,1))[:,:,:]\n",
    "            # data      = data * tf.reduce_mean(tf.transpose(tf.abs(V),(0,2,1)) * scaleds[:,:,None] ,(1),keepdims=True)\n",
    "            update = []\n",
    "            update.append(tf.assign(self.moving_u,self.moving_u))\n",
    "            update.append(tf.assign(self.moving_v,self.moving_v))\n",
    "            return data,update\n",
    "        \n",
    "        data,update  = tf.cond(training_phase,true_fn=training_fn,false_fn=testing_fn)\n",
    "        return data,update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-12T17:45:28.446350Z",
     "start_time": "2018-12-12T17:45:28.106260Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# restart the graph \n",
    "# sess.close()\n",
    "# tf.reset_default_graph()\n",
    "learning_rate = 0.0008; batch_size = 100\n",
    "\n",
    "l1 = CNN(3,3, 16)\n",
    "l2 = CNN(3,16,16)\n",
    "l3 = CNN(3,16,16)\n",
    "\n",
    "l4 = CNN(3,16,32)\n",
    "l5 = CNN(3,32,32)\n",
    "l6 = CNN(3,32,32)\n",
    "\n",
    "l7 = CNN(3,32,64)\n",
    "l8 = CNN(3,64,64)\n",
    "l9 = CNN(3,64,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-12T17:45:29.408779Z",
     "start_time": "2018-12-12T17:45:28.817358Z"
    },
    "code_folding": [
     1
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# build graph \n",
    "x = tf.placeholder(shape=(batch_size,96,96,3),dtype=tf.float32)\n",
    "y = tf.placeholder(shape=(batch_size,10),dtype=tf.float32)\n",
    "is_train = tf.placeholder_with_default(False,())\n",
    "\n",
    "layer1 = l1.feedforward(x       ,padding='SAME',stride=2)\n",
    "layer2 = l2.feedforward(layer1  ,padding='SAME',stride=2) \n",
    "layer3 = l3.feedforward(layer2  ,padding='SAME',stride=1)\n",
    "\n",
    "layer4 = l4.feedforward(layer3  ,padding='SAME',stride=2) \n",
    "layer5 = l5.feedforward(layer4  ,padding='SAME',stride=2)\n",
    "layer6 = l6.feedforward(layer5  ,padding='SAME',stride=1)\n",
    "\n",
    "layer7 = l7.feedforward(layer6  ,padding='SAME',stride=1)\n",
    "layer8 = l8.feedforward(layer7  ,padding='SAME',stride=1) \n",
    "layer9 = l9.feedforward(layer8  ,padding='SAME',stride=1)\n",
    "\n",
    "all_update  = l1.updatew() + l2.updatew() + l3.updatew() + l4.updatew() + l5.updatew() + l6.updatew() + l7.updatew() + l8.updatew() + l9.updatew() \n",
    "final_layer = tf.reduce_mean(layer9,axis=(1,2))\n",
    "cost        = tf.nn.softmax_cross_entropy_with_logits_v2(logits=final_layer,labels=y)\n",
    "optimizer   = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "reset_optimizer_op = tf.variables_initializer(optimizer.variables())\n",
    "auto_train  = optimizer.minimize(cost)\n",
    "\n",
    "final_softmax      = tf_softmax(final_layer)\n",
    "correct_prediction = tf.equal(tf.argmax(final_softmax, 1), tf.argmax(y, 1))\n",
    "accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-12T17:50:31.490Z"
    },
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current : 0 Acc : 0.14179999984800815 Test Acc : 0.1877500002272427\n",
      "Current : 1 Acc : 0.21279999986290932 Test Acc : 0.24812500085681677\n",
      "Current : 2 Acc : 0.25740000039339067 Test Acc : 0.27800000067800285\n",
      "Current : 3 Acc : 0.2812000003457069 Test Acc : 0.2796250004321337\n",
      "Current : 4 Acc : 0.2898000007867813 Test Acc : 0.3018750002607703\n",
      "Current : 5 Acc : 0.3252000018954277 Test Acc : 0.3335000041872263\n",
      "Current : 6 Acc : 0.3437999990582466 Test Acc : 0.33750000279396775\n",
      "Current : 7 Acc : 0.3551999992132187 Test Acc : 0.34249999970197675\n",
      "Current : 8 Acc : 0.36520000010728837 Test Acc : 0.3506250001490116\n",
      "Current : 9 Acc : 0.37679999828338623 Test Acc : 0.36162499971687795\n",
      "Current : 10 Acc : 0.3917999976873398 Test Acc : 0.3657500000670552\n",
      "Current : 11 Acc : 0.41079999566078185 Test Acc : 0.3980000000447035\n",
      "Current : 12 Acc : 0.4160000002384186 Test Acc : 0.3855000000447035\n",
      "Current : 13 Acc : 0.4283999979496002 Test Acc : 0.4061249990016222\n",
      "Current : 14 Acc : 0.43419999837875367 Test Acc : 0.396374998986721\n",
      "Current : 15 Acc : 0.4525999981164932 Test Acc : 0.42399999760091306\n",
      "Current : 16 Acc : 0.47740000128746035 Test Acc : 0.4363749984651804\n",
      "Current : 17 Acc : 0.4931999975442886 Test Acc : 0.42999999672174455\n",
      "Current : 18 Acc : 0.12800000011920928 Test Acc : 0.16412500003352762\n",
      "Current : 19 Acc : 0.1623999997228384 Test Acc : 0.1988749999552965\n",
      "Current : 20 Acc : 0.2328000009059906 Test Acc : 0.24224999994039537\n",
      "Current : 21 Acc : 0.26239999920129775 Test Acc : 0.25150000024586916\n",
      "Current : 22 Acc : 0.2902000007033348 Test Acc : 0.29475000146776437\n",
      "Current : 23 Acc : 0.3030000016093254 Test Acc : 0.2863750014454126\n",
      "Current : 24 Acc : 0.3258000022172928 Test Acc : 0.33200000170618293\n",
      "Current : 25 Acc : 0.35439999878406525 Test Acc : 0.32762500178068876\n",
      "Current : 26 Acc : 0.36280000060796735 Test Acc : 0.3473750000819564\n",
      "Current : 27 Acc : 0.37919999957084655 Test Acc : 0.37150000035762787\n",
      "Current : 28 Acc : 0.3955999970436096 Test Acc : 0.3942500002682209\n",
      "Current : 29 Acc : 0.4159999966621399 Test Acc : 0.3913750000298023\n",
      "Current : 30 Acc : 0.42679999887943265 Test Acc : 0.402250000089407\n",
      "Current : 31 Acc : 0.44679999589920044 Test Acc : 0.3972500003874302\n",
      "Current : 32 Acc : 0.44959999799728395 Test Acc : 0.4123749990016222\n",
      "Current : 33 Acc : 0.4703999972343445 Test Acc : 0.42374999932944774\n",
      "Current : 34 Acc : 0.4741999977827072 Test Acc : 0.4423749979585409\n",
      "Current : 35 Acc : 0.46779999673366546 Test Acc : 0.434749998152256\n",
      "Current : 36 Acc : 0.4995999962091446 Test Acc : 0.4326249975711107\n",
      "Current : 37 Acc : 0.13840000003576278 Test Acc : 0.1778749999590218\n",
      "Current : 38 Acc : 0.19719999939203262 Test Acc : 0.1944999996572733\n",
      "Current : 39 Acc : 0.23440000027418137 Test Acc : 0.27987500056624415\n",
      "Current : 40 Acc : 0.2856000018119812 Test Acc : 0.27562500033527615\n",
      "Current : 41 Acc : 0.3000000002980232 Test Acc : 0.3057500025257468\n",
      "Current : 42 Acc : 0.32860000103712084 Test Acc : 0.3437500011175871\n",
      "Current : 43 Acc : 0.3612000024318695 Test Acc : 0.3226250022649765\n",
      "Current : 44 Acc : 0.38439999997615815 Test Acc : 0.36487499959766867\n",
      "Current : 45 Acc : 0.3968000012636185 Test Acc : 0.3742499999701977\n",
      "Current : 46 Acc : 0.41580000162124636 Test Acc : 0.38462499864399435\n",
      "Current : 47 Acc : 0.43499999701976777 Test Acc : 0.3979999989271164\n",
      "Current : 48 Acc : 0.4387999999523163 Test Acc : 0.40762499868869784\n",
      "Current : 49 Acc : 0.46279999673366545 Test Acc : 0.40949999783188107\n",
      "Current : 50 Acc : 0.10299999974668025 Test Acc : 0.1037499999627471\n",
      "Current : 51 Acc : 0.10299999974668025 Test Acc : 0.10599999958649278\n",
      "Current : 52 Acc : 0.11060000035911799 Test Acc : 0.12050000010058284\n",
      "Current : 53 Acc : 0.12919999971985818 Test Acc : 0.13725000009872018\n",
      "Current : 54 Acc : 0.13700000040233135 Test Acc : 0.13987500010989606\n",
      "Current : 55 Acc : 0.13419999986886977 Test Acc : 0.1328750004991889\n",
      "Current : 56 Acc : 0.1373999997973442 Test Acc : 0.1513749998062849\n",
      "Current : 57 Acc : 0.14380000039935112 Test Acc : 0.1487500007264316\n",
      "Current : 58 Acc : 0.15179999977350234 Test Acc : 0.11687499969266354\n",
      "Current : 59 Acc : 0.10019999898970128 Test Acc : 0.1\n",
      "Current : 60 Acc : 0.09999999992549419 Test Acc : 0.1\n",
      "Current : 61 Acc : 0.1 Test Acc : 0.18000 acc : 0.082\n",
      "Current : 62 Acc : 0.12059999998658895 Test Acc : 0.16412500077858566\n",
      "Current : 63 Acc : 0.15680000007152559 Test Acc : 0.17487500058487057\n",
      "Current : 64 Acc : 0.17160000085830687 Test Acc : 0.1773750003427267\n",
      "Current : 65 Acc : 0.18300000011920928 Test Acc : 0.1981249999254942\n",
      "Current : 66 Acc : 0.21599999994039534 Test Acc : 0.23737500067800282\n",
      "Current : 67 Acc : 0.26 Test Acc : 0.2577500009909272\n",
      "Current : 68 Acc : 0.27259999781847 Test Acc : 0.2755000006407499\n",
      "Current : 69 Acc : 0.29580000042915344 Test Acc : 0.2871250003576279\n",
      "Current : 70 Acc : 0.31340000212192537 Test Acc : 0.31850000098347664\n",
      "Current : 71 Acc : 0.3216000017523766 Test Acc : 0.31987500190734863\n",
      "Current : 72 Acc : 0.32920000195503235 Test Acc : 0.3316250009462237\n",
      "Current : 73 Acc : 0.3529999998211861 Test Acc : 0.3404999999329448\n",
      "Current : 74 Acc : 0.3597999995946884 Test Acc : 0.36587499883025887\n",
      "Current : 75 Acc : 0.3837999975681305 Test Acc : 0.3813749998807907\n",
      "Current : 76 Acc : 0.3970000010728836 Test Acc : 0.39112499859184024\n",
      "Current : 77 Acc : 0.40340000033378604 Test Acc : 0.3749999998137355\n",
      "Current : 78 Acc : 0.42560000002384185 Test Acc : 0.4036249991506338\n",
      "Current : 79 Acc : 0.43479999899864197 Test Acc : 0.40549999959766864\n",
      "Current : 80 Acc : 0.4427999973297119 Test Acc : 0.39500000029802323\n",
      "Current : 81 Acc : 0.45219999849796294 Test Acc : 0.419124998524785\n",
      "Current : 82 Acc : 0.46219999611377716 Test Acc : 0.4251249983906746\n",
      "Current : 83 Acc : 0.4743999981880188 Test Acc : 0.41874999888241293\n",
      "Current : 84 Acc : 0.09339999962598085 Test Acc : 0.09949999996460974\n",
      "Current : 85 Acc : 0.1001999993622303 Test Acc : 0.09987499993294477\n",
      "Current : 86 Acc : 0.09999999959021806 Test Acc : 0.09999999990686774\n",
      "Current : 87 Acc : 0.09999999959021806 Test Acc : 0.09999999990686774\n",
      "Current : 88 Acc : 0.10000000018626451 Test Acc : 0.09999999990686774\n",
      "Current : 89 Acc : 0.10719999998807907 Test Acc : 0.1008749998640269\n",
      "Current : 90 Acc : 0.12379999995231629 Test Acc : 0.1681249998509884\n",
      "Current : 91 Acc : 0.17180000007152557 Test Acc : 0.1601250003091991\n",
      "Current : 92 Acc : 0.18900000020861626 Test Acc : 0.2168750001117587\n",
      "Current : 93 Acc : 0.24659999936819077 Test Acc : 0.26312500014901163\n",
      "Current : 94 Acc : 0.2748000019788742 Test Acc : 0.2950000012293458\n",
      "Current : 95 Acc : 0.3062000024318695 Test Acc : 0.3126250009983778\n",
      "Current : 96 Acc : 0.31880000203847886 Test Acc : 0.32837500143796206\n",
      "Current : 97 Acc : 0.34100000143051146 Test Acc : 0.3401250008493662\n",
      "Current : 98 Acc : 0.3554000017046928 Test Acc : 0.3300000008195639\n",
      "Current : 99 Acc : 0.3802000004053116 Test Acc : 0.3665000002831221\n",
      "Current : 100 Acc : 0.37639999866485596 Test Acc : 0.3717500001192093\n",
      "Current : 101 Acc : 0.4011999976634979 Test Acc : 0.39724999815225603\n",
      "Current : 102 Acc : 0.4223999983072281 Test Acc : 0.39012499935925005\n",
      "Current : 103 Acc : 0.42439999580383303 Test Acc : 0.4037499975413084\n",
      "Current : 104 Acc : 0.4369999980926514 Test Acc : 0.4169999957084656\n",
      "Current : 105 Acc : 0.45779999732971194 Test Acc : 0.42212499752640725\n",
      "Current : 106 Acc : 0.4653999948501587 Test Acc : 0.42212499752640725\n",
      "Current : 107 Acc : 0.4701999962329865 Test Acc : 0.42399999797344207\n",
      "Current : 108 Acc : 0.4877999997138977 Test Acc : 0.4292499989271164\n",
      "Current : 109 Acc : 0.10140000030398369 Test Acc : 0.10137500008568168\n",
      "Current : 110 Acc : 0.10079999953508377 Test Acc : 0.12199999997392297\n",
      "Current : 111 Acc : 0.11920000068843364 Test Acc : 0.15687500005587934\n",
      "Current : 112 Acc : 0.20400000005960464 Test Acc : 0.23700000001117588\n",
      "Current : 113 Acc : 0.2750000011920929 Test Acc : 0.2568749997764826\n",
      "Current : 114 Acc : 0.3146000015735626 Test Acc : 0.31950000058859584\n",
      "Current : 115 Acc : 0.3316000008583069 Test Acc : 0.3441249992698431\n",
      "Current : 116 Acc : 0.36780000030994414 Test Acc : 0.3622500002384186\n",
      "Current : 117 Acc : 0.38420000225305556 Test Acc : 0.36400000005960464\n",
      "Current : 118 Acc : 0.39999999940395353 Test Acc : 0.3697500007227063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current : 119 Acc : 0.4139999973773956 Test Acc : 0.37887500002980234\n",
      "Current : 120 Acc : 0.42859999895095824 Test Acc : 0.39812499955296515\n",
      "Current : 121 Acc : 0.44039999961853027 Test Acc : 0.4193749979138374\n",
      "Current : 122 Acc : 0.45879999935626986 Test Acc : 0.4208749957382679\n",
      "Current : 123 Acc : 0.47339999735355376 Test Acc : 0.4294999986886978\n",
      "Current : 124 Acc : 0.4977999991178513 Test Acc : 0.42624999918043616\n",
      "Current : 125 Acc : 0.10979999989271164 Test Acc : 0.09962499989196658\n",
      "Current : 126 Acc : 0.10599999986588955 Test Acc : 0.13549999948590993\n",
      "Current : 127 Acc : 0.14799999989569187 Test Acc : 0.17750000013038517\n",
      "Current : 128 Acc : 0.17740000054240226 Test Acc : 0.21137499902397394\n",
      "Current : 129 Acc : 0.21300000086426735 Test Acc : 0.2248749990016222\n",
      "Current : 130 Acc : 0.2338000002503395 Test Acc : 0.2641250007785857\n",
      "Current : 131 Acc : 0.30280000030994414 Test Acc : 0.32087500151246784\n",
      "Current : 132 Acc : 0.3366000032424927 Test Acc : 0.34900000132620335\n",
      "Current : 133 Acc : 0.35459999918937685 Test Acc : 0.3681249987334013\n",
      "Current : 134 Acc : 0.3805999979376793 Test Acc : 0.38262500185519455\n",
      "Current : 135 Acc : 0.39439999997615816 Test Acc : 0.3993749987334013\n",
      "Current : 136 Acc : 0.4147999978065491 Test Acc : 0.39762500002980233\n",
      "Current : 137 Acc : 0.4353999978303909 Test Acc : 0.425374997779727\n",
      "Current : 138 Acc : 0.4625999957323074 Test Acc : 0.43575000017881393\n",
      "Current : 139 Acc : 0.4713999980688095 Test Acc : 0.44374999664723874\n",
      "Current : 140 Acc : 0.4749999964237213 Test Acc : 0.4408749982714653\n",
      "Current : 141 Acc : 0.4923999959230423 Test Acc : 0.4316249996423721\n",
      "Current : 142 Acc : 0.10639999963343144 Test Acc : 0.10037499978207051\n",
      "Current : 143 Acc : 0.11800000041723252 Test Acc : 0.1620000010356307\n",
      "Current : 144 Acc : 0.14580000020563602 Test Acc : 0.14325000010430813\n",
      "Current : 145 Acc : 0.17420000106096267 Test Acc : 0.1735000006854534\n",
      "Current : 146 Acc : 0.19239999935030938 Test Acc : 0.23825000058859586\n",
      "Current : 147 Acc : 0.23380000084638597 Test Acc : 0.27249999884516\n",
      "Current : 148 Acc : 0.2780000001192093 Test Acc : 0.2767500013113022\n",
      "Current : 149 Acc : 0.2898000028729439 Test Acc : 0.3333750024437904\n",
      "Current : 150 Acc : 0.33500000059604645 Test Acc : 0.34400000162422656\n",
      "Current : 151 Acc : 0.368199999332428 Test Acc : 0.35049999952316285\n",
      "Current : 152 Acc : 0.3740000009536743 Test Acc : 0.3677500009536743\n",
      "Current : 153 Acc : 0.39379999876022337 Test Acc : 0.38362500034272673\n",
      "Current : 154 Acc : 0.40079999923706056 Test Acc : 0.4042499974370003\n",
      "Current : 155 Acc : 0.42799999594688415 Test Acc : 0.3974999986588955\n",
      "Current : 156 Acc : 0.4391999965906143 Test Acc : 0.4086249988526106\n",
      "Current : 157 Acc : 0.44879999697208406 Test Acc : 0.41412500105798244\n",
      "Current : 158 Acc : 0.45659999787807465 Test Acc : 0.4228749979287386\n",
      "Current : 159 Acc : 0.46999999940395354 Test Acc : 0.42337499782443044\n",
      "Current : 160 Acc : 0.4801999986171722 Test Acc : 0.437499999627471\n",
      "Current : 161 Acc : 0.49499999403953554 Test Acc : 0.43487499803304674\n",
      "Current : 162 Acc : 0.10180000014603138 Test Acc : 0.10000000023283065\n",
      "Current : 163 Acc : 0.10000000044703483 Test Acc : 0.10050000012852252\n",
      "Current : 164 Acc : 0.10299999974668025 Test Acc : 0.09925000004004687\n",
      "Current : 165 Acc : 0.1002000005915761 Test Acc : 0.19\n",
      "Current : 166 Acc : 0.09999999977648258 Test Acc : 0.1\n",
      "Current : 167 Acc : 0.09999999985098838 Test Acc : 0.1\n",
      "Current : 168 Acc : 0.1019999997317791 Test Acc : 0.11\n",
      "Current : 169 Acc : 0.10099999949336053 Test Acc : 0.10824999962933361\n",
      "Current : 170 Acc : 0.14919999971985817 Test Acc : 0.1715000013820827\n",
      "Current : 171 Acc : 0.19899999991059303 Test Acc : 0.22612499920651316\n",
      "Current : 172 Acc : 0.2510000014305115 Test Acc : 0.2349999975413084\n",
      "Current : 173 Acc : 0.26799999982118605 Test Acc : 0.2890000019222498\n",
      "Current : 174 Acc : 0.2948000001907349 Test Acc : 0.29362500123679636\n",
      "Current : 175 Acc : 0.3179999986290932 Test Acc : 0.3031250011175871\n",
      "Current : 176 Acc : 0.32720000147819517 Test Acc : 0.3298750001937151\n",
      "Current : 177 Acc : 0.34940000116825104 Test Acc : 0.3561249999329448\n",
      "Current : 178 Acc : 0.36659999907016755 Test Acc : 0.36700000129640103\n",
      "Current : 179 Acc : 0.3855999982357025 Test Acc : 0.35750000141561034\n",
      "Current : 180 Acc : 0.39879999935626986 Test Acc : 0.3917500000447035\n",
      "Current : 181 Acc : 0.4219999986886978 Test Acc : 0.4088749971240759\n",
      "Current : 182 Acc : 0.4395999985933304 Test Acc : 0.4122499983757734\n",
      "Current : 183 Acc : 0.45519999563694 Test Acc : 0.42487499862909317\n",
      "Current : 184 Acc : 0.4605999976396561 Test Acc : 0.4384999983012676\n",
      "Current : 185 Acc : 0.4805999970436096 Test Acc : 0.43949999697506426\n",
      "Current : 186 Acc : 0.5053999978303909 Test Acc : 0.4393749997019768\n",
      "Current : 187 Acc : 0.09659999992698431 Test Acc : 0.1\n",
      "Current : 188 Acc : 0.10560000069439411 Test Acc : 0.14400000004097818\n",
      "Current : 189 Acc : 0.1519999997317791 Test Acc : 0.15637499978765845\n",
      "Current : 190 Acc : 0.17519999980926515 Test Acc : 0.18962500067427754\n",
      "Current : 191 Acc : 0.22180000066757202 Test Acc : 0.23737499825656413\n",
      "Current : 192 Acc : 0.28340000063180926 Test Acc : 0.2943750018253922\n",
      "Current Iter : 193/10000 batch : 2900/8000 acc : 0.273\r"
     ]
    }
   ],
   "source": [
    "# start the training \n",
    "# sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "num_epoch = 10000 ; avg_acc_train = 0; avg_acc_test  = 0; \n",
    "\n",
    "for iter in range(num_epoch):\n",
    "    train_images,train_labels = shuffle(train_images,train_labels);\n",
    "    for current_batch_index in range(0,len(train_images),batch_size):\n",
    "        current_data  = train_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = train_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy,auto_train],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write(' Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(train_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_train = avg_acc_train + sess_results[0]\n",
    "    for current_batch_index in range(0,len(test_images), batch_size):\n",
    "        current_data  = test_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = test_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(test_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_test = avg_acc_test + sess_results[0]        \n",
    "    print(\"Current : \"+ str(iter) + \" Acc : \" + str(avg_acc_train/(len(train_images)/batch_size)) + \" Test Acc : \" + str(avg_acc_test/(len(test_images)/batch_size)) )\n",
    "    if np.abs(avg_acc_train/(len(train_images)/batch_size)-avg_acc_test/(len(test_images)/batch_size))>0.05 and avg_acc_train/(len(train_images)/batch_size)>avg_acc_test/(len(test_images)/batch_size): \n",
    "        sess.run([all_update,reset_optimizer_op]);   \n",
    "    avg_acc_train = 0 ; avg_acc_test  = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reset Optimizer : Optimizer, R. (2018). Reset tensorflow Optimizer. Stack Overflow. Retrieved 12 December 2018, from https://stackoverflow.com/questions/39607566/reset-tensorflow-optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference \n",
    "1. Brownlee, J. (2017). How to One Hot Encode Sequence Data in Python. Machine Learning Mastery. Retrieved 9 December 2018, from https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/\n",
    "2. tf.placeholder_with_default | TensorFlow. (2018). TensorFlow. Retrieved 9 December 2018, from https://www.tensorflow.org/api_docs/python/tf/placeholder_with_default\n",
    "3. tf.nn.softmax_cross_entropy_with_logits | TensorFlow. (2018). TensorFlow. Retrieved 9 December 2018, from https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits\n",
    "4. line, O. (2018). Output without new line. Stack Overflow. Retrieved 9 December 2018, from https://stackoverflow.com/questions/2623470/output-without-new-line\n",
    "5. shell?, H. (2018). How to tell if tensorflow is using gpu acceleration from inside python shell?. Stack Overflow. Retrieved 9 December 2018, from https://stackoverflow.com/questions/38009682/how-to-tell-if-tensorflow-is-using-gpu-acceleration-from-inside-python-shell\n",
    "6. GPU?, H. (2018). How to use TensorFlow GPU?. Stack Overflow. Retrieved 9 December 2018, from https://stackoverflow.com/questions/51306862/how-to-use-tensorflow-gpu\n",
    "7. Batch normalization: theory and how to use it with Tensorflow. (2018). Towards Data Science. Retrieved 9 December 2018, from https://towardsdatascience.com/batch-normalization-theory-and-how-to-use-it-with-tensorflow-1892ca0173ad\n",
    "8. Batch normalization: theory and how to use it with Tensorflow. (2018). Towards Data Science. Retrieved 9 December 2018, from https://towardsdatascience.com/batch-normalization-theory-and-how-to-use-it-with-tensorflow-1892ca0173ad\n",
    "9. tf.reset_default_graph | TensorFlow. (2018). TensorFlow. Retrieved 9 December 2018, from https://www.tensorflow.org/api_docs/python/tf/reset_default_graph\n",
    "10. tf.Session | TensorFlow. (2018). TensorFlow. Retrieved 9 December 2018, from https://www.tensorflow.org/api_docs/python/tf/Session\n",
    "11. tf.nn.moments | TensorFlow. (2018). TensorFlow. Retrieved 9 December 2018, from https://www.tensorflow.org/api_docs/python/tf/nn/moments\n",
    "12. CMD?, H. (2018). How do I run two commands in one line in Windows CMD?. Stack Overflow. Retrieved 9 December 2018, from https://stackoverflow.com/questions/8055371/how-do-i-run-two-commands-in-one-line-in-windows-cmd\n",
    "13. loop, B. (2018). Batch script loop. Stack Overflow. Retrieved 9 December 2018, from https://stackoverflow.com/questions/2591758/batch-script-loop\n",
    "14. tf.train.MomentumOptimizer | TensorFlow. (2018). TensorFlow. Retrieved 9 December 2018, from https://www.tensorflow.org/api_docs/python/tf/train/MomentumOptimizer\n",
    "15. Test if two numpy arrays are (close to) equal, i. (2018). Test if two numpy arrays are (close to) equal, including shape. Stack Overflow. Retrieved 9 December 2018, from https://stackoverflow.com/questions/32874840/test-if-two-numpy-arrays-are-close-to-equal-including-shape\n",
    "16. tf.linalg.diag | TensorFlow. (2018). TensorFlow. Retrieved 9 December 2018, from https://www.tensorflow.org/api_docs/python/tf/linalg/diag\n",
    "17. tf.layers.batch_normalization | TensorFlow. (2018). TensorFlow. Retrieved 10 December 2018, from https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization\n",
    "18. tf.layers.batch_normalization | TensorFlow. (2018). TensorFlow. Retrieved 10 December 2018, from https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization\n",
    "19. error, t. (2018). tf.layers.batch_normalization large test error. Stack Overflow. Retrieved 10 December 2018, from https://stackoverflow.com/questions/43234667/tf-layers-batch-normalization-large-test-error\n",
    "20. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
