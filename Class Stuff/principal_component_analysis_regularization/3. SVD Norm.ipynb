{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-11T20:31:13.834409Z",
     "start_time": "2018-12-11T20:31:10.536400Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# import lib\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys, os,cv2\n",
    "from sklearn.utils import shuffle\n",
    "from scipy.misc import imread,imresize\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from skimage.transform import resize\n",
    "from imgaug import augmenters as iaa\n",
    "import imgaug as ia\n",
    "from scipy.ndimage import zoom\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-11T20:31:21.251700Z",
     "start_time": "2018-12-11T20:31:19.198087Z"
    },
    "code_folding": [
     1,
     28
    ]
   },
   "outputs": [],
   "source": [
    "# read all of the data\n",
    "def read_all_images(path_to_data):\n",
    "    \"\"\"\n",
    "    :param path_to_data: the file containing the binary images from the STL-10 dataset\n",
    "    :return: an array containing all the images\n",
    "    \"\"\"\n",
    "\n",
    "    with open(path_to_data, 'rb') as f:\n",
    "        # read whole file in uint8 chunks\n",
    "        everything = np.fromfile(f, dtype=np.uint8)\n",
    "\n",
    "        # We force the data into 3x96x96 chunks, since the\n",
    "        # images are stored in \"column-major order\", meaning\n",
    "        # that \"the first 96*96 values are the red channel,\n",
    "        # the next 96*96 are green, and the last are blue.\"\n",
    "        # The -1 is since the size of the pictures depends\n",
    "        # on the input file, and this way numpy determines\n",
    "        # the size on its own.\n",
    "\n",
    "        images = np.reshape(everything, (-1, 3, 96, 96))\n",
    "\n",
    "        # Now transpose the images into a standard image format\n",
    "        # readable by, for example, matplotlib.imshow\n",
    "        # You might want to comment this line or reverse the shuffle\n",
    "        # if you will use a learning algorithm like CNN, since they like\n",
    "        # their channels separated.\n",
    "        images = np.transpose(images, (0, 3, 2, 1))\n",
    "        return images\n",
    "def read_labels(path_to_labels):\n",
    "    \"\"\"\n",
    "    :param path_to_labels: path to the binary file containing labels from the STL-10 dataset\n",
    "    :return: an array containing the labels\n",
    "    \"\"\"\n",
    "    with open(path_to_labels, 'rb') as f:\n",
    "        labels = np.fromfile(f, dtype=np.uint8)\n",
    "        return labels\n",
    "    \n",
    "train_images = read_all_images(\"../../DataSet/STL10/stl10_binary/train_X.bin\") / 255.0\n",
    "train_labels = read_labels    (\"../../DataSet/STL10/stl10_binary/train_Y.bin\")\n",
    "test_images  = read_all_images(\"../../DataSet/STL10/stl10_binary/test_X.bin\")  / 255.0\n",
    "test_labels  = read_labels    (\"../../DataSet/STL10/stl10_binary/test_y.bin\")\n",
    "\n",
    "label_encoder= OneHotEncoder(sparse=False,categories='auto')\n",
    "train_labels = label_encoder.fit_transform(train_labels.reshape((-1,1)))\n",
    "test_labels  = label_encoder.fit_transform(test_labels.reshape((-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-11T20:31:29.419455Z",
     "start_time": "2018-12-11T20:31:25.705387Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1.] [0. 0. 0.] [0.44671062 0.43980984 0.40664645] [0.26034098 0.25657727 0.27126738]\n",
      "(5000, 96, 96, 3)\n",
      "(5000, 10)\n",
      "[1. 1. 1.] [0. 0. 0.] [0.44723063 0.43964247 0.40495725] [0.2605645  0.25666146 0.26997382]\n",
      "(8000, 96, 96, 3)\n",
      "(8000, 10)\n"
     ]
    }
   ],
   "source": [
    "# some basic statistic of train and test image // hyper\n",
    "print(train_images.max((0,1,2)),train_images.min((0,1,2)),train_images.mean((0,1,2)),train_images.std((0,1,2)) )\n",
    "print(train_images.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_images.max((0,1,2)),test_images.min((0,1,2)),test_images.mean((0,1,2)),test_images.std((0,1,2)) )\n",
    "print(test_images.shape)\n",
    "print(test_labels.shape)\n",
    "num_epoch = 50 ; learning_rate = 0.0008; batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-12T04:43:48.746608Z",
     "start_time": "2018-12-12T04:43:48.709379Z"
    },
    "code_folding": [
     25
    ]
   },
   "outputs": [],
   "source": [
    "# import layers\n",
    "def tf_softmax(x): return tf.nn.softmax(x)\n",
    "def tf_elu(x):     return tf.nn.elu(x)\n",
    "def d_tf_elu(x):   return tf.cast(tf.greater(x,0),tf.float32)  + (tf_elu(tf.cast(tf.less_equal(x,0),tf.float32) * x) + 1.0)\n",
    "def tf_relu(x):    return tf.nn.relu(x)\n",
    "def d_tf_relu(x):  return tf.cast(tf.greater(x,0),tf.float32)\n",
    "def tf_iden(x): return x\n",
    "\n",
    "class CNN():\n",
    "\n",
    "    def __init__(self,k,inc,out, stddev=0.05,which_reg=0,act=tf_elu,d_act=d_tf_elu):\n",
    "        self.w          = tf.Variable(tf.random_normal([k,k,inc,out],stddev=stddev,seed=2,dtype=tf.float32))\n",
    "        self.moving_w   = tf.Variable(tf.random_normal([k,k,inc,out],stddev=stddev,seed=2,dtype=tf.float32))\n",
    "        self.b          = tf.Variable(tf.zeros(out,dtype=tf.float32))\n",
    "        self.m,self.v   = tf.Variable(tf.zeros_like(self.w)),tf.Variable(tf.zeros_like(self.w))\n",
    "        self.mb,self.vb = tf.Variable(tf.zeros_like(self.b)),tf.Variable(tf.zeros_like(self.b))\n",
    "        self.act,self.d_act = act,d_act\n",
    "        self.which_reg  = which_reg\n",
    "        \n",
    "    def getw(self): return [self.w,self.b]\n",
    "    def feedforward(self,input,stride=1,padding='VALID'):\n",
    "        self.input  = input\n",
    "        self.layer  = tf.nn.conv2d(input,self.w,strides=[1,stride,stride,1],padding=padding)\n",
    "        self.layerA = self.act(self.layer)\n",
    "        return self.layerA\n",
    "\n",
    "    def backprop(self,gradient,stride=1,padding='VALID'):\n",
    "        grad_part_1 = gradient\n",
    "        grad_part_2 = self.d_act(self.layer)\n",
    "        grad_part_3 = self.input\n",
    "\n",
    "        grad_middle = grad_part_1 * grad_part_2\n",
    "        grad_b      = tf.reduce_mean(grad_middle,(0,1,2))/batch_size\n",
    "        grad        = tf.nn.conv2d_backprop_filter(input = grad_part_3,filter_sizes = tf.shape(self.w),  out_backprop = grad_middle,strides=[1,stride,stride,1],padding=padding) / batch_size\n",
    "        grad_pass   = tf.nn.conv2d_backprop_input (input_sizes = tf.shape(self.input),filter= self.w,out_backprop = grad_middle,strides=[1,stride,stride,1],padding=padding)\n",
    "\n",
    "        if self.which_reg == 0:   grad = grad\n",
    "        if self.which_reg == 0.5: grad = grad + lamda * (tf.sqrt(tf.abs(self.w))) * (1.0/tf.sqrt(tf.abs(self.w)+ 10e-5)) * tf.sign(self.w)\n",
    "        if self.which_reg == 1:   grad = grad + lamda * tf.sign(self.w)\n",
    "        if self.which_reg == 1.5: grad = grad + lamda * 1.0/(tf.sqrt(tf.square(self.w) + 10e-5)) * self.w\n",
    "        if self.which_reg == 2:   grad = grad + lamda * (1.0/tf.sqrt(tf.square(tf.abs(self.w))+ 10e-5)) * tf.abs(self.w) * tf.sign(self.w)\n",
    "        if self.which_reg == 2.5: grad = grad + lamda * 2.0 * self.w\n",
    "        if self.which_reg == 3:   grad = grad + lamda * tf.pow(tf.pow(tf.abs(self.w),3)+ 10e-5,-0.66) * tf.pow(tf.abs(self.w),2) * tf.sign(self.w)\n",
    "        if self.which_reg == 4:   grad = grad + lamda * tf.pow(tf.pow(tf.abs(self.w),4)+ 10e-5,-0.75) * tf.pow(tf.abs(self.w),3) * tf.sign(self.w)\n",
    "\n",
    "        update_w = []\n",
    "        \n",
    "        update_w.append(tf.assign( self.m,self.m*beta1 + (1-beta1) * (grad)   ))\n",
    "        update_w.append(tf.assign( self.v,self.v*beta2 + (1-beta2) * (grad ** 2)   ))\n",
    "        m_hat = self.m / (1-beta1) ; v_hat = self.v / (1-beta2)\n",
    "        adam_middle = m_hat * learning_rate/(tf.sqrt(v_hat) + adam_e)\n",
    "        update_w.append(tf.assign(self.w,tf.subtract(self.w,adam_middle  )))\n",
    "        \n",
    "        update_w.append(tf.assign( self.mb,self.mb*beta1 + (1-beta1) * (grad_b)   ))\n",
    "        update_w.append(tf.assign( self.vb,self.vb*beta2 + (1-beta2) * (grad_b ** 2)   ))\n",
    "        m_hatb = self.mb / (1-beta1) ; v_hatb = self.vb / (1-beta2)\n",
    "        adam_middleb = m_hatb * learning_rate/(tf.sqrt(v_hatb) + adam_e)\n",
    "        update_w.append(tf.assign(self.b,tf.subtract(self.b,adam_middleb  )))\n",
    "        \n",
    "        return grad_pass,update_w\n",
    "    \n",
    "    def updatew(self):\n",
    "        w,h,inc,outc = self.w.shape\n",
    "        \n",
    "        tempw = tf.reshape(self.w,(w*h,inc,outc))\n",
    "        tempw = tf.transpose(tempw,(1,2,0))\n",
    "        s,U,V = tf.svd(tempw)\n",
    "        print(s)\n",
    "        print(tf.reduce_min(s,1,True))\n",
    "        scaleds = (s-tf.reduce_min(s,1,True))/(tf.reduce_max(s,1,True)-tf.reduce_min(s,1,True))\n",
    "        _,n   = s.shape\n",
    "        n     = n//2\n",
    "        print(n)\n",
    "        neww  = U[:,:,:n] @ tf.matrix_diag(scaleds)[:,:n,:] \n",
    "        neww  = tf.transpose(neww,(2,0,1))\n",
    "        neww  = tf.reshape(neww,(w,h,inc,outc))\n",
    "        \n",
    "        update = []\n",
    "        update.append(tf.assign(self.moving_w,self.moving_w * 0.9 + 0.1 * neww))\n",
    "        update.append(tf.assign(self.w,self.moving_w))\n",
    "        return update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-12T04:43:49.158304Z",
     "start_time": "2018-12-12T04:43:49.143309Z"
    },
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# create the svd layer\n",
    "class svd_layer():\n",
    "    \n",
    "    def __init__(self,batch,size,width):\n",
    "        self.n = size\n",
    "        self.moving_s = tf.Variable(tf.zeros((batch_size,size),dtype=tf.float32))\n",
    "        self.moving_u = tf.Variable(tf.zeros((batch_size,width**2,size),dtype=tf.float32))\n",
    "        self.moving_v = tf.Variable(tf.zeros((batch_size,size,size),dtype=tf.float32))\n",
    "    \n",
    "    def feedforward(self,data,training_phase):\n",
    "        \n",
    "        with tf.device('/cpu:0'):\n",
    "            s,U,V = tf.svd(data)\n",
    "        print(s)\n",
    "        print(U)\n",
    "        print(V)\n",
    "        smin = tf.reduce_min(s,1,keepdims=True)\n",
    "        smax = tf.reduce_max(s,1,keepdims=True)\n",
    "        scaleds = (s - smin)/(smax-smin + 1e-8)\n",
    "        def training_fn():\n",
    "            data      = U[:,:,:] @ tf.matrix_diag(s)[:,:,:] @ tf.transpose(V,(0,2,1))[:,:,:]\n",
    "            data = data  * tf.reduce_mean(tf.transpose(tf.abs(V),(0,2,1)) * scaleds[:,:,None] ,(1),keepdims=True)\n",
    "            # data      = data * tf.reduce_mean(tf.transpose(tf.abs(V),(0,2,1)) * scaleds[:,:,None] ,(1),keepdims=True)\n",
    "            update = []\n",
    "            update.append(tf.assign(self.moving_u,self.moving_u*0.9 + 0.1 * U))\n",
    "            update.append(tf.assign(self.moving_v,self.moving_v*0.9 + 0.1 * V))\n",
    "            return data,update\n",
    "            \n",
    "        def testing_fn():\n",
    "            data      = U[:,:,:] @ tf.matrix_diag(s)[:,:,:] @ tf.transpose(V,(0,2,1))[:,:,:]\n",
    "            # data      = data * tf.reduce_mean(tf.transpose(tf.abs(V),(0,2,1)) * scaleds[:,:,None] ,(1),keepdims=True)\n",
    "            update = []\n",
    "            update.append(tf.assign(self.moving_u,self.moving_u))\n",
    "            update.append(tf.assign(self.moving_v,self.moving_v))\n",
    "            return data,update\n",
    "        \n",
    "        data,update  = tf.cond(training_phase,true_fn=training_fn,false_fn=testing_fn)\n",
    "        return data,update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-12T04:43:50.137782Z",
     "start_time": "2018-12-12T04:43:49.724867Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# restart the graph \n",
    "# sess.close()\n",
    "# tf.reset_default_graph()\n",
    "learning_rate = 0.0008; batch_size = 100\n",
    "\n",
    "l1 = CNN(3,3, 16)\n",
    "l2 = CNN(3,16,16)\n",
    "l3 = CNN(3,16,16)\n",
    "l3_svd = svd_layer(100,16,24)\n",
    "\n",
    "l4 = CNN(3,16,32)\n",
    "l5 = CNN(3,32,32)\n",
    "l6 = CNN(3,32,32)\n",
    "l6_svd = svd_layer(100,32,6)\n",
    "\n",
    "l7 = CNN(3,32,64)\n",
    "l8 = CNN(3,64,64)\n",
    "l9 = CNN(3,64,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-12T04:43:51.981477Z",
     "start_time": "2018-12-12T04:43:50.468373Z"
    },
    "code_folding": [
     1
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Svd_383:0\", shape=(3, 9), dtype=float32)\n",
      "Tensor(\"Min_391:0\", shape=(3, 1), dtype=float32)\n",
      "4\n",
      "Tensor(\"Svd_384:0\", shape=(16, 9), dtype=float32)\n",
      "Tensor(\"Min_394:0\", shape=(16, 1), dtype=float32)\n",
      "4\n",
      "Tensor(\"Svd_385:0\", shape=(16, 9), dtype=float32)\n",
      "Tensor(\"Min_397:0\", shape=(16, 1), dtype=float32)\n",
      "4\n",
      "Tensor(\"Svd_386:0\", shape=(16, 9), dtype=float32)\n",
      "Tensor(\"Min_400:0\", shape=(16, 1), dtype=float32)\n",
      "4\n",
      "Tensor(\"Svd_387:0\", shape=(32, 9), dtype=float32)\n",
      "Tensor(\"Min_403:0\", shape=(32, 1), dtype=float32)\n",
      "4\n",
      "Tensor(\"Svd_388:0\", shape=(32, 9), dtype=float32)\n",
      "Tensor(\"Min_406:0\", shape=(32, 1), dtype=float32)\n",
      "4\n",
      "Tensor(\"Svd_389:0\", shape=(32, 9), dtype=float32)\n",
      "Tensor(\"Min_409:0\", shape=(32, 1), dtype=float32)\n",
      "4\n",
      "Tensor(\"Svd_390:0\", shape=(64, 9), dtype=float32)\n",
      "Tensor(\"Min_412:0\", shape=(64, 1), dtype=float32)\n",
      "4\n",
      "Tensor(\"Svd_391:0\", shape=(64, 9), dtype=float32)\n",
      "Tensor(\"Min_415:0\", shape=(64, 1), dtype=float32)\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# build graph \n",
    "x = tf.placeholder(shape=(batch_size,96,96,3),dtype=tf.float32)\n",
    "y = tf.placeholder(shape=(batch_size,10),dtype=tf.float32)\n",
    "is_train = tf.placeholder_with_default(False,())\n",
    "\n",
    "layer1 = l1.feedforward(x       ,padding='SAME',stride=2)\n",
    "layer2 = l2.feedforward(layer1  ,padding='SAME',stride=2) \n",
    "layer3 = l3.feedforward(layer2  ,padding='SAME',stride=1)\n",
    "\n",
    "layer4 = l4.feedforward(layer3  ,padding='SAME',stride=2) \n",
    "layer5 = l5.feedforward(layer4  ,padding='SAME',stride=2)\n",
    "layer6 = l6.feedforward(layer5  ,padding='SAME',stride=1)\n",
    "\n",
    "layer7 = l7.feedforward(layer6  ,padding='SAME',stride=1)\n",
    "layer8 = l8.feedforward(layer7  ,padding='SAME',stride=1) \n",
    "layer9 = l9.feedforward(layer8  ,padding='SAME',stride=1)\n",
    "\n",
    "all_update = l1.updatew() + l2.updatew() + l3.updatew() + \\\n",
    "             l4.updatew() + l5.updatew() + l6.updatew() + \\\n",
    "             l7.updatew() + l8.updatew() + l9.updatew() \n",
    "\n",
    "final_layer = tf.reduce_mean(layer9,axis=(1,2))\n",
    "cost        = tf.nn.softmax_cross_entropy_with_logits_v2(logits=final_layer,labels=y)\n",
    "auto_train  = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "final_softmax      = tf_softmax(final_layer)\n",
    "correct_prediction = tf.equal(tf.argmax(final_softmax, 1), tf.argmax(y, 1))\n",
    "accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-12T04:49:23.151Z"
    },
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current : 0 Acc : 0.5501999992132187 Test Acc : 0.4532499972730875\n",
      "\n",
      "Current : 1 Acc : 0.5513999980688095 Test Acc : 0.46212499812245367\n",
      "\n",
      "Current : 2 Acc : 0.5718000024557114 Test Acc : 0.4528749980032444\n",
      "\n",
      "Current : 3 Acc : 0.5822000032663346 Test Acc : 0.46037499867379666\n",
      "\n",
      "Current : 4 Acc : 0.5914000022411346 Test Acc : 0.46399999782443047\n",
      "\n",
      "Current : 5 Acc : 0.6077999985218048 Test Acc : 0.46312499791383743\n",
      "\n",
      "Current : 6 Acc : 0.6226000010967254 Test Acc : 0.4641249947249889\n",
      "\n",
      "Current : 7 Acc : 0.6413999998569488 Test Acc : 0.46674999594688416\n",
      "\n",
      "Current : 8 Acc : 0.6484000027179718 Test Acc : 0.4563749972730875\n",
      "\n",
      "Current : 9 Acc : 0.6692000043392181 Test Acc : 0.44937499761581423\n",
      "\n",
      "Current : 10 Acc : 0.6896000027656555 Test Acc : 0.47649999558925626\n",
      "\n",
      "Current : 11 Acc : 0.7181999969482422 Test Acc : 0.46312499307096006\n",
      "\n",
      "Current : 12 Acc : 0.7177999997138977 Test Acc : 0.46712499521672723\n",
      "\n",
      "Current : 13 Acc : 0.7619999992847443 Test Acc : 0.4692499987781048\n",
      "\n",
      "Current : 14 Acc : 0.7561999988555909 Test Acc : 0.4489999979734421\n",
      "\n",
      "Current : 15 Acc : 0.7823999989032745 Test Acc : 0.4556249979883432\n",
      "\n",
      "Current : 16 Acc : 0.7889999973773957 Test Acc : 0.45662499889731406\n",
      "\n",
      "Current : 17 Acc : 0.8338000011444092 Test Acc : 0.4471249967813492\n",
      "\n",
      "Current : 18 Acc : 0.8539999985694885 Test Acc : 0.4504999976605177\n",
      "\n",
      "Current : 19 Acc : 0.8556000018119811 Test Acc : 0.4333749979734421\n",
      "\n",
      "Current : 20 Acc : 0.8228000020980835 Test Acc : 0.4381249975413084\n",
      "\n",
      "Current : 21 Acc : 0.8831999969482421 Test Acc : 0.45874999538064004\n",
      "\n",
      "Current Iter : 22/30000 batch : 7900/8000 acc : 0.4183\n",
      "-----------reset\n",
      "Current : 22 Acc : 0.9266000020503998 Test Acc : 0.45249999649822714\n",
      "\n",
      "Current : 23 Acc : 0.08680000085383653 Test Acc : 0.09925000004004687\n",
      "\n",
      "Current : 24 Acc : 0.10260000005364418 Test Acc : 0.10049999996554107\n",
      "\n",
      "Current : 25 Acc : 0.09999999992549419 Test Acc : 0.10000000020954758\n",
      "\n",
      "Current : 26 Acc : 0.10000000018626451 Test Acc : 0.1000000003958121\n",
      "\n",
      "Current : 27 Acc : 0.10019999973475933 Test Acc : 0.10025000013411045\n",
      "\n",
      "Current : 28 Acc : 0.14019999966025354 Test Acc : 0.16625000033527612\n",
      "\n",
      "Current : 29 Acc : 0.16180000066757202 Test Acc : 0.1702500008046627\n",
      "\n",
      "Current : 30 Acc : 0.1792000015079975 Test Acc : 0.18462499957531692\n",
      "\n",
      "Current : 31 Acc : 0.1818000002205372 Test Acc : 0.21050000116229056\n",
      "\n",
      "Current : 32 Acc : 0.20619999885559082 Test Acc : 0.20237500043585896\n",
      "\n",
      "Current : 33 Acc : 0.23679999902844429 Test Acc : 0.2372499999590218\n",
      "\n",
      "Current : 34 Acc : 0.2506000006198883 Test Acc : 0.25437499955296516\n",
      "\n",
      "Current : 35 Acc : 0.2713999992609024 Test Acc : 0.2781250011175871\n",
      "\n",
      "Current : 36 Acc : 0.2944000020623207 Test Acc : 0.3070000007748604\n",
      "\n",
      "Current : 37 Acc : 0.31880000174045564 Test Acc : 0.3246250020340085\n",
      "\n",
      "Current : 38 Acc : 0.3278000012040138 Test Acc : 0.34950000159442424\n",
      "\n",
      "Current : 39 Acc : 0.34979999929666517 Test Acc : 0.35599999912083147\n",
      "\n",
      "Current : 40 Acc : 0.36679999947547914 Test Acc : 0.35200000163167716\n",
      "\n",
      "Current : 41 Acc : 0.3720000007748604 Test Acc : 0.3704999998211861\n",
      "\n",
      "Current : 42 Acc : 0.3893999981880188 Test Acc : 0.3773750003427267\n",
      "\n",
      "Current : 43 Acc : 0.4019999992847443 Test Acc : 0.37174999974668027\n",
      "\n",
      "Current : 44 Acc : 0.4011999988555908 Test Acc : 0.3957499984651804\n",
      "\n",
      "Current : 45 Acc : 0.413999999165535 Test Acc : 0.39574999883770945\n",
      "\n",
      "Current : 46 Acc : 0.43179999709129335 Test Acc : 0.3894999992102385\n",
      "\n",
      "Current : 47 Acc : 0.4440000009536743 Test Acc : 0.40899999663233755\n",
      "\n",
      "Current : 48 Acc : 0.44619999766349794 Test Acc : 0.416499999538064\n",
      "\n",
      "Current : 49 Acc : 0.4573999971151352 Test Acc : 0.416874996945262\n",
      "\n",
      "Current : 50 Acc : 0.46819999933242795 Test Acc : 0.42450000047683717\n",
      "\n",
      "Current : 51 Acc : 0.480599998831749 Test Acc : 0.40737499892711637\n",
      "\n",
      "Current : 52 Acc : 0.4795999962091446 Test Acc : 0.44174999594688413\n",
      "\n",
      "Current : 53 Acc : 0.5125999975204468 Test Acc : 0.44237499721348283\n",
      "\n",
      "Current : 54 Acc : 0.5201999962329864 Test Acc : 0.4327499955892563\n",
      "\n",
      "Current : 55 Acc : 0.5261999952793122 Test Acc : 0.45024999752640726\n",
      "\n",
      "Current : 56 Acc : 0.541999996304512 Test Acc : 0.43174999840557576\n",
      "\n",
      "Current : 57 Acc : 0.5460000002384185 Test Acc : 0.4497499980032444\n",
      "\n",
      "Current : 58 Acc : 0.5700000005960465 Test Acc : 0.44449999667704104\n",
      "\n",
      "Current : 59 Acc : 0.5801999986171722 Test Acc : 0.43774999789893626\n",
      "\n",
      "Current : 60 Acc : 0.596000000834465 Test Acc : 0.44649999737739565\n",
      "\n",
      "Current : 61 Acc : 0.622999997138977 Test Acc : 0.45124999806284904\n",
      "\n",
      "Current : 62 Acc : 0.6173999983072281 Test Acc : 0.44774999693036077\n",
      "\n",
      "Current : 63 Acc : 0.6512000036239624 Test Acc : 0.45462499633431436\n",
      "\n",
      "Current : 64 Acc : 0.6676000070571899 Test Acc : 0.4368749979883432\n",
      "\n",
      "Current : 65 Acc : 0.6791999995708465 Test Acc : 0.45362499691545966\n",
      "\n",
      "Current : 66 Acc : 0.7060000050067902 Test Acc : 0.4533749960362911\n",
      "\n",
      "Current : 67 Acc : 0.7314000022411347 Test Acc : 0.43749999850988386\n",
      "\n",
      "Current : 68 Acc : 0.7144000017642975 Test Acc : 0.441874998062849\n",
      "\n",
      "Current : 69 Acc : 0.7576000034809113 Test Acc : 0.4388749983161688\n",
      "\n",
      "Current : 70 Acc : 0.7839999961853027 Test Acc : 0.43862499892711637\n",
      "\n",
      "Current : 71 Acc : 0.8043999934196472 Test Acc : 0.4447499979287386\n",
      "\n",
      "Current : 72 Acc : 0.8355999958515167 Test Acc : 0.44337499774992467\n",
      "\n",
      "Current : 73 Acc : 0.8394000029563904 Test Acc : 0.43924999721348285\n",
      "\n",
      "Current : 74 Acc : 0.8692000019550323 Test Acc : 0.4386249970644712\n",
      "\n",
      "Current : 75 Acc : 0.870200002193451 Test Acc : 0.4162499997764826\n",
      "\n",
      "Current : 76 Acc : 0.8904000043869018 Test Acc : 0.43424999713897705\n",
      "\n",
      "Current Iter : 77/30000 batch : 7900/8000 acc : 0.4286\n",
      "-----------reset\n",
      "Current : 77 Acc : 0.9330000007152557 Test Acc : 0.4323749989271164\n",
      "\n",
      "Current : 78 Acc : 0.09960000030696392 Test Acc : 0.10199999941978603\n",
      "\n",
      "Current : 79 Acc : 0.09999999891966581 Test Acc : 0.10962499983143062\n",
      "\n",
      "Current : 80 Acc : 0.11200000055134296 Test Acc : 0.10137499975971878\n",
      "\n",
      "Current : 81 Acc : 0.09939999997615814 Test Acc : 0.10012499978765846\n",
      "\n",
      "Current : 82 Acc : 0.1 Test Acc : 0.100000000069849197\n",
      "\n",
      "Current : 83 Acc : 0.11540000058710576 Test Acc : 0.1661250007804483\n",
      "\n",
      "Current : 84 Acc : 0.15979999974370002 Test Acc : 0.16600000048056246\n",
      "\n",
      "Current : 85 Acc : 0.164200000166893 Test Acc : 0.17137500047683715\n",
      "\n",
      "Current : 86 Acc : 0.17879999950528144 Test Acc : 0.20174999963492155\n",
      "\n",
      "Current : 87 Acc : 0.18879999935626984 Test Acc : 0.20549999978393316\n",
      "\n",
      "Current : 88 Acc : 0.21839999958872794 Test Acc : 0.22662499947473408\n",
      "\n",
      "Current : 89 Acc : 0.22560000091791152 Test Acc : 0.23187499977648257\n",
      "\n",
      "Current : 90 Acc : 0.24480000078678132 Test Acc : 0.26475000008940697\n",
      "\n",
      "Current : 91 Acc : 0.25980000078678134 Test Acc : 0.2721249995753169\n",
      "\n",
      "Current : 92 Acc : 0.2810000017285347 Test Acc : 0.2753750000149012\n",
      "\n",
      "Current : 93 Acc : 0.30199999988079074 Test Acc : 0.31962500102818014\n",
      "\n",
      "Current : 94 Acc : 0.328200002014637 Test Acc : 0.33300000075250863\n",
      "\n",
      "Current : 95 Acc : 0.34519999980926513 Test Acc : 0.32912500090897084\n",
      "\n",
      "Current : 96 Acc : 0.34880000352859497 Test Acc : 0.3520000012591481\n",
      "\n",
      "Current : 97 Acc : 0.3608000001311302 Test Acc : 0.3590000020340085\n",
      "\n",
      "Current : 98 Acc : 0.3743999993801117 Test Acc : 0.36299999989569187\n",
      "\n",
      "Current : 99 Acc : 0.3883999988436699 Test Acc : 0.3542500015348196\n",
      "\n",
      "Current : 100 Acc : 0.40539999842643737 Test Acc : 0.3926249995827675\n",
      "\n",
      "Current : 101 Acc : 0.40739999890327455 Test Acc : 0.3754999987781048\n",
      "\n",
      "Current : 102 Acc : 0.42539999783039095 Test Acc : 0.4022499989718199\n",
      "\n",
      "Current : 103 Acc : 0.4283999979496002 Test Acc : 0.3992499988526106\n",
      "\n",
      "Current : 104 Acc : 0.44139999747276304 Test Acc : 0.40012500025331976\n",
      "\n",
      "Current : 105 Acc : 0.4557999974489212 Test Acc : 0.39599999859929086\n",
      "\n",
      "Current : 106 Acc : 0.47719999790191653 Test Acc : 0.42362499944865706\n",
      "\n",
      "Current : 107 Acc : 0.4907999986410141 Test Acc : 0.4354999966919422\n",
      "\n",
      "Current : 108 Acc : 0.4995999962091446 Test Acc : 0.4043750002980232\n",
      "\n",
      "Current : 109 Acc : 0.5033999925851822 Test Acc : 0.4251249980181456\n",
      "\n",
      "0 Current Iter : 110/30000 batch : 3000/5000 acc : 0.52\r"
     ]
    }
   ],
   "source": [
    "# start the training \n",
    "# sess = tf.InteractiveSession()\n",
    "# sess.run(tf.global_variables_initializer())\n",
    "num_epoch = 30000 ; avg_acc_train = 0; avg_acc_test  = 0; train_more = 1\n",
    "\n",
    "for iter in range(num_epoch):\n",
    "    \n",
    "    test_images,test_labels   = shuffle(test_images,test_labels);\n",
    "\n",
    "    for i in range(train_more):\n",
    "        train_images,train_labels = shuffle(train_images,train_labels);\n",
    "        for current_batch_index in range(0,len(train_images),batch_size):\n",
    "            current_data  = train_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "            current_label = train_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "            sess_results  = sess.run([accuracy,auto_train],feed_dict={x:current_data,y:current_label})\n",
    "            sys.stdout.write(str(i) + ' Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(train_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "            sys.stdout.flush(); avg_acc_train = avg_acc_train + sess_results[0]\n",
    "        \n",
    "    for current_batch_index in range(0,len(test_images), batch_size):\n",
    "        current_data  = test_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = test_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(test_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_test = avg_acc_test + sess_results[0]        \n",
    "        \n",
    "\n",
    "    if avg_acc_train/(len(train_images)/batch_size)/train_more > 0.9 and avg_acc_test/(len(test_images)/batch_size) < 0.5 :  \n",
    "        print('\\n-----------reset')\n",
    "        sess.run(all_update)\n",
    "    if avg_acc_test/(len(test_images)/batch_size) > 0.5:\n",
    "        print('-----------------------------------------')\n",
    "        print(\"Current : \"+ str(iter) + \" Acc : \" + str(avg_acc_train/(len(train_images)/batch_size)/train_more) + \" Test Acc : \" + str(avg_acc_test/(len(test_images)/batch_size)) + '\\n')\n",
    "        print(\"Current : \"+ str(iter) + \" Acc : \" + str(avg_acc_train/(len(train_images)/batch_size)/train_more) + \" Test Acc : \" + str(avg_acc_test/(len(test_images)/batch_size)) + '\\n')\n",
    "        print(\"Current : \"+ str(iter) + \" Acc : \" + str(avg_acc_train/(len(train_images)/batch_size)/train_more) + \" Test Acc : \" + str(avg_acc_test/(len(test_images)/batch_size)) + '\\n')\n",
    "        print(\"Current : \"+ str(iter) + \" Acc : \" + str(avg_acc_train/(len(train_images)/batch_size)/train_more) + \" Test Acc : \" + str(avg_acc_test/(len(test_images)/batch_size)) + '\\n')\n",
    "        print(\"Current : \"+ str(iter) + \" Acc : \" + str(avg_acc_train/(len(train_images)/batch_size)/train_more) + \" Test Acc : \" + str(avg_acc_test/(len(test_images)/batch_size)) + '\\n')\n",
    "        print(\"Current : \"+ str(iter) + \" Acc : \" + str(avg_acc_train/(len(train_images)/batch_size)/train_more) + \" Test Acc : \" + str(avg_acc_test/(len(test_images)/batch_size)) + '\\n')\n",
    "        print(\"Current : \"+ str(iter) + \" Acc : \" + str(avg_acc_train/(len(train_images)/batch_size)/train_more) + \" Test Acc : \" + str(avg_acc_test/(len(test_images)/batch_size)) + '\\n')\n",
    "        print(\"Current : \"+ str(iter) + \" Acc : \" + str(avg_acc_train/(len(train_images)/batch_size)/train_more) + \" Test Acc : \" + str(avg_acc_test/(len(test_images)/batch_size)) + '\\n')\n",
    "        print(\"Current : \"+ str(iter) + \" Acc : \" + str(avg_acc_train/(len(train_images)/batch_size)/train_more) + \" Test Acc : \" + str(avg_acc_test/(len(test_images)/batch_size)) + '\\n')\n",
    "        print(\"Current : \"+ str(iter) + \" Acc : \" + str(avg_acc_train/(len(train_images)/batch_size)/train_more) + \" Test Acc : \" + str(avg_acc_test/(len(test_images)/batch_size)) + '\\n')\n",
    "        print(\"Current : \"+ str(iter) + \" Acc : \" + str(avg_acc_train/(len(train_images)/batch_size)/train_more) + \" Test Acc : \" + str(avg_acc_test/(len(test_images)/batch_size)) + '\\n')\n",
    "        print(\"Current : \"+ str(iter) + \" Acc : \" + str(avg_acc_train/(len(train_images)/batch_size)/train_more) + \" Test Acc : \" + str(avg_acc_test/(len(test_images)/batch_size)) + '\\n')\n",
    "        print('-----------------------------------------')\n",
    "\n",
    "    print(\"Current : \"+ str(iter) + \" Acc : \" + str(avg_acc_train/(len(train_images)/batch_size)/train_more) + \" Test Acc : \" + str(avg_acc_test/(len(test_images)/batch_size)) + '\\n')\n",
    "    avg_acc_train = 0 ; avg_acc_test  = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T21:03:17.629751Z",
     "start_time": "2018-12-10T21:03:16.795967Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference \n",
    "1. Brownlee, J. (2017). How to One Hot Encode Sequence Data in Python. Machine Learning Mastery. Retrieved 9 December 2018, from https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/\n",
    "2. tf.placeholder_with_default | TensorFlow. (2018). TensorFlow. Retrieved 9 December 2018, from https://www.tensorflow.org/api_docs/python/tf/placeholder_with_default\n",
    "3. tf.nn.softmax_cross_entropy_with_logits | TensorFlow. (2018). TensorFlow. Retrieved 9 December 2018, from https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits\n",
    "4. line, O. (2018). Output without new line. Stack Overflow. Retrieved 9 December 2018, from https://stackoverflow.com/questions/2623470/output-without-new-line\n",
    "5. shell?, H. (2018). How to tell if tensorflow is using gpu acceleration from inside python shell?. Stack Overflow. Retrieved 9 December 2018, from https://stackoverflow.com/questions/38009682/how-to-tell-if-tensorflow-is-using-gpu-acceleration-from-inside-python-shell\n",
    "6. GPU?, H. (2018). How to use TensorFlow GPU?. Stack Overflow. Retrieved 9 December 2018, from https://stackoverflow.com/questions/51306862/how-to-use-tensorflow-gpu\n",
    "7. Batch normalization: theory and how to use it with Tensorflow. (2018). Towards Data Science. Retrieved 9 December 2018, from https://towardsdatascience.com/batch-normalization-theory-and-how-to-use-it-with-tensorflow-1892ca0173ad\n",
    "8. Batch normalization: theory and how to use it with Tensorflow. (2018). Towards Data Science. Retrieved 9 December 2018, from https://towardsdatascience.com/batch-normalization-theory-and-how-to-use-it-with-tensorflow-1892ca0173ad\n",
    "9. tf.reset_default_graph | TensorFlow. (2018). TensorFlow. Retrieved 9 December 2018, from https://www.tensorflow.org/api_docs/python/tf/reset_default_graph\n",
    "10. tf.Session | TensorFlow. (2018). TensorFlow. Retrieved 9 December 2018, from https://www.tensorflow.org/api_docs/python/tf/Session\n",
    "11. tf.nn.moments | TensorFlow. (2018). TensorFlow. Retrieved 9 December 2018, from https://www.tensorflow.org/api_docs/python/tf/nn/moments\n",
    "12. CMD?, H. (2018). How do I run two commands in one line in Windows CMD?. Stack Overflow. Retrieved 9 December 2018, from https://stackoverflow.com/questions/8055371/how-do-i-run-two-commands-in-one-line-in-windows-cmd\n",
    "13. loop, B. (2018). Batch script loop. Stack Overflow. Retrieved 9 December 2018, from https://stackoverflow.com/questions/2591758/batch-script-loop\n",
    "14. tf.train.MomentumOptimizer | TensorFlow. (2018). TensorFlow. Retrieved 9 December 2018, from https://www.tensorflow.org/api_docs/python/tf/train/MomentumOptimizer\n",
    "15. Test if two numpy arrays are (close to) equal, i. (2018). Test if two numpy arrays are (close to) equal, including shape. Stack Overflow. Retrieved 9 December 2018, from https://stackoverflow.com/questions/32874840/test-if-two-numpy-arrays-are-close-to-equal-including-shape\n",
    "16. tf.linalg.diag | TensorFlow. (2018). TensorFlow. Retrieved 9 December 2018, from https://www.tensorflow.org/api_docs/python/tf/linalg/diag\n",
    "17. tf.layers.batch_normalization | TensorFlow. (2018). TensorFlow. Retrieved 10 December 2018, from https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization\n",
    "18. tf.layers.batch_normalization | TensorFlow. (2018). TensorFlow. Retrieved 10 December 2018, from https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization\n",
    "19. error, t. (2018). tf.layers.batch_normalization large test error. Stack Overflow. Retrieved 10 December 2018, from https://stackoverflow.com/questions/43234667/tf-layers-batch-normalization-large-test-error\n",
    "20. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
