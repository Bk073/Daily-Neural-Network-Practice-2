{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T14:20:42.901157Z",
     "start_time": "2018-12-10T14:20:39.433450Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# import lib\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys, os,cv2\n",
    "from sklearn.utils import shuffle\n",
    "from scipy.misc import imread,imresize\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from skimage.transform import resize\n",
    "from imgaug import augmenters as iaa\n",
    "import imgaug as ia\n",
    "from scipy.ndimage import zoom\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T14:20:44.640534Z",
     "start_time": "2018-12-10T14:20:42.948026Z"
    },
    "code_folding": [
     0,
     1,
     28
    ]
   },
   "outputs": [],
   "source": [
    "# read all of the data\n",
    "def read_all_images(path_to_data):\n",
    "    \"\"\"\n",
    "    :param path_to_data: the file containing the binary images from the STL-10 dataset\n",
    "    :return: an array containing all the images\n",
    "    \"\"\"\n",
    "\n",
    "    with open(path_to_data, 'rb') as f:\n",
    "        # read whole file in uint8 chunks\n",
    "        everything = np.fromfile(f, dtype=np.uint8)\n",
    "\n",
    "        # We force the data into 3x96x96 chunks, since the\n",
    "        # images are stored in \"column-major order\", meaning\n",
    "        # that \"the first 96*96 values are the red channel,\n",
    "        # the next 96*96 are green, and the last are blue.\"\n",
    "        # The -1 is since the size of the pictures depends\n",
    "        # on the input file, and this way numpy determines\n",
    "        # the size on its own.\n",
    "\n",
    "        images = np.reshape(everything, (-1, 3, 96, 96))\n",
    "\n",
    "        # Now transpose the images into a standard image format\n",
    "        # readable by, for example, matplotlib.imshow\n",
    "        # You might want to comment this line or reverse the shuffle\n",
    "        # if you will use a learning algorithm like CNN, since they like\n",
    "        # their channels separated.\n",
    "        images = np.transpose(images, (0, 3, 2, 1))\n",
    "        return images\n",
    "def read_labels(path_to_labels):\n",
    "    \"\"\"\n",
    "    :param path_to_labels: path to the binary file containing labels from the STL-10 dataset\n",
    "    :return: an array containing the labels\n",
    "    \"\"\"\n",
    "    with open(path_to_labels, 'rb') as f:\n",
    "        labels = np.fromfile(f, dtype=np.uint8)\n",
    "        return labels\n",
    "    \n",
    "train_images = read_all_images(\"../../../DataSet/STL10/stl10_binary/train_X.bin\") / 255.0\n",
    "train_labels = read_labels    (\"../../../DataSet/STL10/stl10_binary/train_Y.bin\")\n",
    "test_images  = read_all_images(\"../../../DataSet/STL10/stl10_binary/test_X.bin\")  / 255.0\n",
    "test_labels  = read_labels    (\"../../../DataSet/STL10/stl10_binary/test_y.bin\")\n",
    "\n",
    "label_encoder= OneHotEncoder(sparse=False,categories='auto')\n",
    "train_labels = label_encoder.fit_transform(train_labels.reshape((-1,1)))\n",
    "test_labels  = label_encoder.fit_transform(test_labels.reshape((-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T15:54:30.999642Z",
     "start_time": "2018-12-10T15:54:27.234331Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1.] [0. 0. 0.] [0.44671062 0.43980984 0.40664645] [0.26034098 0.25657727 0.27126738]\n",
      "(5000, 96, 96, 3)\n",
      "(5000, 10)\n",
      "[1. 1. 1.] [0. 0. 0.] [0.44723063 0.43964247 0.40495725] [0.2605645  0.25666146 0.26997382]\n",
      "(8000, 96, 96, 3)\n",
      "(8000, 10)\n"
     ]
    }
   ],
   "source": [
    "# some basic statistic of train and test image // hyper\n",
    "print(train_images.max((0,1,2)),train_images.min((0,1,2)),train_images.mean((0,1,2)),train_images.std((0,1,2)) )\n",
    "print(train_images.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_images.max((0,1,2)),test_images.min((0,1,2)),test_images.mean((0,1,2)),test_images.std((0,1,2)) )\n",
    "print(test_images.shape)\n",
    "print(test_labels.shape)\n",
    "num_epoch = 50 ; learning_rate = 0.0008; batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T15:54:31.455387Z",
     "start_time": "2018-12-10T15:54:31.428459Z"
    },
    "code_folding": [
     7
    ]
   },
   "outputs": [],
   "source": [
    "# import layers\n",
    "def tf_softmax(x): return tf.nn.softmax(x)\n",
    "def tf_elu(x):     return tf.nn.elu(x)\n",
    "def d_tf_elu(x):   return tf.cast(tf.greater(x,0),tf.float32)  + (tf_elu(tf.cast(tf.less_equal(x,0),tf.float32) * x) + 1.0)\n",
    "def tf_relu(x):    return tf.nn.relu(x)\n",
    "def d_tf_relu(x):  return tf.cast(tf.greater(x,0),tf.float32)\n",
    "\n",
    "class CNN():\n",
    "\n",
    "    def __init__(self,k,inc,out, stddev=0.05,which_reg=0,act=tf_elu,d_act=d_tf_elu):\n",
    "        self.w          = tf.Variable(tf.random_normal([k,k,inc,out],stddev=stddev,seed=2,dtype=tf.float32))\n",
    "        self.b          = tf.Variable(tf.zeros(out,dtype=tf.float32))\n",
    "        self.m,self.v   = tf.Variable(tf.zeros_like(self.w)),tf.Variable(tf.zeros_like(self.w))\n",
    "        self.mb,self.vb = tf.Variable(tf.zeros_like(self.b)),tf.Variable(tf.zeros_like(self.b))\n",
    "        self.act,self.d_act = act,d_act\n",
    "        self.which_reg  = which_reg\n",
    "        \n",
    "    def getw(self): return [self.w,self.b]\n",
    "\n",
    "    def feedforward(self,input,stride=1,padding='VALID'):\n",
    "        self.input  = input\n",
    "        self.layer  = tf.nn.conv2d(input,self.w,strides=[1,stride,stride,1],padding=padding) \n",
    "        self.layerA = self.act(self.layer)\n",
    "        return self.layerA\n",
    "\n",
    "    def backprop(self,gradient,stride=1,padding='VALID'):\n",
    "        grad_part_1 = gradient\n",
    "        grad_part_2 = self.d_act(self.layer)\n",
    "        grad_part_3 = self.input\n",
    "\n",
    "        grad_middle = grad_part_1 * grad_part_2\n",
    "        grad_b      = tf.reduce_mean(grad_middle,(0,1,2))/batch_size\n",
    "        grad        = tf.nn.conv2d_backprop_filter(input = grad_part_3,filter_sizes = tf.shape(self.w),  out_backprop = grad_middle,strides=[1,stride,stride,1],padding=padding) / batch_size\n",
    "        grad_pass   = tf.nn.conv2d_backprop_input (input_sizes = tf.shape(self.input),filter= self.w,out_backprop = grad_middle,strides=[1,stride,stride,1],padding=padding)\n",
    "\n",
    "        if self.which_reg == 0:   grad = grad\n",
    "        if self.which_reg == 0.5: grad = grad + lamda * (tf.sqrt(tf.abs(self.w))) * (1.0/tf.sqrt(tf.abs(self.w)+ 10e-5)) * tf.sign(self.w)\n",
    "        if self.which_reg == 1:   grad = grad + lamda * tf.sign(self.w)\n",
    "        if self.which_reg == 1.5: grad = grad + lamda * 1.0/(tf.sqrt(tf.square(self.w) + 10e-5)) * self.w\n",
    "        if self.which_reg == 2:   grad = grad + lamda * (1.0/tf.sqrt(tf.square(tf.abs(self.w))+ 10e-5)) * tf.abs(self.w) * tf.sign(self.w)\n",
    "        if self.which_reg == 2.5: grad = grad + lamda * 2.0 * self.w\n",
    "        if self.which_reg == 3:   grad = grad + lamda * tf.pow(tf.pow(tf.abs(self.w),3)+ 10e-5,-0.66) * tf.pow(tf.abs(self.w),2) * tf.sign(self.w)\n",
    "        if self.which_reg == 4:   grad = grad + lamda * tf.pow(tf.pow(tf.abs(self.w),4)+ 10e-5,-0.75) * tf.pow(tf.abs(self.w),3) * tf.sign(self.w)\n",
    "\n",
    "        update_w = []\n",
    "        \n",
    "        update_w.append(tf.assign( self.m,self.m*beta1 + (1-beta1) * (grad)   ))\n",
    "        update_w.append(tf.assign( self.v,self.v*beta2 + (1-beta2) * (grad ** 2)   ))\n",
    "        m_hat = self.m / (1-beta1) ; v_hat = self.v / (1-beta2)\n",
    "        adam_middle = m_hat * learning_rate/(tf.sqrt(v_hat) + adam_e)\n",
    "        update_w.append(tf.assign(self.w,tf.subtract(self.w,adam_middle  )))\n",
    "        \n",
    "        update_w.append(tf.assign( self.mb,self.mb*beta1 + (1-beta1) * (grad_b)   ))\n",
    "        update_w.append(tf.assign( self.vb,self.vb*beta2 + (1-beta2) * (grad_b ** 2)   ))\n",
    "        m_hatb = self.mb / (1-beta1) ; v_hatb = self.vb / (1-beta2)\n",
    "        adam_middleb = m_hatb * learning_rate/(tf.sqrt(v_hatb) + adam_e)\n",
    "        update_w.append(tf.assign(self.b,tf.subtract(self.b,adam_middleb  )))\n",
    "        \n",
    "        return grad_pass,update_w\n",
    "    \n",
    "# declare layer \n",
    "def tf_pca_svd(X,mmax=0.8,mmin=0.0): \n",
    "    s,U,V  = tf.svd(X,full_matrices=False)\n",
    "    smin = tf.reduce_min(s,0)\n",
    "    smax = tf.reduce_max(s,0)\n",
    "    ScaledS= (mmax-mmin)*((s-smin)/(smax-smin)) + mmin\n",
    "    recon_data = U @ tf.diag(s) @ tf.transpose(V) * tf.reduce_mean(tf.abs(V) * ScaledS[None,:],0,keepdims =True)\n",
    "    return recon_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T15:54:34.505494Z",
     "start_time": "2018-12-10T15:54:34.194840Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# restart the graph \n",
    "# sess.close()\n",
    "# tf.reset_default_graph()\n",
    "\n",
    "l1 = CNN(3,3, 32)\n",
    "l2 = CNN(3,32,32)\n",
    "l3 = CNN(3,32,32)\n",
    "\n",
    "l4 = CNN(3,32,64)\n",
    "l5 = CNN(3,64,64)\n",
    "l6 = CNN(3,64,64)\n",
    "\n",
    "l7 = CNN(3,64,128)\n",
    "l8 = CNN(1,128,128)\n",
    "l9 = CNN(1,128,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T15:54:39.318893Z",
     "start_time": "2018-12-10T15:54:38.759131Z"
    },
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# build graph \n",
    "def do(layer9,num=10):\n",
    "    with tf.device('/cpu:0'):\n",
    "        s,U,V = tf.svd(layer9)\n",
    "    S      = tf.matrix_diag(s)\n",
    "    layer9 = U @ S @ tf.transpose(V,(0,1,3,2))[:,:,:,:num] \n",
    "    return layer9\n",
    "\n",
    "x = tf.placeholder(shape=(batch_size,96,96,3),dtype=tf.float32)\n",
    "y = tf.placeholder(shape=(batch_size,10),dtype=tf.float32)\n",
    "n = tf.placeholder(shape=(),dtype=tf.int32)\n",
    "is_training = tf.placeholder_with_default(input=False,shape=())\n",
    "\n",
    "layer1 = l1.feedforward(x       ,padding='VALID',stride=1)\n",
    "layer2 = l2.feedforward(layer1  ,padding='VALID',stride=1)\n",
    "layer3 = l3.feedforward(layer2  ,padding='VALID',stride=2)\n",
    "\n",
    "layer4 = l4.feedforward(layer3  ,padding='VALID',stride=1)\n",
    "layer5 = l5.feedforward(layer4  ,padding='VALID',stride=1)\n",
    "layer6 = l6.feedforward(layer5  ,padding='VALID',stride=2)\n",
    " \n",
    "layer7 = l7.feedforward(layer6  ,padding='VALID',stride=1)\n",
    "layer8 = l8.feedforward(layer7  ,padding='VALID',stride=1)\n",
    "layer9 = l9.feedforward(layer8  ,padding='VALID',stride=1)\n",
    "\n",
    "layer9_t = tf.transpose(layer9,(1,2,0,3))\n",
    "with tf.device('/cpu:0'):\n",
    "    s,U,V = tf.svd(layer9_t)\n",
    "S        = tf.matrix_diag(s)\n",
    "layer9_2 = U[:,:,:,:n] @ S[:,:,:n,:n] @ tf.transpose(V,(0,1,3,2))[:,:,:n,:]\n",
    "layer9_3 = tf.transpose(layer9_2,(2,0,1,3)) \n",
    "\n",
    "final_layer = tf.reduce_mean(layer9_3,axis=(1,2))\n",
    "cost        = tf.nn.softmax_cross_entropy_with_logits_v2(logits=final_layer,labels=y)\n",
    "auto_train  = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "final_softmax      = tf_softmax(final_layer)\n",
    "correct_prediction = tf.equal(tf.argmax(final_softmax, 1), tf.argmax(y, 1))\n",
    "accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-10T15:55:05.825Z"
    },
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Iter : 0/50 batch : 7900/8000 acc : 0.17\n",
      " Current : 0 Acc : 0.1522000002115965 Test Acc : 0.18775000032037498\n",
      "\n",
      "Current Iter : 1/50 batch : 7900/8000 acc : 0.23\n",
      " Current : 1 Acc : 0.17600000008940697 Test Acc : 0.17649999959394336\n",
      "\n",
      "Current Iter : 2/50 batch : 7900/8000 acc : 0.21\n",
      " Current : 2 Acc : 0.21019999906420708 Test Acc : 0.2067500002682209\n",
      "\n",
      "Current Iter : 3/50 batch : 7900/8000 acc : 0.26\n",
      " Current : 3 Acc : 0.22680000022053717 Test Acc : 0.2516249995678663\n",
      "\n",
      "Current Iter : 4/50 batch : 7900/8000 acc : 0.36\n",
      " Current : 4 Acc : 0.25640000015497205 Test Acc : 0.2861250014975667\n",
      "\n",
      "Current Iter : 5/50 batch : 7900/8000 acc : 0.25\n",
      " Current : 5 Acc : 0.2830000004172325 Test Acc : 0.28412500079721215\n",
      "\n",
      "Current Iter : 6/50 batch : 7900/8000 acc : 0.32\n",
      " Current : 6 Acc : 0.30120000153779986 Test Acc : 0.3073750000447035\n",
      "\n",
      "Current Iter : 7/50 batch : 7900/8000 acc : 0.32\n",
      " Current : 7 Acc : 0.3072000008821487 Test Acc : 0.3055000018328428\n",
      "\n",
      "Current Iter : 8/50 batch : 7900/8000 acc : 0.43\n",
      " Current : 8 Acc : 0.31880000203847886 Test Acc : 0.3296250019222498\n",
      "\n",
      "Current Iter : 9/50 batch : 7900/8000 acc : 0.29\n",
      " Current : 9 Acc : 0.32420000225305556 Test Acc : 0.32975000254809855\n",
      "\n",
      "Current Iter : 10/50 batch : 7900/8000 acc : 0.28\n",
      " Current : 10 Acc : 0.3344000008702278 Test Acc : 0.3572500005364418\n",
      "\n",
      "Current Iter : 11/50 batch : 7900/8000 acc : 0.37\n",
      " Current : 11 Acc : 0.35240000128746035 Test Acc : 0.3201250027865171\n",
      "\n",
      "Current Iter : 12/50 batch : 7900/8000 acc : 0.44\n",
      " Current : 12 Acc : 0.3668000000715256 Test Acc : 0.3980000004172325\n",
      "\n",
      "Current Iter : 13/50 batch : 7900/8000 acc : 0.41\n",
      " Current : 13 Acc : 0.39279999971389773 Test Acc : 0.399499998241663\n",
      "\n",
      "Current Iter : 14/50 batch : 7900/8000 acc : 0.49\n",
      " Current : 14 Acc : 0.4041999977827072 Test Acc : 0.42724999897181987\n",
      "\n",
      "Current Iter : 15/50 batch : 7900/8000 acc : 0.39\n",
      " Current : 15 Acc : 0.41559999823570254 Test Acc : 0.42149999774992464\n",
      "\n",
      "Current Iter : 16/50 batch : 7900/8000 acc : 0.39\n",
      " Current : 16 Acc : 0.42919999778270723 Test Acc : 0.4572499979287386\n",
      "\n",
      "Current Iter : 17/50 batch : 7900/8000 acc : 0.39\n",
      " Current : 17 Acc : 0.4475999981164932 Test Acc : 0.444999997317791\n",
      "\n",
      "Current Iter : 18/50 batch : 7900/8000 acc : 0.41\n",
      " Current : 18 Acc : 0.4631999963521957 Test Acc : 0.4652499984949827\n",
      "\n",
      "Current Iter : 19/50 batch : 7900/8000 acc : 0.42\n",
      " Current : 19 Acc : 0.4865999960899353 Test Acc : 0.45724999755620954\n",
      "\n",
      "Current Iter : 20/50 batch : 7900/8000 acc : 0.39\n",
      " Current : 20 Acc : 0.47959999859333036 Test Acc : 0.4669999971985817\n",
      "\n",
      "Current Iter : 21/50 batch : 7900/8000 acc : 0.44\n",
      " Current : 21 Acc : 0.4867999964952469 Test Acc : 0.47125000022351743\n",
      "\n",
      "Current Iter : 22/50 batch : 7900/8000 acc : 0.39\n",
      " Current : 22 Acc : 0.49199999511241915 Test Acc : 0.4793749962002039\n",
      "\n",
      "Current Iter : 23/50 batch : 7900/8000 acc : 0.37\n",
      " Current : 23 Acc : 0.4999999970197678 Test Acc : 0.4709999978542328\n",
      "\n",
      "Current Iter : 24/50 batch : 7900/8000 acc : 0.51\n",
      " Current : 24 Acc : 0.5011999952793121 Test Acc : 0.4696249973028898\n",
      "\n",
      "Current Iter : 25/50 batch : 7900/8000 acc : 0.49\n",
      " Current : 25 Acc : 0.5057999932765961 Test Acc : 0.5024999961256981\n",
      "\n",
      "Current Iter : 26/50 batch : 7900/8000 acc : 0.51\n",
      " Current : 26 Acc : 0.5255999982357025 Test Acc : 0.4988749995827675\n",
      "\n",
      "Current Iter : 27/50 batch : 7900/8000 acc : 0.48\n",
      " Current : 27 Acc : 0.5229999983310699 Test Acc : 0.5124999992549419\n",
      "\n",
      "Current Iter : 28/50 batch : 7900/8000 acc : 0.49\n",
      " Current : 28 Acc : 0.5263999974727631 Test Acc : 0.49862499684095385\n",
      "\n",
      "Current Iter : 29/50 batch : 7900/8000 acc : 0.52\n",
      " Current : 29 Acc : 0.5365999972820282 Test Acc : 0.5181249979883432\n",
      "\n",
      "Current Iter : 30/50 batch : 7900/8000 acc : 0.56\n",
      " Current : 30 Acc : 0.5467999935150146 Test Acc : 0.5041249953210354\n",
      "\n",
      "Current Iter : 31/50 batch : 7900/8000 acc : 0.56\n",
      " Current : 31 Acc : 0.5529999965429306 Test Acc : 0.48962499797344206\n",
      "\n",
      "Current Iter : 32/50 batch : 7900/8000 acc : 0.39\n",
      " Current : 32 Acc : 0.5487999951839447 Test Acc : 0.5111249979585409\n",
      "\n",
      "Current Iter : 33/50 batch : 7900/8000 acc : 0.57\n",
      " Current : 33 Acc : 0.5655999958515168 Test Acc : 0.521374997869134\n",
      "\n",
      "Current Iter : 34/50 batch : 7900/8000 acc : 0.51\n",
      " Current : 34 Acc : 0.5715999966859817 Test Acc : 0.5307499978691339\n",
      "\n",
      "Current Iter : 35/50 batch : 7900/8000 acc : 0.53\n",
      " Current : 35 Acc : 0.5745999985933303 Test Acc : 0.5136249963194132\n",
      "\n",
      "Current Iter : 36/50 batch : 7900/8000 acc : 0.46\n",
      " Current : 36 Acc : 0.574600002169609 Test Acc : 0.5283749982714653\n",
      "\n",
      "Current Iter : 37/50 batch : 7900/8000 acc : 0.46\n",
      " Current : 37 Acc : 0.5944000005722045 Test Acc : 0.5264999963343143\n",
      "\n",
      "Current Iter : 38/50 batch : 7900/8000 acc : 0.53\n",
      " Current : 38 Acc : 0.5871999943256379 Test Acc : 0.5344999950379133\n",
      "\n",
      "Current Iter : 39/50 batch : 7900/8000 acc : 0.52\n",
      " Current : 39 Acc : 0.6154000002145767 Test Acc : 0.549499997869134\n",
      "\n",
      "Current Iter : 40/50 batch : 7900/8000 acc : 0.62\n",
      " Current : 40 Acc : 0.612000002861023 Test Acc : 0.5399999983608723\n",
      "\n",
      "Current Iter : 41/50 batch : 7900/8000 acc : 0.63\n",
      " Current : 41 Acc : 0.6219999992847443 Test Acc : 0.5492499947547913\n",
      "\n",
      "Current Iter : 42/50 batch : 7900/8000 acc : 0.55\n",
      " Current : 42 Acc : 0.6310000014305115 Test Acc : 0.5487499959766865\n",
      "\n",
      "Current Iter : 43/50 batch : 7900/8000 acc : 0.63\n",
      " Current : 43 Acc : 0.6336000007390976 Test Acc : 0.5527500003576279\n",
      "\n",
      "Current Iter : 44/50 batch : 7900/8000 acc : 0.53\n",
      " Current : 44 Acc : 0.6474000012874603 Test Acc : 0.533374996110797\n",
      "\n",
      "Current Iter : 45/50 batch : 7900/8000 acc : 0.47\n",
      " Current : 45 Acc : 0.6339999985694885 Test Acc : 0.5208749942481518\n",
      "\n",
      "Current Iter : 46/50 batch : 7900/8000 acc : 0.47\n",
      " Current : 46 Acc : 0.6446000027656555 Test Acc : 0.5428749974817038\n",
      "\n",
      "Current Iter : 47/50 batch : 7900/8000 acc : 0.61\n",
      " Current : 47 Acc : 0.6542000043392181 Test Acc : 0.5456249997019768\n",
      "\n",
      "Current Iter : 48/50 batch : 7900/8000 acc : 0.53\n",
      " Current : 48 Acc : 0.6712000012397766 Test Acc : 0.5569999985396862\n",
      "\n",
      "Current Iter : 49/50 batch : 7900/8000 acc : 0.56\n",
      " Current : 49 Acc : 0.675200001001358 Test Acc : 0.542249995842576\n",
      "\n",
      "Current Iter : 50/50 batch : 7900/8000 acc : 0.51\n",
      " Current : 50 Acc : 0.6806000053882599 Test Acc : 0.5533749971538782\n",
      "\n",
      "Current Iter : 51/50 batch : 7900/8000 acc : 0.63\n",
      " Current : 51 Acc : 0.7084000039100647 Test Acc : 0.5467499993741513\n",
      "\n",
      "Current Iter : 52/50 batch : 7900/8000 acc : 0.57\n",
      " Current : 52 Acc : 0.7114000034332275 Test Acc : 0.5502499952912331\n",
      "\n",
      "Current Iter : 53/50 batch : 7900/8000 acc : 0.56\n",
      " Current : 53 Acc : 0.7036000037193298 Test Acc : 0.5567499946802854\n",
      "\n",
      "Current Iter : 54/50 batch : 7900/8000 acc : 0.51\n",
      " Current : 54 Acc : 0.7058000016212463 Test Acc : 0.560499994456768\n",
      "\n",
      "Current Iter : 55/50 batch : 7900/8000 acc : 0.42\n",
      " Current : 55 Acc : 0.7300000011920929 Test Acc : 0.5491249967366457\n",
      "\n",
      "Current Iter : 56/50 batch : 7900/8000 acc : 0.62\n",
      " Current : 56 Acc : 0.7417999982833863 Test Acc : 0.5371249962598086\n",
      "\n",
      "Current Iter : 57/50 batch : 7900/8000 acc : 0.47\n",
      " Current : 57 Acc : 0.7654000020027161 Test Acc : 0.5554999973624944\n",
      "\n",
      "Current Iter : 58/50 batch : 7900/8000 acc : 0.64\n",
      " Current : 58 Acc : 0.7641999971866608 Test Acc : 0.5253750007599592\n",
      "\n",
      "Current Iter : 59/50 batch : 7900/8000 acc : 0.53\n",
      " Current : 59 Acc : 0.7359999990463257 Test Acc : 0.5561249952763319\n",
      "\n",
      "Current Iter : 60/50 batch : 7900/8000 acc : 0.49\n",
      " Current : 60 Acc : 0.7810000002384185 Test Acc : 0.534875001385808\n",
      "\n",
      "Current Iter : 61/50 batch : 7900/8000 acc : 0.55\n",
      " Current : 61 Acc : 0.7799999988079072 Test Acc : 0.5353749994188547\n",
      "\n",
      "Current Iter : 62/50 batch : 7900/8000 acc : 0.53\n",
      " Current : 62 Acc : 0.7837999999523163 Test Acc : 0.5422499977052212\n",
      "\n",
      "Current Iter : 63/50 batch : 7900/8000 acc : 0.66\n",
      " Current : 63 Acc : 0.8033999991416931 Test Acc : 0.549499998241663\n",
      "\n",
      "Current Iter : 64/50 batch : 7900/8000 acc : 0.53\n",
      " Current : 64 Acc : 0.7983999979496003 Test Acc : 0.5529999975115061\n",
      "\n",
      "Current Iter : 65/50 batch : 7900/8000 acc : 0.41\n",
      " Current : 65 Acc : 0.8221999967098236 Test Acc : 0.5337499983608722\n",
      "\n",
      "Current Iter : 66/50 batch : 200/5000 acc : 0.81\r"
     ]
    }
   ],
   "source": [
    "# start the training \n",
    "# sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "avg_acc_train = 0\n",
    "avg_acc_test  = 0\n",
    "for iter in range(num_epoch*100):\n",
    "    \n",
    "    train_images,train_labels = shuffle(train_images,train_labels); test_images,test_labels   = shuffle(test_images,test_labels)\n",
    "    \n",
    "    for current_batch_index in range(0,len(train_images),batch_size):\n",
    "        current_data  = train_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = train_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy,auto_train],feed_dict={x:current_data,y:current_label,is_training:True,n:5})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(train_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush()\n",
    "        avg_acc_train = avg_acc_train + sess_results[0]\n",
    "        \n",
    "    for current_batch_index in range(0,len(test_images), batch_size):\n",
    "        current_data  = test_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = test_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy],feed_dict={x:current_data,y:current_label,n:10})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(test_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush()\n",
    "        avg_acc_test = avg_acc_test + sess_results[0]        \n",
    "        \n",
    "    print(\"\\n Current : \"+ str(iter) + \" Acc : \" + str(avg_acc_train/(len(train_images)/batch_size)) + \" Test Acc : \" + str(avg_acc_test/(len(test_images)/batch_size)) + '\\n')\n",
    "    avg_acc_train = 0 \n",
    "    avg_acc_test  = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# build graph (batch norm)\n",
    "def do(layer9,num=10):\n",
    "    with tf.device('/cpu:0'):\n",
    "        s,U,V = tf.svd(layer9)\n",
    "    S      = tf.matrix_diag(s)\n",
    "    layer9 = U @ S @ tf.transpose(V,(0,1,3,2))[:,:,:,:num] \n",
    "    return layer9\n",
    "\n",
    "x = tf.placeholder(shape=(batch_size,96,96,3),dtype=tf.float32)\n",
    "y = tf.placeholder(shape=(batch_size,10),dtype=tf.float32)\n",
    "is_training = tf.placeholder_with_default(input=False,shape=())\n",
    "\n",
    "layer1 = l1.feedforward(x,     padding='SAME',stride=1)\n",
    "layer2 = l2.feedforward(layer1,padding='SAME',stride=1)\n",
    "layer3 = l3.feedforward(layer2,padding='SAME',stride=2)\n",
    "# layer3 = tf.layers.batch_normalization(layer3,training=is_training)\n",
    "\n",
    "layer4 = l4.feedforward(layer3,padding='SAME',stride=1)\n",
    "layer5 = l5.feedforward(layer4,padding='SAME',stride=1)\n",
    "layer6 = l6.feedforward(layer5,padding='SAME',stride=2)\n",
    "# layer6 = tf.layers.batch_normalization(layer6,training=is_training)\n",
    "\n",
    "# layer6_t = tf.transpose(layer6,(1,2,0,3))\n",
    "# with tf.device('/cpu:0'):\n",
    "#     s,U,V = tf.svd(layer6_t)\n",
    "# S        = tf.matrix_diag(s)\n",
    "# layer6_2 = U @ S @ tf.transpose(V,(0,1,3,2))[:,:,:,:64]\n",
    "# layer6_3 = tf.transpose(layer6_2,(2,0,1,3))\n",
    "# print(U,S,V)\n",
    "# print(layer6_2)\n",
    "# print(layer6_3)\n",
    "\n",
    "layer7 = l7.feedforward(layer6,padding='VALID',stride=1)\n",
    "layer8 = l8.feedforward(layer7,padding='VALID',stride=1)\n",
    "layer9 = l9.feedforward(layer8,padding='VALID',stride=1)\n",
    "\n",
    "final_layer = tf.reduce_mean(layer9,axis=(1,2))\n",
    "cost        = tf.nn.softmax_cross_entropy_with_logits_v2(logits=final_layer,labels=y)\n",
    "# update_ops  = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "auto_train  = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "final_softmax      = tf_softmax(final_layer)\n",
    "correct_prediction = tf.equal(tf.argmax(final_softmax, 1), tf.argmax(y, 1))\n",
    "accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T14:17:25.431311Z",
     "start_time": "2018-12-10T14:17:14.977Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# build graph \n",
    "x = tf.placeholder(shape=(batch_size,96,96,3),dtype=tf.float32)\n",
    "y = tf.placeholder(shape=(batch_size,10),dtype=tf.float32)\n",
    "is_training = tf.placeholder_with_default(input=False,shape=())\n",
    "\n",
    "layer1 = l1.feedforward(x,padding='VALID',stride=2)\n",
    "layer2 = l2.feedforward(layer1,padding='VALID',stride=2)\n",
    "layer3 = l3.feedforward(layer2,padding='VALID',stride=2)\n",
    "\n",
    "layer4 = l4.feedforward(layer3,padding='VALID',stride=2)\n",
    "layer5 = l5.feedforward(layer4,padding='VALID')\n",
    "layer6 = l6.feedforward(layer5,padding='VALID',stride=2)\n",
    "\n",
    "layer7 = l7.feedforward(layer6,padding='VALID')\n",
    "layer8 = l8.feedforward(layer7,padding='VALID')\n",
    "layer9 = l9.feedforward(layer8,padding='VALID')\n",
    "final_layer   = tf.reduce_mean(layer9,axis=(1,2))\n",
    "\n",
    "cost       = tf.nn.softmax_cross_entropy_with_logits_v2(logits=final_layer,labels=y)\n",
    "auto_train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "final_softmax      = tf_softmax(final_layer)\n",
    "correct_prediction = tf.equal(tf.argmax(final_softmax, 1), tf.argmax(y, 1))\n",
    "accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T14:17:25.433341Z",
     "start_time": "2018-12-10T14:17:14.981Z"
    },
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# start the training \n",
    "# sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "avg_acc_train = 0\n",
    "avg_acc_test  = 0\n",
    "for iter in range(num_epoch):\n",
    "    \n",
    "    train_images,train_labels = shuffle(train_images,train_labels)\n",
    "    test_images,test_labels   = shuffle(test_images,test_labels)\n",
    "    \n",
    "    for current_batch_index in range(0,len(train_images),batch_size):\n",
    "        current_data  = train_images[current_batch_index:current_batch_index+batch_size]\n",
    "        current_label = train_labels[current_batch_index:current_batch_index+batch_size]\n",
    "        sess_results  = sess.run([accuracy,auto_train],feed_dict={x:current_data,y:current_label,is_training:True})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  +\n",
    "                         ' batch : ' + str(current_batch_index) + '/'+ str(len(train_images)) + \n",
    "                         ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush()\n",
    "        avg_acc_train = avg_acc_train + sess_results[0]\n",
    "        \n",
    "    for current_batch_index in range(0,len(test_images),batch_size):\n",
    "        current_data  = test_images[current_batch_index:current_batch_index+batch_size]\n",
    "        current_label = test_labels[current_batch_index:current_batch_index+batch_size]\n",
    "        sess_results  = sess.run([accuracy],feed_dict={x:current_data,y:current_label,is_training:False})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  +\n",
    "                         ' batch : ' + str(current_batch_index) + '/'+ str(len(test_images)) + \n",
    "                         ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush()\n",
    "        avg_acc_test = avg_acc_test + sess_results[0]        \n",
    "        \n",
    "    print(\"\\n Current : \"+ str(iter) + \" Acc : \" + str(avg_acc_train/(len(train_images)/batch_size)) + \" Test Acc : \" + str(avg_acc_test/(len(test_images)/batch_size)) + '\\n')\n",
    "    avg_acc_train = 0 \n",
    "    avg_acc_test  = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-09T16:56:07.568679Z",
     "start_time": "2018-12-09T16:56:07.550727Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference \n",
    "1. Brownlee, J. (2017). How to One Hot Encode Sequence Data in Python. Machine Learning Mastery. Retrieved 9 December 2018, from https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/\n",
    "2. tf.placeholder_with_default | TensorFlow. (2018). TensorFlow. Retrieved 9 December 2018, from https://www.tensorflow.org/api_docs/python/tf/placeholder_with_default\n",
    "3. tf.nn.softmax_cross_entropy_with_logits | TensorFlow. (2018). TensorFlow. Retrieved 9 December 2018, from https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits\n",
    "4. line, O. (2018). Output without new line. Stack Overflow. Retrieved 9 December 2018, from https://stackoverflow.com/questions/2623470/output-without-new-line\n",
    "5. shell?, H. (2018). How to tell if tensorflow is using gpu acceleration from inside python shell?. Stack Overflow. Retrieved 9 December 2018, from https://stackoverflow.com/questions/38009682/how-to-tell-if-tensorflow-is-using-gpu-acceleration-from-inside-python-shell\n",
    "6. GPU?, H. (2018). How to use TensorFlow GPU?. Stack Overflow. Retrieved 9 December 2018, from https://stackoverflow.com/questions/51306862/how-to-use-tensorflow-gpu\n",
    "7. Batch normalization: theory and how to use it with Tensorflow. (2018). Towards Data Science. Retrieved 9 December 2018, from https://towardsdatascience.com/batch-normalization-theory-and-how-to-use-it-with-tensorflow-1892ca0173ad\n",
    "8. Batch normalization: theory and how to use it with Tensorflow. (2018). Towards Data Science. Retrieved 9 December 2018, from https://towardsdatascience.com/batch-normalization-theory-and-how-to-use-it-with-tensorflow-1892ca0173ad\n",
    "9. tf.reset_default_graph | TensorFlow. (2018). TensorFlow. Retrieved 9 December 2018, from https://www.tensorflow.org/api_docs/python/tf/reset_default_graph\n",
    "10. tf.Session | TensorFlow. (2018). TensorFlow. Retrieved 9 December 2018, from https://www.tensorflow.org/api_docs/python/tf/Session\n",
    "11. tf.nn.moments | TensorFlow. (2018). TensorFlow. Retrieved 9 December 2018, from https://www.tensorflow.org/api_docs/python/tf/nn/moments\n",
    "12. CMD?, H. (2018). How do I run two commands in one line in Windows CMD?. Stack Overflow. Retrieved 9 December 2018, from https://stackoverflow.com/questions/8055371/how-do-i-run-two-commands-in-one-line-in-windows-cmd\n",
    "13. loop, B. (2018). Batch script loop. Stack Overflow. Retrieved 9 December 2018, from https://stackoverflow.com/questions/2591758/batch-script-loop\n",
    "14. tf.train.MomentumOptimizer | TensorFlow. (2018). TensorFlow. Retrieved 9 December 2018, from https://www.tensorflow.org/api_docs/python/tf/train/MomentumOptimizer\n",
    "15. Test if two numpy arrays are (close to) equal, i. (2018). Test if two numpy arrays are (close to) equal, including shape. Stack Overflow. Retrieved 9 December 2018, from https://stackoverflow.com/questions/32874840/test-if-two-numpy-arrays-are-close-to-equal-including-shape\n",
    "16. tf.linalg.diag | TensorFlow. (2018). TensorFlow. Retrieved 9 December 2018, from https://www.tensorflow.org/api_docs/python/tf/linalg/diag\n",
    "17. tf.layers.batch_normalization | TensorFlow. (2018). TensorFlow. Retrieved 10 December 2018, from https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization\n",
    "18. tf.layers.batch_normalization | TensorFlow. (2018). TensorFlow. Retrieved 10 December 2018, from https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization\n",
    "19. error, t. (2018). tf.layers.batch_normalization large test error. Stack Overflow. Retrieved 10 December 2018, from https://stackoverflow.com/questions/43234667/tf-layers-batch-normalization-large-test-error\n",
    "20. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T15:52:23.075289Z",
     "start_time": "2018-12-10T15:52:21.780293Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"MatrixDiag_54:0\", shape=(100, 32, 3, 3), dtype=float64)\n",
      "Tensor(\"Svd_54:1\", shape=(100, 32, 32, 3), dtype=float64) Tensor(\"Svd_54:0\", shape=(100, 32, 3), dtype=float64) Tensor(\"Svd_54:2\", shape=(100, 32, 3, 3), dtype=float64)\n",
      "Tensor(\"Diag:0\", shape=(100, 32, 3, 100, 32, 3), dtype=float64)\n",
      "[[[[6.55823893 0.         0.        ]\n",
      "   [0.         5.56346808 0.        ]\n",
      "   [0.         0.         4.8413144 ]]\n",
      "\n",
      "  [[7.25037232 0.         0.        ]\n",
      "   [0.         5.49028313 0.        ]\n",
      "   [0.         0.         4.46036365]]\n",
      "\n",
      "  [[7.15437884 0.         0.        ]\n",
      "   [0.         5.90717244 0.        ]\n",
      "   [0.         0.         4.85589987]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[6.54711788 0.         0.        ]\n",
      "   [0.         5.23255879 0.        ]\n",
      "   [0.         0.         4.90220369]]\n",
      "\n",
      "  [[6.50449194 0.         0.        ]\n",
      "   [0.         5.96788598 0.        ]\n",
      "   [0.         0.         4.59075861]]\n",
      "\n",
      "  [[6.98222117 0.         0.        ]\n",
      "   [0.         5.18923976 0.        ]\n",
      "   [0.         0.         4.57867129]]]\n",
      "\n",
      "\n",
      " [[[6.65563915 0.         0.        ]\n",
      "   [0.         5.41452507 0.        ]\n",
      "   [0.         0.         4.19351541]]\n",
      "\n",
      "  [[7.04990175 0.         0.        ]\n",
      "   [0.         5.73566958 0.        ]\n",
      "   [0.         0.         4.94013733]]\n",
      "\n",
      "  [[6.59232613 0.         0.        ]\n",
      "   [0.         5.1820523  0.        ]\n",
      "   [0.         0.         4.56623391]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[6.99361756 0.         0.        ]\n",
      "   [0.         4.76220651 0.        ]\n",
      "   [0.         0.         3.78501403]]\n",
      "\n",
      "  [[6.70257066 0.         0.        ]\n",
      "   [0.         5.04106367 0.        ]\n",
      "   [0.         0.         3.62371423]]\n",
      "\n",
      "  [[6.44436126 0.         0.        ]\n",
      "   [0.         6.09610488 0.        ]\n",
      "   [0.         0.         4.23362188]]]\n",
      "\n",
      "\n",
      " [[[6.0584267  0.         0.        ]\n",
      "   [0.         5.21255356 0.        ]\n",
      "   [0.         0.         4.84853997]]\n",
      "\n",
      "  [[6.4555343  0.         0.        ]\n",
      "   [0.         5.5466113  0.        ]\n",
      "   [0.         0.         5.37977099]]\n",
      "\n",
      "  [[6.57745782 0.         0.        ]\n",
      "   [0.         4.88388699 0.        ]\n",
      "   [0.         0.         4.52899489]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[6.71141229 0.         0.        ]\n",
      "   [0.         5.08802657 0.        ]\n",
      "   [0.         0.         4.46674932]]\n",
      "\n",
      "  [[7.4053751  0.         0.        ]\n",
      "   [0.         5.47949032 0.        ]\n",
      "   [0.         0.         4.62222148]]\n",
      "\n",
      "  [[6.88954481 0.         0.        ]\n",
      "   [0.         5.75814449 0.        ]\n",
      "   [0.         0.         5.22155675]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[6.3636869  0.         0.        ]\n",
      "   [0.         5.42808712 0.        ]\n",
      "   [0.         0.         4.36875603]]\n",
      "\n",
      "  [[7.64153839 0.         0.        ]\n",
      "   [0.         5.85689381 0.        ]\n",
      "   [0.         0.         4.79103845]]\n",
      "\n",
      "  [[7.38098974 0.         0.        ]\n",
      "   [0.         5.62608207 0.        ]\n",
      "   [0.         0.         4.92903158]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[6.05271701 0.         0.        ]\n",
      "   [0.         5.67676784 0.        ]\n",
      "   [0.         0.         4.09923391]]\n",
      "\n",
      "  [[7.40547838 0.         0.        ]\n",
      "   [0.         5.87293664 0.        ]\n",
      "   [0.         0.         5.51400779]]\n",
      "\n",
      "  [[6.24778885 0.         0.        ]\n",
      "   [0.         5.38525575 0.        ]\n",
      "   [0.         0.         4.24792237]]]\n",
      "\n",
      "\n",
      " [[[6.68074436 0.         0.        ]\n",
      "   [0.         5.43293724 0.        ]\n",
      "   [0.         0.         4.46435496]]\n",
      "\n",
      "  [[6.89766143 0.         0.        ]\n",
      "   [0.         6.08170519 0.        ]\n",
      "   [0.         0.         4.68046135]]\n",
      "\n",
      "  [[6.26705532 0.         0.        ]\n",
      "   [0.         5.73080523 0.        ]\n",
      "   [0.         0.         4.73913252]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[6.33614622 0.         0.        ]\n",
      "   [0.         5.08085514 0.        ]\n",
      "   [0.         0.         4.70218318]]\n",
      "\n",
      "  [[6.63657168 0.         0.        ]\n",
      "   [0.         5.23492603 0.        ]\n",
      "   [0.         0.         4.80648337]]\n",
      "\n",
      "  [[5.73857265 0.         0.        ]\n",
      "   [0.         5.27168151 0.        ]\n",
      "   [0.         0.         4.16750616]]]\n",
      "\n",
      "\n",
      " [[[6.1818294  0.         0.        ]\n",
      "   [0.         5.42844189 0.        ]\n",
      "   [0.         0.         5.01108706]]\n",
      "\n",
      "  [[6.32672751 0.         0.        ]\n",
      "   [0.         5.93742272 0.        ]\n",
      "   [0.         0.         5.25498651]]\n",
      "\n",
      "  [[6.41676669 0.         0.        ]\n",
      "   [0.         5.65869776 0.        ]\n",
      "   [0.         0.         4.61444676]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[6.05626462 0.         0.        ]\n",
      "   [0.         5.43656193 0.        ]\n",
      "   [0.         0.         3.97818298]]\n",
      "\n",
      "  [[7.0781321  0.         0.        ]\n",
      "   [0.         4.84326143 0.        ]\n",
      "   [0.         0.         4.31987294]]\n",
      "\n",
      "  [[6.65490105 0.         0.        ]\n",
      "   [0.         5.76119643 0.        ]\n",
      "   [0.         0.         5.55471995]]]]\n",
      "None\n",
      "True\n",
      "19489\n",
      "307200\n"
     ]
    }
   ],
   "source": [
    "temp   = np.random.randn(100,32,32,3)\n",
    "s,U,V  = tf.svd(temp)\n",
    "S = tf.matrix_diag(s)\n",
    "print(S)\n",
    "print(U,s,V)\n",
    "print(tf.diag(s))\n",
    "\n",
    "result = U @ S @ tf.transpose(V,(0,1,3,2))\n",
    "print(S.eval())\n",
    "result = result.eval()\n",
    "\n",
    "print(np.testing.assert_allclose(temp, result))\n",
    "print(np.allclose(temp,result))\n",
    "print(np.equal(temp,result).sum())\n",
    "print(100*32*32*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
