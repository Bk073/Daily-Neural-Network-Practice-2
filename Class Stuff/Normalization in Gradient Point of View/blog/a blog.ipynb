{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T06:25:48.211729Z",
     "start_time": "2018-12-20T06:25:44.682125Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# import Library and some random image data set\n",
    "import tensorflow as tf\n",
    "import numpy      as np\n",
    "import seaborn    as sns \n",
    "import pandas     as pd\n",
    "import os,sys\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(78); tf.set_random_seed(78)\n",
    "\n",
    "# get some of the STL data set\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from skimage import util \n",
    "from skimage.transform import resize\n",
    "from skimage.io import imread\n",
    "import warnings\n",
    "from numpy import inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T06:25:53.354955Z",
     "start_time": "2018-12-20T06:25:50.789791Z"
    },
    "code_folding": [
     0,
     2,
     29,
     37
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 96, 96, 3) 1.0 0.0\n",
      "(5000, 10) 1.0 0.0\n",
      "(8000, 96, 96, 3) 1.0 0.0\n",
      "(8000, 10) 1.0 0.0\n"
     ]
    }
   ],
   "source": [
    "# read all of the data\n",
    "# https://github.com/mttk/STL10\n",
    "def read_all_images(path_to_data):\n",
    "    \"\"\"\n",
    "    :param path_to_data: the file containing the binary images from the STL-10 dataset\n",
    "    :return: an array containing all the images\n",
    "    \"\"\"\n",
    "\n",
    "    with open(path_to_data, 'rb') as f:\n",
    "        # read whole file in uint8 chunks\n",
    "        everything = np.fromfile(f, dtype=np.uint8)\n",
    "\n",
    "        # We force the data into 3x96x96 chunks, since the\n",
    "        # images are stored in \"column-major order\", meaning\n",
    "        # that \"the first 96*96 values are the red channel,\n",
    "        # the next 96*96 are green, and the last are blue.\"\n",
    "        # The -1 is since the size of the pictures depends\n",
    "        # on the input file, and this way numpy determines\n",
    "        # the size on its own.\n",
    "\n",
    "        images = np.reshape(everything, (-1, 3, 96, 96))\n",
    "\n",
    "        # Now transpose the images into a standard image format\n",
    "        # readable by, for example, matplotlib.imshow\n",
    "        # You might want to comment this line or reverse the shuffle\n",
    "        # if you will use a learning algorithm like CNN, since they like\n",
    "        # their channels separated.\n",
    "        images = np.transpose(images, (0, 3, 2, 1))\n",
    "        return images\n",
    "def read_labels(path_to_labels):\n",
    "    \"\"\"\n",
    "    :param path_to_labels: path to the binary file containing labels from the STL-10 dataset\n",
    "    :return: an array containing the labels\n",
    "    \"\"\"\n",
    "    with open(path_to_labels, 'rb') as f:\n",
    "        labels = np.fromfile(f, dtype=np.uint8)\n",
    "        return labels\n",
    "def show_images(data,row=1,col=1):\n",
    "    fig=plt.figure(figsize=(10,10))\n",
    "    columns = col; rows = row\n",
    "    for i in range(1, columns*rows +1):\n",
    "        fig.add_subplot(rows, columns, i)\n",
    "        plt.imshow(data[i-1])\n",
    "    plt.show()\n",
    "\n",
    "train_images = read_all_images(\"../../../DataSet/STL10/stl10_binary/train_X.bin\") / 255.0\n",
    "train_labels = read_labels    (\"../../../DataSet/STL10/stl10_binary/train_Y.bin\")\n",
    "test_images  = read_all_images(\"../../../DataSet/STL10/stl10_binary/test_X.bin\")  / 255.0\n",
    "test_labels  = read_labels    (\"../../../DataSet/STL10/stl10_binary/test_y.bin\")\n",
    "\n",
    "label_encoder= OneHotEncoder(sparse=False,categories='auto')\n",
    "train_labels = label_encoder.fit_transform(train_labels.reshape((-1,1)))\n",
    "test_labels  = label_encoder.fit_transform(test_labels.reshape((-1,1)))\n",
    "\n",
    "print(train_images.shape,train_images.max(),train_images.min())\n",
    "print(train_labels.shape,train_labels.max(),train_labels.min())\n",
    "print(test_images.shape,test_images.max(),test_images.min())\n",
    "print(test_labels.shape,test_labels.max(),test_labels.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T07:16:29.956183Z",
     "start_time": "2018-12-20T07:16:29.883343Z"
    },
    "code_folding": [
     15,
     58,
     99,
     140
    ]
   },
   "outputs": [],
   "source": [
    "# create the layers\n",
    "def tf_softmax(x): return tf.nn.softmax(x)\n",
    "\n",
    "def tf_elu(x):   return tf.nn.elu(x)\n",
    "def d_tf_elu(x): return tf.cast(tf.greater(x,0),tf.float32)  + (tf_elu(tf.cast(tf.less_equal(x,0),tf.float32) * x) + 1.0)\n",
    "\n",
    "def tf_relu(x):   return tf.nn.relu(x)\n",
    "def d_tf_relu(x): return tf.cast(tf.greater(x,0),tf.float32)\n",
    "\n",
    "def tf_tanh(x):   return tf.nn.tanh(x)\n",
    "def d_tf_tanh(x): return 1 - tf_tanh(x) ** 2\n",
    "\n",
    "def tf_sigmoid(x):   return tf.nn.sigmoid(x)\n",
    "def d_tf_sigmoid(x): return tf_sigmoid(x) * (1.0-tf_sigmoid(x))\n",
    "\n",
    "class CNN():\n",
    "\n",
    "    def __init__(self,k,inc,out, stddev=0.05,which_reg=0,act=tf_relu,d_act=d_tf_relu):\n",
    "        self.w          = tf.Variable(tf.random_normal([k,k,inc,out],stddev=stddev,seed=2,dtype=tf.float32))\n",
    "        self.m,self.v   = tf.Variable(tf.zeros_like(self.w)),tf.Variable(tf.zeros_like(self.w))\n",
    "        self.act,self.d_act = act,d_act\n",
    "        self.which_reg  = which_reg\n",
    "        \n",
    "    def getw(self): return self.w\n",
    "\n",
    "    def feedforward(self,input,stride=1,padding='SAME'):\n",
    "        self.input  = input\n",
    "        self.layer  = tf.nn.conv2d(input,self.w,strides=[1,stride,stride,1],padding=padding) \n",
    "        self.layerA = self.act(self.layer)\n",
    "        return [self.layer,self.layerA]\n",
    "    \n",
    "    def backprop(self,gradient,stride=1,padding='SAME'):\n",
    "        grad_part_1 = gradient\n",
    "        grad_part_2 = self.d_act(self.layer)\n",
    "        grad_part_3 = self.input\n",
    "\n",
    "        grad_middle = grad_part_1 * grad_part_2\n",
    "        grad        = tf.nn.conv2d_backprop_filter(input = grad_part_3,filter_sizes = tf.shape(self.w),  out_backprop = grad_middle,strides=[1,stride,stride,1],padding=padding) / batch_size\n",
    "        grad_pass   = tf.nn.conv2d_backprop_input (input_sizes = tf.shape(self.input),filter= self.w,out_backprop = grad_middle,strides=[1,stride,stride,1],padding=padding)\n",
    "\n",
    "        if self.which_reg == 0:   grad = grad\n",
    "        if self.which_reg == 0.5: grad = grad + lamda * (tf.sqrt(tf.abs(self.w))) * (1.0/tf.sqrt(tf.abs(self.w)+ 10e-5)) * tf.sign(self.w)\n",
    "        if self.which_reg == 1:   grad = grad + lamda * tf.sign(self.w)\n",
    "        if self.which_reg == 1.5: grad = grad + lamda * 1.0/(tf.sqrt(tf.square(self.w) + 10e-5)) * self.w\n",
    "        if self.which_reg == 2:   grad = grad + lamda * (1.0/tf.sqrt(tf.square(tf.abs(self.w))+ 10e-5)) * tf.abs(self.w) * tf.sign(self.w)\n",
    "        if self.which_reg == 2.5: grad = grad + lamda * 2.0 * self.w\n",
    "        if self.which_reg == 3:   grad = grad + lamda * tf.pow(tf.pow(tf.abs(self.w),3)+ 10e-5,-0.66) * tf.pow(tf.abs(self.w),2) * tf.sign(self.w)\n",
    "        if self.which_reg == 4:   grad = grad + lamda * tf.pow(tf.pow(tf.abs(self.w),4)+ 10e-5,-0.75) * tf.pow(tf.abs(self.w),3) * tf.sign(self.w)\n",
    "\n",
    "        update_w = []\n",
    "        update_w.append(tf.assign( self.m,self.m*beta1 + (1-beta1) * (grad)   ))\n",
    "        update_w.append(tf.assign( self.v,self.v*beta2 + (1-beta2) * (grad ** 2)   ))\n",
    "        m_hat = self.m / (1-beta1) ; v_hat = self.v / (1-beta2)\n",
    "        adam_middle = m_hat * learning_rate/(tf.sqrt(v_hat) + adam_e)\n",
    "        update_w.append(tf.assign(self.w,tf.subtract(self.w,adam_middle  )))\n",
    "        \n",
    "        return grad_pass,grad,update_w\n",
    "    \n",
    "class tf_batch_norm_layer():\n",
    "    \n",
    "    def __init__(self,vector_shape,axis):\n",
    "        self.moving_mean = tf.Variable(tf.zeros(shape=[1,1,1,vector_shape],dtype=tf.float32))\n",
    "        self.moving_vari = tf.Variable(tf.zeros(shape=[1,1,1,vector_shape],dtype=tf.float32))\n",
    "        self.axis        = axis\n",
    "        \n",
    "    def feedforward(self,input,training_phase=True,eps = 1e-8):\n",
    "        self.input = input\n",
    "        self.input_size          = self.input.shape\n",
    "        self.batch,self.h,self.w,self.c = self.input_size[0].value,self.input_size[1].value,self.input_size[2].value,self.input_size[3].value\n",
    "\n",
    "        # Training Moving Average Mean         \n",
    "        def training_fn():\n",
    "            self.mean    = tf.reduce_mean(self.input,axis=self.axis ,keepdims=True)\n",
    "            self.var     = tf.reduce_mean(tf.square(self.input-self.mean),axis=self.axis,keepdims=True)\n",
    "            centered_data= (self.input - self.mean)/tf.sqrt(self.var + eps)\n",
    "            \n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean*0.9 + 0.1 * self.mean ))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari*0.9 + 0.1 * self.var  ))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        # Testing Moving Average Mean        \n",
    "        def  testing_fn():\n",
    "            centered_data   = (self.input - self.moving_mean)/tf.sqrt(self.moving_vari + eps)\n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        self.output,update_variable = tf.cond(training_phase,true_fn=training_fn,false_fn=testing_fn)\n",
    "        return self.output,update_variable\n",
    "    \n",
    "    def backprop(self,grad,eps = 1e-8):\n",
    "        change_parts = 1.0 /(self.batch * self.h * self.w)\n",
    "        grad_sigma   = tf.reduce_sum( grad *  (self.input-self.mean)     ,axis=self.axis,keepdims=True) * -0.5 * (self.var+eps) ** -1.5\n",
    "        grad_mean    = tf.reduce_sum( grad *  (-1./tf.sqrt(self.var+eps)),axis=self.axis,keepdims=True) + grad_sigma * change_parts * 2.0 * tf.reduce_sum((self.input-self.mean),axis=self.axis,keepdims=True) * -1\n",
    "        grad_x       = grad * 1/(tf.sqrt(self.var+eps)) + grad_sigma * change_parts * 2.0 * (self.input-self.mean) + grad_mean * change_parts\n",
    "        return grad_x\n",
    "\n",
    "class tf_layer_norm_layer():\n",
    "    \n",
    "    def __init__(self,vector_shape,axis):\n",
    "        self.moving_mean = tf.Variable(tf.zeros(shape=[vector_shape,1,1,1],dtype=tf.float32))\n",
    "        self.moving_vari = tf.Variable(tf.zeros(shape=[vector_shape,1,1,1],dtype=tf.float32))\n",
    "        self.axis        = axis\n",
    "        \n",
    "    def feedforward(self,input,training_phase=True,eps = 1e-8):\n",
    "        self.input = input\n",
    "        self.input_size          = self.input.shape\n",
    "        self.batch,self.h,self.w,self.c = self.input_size[0].value,self.input_size[1].value,self.input_size[2].value,self.input_size[3].value\n",
    "\n",
    "        # Training Moving Average Mean         \n",
    "        def training_fn():\n",
    "            self.mean    = tf.reduce_mean(self.input,axis=self.axis ,keepdims=True)\n",
    "            self.var     = tf.reduce_mean(tf.square(self.input-self.mean),axis=self.axis,keepdims=True)\n",
    "            centered_data= (self.input - self.mean)/tf.sqrt(self.var + eps)\n",
    "            \n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean*0.9 + 0.1 * self.mean ))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari*0.9 + 0.1 * self.var  ))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        # Testing Moving Average Mean        \n",
    "        def  testing_fn():\n",
    "            centered_data   = (self.input - self.moving_mean)/tf.sqrt(self.moving_vari + eps)\n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        self.output,update_variable = tf.cond(training_phase,true_fn=training_fn,false_fn=testing_fn)\n",
    "        return self.output,update_variable\n",
    "    \n",
    "    def backprop(self,grad,eps = 1e-8):\n",
    "        change_parts = 1.0 /(self.h * self.w * self.c)\n",
    "        grad_sigma   = tf.reduce_sum( grad *  (self.input-self.mean)     ,axis=self.axis,keepdims=True) * -0.5 * (self.var+eps) ** -1.5\n",
    "        grad_mean    = tf.reduce_sum( grad *  (-1./tf.sqrt(self.var+eps)),axis=self.axis,keepdims=True) + grad_sigma * change_parts * 2.0 * tf.reduce_sum((self.input-self.mean),axis=self.axis,keepdims=True) * -1\n",
    "        grad_x       = grad * 1/(tf.sqrt(self.var+eps)) + grad_sigma * change_parts * 2.0 * (self.input-self.mean) + grad_mean * change_parts\n",
    "        return grad_x\n",
    "    \n",
    "class tf_instance_norm_layer():\n",
    "    \n",
    "    def __init__(self,batch_size,vector_shape,axis):\n",
    "        self.moving_mean = tf.Variable(tf.zeros(shape=[batch_size,1,1,vector_shape],dtype=tf.float32))\n",
    "        self.moving_vari = tf.Variable(tf.zeros(shape=[batch_size,1,1,vector_shape],dtype=tf.float32))\n",
    "        self.axis        = axis\n",
    "        \n",
    "    def feedforward(self,input,training_phase=True,eps = 1e-8):\n",
    "        self.input = input\n",
    "        self.input_size          = self.input.shape\n",
    "        self.batch,self.h,self.w,self.c = self.input_size[0].value,self.input_size[1].value,self.input_size[2].value,self.input_size[3].value\n",
    "\n",
    "        # Training Moving Average Mean         \n",
    "        def training_fn():\n",
    "            self.mean    = tf.reduce_mean(self.input,axis=self.axis ,keepdims=True)\n",
    "            self.var     = tf.reduce_mean(tf.square(self.input-self.mean),axis=self.axis,keepdims=True)\n",
    "            centered_data= (self.input - self.mean)/tf.sqrt(self.var + eps)\n",
    "            \n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean*0.9 + 0.1 * self.mean ))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari*0.9 + 0.1 * self.var  ))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        # Testing Moving Average Mean        \n",
    "        def  testing_fn():\n",
    "            centered_data   = (self.input - self.moving_mean)/tf.sqrt(self.moving_vari + eps)\n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        self.output,update_variable = tf.cond(training_phase,true_fn=training_fn,false_fn=testing_fn)\n",
    "        return self.output,update_variable\n",
    "    \n",
    "    def backprop(self,grad,eps = 1e-8):\n",
    "        change_parts = 1.0 /(self.h * self.w)\n",
    "        grad_sigma   = tf.reduce_sum( grad *  (self.input-self.mean)     ,axis=self.axis,keepdims=True) * -0.5 * (self.var+eps) ** -1.5\n",
    "        grad_mean    = tf.reduce_sum( grad *  (-1./tf.sqrt(self.var+eps)),axis=self.axis,keepdims=True) + grad_sigma * change_parts * 2.0 * tf.reduce_sum((self.input-self.mean),axis=self.axis,keepdims=True) * -1\n",
    "        grad_x       = grad * 1/(tf.sqrt(self.var+eps)) + grad_sigma * change_parts * 2.0 * (self.input-self.mean) + grad_mean * change_parts\n",
    "        return grad_x\n",
    "  \n",
    "class tf_box_cox():\n",
    "    \n",
    "    def __init__(self,channel):\n",
    "        self.lmbda    = tf.Variable(tf.ones([1,1,1,channel],tf.float32)* 2.0) \n",
    "#         self.lmbda    = tf.Variable(2.0) \n",
    "        self.m,self.v = tf.Variable(tf.zeros_like(self.lmbda)),tf.Variable(tf.zeros_like(self.lmbda))\n",
    "    def getw(self): return self.lmbda\n",
    "    \n",
    "    def feedforward(self,data):\n",
    "        self.input = data\n",
    "        self.layer = tf.pow((self.input + 1.0),self.lmbda)\n",
    "        return (self.layer - 1.0)/(self.lmbda + 1e-8)\n",
    "    \n",
    "    def backprop(self,grad):\n",
    "        \n",
    "        # Gradient that gets passed along\n",
    "        grad_pass = tf.pow((self.input + 1),self.lmbda-1.0) * grad\n",
    "        \n",
    "        # Grad respect to the lmbda value (not tested!)\n",
    "        grad_lmbda1 =   (self.layer * tf.log(self.input + 1 ))/(self.lmbda + 1e-8)\n",
    "        grad_lmbda2 = - (self.layer - 1)/(self.lmbda ** 2 + 1e-8)\n",
    "        grad_lmbda  = tf.reduce_mean((grad_lmbda1 + grad_lmbda2)*grad,(0,1,2),True)\n",
    "\n",
    "        update_w = []\n",
    "        update_w.append(tf.assign( self.m,self.m*beta1 + (1-beta1) * (grad_lmbda)   ))\n",
    "        update_w.append(tf.assign( self.v,self.v*beta2 + (1-beta2) * (grad_lmbda ** 2)   ))\n",
    "        m_hat = self.m / (1-beta1) ; v_hat = self.v / (1-beta2)\n",
    "        adam_middle = m_hat * learning_rate/(tf.sqrt(v_hat) + adam_e)\n",
    "        update_w.append(tf.assign(self.lmbda,tf.subtract(self.lmbda,adam_middle  )))\n",
    "        \n",
    "        return grad_pass,grad_lmbda,update_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T06:21:04.871403Z",
     "start_time": "2018-12-20T06:21:04.769676Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# hyper parameter\n",
    "num_epoch = 300; learning_rate = 0.0008; batch_size = 20\n",
    "beta1,beta2,adam_e = 0.9,0.999,1e-9\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T06:07:37.013560Z",
     "start_time": "2018-12-20T06:07:36.830997Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# create layers\n",
    "l1 = CNN(3,3, 16); \n",
    "l2 = CNN(3,16,16); \n",
    "l3 = CNN(3,16,16); \n",
    "\n",
    "l4 = CNN(3,16,32); \n",
    "l5 = CNN(3,32,32); \n",
    "l6 = CNN(3,32,10); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T06:08:47.504461Z",
     "start_time": "2018-12-20T06:08:47.299795Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# create the graph \n",
    "x = tf.placeholder(tf.float32,[batch_size,96,96,3])\n",
    "y = tf.placeholder(tf.float32,[batch_size,10])\n",
    "\n",
    "layer1,layer1a = l1.feedforward(x,stride=2)      ;          \n",
    "layer2,layer2a = l2.feedforward(layer1a,stride=2);         \n",
    "layer3,layer3a = l3.feedforward(layer2a,stride=2); \n",
    "\n",
    "layer4,layer4a = l4.feedforward(layer3a,stride=2);          \n",
    "layer5,layer5a = l5.feedforward(layer4a,stride=1);         \n",
    "layer6,layer6a = l6.feedforward(layer5a,stride=1); \n",
    "\n",
    "final_layer   = tf.reduce_mean(layer6a,(1,2))\n",
    "final_softmax = tf_softmax(final_layer)\n",
    "cost          = -tf.reduce_mean(y * tf.log(final_softmax + 1e-8))\n",
    "correct_prediction = tf.equal(tf.argmax(final_softmax, 1), tf.argmax(y, 1))\n",
    "accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "gradient = tf.tile((final_softmax-y)[:,None,None,:],[1,6,6,1])/batch_size\n",
    "grad6w,grad6p,grad6_up = l6.backprop(gradient)\n",
    "grad5w,grad5p,grad5_up = l5.backprop(grad6p)\n",
    "grad4w,grad4p,grad4_up = l4.backprop(grad5p,stride=2)\n",
    "\n",
    "grad3w,grad3p,grad3_up = l3.backprop(grad4p,stride=2)\n",
    "grad2w,grad2p,grad2_up = l2.backprop(grad3p,stride=2)\n",
    "grad1w,grad1p,grad1_up = l1.backprop(grad2p,stride=2)\n",
    "\n",
    "gradient_update = grad6_up + \\\n",
    "                   grad5_up + \\\n",
    "                   grad4_up + \\\n",
    "                   grad3_up + \\\n",
    "                   grad2_up + \\\n",
    "                   grad1_up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T06:09:14.239248Z",
     "start_time": "2018-12-20T06:08:54.084190Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Iter : 0/300 batch : 7980/8000 acc : 0.35\n",
      " Current : 0 Acc : 0.12840000288188458 Test Acc : 0.21200000332668423\n",
      "\n",
      "Current Iter : 1/300 batch : 7980/8000 acc : 0.45\n",
      " Current : 1 Acc : 0.22900000382959843 Test Acc : 0.25975000286474825\n",
      "\n",
      "Current Iter : 2/300 batch : 7980/8000 acc : 0.55\n",
      " Current : 2 Acc : 0.2868000023066998 Test Acc : 0.3093750014156103\n",
      "\n",
      "Current Iter : 3/300 batch : 900/8000 acc : 0.455\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-fbb428fac211>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mcurrent_data\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mtest_images\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcurrent_batch_index\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mcurrent_batch_index\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mcurrent_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcurrent_batch_index\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mcurrent_batch_index\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0msess_results\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mcurrent_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mcurrent_label\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Current Iter : '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/'\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_epoch\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;33m+\u001b[0m \u001b[1;34m' batch : '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_batch_index\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/'\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' acc : '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'\\r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0mavg_acc_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mavg_acc_test\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msess_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1332\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train\n",
    "sess.run(tf.global_variables_initializer())\n",
    "avg_acc_train = 0; avg_acc_test  = 0; \n",
    "train_acc     = [];test_acc = []\n",
    "for iter in range(num_epoch):\n",
    "\n",
    "    for current_batch_index in range(0,len(train_images),batch_size):\n",
    "        current_data  = train_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = train_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy,gradient_update],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(train_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_train = avg_acc_train + sess_results[0]\n",
    "        \n",
    "    for current_batch_index in range(0,len(test_images), batch_size):\n",
    "        current_data  = test_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = test_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(test_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_test = avg_acc_test + sess_results[0]   \n",
    "        \n",
    "    print(\"\\n Current : \"+ str(iter) + \" Acc : \" + str(avg_acc_train/(len(train_images)/batch_size)) + \" Test Acc : \" + str(avg_acc_test/(len(test_images)/batch_size)) + '\\n')\n",
    "    \n",
    "    # save the training\n",
    "    train_acc.append(avg_acc_train/(len(train_images)/batch_size))\n",
    "    test_acc .append(avg_acc_test/(len(test_images)/batch_size)  )\n",
    "    \n",
    "    \n",
    "    avg_acc_train = 0 ; avg_acc_test  = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T07:16:38.343541Z",
     "start_time": "2018-12-20T07:16:38.106960Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# create layers\n",
    "num_epoch = 150; learning_rate = 0.0008; batch_size = 50\n",
    "beta1,beta2,adam_e = 0.9,0.999,1e-8\n",
    "tf.reset_default_graph(); sess.close()\n",
    "sess = tf.InteractiveSession()\n",
    "l1 = CNN(3,3, 16); l1n = tf_box_cox(16)\n",
    "l2 = CNN(3,16,16); l2n = tf_box_cox(16)\n",
    "l3 = CNN(3,16,16); l3n = tf_box_cox(16)\n",
    "\n",
    "l4 = CNN(3,16,32); l4n = tf_box_cox(32)\n",
    "l5 = CNN(3,32,32); l5n = tf_box_cox(32)\n",
    "l6 = CNN(3,32,10); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T07:16:41.668958Z",
     "start_time": "2018-12-20T07:16:40.835942Z"
    }
   },
   "outputs": [],
   "source": [
    "# create the graph \n",
    "x = tf.placeholder(tf.float32,[batch_size,96,96,3])\n",
    "y = tf.placeholder(tf.float32,[batch_size,10])\n",
    "\n",
    "layer1,layer1a = l1.feedforward(x,stride=2)      ;          \n",
    "layer1n = l1n.feedforward(layer1a)\n",
    "layer2,layer2a = l2.feedforward(layer1n,stride=2);          \n",
    "layer2n = l2n.feedforward(layer2a)\n",
    "layer3,layer3a = l3.feedforward(layer2n,stride=2); \n",
    "layer3n = l3n.feedforward(layer3a)\n",
    "\n",
    "layer4,layer4a = l4.feedforward(layer3n,stride=2);          \n",
    "layer4n = l4n.feedforward(layer4a)\n",
    "layer5,layer5a = l5.feedforward(layer4n,stride=1);          \n",
    "layer5n = l5n.feedforward(layer5a)\n",
    "layer6,layer6a = l6.feedforward(layer5n,stride=1); \n",
    "\n",
    "final_layer   = tf.reduce_mean(layer6a,(1,2))\n",
    "final_softmax = tf_softmax(final_layer)\n",
    "cost          = -tf.reduce_mean(y * tf.log(final_softmax + 1e-8))\n",
    "auto_train = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "correct_prediction = tf.equal(tf.argmax(final_softmax, 1), tf.argmax(y, 1))\n",
    "accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "gradient = tf.tile((final_softmax-y)[:,None,None,:],[1,6,6,1])/batch_size\n",
    "grad6p,grad6w,grad6_up  = l6.backprop(gradient)\n",
    "grad5n,grad5l,grad5n_up = l5n.backprop(grad6p); \n",
    "grad5p,grad5w,grad5_up  = l5.backprop(grad5n)\n",
    "grad4n,grad4l,grad4n_up = l4n.backprop(grad5p); \n",
    "grad4p,grad4w,grad4_up  = l4.backprop(grad4n,stride=2)\n",
    "\n",
    "grad3n,grad3l,grad3n_up = l3n.backprop(grad4p);\n",
    "grad3p,grad3w,grad3_up  = l3.backprop(grad3n,stride=2)\n",
    "grad2n,grad2l,grad2n_up = l2n.backprop(grad3p); \n",
    "grad2p,grad2w,grad2_up  = l2.backprop(grad2n,stride=2)\n",
    "grad1n,grad1l,grad1n_up = l1n.backprop(grad2p); \n",
    "grad1p,grad1w,grad1_up  = l1.backprop(grad1n,stride=2)\n",
    "\n",
    "gradient_update = grad6_up + \\\n",
    "                  grad5n_up + grad5_up + \\\n",
    "                  grad4n_up + grad4_up + \\\n",
    "                  grad3n_up + grad3_up + \\\n",
    "                  grad2n_up + grad2_up + \\\n",
    "                  grad1n_up + grad1_up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-20T07:17:05.096Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Iter : 0/150 batch : 7950/8000 acc : 0.12\n",
      " Current : 0 Acc : 0.11179999973624945 Test Acc : 0.0999999994179234\n",
      "\n",
      "Current Iter : 1/150 batch : 7950/8000 acc : 0.26\n",
      " Current : 1 Acc : 0.13760000016540289 Test Acc : 0.17925000013783574\n",
      "\n",
      "Current Iter : 2/150 batch : 7950/8000 acc : 0.28\n",
      " Current : 2 Acc : 0.18520000029355288 Test Acc : 0.19087500020395964\n",
      "\n",
      "Current Iter : 3/150 batch : 7950/8000 acc : 0.34\n",
      " Current : 3 Acc : 0.20580000020563602 Test Acc : 0.21949999993667008\n",
      "\n",
      "Current Iter : 4/150 batch : 7950/8000 acc : 0.42\n",
      " Current : 4 Acc : 0.2301999992877245 Test Acc : 0.23662499939091503\n",
      "\n",
      "Current Iter : 5/150 batch : 7950/8000 acc : 0.42\n",
      " Current : 5 Acc : 0.23599999994039536 Test Acc : 0.23987499983049929\n",
      "\n",
      "Current Iter : 6/150 batch : 7950/8000 acc : 0.38\n",
      " Current : 6 Acc : 0.24299999989569188 Test Acc : 0.24262499930337073\n",
      "\n",
      "Current Iter : 7/150 batch : 7950/8000 acc : 0.38\n",
      " Current : 7 Acc : 0.24539999961853026 Test Acc : 0.24687499972060323\n",
      "\n",
      "Current Iter : 8/150 batch : 7950/8000 acc : 0.38\n",
      " Current : 8 Acc : 0.253999999538064 Test Acc : 0.25349999917671084\n",
      "\n",
      "Current Iter : 9/150 batch : 7950/8000 acc : 0.34\n",
      " Current : 9 Acc : 0.2619999988377094 Test Acc : 0.25662499931640925\n",
      "\n",
      "Current Iter : 10/150 batch : 7950/8000 acc : 0.36\n",
      " Current : 10 Acc : 0.26240000031888483 Test Acc : 0.2678749999497086\n",
      "\n",
      "Current Iter : 11/150 batch : 7950/8000 acc : 0.34\n",
      " Current : 11 Acc : 0.2788000014424324 Test Acc : 0.28512500040233135\n",
      "\n",
      "Current Iter : 12/150 batch : 7950/8000 acc : 0.36\n",
      " Current : 12 Acc : 0.28600000075995924 Test Acc : 0.2812499995343387\n",
      "\n",
      "Current Iter : 13/150 batch : 7950/8000 acc : 0.36\n",
      " Current : 13 Acc : 0.29880000054836275 Test Acc : 0.29725000010803343\n",
      "\n",
      "Current Iter : 14/150 batch : 7950/8000 acc : 0.34\n",
      " Current : 14 Acc : 0.2998000017553568 Test Acc : 0.3001250005327165\n",
      "\n",
      "Current Iter : 15/150 batch : 7950/8000 acc : 0.36\n",
      " Current : 15 Acc : 0.3114000003039837 Test Acc : 0.3115000014193356\n",
      "\n",
      "Current Iter : 16/150 batch : 7950/8000 acc : 0.46\n",
      " Current : 16 Acc : 0.3152000004053116 Test Acc : 0.31674999967217443\n",
      "\n",
      "Current Iter : 17/150 batch : 7950/8000 acc : 0.42\n",
      " Current : 17 Acc : 0.3194000005722046 Test Acc : 0.3210000002756715\n",
      "\n",
      "Current Iter : 18/150 batch : 7950/8000 acc : 0.46\n",
      " Current : 18 Acc : 0.3223999997973442 Test Acc : 0.3233750005252659\n",
      "\n",
      "Current Iter : 19/150 batch : 7950/8000 acc : 0.44\n",
      " Current : 19 Acc : 0.33440000236034395 Test Acc : 0.3317500015720725\n",
      "\n",
      "Current Iter : 20/150 batch : 7950/8000 acc : 0.44\n",
      " Current : 20 Acc : 0.3393999998271465 Test Acc : 0.3355000006966293\n",
      "\n",
      "Current Iter : 21/150 batch : 7950/8000 acc : 0.46\n",
      " Current : 21 Acc : 0.34660000041127204 Test Acc : 0.3405000023543835\n",
      "\n",
      "Current Iter : 22/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 22 Acc : 0.3527999997138977 Test Acc : 0.3462500005029142\n",
      "\n",
      "Current Iter : 23/150 batch : 7950/8000 acc : 0.42\n",
      " Current : 23 Acc : 0.35100000143051147 Test Acc : 0.3412500010803342\n",
      "\n",
      "Current Iter : 24/150 batch : 7950/8000 acc : 0.46\n",
      " Current : 24 Acc : 0.3623999999463558 Test Acc : 0.3547500018961728\n",
      "\n",
      "Current Iter : 25/150 batch : 7950/8000 acc : 0.42\n",
      " Current : 25 Acc : 0.36279999911785127 Test Acc : 0.3560000007972121\n",
      "\n",
      "Current Iter : 26/150 batch : 7950/8000 acc : 0.42\n",
      " Current : 26 Acc : 0.3653999990224838 Test Acc : 0.3562500009313226\n",
      "\n",
      "Current Iter : 27/150 batch : 7950/8000 acc : 0.42\n",
      " Current : 27 Acc : 0.36699999928474425 Test Acc : 0.3616250013932586\n",
      "\n",
      "Current Iter : 28/150 batch : 7950/8000 acc : 0.42\n",
      " Current : 28 Acc : 0.37019999951124194 Test Acc : 0.363625000603497\n",
      "\n",
      "Current Iter : 29/150 batch : 7950/8000 acc : 0.42\n",
      " Current : 29 Acc : 0.3745999999344349 Test Acc : 0.36937500033527615\n",
      "\n",
      "Current Iter : 30/150 batch : 7950/8000 acc : 0.42\n",
      " Current : 30 Acc : 0.3768000011146069 Test Acc : 0.3700000014156103\n",
      "\n",
      "Current Iter : 31/150 batch : 7950/8000 acc : 0.42\n",
      " Current : 31 Acc : 0.3772000004351139 Test Acc : 0.37337500127032397\n",
      "\n",
      "Current Iter : 32/150 batch : 7950/8000 acc : 0.42\n",
      " Current : 32 Acc : 0.3814000016450882 Test Acc : 0.38000000165775416\n",
      "\n",
      "Current Iter : 33/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 33 Acc : 0.38220000192523 Test Acc : 0.37550000101327896\n",
      "\n",
      "Current Iter : 34/150 batch : 7950/8000 acc : 0.42\n",
      " Current : 34 Acc : 0.38919999927282334 Test Acc : 0.3827500006183982\n",
      "\n",
      "Current Iter : 35/150 batch : 7950/8000 acc : 0.38\n",
      " Current : 35 Acc : 0.38580000057816505 Test Acc : 0.38237500144168735\n",
      "\n",
      "Current Iter : 36/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 36 Acc : 0.3921999999880791 Test Acc : 0.38500000182539224\n",
      "\n",
      "Current Iter : 37/150 batch : 7950/8000 acc : 0.38\n",
      " Current : 37 Acc : 0.3932000003755093 Test Acc : 0.382875001616776\n",
      "\n",
      "Current Iter : 38/150 batch : 7950/8000 acc : 0.42\n",
      " Current : 38 Acc : 0.396600000411272 Test Acc : 0.3880000001750886\n",
      "\n",
      "Current Iter : 39/150 batch : 7950/8000 acc : 0.38\n",
      " Current : 39 Acc : 0.39679999992251397 Test Acc : 0.3865000013262033\n",
      "\n",
      "Current Iter : 40/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 40 Acc : 0.4016000011563301 Test Acc : 0.387124999333173\n",
      "\n",
      "Current Iter : 41/150 batch : 7950/8000 acc : 0.42\n",
      " Current : 41 Acc : 0.4016000024974346 Test Acc : 0.38862499995157124\n",
      "\n",
      "Current Iter : 42/150 batch : 7950/8000 acc : 0.44\n",
      " Current : 42 Acc : 0.40340000063180925 Test Acc : 0.3899999987334013\n",
      "\n",
      "Current Iter : 43/150 batch : 7950/8000 acc : 0.44\n",
      " Current : 43 Acc : 0.4046000002324581 Test Acc : 0.3921250001527369\n",
      "\n",
      "Current Iter : 44/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 44 Acc : 0.4116000011563301 Test Acc : 0.39437499893829225\n",
      "\n",
      "Current Iter : 45/150 batch : 7950/8000 acc : 0.44\n",
      " Current : 45 Acc : 0.4113999997079372 Test Acc : 0.3956249995157123\n",
      "\n",
      "Current Iter : 46/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 46 Acc : 0.41300000056624414 Test Acc : 0.39724999805912375\n",
      "\n",
      "Current Iter : 47/150 batch : 7950/8000 acc : 0.46\n",
      " Current : 47 Acc : 0.415599998831749 Test Acc : 0.3989999992772937\n",
      "\n",
      "Current Iter : 48/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 48 Acc : 0.4193999986350536 Test Acc : 0.4004999989643693\n",
      "\n",
      "Current Iter : 49/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 49 Acc : 0.4183999989926815 Test Acc : 0.4018749982118607\n",
      "\n",
      "Current Iter : 50/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 50 Acc : 0.42299999997019766 Test Acc : 0.40287499902769924\n",
      "\n",
      "Current Iter : 51/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 51 Acc : 0.423999999165535 Test Acc : 0.4027499992400408\n",
      "\n",
      "Current Iter : 52/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 52 Acc : 0.4250000002980232 Test Acc : 0.40499999821186067\n",
      "\n",
      "Current Iter : 53/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 53 Acc : 0.4266000005602837 Test Acc : 0.4073749979957938\n",
      "\n",
      "Current Iter : 54/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 54 Acc : 0.42759999915957453 Test Acc : 0.4097499992698431\n",
      "\n",
      "Current Iter : 55/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 55 Acc : 0.43080000042915345 Test Acc : 0.40999999940395354\n",
      "\n",
      "Current Iter : 56/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 56 Acc : 0.43400000035762787 Test Acc : 0.4111250001937151\n",
      "\n",
      "Current Iter : 57/150 batch : 7950/8000 acc : 0.46\n",
      " Current : 57 Acc : 0.4365999999642372 Test Acc : 0.4107500000856817\n",
      "\n",
      "Current Iter : 58/150 batch : 7950/8000 acc : 0.56\n",
      " Current : 58 Acc : 0.4361999994516373 Test Acc : 0.41062499973922967\n",
      "\n",
      "Current Iter : 59/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 59 Acc : 0.4401999995112419 Test Acc : 0.4103749988600612\n",
      "\n",
      "Current Iter : 60/150 batch : 7950/8000 acc : 0.58\n",
      " Current : 60 Acc : 0.4448000003397465 Test Acc : 0.41150000002235176\n",
      "\n",
      "Current Iter : 61/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 61 Acc : 0.44540000036358834 Test Acc : 0.4136249993927777\n",
      "\n",
      "Current Iter : 62/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 62 Acc : 0.4463999989628792 Test Acc : 0.4141249992884696\n",
      "\n",
      "Current Iter : 63/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 63 Acc : 0.44839999854564666 Test Acc : 0.4118750002235174\n",
      "\n",
      "Current Iter : 64/150 batch : 7950/8000 acc : 0.58\n",
      " Current : 64 Acc : 0.45159999817609786 Test Acc : 0.40874999975785614\n",
      "\n",
      "Current Iter : 65/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 65 Acc : 0.45479999870061877 Test Acc : 0.40762499952688813\n",
      "\n",
      "Current Iter : 66/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 66 Acc : 0.458000001013279 Test Acc : 0.40624999925494193\n",
      "\n",
      "Current Iter : 67/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 67 Acc : 0.4613999992609024 Test Acc : 0.4099999999627471\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Iter : 68/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 68 Acc : 0.46140000015497207 Test Acc : 0.4122499990276992\n",
      "\n",
      "Current Iter : 69/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 69 Acc : 0.4653999999165535 Test Acc : 0.4152499997057021\n",
      "\n",
      "Current Iter : 70/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 70 Acc : 0.46619999945163726 Test Acc : 0.41675000051036476\n",
      "\n",
      "Current Iter : 71/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 71 Acc : 0.4667999988794327 Test Acc : 0.41637499909847975\n",
      "\n",
      "Current Iter : 72/150 batch : 7950/8000 acc : 0.44\n",
      " Current : 72 Acc : 0.4677999982237816 Test Acc : 0.4096249993890524\n",
      "\n",
      "Current Iter : 73/150 batch : 7950/8000 acc : 0.42\n",
      " Current : 73 Acc : 0.46639999985694885 Test Acc : 0.3989999992772937\n",
      "\n",
      "Current Iter : 74/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 74 Acc : 0.47139999747276307 Test Acc : 0.4143749986775219\n",
      "\n",
      "Current Iter : 75/150 batch : 7950/8000 acc : 0.54\n",
      " Current : 75 Acc : 0.47420000106096266 Test Acc : 0.4218749999068677\n",
      "\n",
      "Current Iter : 76/150 batch : 7950/8000 acc : 0.56\n",
      " Current : 76 Acc : 0.48060000121593477 Test Acc : 0.4254999982193112\n",
      "\n",
      "Current Iter : 77/150 batch : 7950/8000 acc : 0.46\n",
      " Current : 77 Acc : 0.47979999989271166 Test Acc : 0.42724999897181987\n",
      "\n",
      "Current Iter : 78/150 batch : 7950/8000 acc : 0.46\n",
      " Current : 78 Acc : 0.48279999911785126 Test Acc : 0.428874999191612\n",
      "\n",
      "Current Iter : 79/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 79 Acc : 0.4815999984741211 Test Acc : 0.42562499986961483\n",
      "\n",
      "Current Iter : 80/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 80 Acc : 0.48220000088214876 Test Acc : 0.42549999989569187\n",
      "\n",
      "Current Iter : 81/150 batch : 7950/8000 acc : 0.46\n",
      " Current : 81 Acc : 0.481599999666214 Test Acc : 0.40675000017508867\n",
      "\n",
      "Current Iter : 82/150 batch : 7950/8000 acc : 0.42\n",
      " Current : 82 Acc : 0.47500000059604647 Test Acc : 0.39812499936670065\n",
      "\n",
      "Current Iter : 83/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 83 Acc : 0.48319999873638153 Test Acc : 0.42699999948963524\n",
      "\n",
      "Current Iter : 84/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 84 Acc : 0.48859999746084215 Test Acc : 0.417625000141561\n",
      "\n",
      "Current Iter : 85/150 batch : 7950/8000 acc : 0.44\n",
      " Current : 85 Acc : 0.48539999932050704 Test Acc : 0.4069999990053475\n",
      "\n",
      "Current Iter : 86/150 batch : 7950/8000 acc : 0.44\n",
      " Current : 86 Acc : 0.4865999999642372 Test Acc : 0.4041249987669289\n",
      "\n",
      "Current Iter : 87/150 batch : 7950/8000 acc : 0.44\n",
      " Current : 87 Acc : 0.4914000001549721 Test Acc : 0.4205000008456409\n",
      "\n",
      "Current Iter : 88/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 88 Acc : 0.497399999499321 Test Acc : 0.4213749995455146\n",
      "\n",
      "Current Iter : 89/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 89 Acc : 0.49759999811649325 Test Acc : 0.4211250000633299\n",
      "\n",
      "Current Iter : 90/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 90 Acc : 0.5009999990463256 Test Acc : 0.42174999937415125\n",
      "\n",
      "Current Iter : 91/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 91 Acc : 0.4971999987959862 Test Acc : 0.4189999999478459\n",
      "\n",
      "Current Iter : 92/150 batch : 7950/8000 acc : 0.44\n",
      " Current : 92 Acc : 0.49180000185966494 Test Acc : 0.40612499983981254\n",
      "\n",
      "Current Iter : 93/150 batch : 7950/8000 acc : 0.44\n",
      " Current : 93 Acc : 0.4949999997019768 Test Acc : 0.4122499991208315\n",
      "\n",
      "Current Iter : 94/150 batch : 7950/8000 acc : 0.56\n",
      " Current : 94 Acc : 0.5013999992609024 Test Acc : 0.43074999898672106\n",
      "\n",
      "Current Iter : 95/150 batch : 7950/8000 acc : 0.54\n",
      " Current : 95 Acc : 0.5051999992132187 Test Acc : 0.43124999944120646\n",
      "\n",
      "Current Iter : 96/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 96 Acc : 0.5093999999761581 Test Acc : 0.4313749998807907\n",
      "\n",
      "Current Iter : 97/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 97 Acc : 0.5076000016927719 Test Acc : 0.43099999874830247\n",
      "\n",
      "Current Iter : 98/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 98 Acc : 0.5076000010967254 Test Acc : 0.4334999997168779\n",
      "\n",
      "Current Iter : 99/150 batch : 3850/5000 acc : 0.42\r"
     ]
    }
   ],
   "source": [
    "# train\n",
    "sess.run(tf.global_variables_initializer())\n",
    "avg_acc_train = 0; avg_acc_test  = 0; \n",
    "train_acc     = [];test_acc = []\n",
    "for iter in range(num_epoch):\n",
    "\n",
    "    for current_batch_index in range(0,len(train_images),batch_size):\n",
    "        current_data  = train_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = train_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy,gradient_update],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(train_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_train = avg_acc_train + sess_results[0]\n",
    "        \n",
    "    for current_batch_index in range(0,len(test_images), batch_size):\n",
    "        current_data  = test_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = test_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(test_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_test = avg_acc_test + sess_results[0]   \n",
    "        \n",
    "    print(\"\\n Current : \"+ str(iter) + \" Acc : \" + str(avg_acc_train/(len(train_images)/batch_size)) + \" Test Acc : \" + str(avg_acc_test/(len(test_images)/batch_size)) + '\\n')\n",
    "    \n",
    "    # save the training\n",
    "    train_acc.append(avg_acc_train/(len(train_images)/batch_size))\n",
    "    test_acc .append(avg_acc_test/(len(test_images)/batch_size)  )\n",
    "    \n",
    "    avg_acc_train = 0 ; avg_acc_test  = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T06:52:08.136334Z",
     "start_time": "2018-12-20T06:52:08.051288Z"
    }
   },
   "outputs": [],
   "source": [
    "print(sess.run([l1n.getw(),l2n.getw(),l3n.getw(),l4n.getw(),l5n.getw()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "1. mttk/STL10. (2018). GitHub. Retrieved 19 December 2018, from https://github.com/mttk/STL10\n",
    "2. [duplicate], H. (2018). How to display multiple images in one figure correctly?. Stack Overflow. Retrieved 19 December 2018, from https://stackoverflow.com/questions/46615554/how-to-display-multiple-images-in-one-figure-correctly"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
