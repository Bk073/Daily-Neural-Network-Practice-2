{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T23:56:34.034594Z",
     "start_time": "2018-12-20T23:56:34.026614Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# import Library and some random image data set\n",
    "import tensorflow as tf\n",
    "import numpy      as np\n",
    "import seaborn    as sns \n",
    "import pandas     as pd\n",
    "import os,sys\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(78); tf.set_random_seed(78)\n",
    "\n",
    "# get some of the STL data set\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from skimage import util \n",
    "from skimage.transform import resize\n",
    "from skimage.io import imread\n",
    "import warnings\n",
    "from numpy import inf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 35})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T23:38:22.125598Z",
     "start_time": "2018-12-20T23:38:19.868571Z"
    },
    "code_folding": [
     0,
     2,
     29,
     37
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 96, 96, 3) 1.0 0.0\n",
      "(5000, 10) 1.0 0.0\n",
      "(8000, 96, 96, 3) 1.0 0.0\n",
      "(8000, 10) 1.0 0.0\n"
     ]
    }
   ],
   "source": [
    "# read all of the data\n",
    "# https://github.com/mttk/STL10\n",
    "def read_all_images(path_to_data):\n",
    "    \"\"\"\n",
    "    :param path_to_data: the file containing the binary images from the STL-10 dataset\n",
    "    :return: an array containing all the images\n",
    "    \"\"\"\n",
    "\n",
    "    with open(path_to_data, 'rb') as f:\n",
    "        # read whole file in uint8 chunks\n",
    "        everything = np.fromfile(f, dtype=np.uint8)\n",
    "\n",
    "        # We force the data into 3x96x96 chunks, since the\n",
    "        # images are stored in \"column-major order\", meaning\n",
    "        # that \"the first 96*96 values are the red channel,\n",
    "        # the next 96*96 are green, and the last are blue.\"\n",
    "        # The -1 is since the size of the pictures depends\n",
    "        # on the input file, and this way numpy determines\n",
    "        # the size on its own.\n",
    "\n",
    "        images = np.reshape(everything, (-1, 3, 96, 96))\n",
    "\n",
    "        # Now transpose the images into a standard image format\n",
    "        # readable by, for example, matplotlib.imshow\n",
    "        # You might want to comment this line or reverse the shuffle\n",
    "        # if you will use a learning algorithm like CNN, since they like\n",
    "        # their channels separated.\n",
    "        images = np.transpose(images, (0, 3, 2, 1))\n",
    "        return images\n",
    "def read_labels(path_to_labels):\n",
    "    \"\"\"\n",
    "    :param path_to_labels: path to the binary file containing labels from the STL-10 dataset\n",
    "    :return: an array containing the labels\n",
    "    \"\"\"\n",
    "    with open(path_to_labels, 'rb') as f:\n",
    "        labels = np.fromfile(f, dtype=np.uint8)\n",
    "        return labels\n",
    "def show_images(data,row=1,col=1):\n",
    "    fig=plt.figure(figsize=(10,10))\n",
    "    columns = col; rows = row\n",
    "    for i in range(1, columns*rows +1):\n",
    "        fig.add_subplot(rows, columns, i)\n",
    "        plt.imshow(data[i-1])\n",
    "    plt.show()\n",
    "\n",
    "train_images = read_all_images(\"../../../DataSet/STL10/stl10_binary/train_X.bin\") / 255.0\n",
    "train_labels = read_labels    (\"../../../DataSet/STL10/stl10_binary/train_Y.bin\")\n",
    "test_images  = read_all_images(\"../../../DataSet/STL10/stl10_binary/test_X.bin\")  / 255.0\n",
    "test_labels  = read_labels    (\"../../../DataSet/STL10/stl10_binary/test_y.bin\")\n",
    "\n",
    "label_encoder= OneHotEncoder(sparse=False,categories='auto')\n",
    "train_labels = label_encoder.fit_transform(train_labels.reshape((-1,1)))\n",
    "test_labels  = label_encoder.fit_transform(test_labels.reshape((-1,1)))\n",
    "\n",
    "print(train_images.shape,train_images.max(),train_images.min())\n",
    "print(train_labels.shape,train_labels.max(),train_labels.min())\n",
    "print(test_images.shape,test_images.max(),test_images.min())\n",
    "print(test_labels.shape,test_labels.max(),test_labels.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T23:38:22.193449Z",
     "start_time": "2018-12-20T23:38:22.127592Z"
    },
    "code_folding": [
     0,
     15,
     48,
     88,
     128,
     199
    ]
   },
   "outputs": [],
   "source": [
    "# create the layers\n",
    "def tf_softmax(x): return tf.nn.softmax(x)\n",
    "\n",
    "def tf_elu(x):   return tf.nn.elu(x)\n",
    "def d_tf_elu(x): return tf.cast(tf.greater(x,0),tf.float32)  + (tf_elu(tf.cast(tf.less_equal(x,0),tf.float32) * x) + 1.0)\n",
    "\n",
    "def tf_relu(x):   return tf.nn.relu(x)\n",
    "def d_tf_relu(x): return tf.cast(tf.greater(x,0),tf.float32)\n",
    "\n",
    "def tf_tanh(x):   return tf.nn.tanh(x)\n",
    "def d_tf_tanh(x): return 1 - tf_tanh(x) ** 2\n",
    "\n",
    "def tf_sigmoid(x):   return tf.nn.sigmoid(x)\n",
    "def d_tf_sigmoid(x): return tf_sigmoid(x) * (1.0-tf_sigmoid(x))\n",
    "\n",
    "class CNN():\n",
    "\n",
    "    def __init__(self,k,inc,out, stddev=0.05,which_reg=0,act=tf_relu,d_act=d_tf_relu):\n",
    "        self.w              = tf.Variable(tf.random_normal([k,k,inc,out],stddev=stddev,seed=4,dtype=tf.float32))\n",
    "        self.m,self.v       = tf.Variable(tf.zeros_like(self.w)),tf.Variable(tf.zeros_like(self.w))\n",
    "        self.act,self.d_act = act,d_act\n",
    "        \n",
    "    def getw(self): return self.w\n",
    "\n",
    "    def feedforward(self,input,stride=1,padding='SAME'):\n",
    "        self.input  = input\n",
    "        self.layer  = tf.nn.conv2d(input,self.w,strides=[1,stride,stride,1],padding=padding) \n",
    "        self.layerA = self.act(self.layer)\n",
    "        return self.layer,self.layerA\n",
    "    \n",
    "    def backprop(self,gradient,stride=1,padding='SAME'):\n",
    "        grad_part_1 = gradient\n",
    "        grad_part_2 = self.d_act(self.layer)\n",
    "        grad_part_3 = self.input\n",
    "\n",
    "        grad_middle = grad_part_1 * grad_part_2\n",
    "        grad        = tf.nn.conv2d_backprop_filter(input = grad_part_3,filter_sizes = tf.shape(self.w),  out_backprop = grad_middle,strides=[1,stride,stride,1],padding=padding) / batch_size\n",
    "        grad_pass   = tf.nn.conv2d_backprop_input (input_sizes = tf.shape(self.input),filter= self.w,out_backprop = grad_middle,strides=[1,stride,stride,1],padding=padding)\n",
    "\n",
    "        update_w = []\n",
    "        update_w.append(tf.assign( self.m,self.m*beta1 + (1-beta1) * (grad)   ))\n",
    "        update_w.append(tf.assign( self.v,self.v*beta2 + (1-beta2) * (grad ** 2)   ))\n",
    "        m_hat = self.m / (1-beta1) ; v_hat = self.v / (1-beta2)\n",
    "        adam_middle = m_hat * learning_rate/(tf.sqrt(v_hat) + adam_e)\n",
    "        update_w.append(tf.assign(self.w,tf.subtract(self.w,adam_middle  )))\n",
    "        \n",
    "        return grad_pass,grad,update_w\n",
    "    \n",
    "class tf_batch_norm_layer():\n",
    "    \n",
    "    def __init__(self,vector_shape,axis):\n",
    "        self.moving_mean = tf.Variable(tf.zeros(shape=[1,1,1,vector_shape],dtype=tf.float32))\n",
    "        self.moving_vari = tf.Variable(tf.zeros(shape=[1,1,1,vector_shape],dtype=tf.float32))\n",
    "        self.axis        = axis\n",
    "        \n",
    "    def feedforward(self,input,training_phase=True,eps = 1e-8):\n",
    "        self.input = input\n",
    "        self.input_size          = self.input.shape\n",
    "        self.batch,self.h,self.w,self.c = self.input_size[0].value,self.input_size[1].value,self.input_size[2].value,self.input_size[3].value\n",
    "\n",
    "        # Training Moving Average Mean         \n",
    "        def training_fn():\n",
    "            self.mean    = tf.reduce_mean(self.input,axis=self.axis ,keepdims=True)\n",
    "            self.var     = tf.reduce_mean(tf.square(self.input-self.mean),axis=self.axis,keepdims=True)\n",
    "            centered_data= (self.input - self.mean)/tf.sqrt(self.var + eps)\n",
    "            \n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean*0.9 + 0.1 * self.mean ))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari*0.9 + 0.1 * self.var  ))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        # Testing Moving Average Mean        \n",
    "        def  testing_fn():\n",
    "            centered_data   = (self.input - self.moving_mean)/tf.sqrt(self.moving_vari + eps)\n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        self.output,update_variable = tf.cond(training_phase,true_fn=training_fn,false_fn=testing_fn)\n",
    "        return self.output,update_variable\n",
    "    \n",
    "    def backprop(self,grad,eps = 1e-8):\n",
    "        change_parts = 1.0 /(self.batch * self.h * self.w)\n",
    "        grad_sigma   = tf.reduce_sum( grad *  (self.input-self.mean)     ,axis=self.axis,keepdims=True) * -0.5 * (self.var+eps) ** -1.5\n",
    "        grad_mean    = tf.reduce_sum( grad *  (-1./tf.sqrt(self.var+eps)),axis=self.axis,keepdims=True) + grad_sigma * change_parts * 2.0 * tf.reduce_sum((self.input-self.mean),axis=self.axis,keepdims=True) * -1\n",
    "        grad_x       = grad * 1/(tf.sqrt(self.var+eps)) + grad_sigma * change_parts * 2.0 * (self.input-self.mean) + grad_mean * change_parts\n",
    "        return grad_x\n",
    "class tf_layer_norm_layer():\n",
    "    \n",
    "    def __init__(self,vector_shape,axis):\n",
    "        self.moving_mean = tf.Variable(tf.zeros(shape=[vector_shape,1,1,1],dtype=tf.float32))\n",
    "        self.moving_vari = tf.Variable(tf.zeros(shape=[vector_shape,1,1,1],dtype=tf.float32))\n",
    "        self.axis        = axis\n",
    "        \n",
    "    def feedforward(self,input,training_phase=True,eps = 1e-8):\n",
    "        self.input = input\n",
    "        self.input_size          = self.input.shape\n",
    "        self.batch,self.h,self.w,self.c = self.input_size[0].value,self.input_size[1].value,self.input_size[2].value,self.input_size[3].value\n",
    "\n",
    "        # Training Moving Average Mean         \n",
    "        def training_fn():\n",
    "            self.mean    = tf.reduce_mean(self.input,axis=self.axis ,keepdims=True)\n",
    "            self.var     = tf.reduce_mean(tf.square(self.input-self.mean),axis=self.axis,keepdims=True)\n",
    "            centered_data= (self.input - self.mean)/tf.sqrt(self.var + eps)\n",
    "            \n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean*0.9 + 0.1 * self.mean ))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari*0.9 + 0.1 * self.var  ))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        # Testing Moving Average Mean        \n",
    "        def  testing_fn():\n",
    "            centered_data   = (self.input - self.moving_mean)/tf.sqrt(self.moving_vari + eps)\n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        self.output,update_variable = tf.cond(training_phase,true_fn=training_fn,false_fn=testing_fn)\n",
    "        return self.output,update_variable\n",
    "    \n",
    "    def backprop(self,grad,eps = 1e-8):\n",
    "        change_parts = 1.0 /(self.h * self.w * self.c)\n",
    "        grad_sigma   = tf.reduce_sum( grad *  (self.input-self.mean)     ,axis=self.axis,keepdims=True) * -0.5 * (self.var+eps) ** -1.5\n",
    "        grad_mean    = tf.reduce_sum( grad *  (-1./tf.sqrt(self.var+eps)),axis=self.axis,keepdims=True) + grad_sigma * change_parts * 2.0 * tf.reduce_sum((self.input-self.mean),axis=self.axis,keepdims=True) * -1\n",
    "        grad_x       = grad * 1/(tf.sqrt(self.var+eps)) + grad_sigma * change_parts * 2.0 * (self.input-self.mean) + grad_mean * change_parts\n",
    "        return grad_x\n",
    "class tf_instance_norm_layer():\n",
    "    \n",
    "    def __init__(self,batch_size,vector_shape,axis):\n",
    "        self.moving_mean = tf.Variable(tf.zeros(shape=[batch_size,1,1,vector_shape],dtype=tf.float32))\n",
    "        self.moving_vari = tf.Variable(tf.zeros(shape=[batch_size,1,1,vector_shape],dtype=tf.float32))\n",
    "        self.axis        = axis\n",
    "        \n",
    "    def feedforward(self,input,training_phase=True,eps = 1e-8):\n",
    "        self.input = input\n",
    "        self.input_size          = self.input.shape\n",
    "        self.batch,self.h,self.w,self.c = self.input_size[0].value,self.input_size[1].value,self.input_size[2].value,self.input_size[3].value\n",
    "\n",
    "        # Training Moving Average Mean         \n",
    "        def training_fn():\n",
    "            self.mean    = tf.reduce_mean(self.input,axis=self.axis ,keepdims=True)\n",
    "            self.var     = tf.reduce_mean(tf.square(self.input-self.mean),axis=self.axis,keepdims=True)\n",
    "            centered_data= (self.input - self.mean)/tf.sqrt(self.var + eps)\n",
    "            \n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean*0.9 + 0.1 * self.mean ))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari*0.9 + 0.1 * self.var  ))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        # Testing Moving Average Mean        \n",
    "        def  testing_fn():\n",
    "            centered_data   = (self.input - self.moving_mean)/tf.sqrt(self.moving_vari + eps)\n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        self.output,update_variable = tf.cond(training_phase,true_fn=training_fn,false_fn=testing_fn)\n",
    "        return self.output,update_variable\n",
    "    \n",
    "    def backprop(self,grad,eps = 1e-8):\n",
    "        change_parts = 1.0 /(self.h * self.w)\n",
    "        grad_sigma   = tf.reduce_sum( grad *  (self.input-self.mean)     ,axis=self.axis,keepdims=True) * -0.5 * (self.var+eps) ** -1.5\n",
    "        grad_mean    = tf.reduce_sum( grad *  (-1./tf.sqrt(self.var+eps)),axis=self.axis,keepdims=True) + grad_sigma * change_parts * 2.0 * tf.reduce_sum((self.input-self.mean),axis=self.axis,keepdims=True) * -1\n",
    "        grad_x       = grad * 1/(tf.sqrt(self.var+eps)) + grad_sigma * change_parts * 2.0 * (self.input-self.mean) + grad_mean * change_parts\n",
    "        return grad_x\n",
    "class tf_box_cox():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.lmbda    = tf.Variable(2.0) \n",
    "        self.m,self.v = tf.Variable(tf.zeros_like(self.lmbda)),tf.Variable(tf.zeros_like(self.lmbda))\n",
    "    def getw(self): return self.lmbda\n",
    "    \n",
    "    def feedforward(self,data):\n",
    "        self.input = data\n",
    "        self.layer = tf.pow((self.input + 1.0),self.lmbda)\n",
    "        return (self.layer - 1.0)/(self.lmbda + 1e-8)\n",
    "    \n",
    "    def backprop(self,grad):\n",
    "        \n",
    "        # Gradient that gets passed along\n",
    "        grad_pass = tf.pow((self.input + 1),self.lmbda-1.0) * grad\n",
    "        \n",
    "        # Grad respect to the lmbda value (not tested!)\n",
    "        grad_lmbda1 =   (self.layer * tf.log(self.input + 1 ))/(self.lmbda + 1e-8)\n",
    "        grad_lmbda2 = - (self.layer - 1)/(self.lmbda ** 2 + 1e-8)\n",
    "        grad_lmbda  = tf.reduce_mean((grad_lmbda1 + grad_lmbda2)*grad)\n",
    "\n",
    "        update_w = []\n",
    "        update_w.append(tf.assign( self.m,self.m*beta1 + (1-beta1) * (grad_lmbda)   ))\n",
    "        update_w.append(tf.assign( self.v,self.v*beta2 + (1-beta2) * (grad_lmbda ** 2)   ))\n",
    "        m_hat = self.m / (1-beta1) ; v_hat = self.v / (1-beta2)\n",
    "        adam_middle = m_hat * learning_rate/(tf.sqrt(v_hat) + adam_e)\n",
    "        update_w.append(tf.assign(self.lmbda,tf.subtract(self.lmbda,adam_middle  )))\n",
    "        \n",
    "        return grad_pass,grad_lmbda,update_w\n",
    "    \n",
    "def save_to_image(data,name):\n",
    "    l1g,l2g,l3g,l4g,l5g,l6g = data\n",
    "    l1g,l2g,l3g,l4g,l5g,l6g = np.asarray(l1g),np.asarray(l2g),np.asarray(l3g),np.asarray(l4g),np.asarray(l5g),np.asarray(l6g)\n",
    "    plt.figure(figsize=(25,15))\n",
    "    plt.suptitle('Current Iter : ' + str(iter))\n",
    "    plt.subplot(231); plt.hist(l1g.ravel(),50); plt.title('layer 1')\n",
    "    plt.subplot(232); plt.hist(l2g.ravel(),50); plt.title('layer 2')\n",
    "    plt.subplot(233); plt.hist(l3g.ravel(),50); plt.title('layer 3')\n",
    "    plt.subplot(234); plt.hist(l4g.ravel(),50); plt.title('layer 4')\n",
    "    plt.subplot(235); plt.hist(l5g.ravel(),50); plt.title('layer 5')\n",
    "    plt.subplot(236); plt.hist(l6g.ravel(),50); plt.title('layer 6')\n",
    "    plt.savefig(name + str(iter)+'.png')\n",
    "    plt.tight_layout()\n",
    "    plt.close('all')     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T23:38:22.207478Z",
     "start_time": "2018-12-20T23:38:22.196410Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# set hyper parameter\n",
    "num_epoch = 200; learning_rate = 0.0008; batch_size = 20; beta1,beta2,adam_e = 0.9,0.999,1e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T00:26:44.047540Z",
     "start_time": "2018-12-20T23:56:40.621136Z"
    },
    "code_folding": [
     0,
     45,
     58
    ],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Iter : 0/200 batch : 7980/8000 acc : 0.25\n",
      " Current : 0 Acc : 0.11400000230967998 Test Acc : 0.18837500314228237\n",
      "\n",
      "Current Iter : 1/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 1 Acc : 0.18660000313818453 Test Acc : 0.23000000271946192\n",
      "\n",
      "Current Iter : 2/200 batch : 7980/8000 acc : 0.25\n",
      " Current : 2 Acc : 0.22940000292658805 Test Acc : 0.2683750024437904\n",
      "\n",
      "Current Iter : 3/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 3 Acc : 0.2522000029683113 Test Acc : 0.2903750020917505\n",
      "\n",
      "Current Iter : 4/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 4 Acc : 0.27060000267624856 Test Acc : 0.29937500176019965\n",
      "\n",
      "Current Iter : 5/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 5 Acc : 0.28920000267028806 Test Acc : 0.30387500166893006\n",
      "\n",
      "Current Iter : 6/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 6 Acc : 0.29280000226199626 Test Acc : 0.30712500154040756\n",
      "\n",
      "Current Iter : 7/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 7 Acc : 0.294200002014637 Test Acc : 0.31300000195391475\n",
      "\n",
      "Current Iter : 8/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 8 Acc : 0.3004000019729137 Test Acc : 0.3180000018142164\n",
      "\n",
      "Current Iter : 9/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 9 Acc : 0.3094000017642975 Test Acc : 0.3217500014975667\n",
      "\n",
      "Current Iter : 10/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 10 Acc : 0.3150000014454126 Test Acc : 0.32937500181607904\n",
      "\n",
      "Current Iter : 11/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 11 Acc : 0.3204000016748905 Test Acc : 0.3350000024959445\n",
      "\n",
      "Current Iter : 12/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 12 Acc : 0.324200002104044 Test Acc : 0.3366250019706786\n",
      "\n",
      "Current Iter : 13/200 batch : 7980/8000 acc : 0.25\n",
      " Current : 13 Acc : 0.3308000017702579 Test Acc : 0.33925000165589153\n",
      "\n",
      "Current Iter : 14/200 batch : 7980/8000 acc : 0.25\n",
      " Current : 14 Acc : 0.33380000209808347 Test Acc : 0.3436250019911677\n",
      "\n",
      "Current Iter : 15/200 batch : 7980/8000 acc : 0.25\n",
      " Current : 15 Acc : 0.33400000125169754 Test Acc : 0.34512500163167714\n",
      "\n",
      "Current Iter : 16/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 16 Acc : 0.33760000151395797 Test Acc : 0.3492500015348196\n",
      "\n",
      "Current Iter : 17/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 17 Acc : 0.33940000176429747 Test Acc : 0.34900000180117785\n",
      "\n",
      "Current Iter : 18/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 18 Acc : 0.3438000020980835 Test Acc : 0.347875001905486\n",
      "\n",
      "Current Iter : 19/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 19 Acc : 0.3472000020891428 Test Acc : 0.3496250008419156\n",
      "\n",
      "Current Iter : 20/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 20 Acc : 0.3504000014960766 Test Acc : 0.3543750006984919\n",
      "\n",
      "Current Iter : 21/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 21 Acc : 0.35400000217556954 Test Acc : 0.3550000005122274\n",
      "\n",
      "Current Iter : 22/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 22 Acc : 0.3576000019609928 Test Acc : 0.35587500107474623\n",
      "\n",
      "Current Iter : 23/200 batch : 7980/8000 acc : 0.25\n",
      " Current : 23 Acc : 0.3610000019967556 Test Acc : 0.3596250008791685\n",
      "\n",
      "Current Iter : 24/200 batch : 7980/8000 acc : 0.25\n",
      " Current : 24 Acc : 0.36220000152289866 Test Acc : 0.36000000111758707\n",
      "\n",
      "Current Iter : 25/200 batch : 7980/8000 acc : 0.25\n",
      " Current : 25 Acc : 0.36800000140070915 Test Acc : 0.36300000119954345\n",
      "\n",
      "Current Iter : 26/200 batch : 7980/8000 acc : 0.25\n",
      " Current : 26 Acc : 0.36840000113844873 Test Acc : 0.36487500090152025\n",
      "\n",
      "Current Iter : 27/200 batch : 7980/8000 acc : 0.25\n",
      " Current : 27 Acc : 0.37400000089406965 Test Acc : 0.3673750007338822\n",
      "\n",
      "Current Iter : 28/200 batch : 7980/8000 acc : 0.25\n",
      " Current : 28 Acc : 0.3750000008046627 Test Acc : 0.36625000069849195\n",
      "\n",
      "Current Iter : 29/200 batch : 7980/8000 acc : 0.25\n",
      " Current : 29 Acc : 0.37560000115633013 Test Acc : 0.3681250007171184\n",
      "\n",
      "Current Iter : 30/200 batch : 7980/8000 acc : 0.25\n",
      " Current : 30 Acc : 0.3792000012695789 Test Acc : 0.3708750005904585\n",
      "\n",
      "Current Iter : 31/200 batch : 7980/8000 acc : 0.25\n",
      " Current : 31 Acc : 0.3838000013232231 Test Acc : 0.3712500004004687\n",
      "\n",
      "Current Iter : 32/200 batch : 7980/8000 acc : 0.25\n",
      " Current : 32 Acc : 0.3848000010251999 Test Acc : 0.3717500007059425\n",
      "\n",
      "Current Iter : 33/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 33 Acc : 0.38760000041127207 Test Acc : 0.37162500105798246\n",
      "\n",
      "Current Iter : 34/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 34 Acc : 0.39260000130534173 Test Acc : 0.37325000135228037\n",
      "\n",
      "Current Iter : 35/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 35 Acc : 0.39480000203847887 Test Acc : 0.37275000136345626\n",
      "\n",
      "Current Iter : 36/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 36 Acc : 0.39360000169277193 Test Acc : 0.3718750013783574\n",
      "\n",
      "Current Iter : 37/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 37 Acc : 0.39860000187158584 Test Acc : 0.3722500015050173\n",
      "\n",
      "Current Iter : 38/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 38 Acc : 0.4020000014901161 Test Acc : 0.3733750016056001\n",
      "\n",
      "Current Iter : 39/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 39 Acc : 0.40120000195503236 Test Acc : 0.37362500121816994\n",
      "\n",
      "Current Iter : 40/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 40 Acc : 0.4052000013887882 Test Acc : 0.37312500094994905\n",
      "\n",
      "Current Iter : 41/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 41 Acc : 0.4074000016152859 Test Acc : 0.3742500009015203\n",
      "\n",
      "Current Iter : 42/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 42 Acc : 0.41000000163912775 Test Acc : 0.3791250013746321\n",
      "\n",
      "Current Iter : 43/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 43 Acc : 0.4110000011622906 Test Acc : 0.37850000146776436\n",
      "\n",
      "Current Iter : 44/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 44 Acc : 0.4124000008404255 Test Acc : 0.3808750015310943\n",
      "\n",
      "Current Iter : 45/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 45 Acc : 0.41480000033974646 Test Acc : 0.38125000132247805\n",
      "\n",
      "Current Iter : 46/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 46 Acc : 0.4162000001370907 Test Acc : 0.3822500017751008\n",
      "\n",
      "Current Iter : 47/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 47 Acc : 0.416400001257658 Test Acc : 0.38250000183470545\n",
      "\n",
      "Current Iter : 48/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 48 Acc : 0.41580000093579295 Test Acc : 0.3832500015385449\n",
      "\n",
      "Current Iter : 49/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 49 Acc : 0.41880000099539755 Test Acc : 0.38300000156275926\n",
      "\n",
      "Current Iter : 50/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 50 Acc : 0.4200000011026859 Test Acc : 0.3841250011138618\n",
      "\n",
      "Current Iter : 51/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 51 Acc : 0.4236000008285046 Test Acc : 0.38350000127218664\n",
      "\n",
      "Current Iter : 52/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 52 Acc : 0.42460000088810923 Test Acc : 0.38550000042654575\n",
      "\n",
      "Current Iter : 53/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 53 Acc : 0.42560000029206274 Test Acc : 0.3846250008419156\n",
      "\n",
      "Current Iter : 54/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 54 Acc : 0.42760000041127205 Test Acc : 0.385750000923872\n",
      "\n",
      "Current Iter : 55/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 55 Acc : 0.4298000002801418 Test Acc : 0.3861250007711351\n",
      "\n",
      "Current Iter : 56/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 56 Acc : 0.43099999997019767 Test Acc : 0.386000000834465\n",
      "\n",
      "Current Iter : 57/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 57 Acc : 0.4316000001728535 Test Acc : 0.38900000070221724\n",
      "\n",
      "Current Iter : 58/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 58 Acc : 0.43400000044703485 Test Acc : 0.38812500044703485\n",
      "\n",
      "Current Iter : 59/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 59 Acc : 0.4356000002324581 Test Acc : 0.3906250003818423\n",
      "\n",
      "Current Iter : 60/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 60 Acc : 0.43800000062584876 Test Acc : 0.38950000039301813\n",
      "\n",
      "Current Iter : 61/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 61 Acc : 0.43740000066161155 Test Acc : 0.392500000083819\n",
      "\n",
      "Current Iter : 62/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 62 Acc : 0.43920000073313714 Test Acc : 0.39325000015087425\n",
      "\n",
      "Current Iter : 63/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 63 Acc : 0.44080000108480455 Test Acc : 0.3917500002216548\n",
      "\n",
      "Current Iter : 64/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 64 Acc : 0.4434000008702278 Test Acc : 0.39325000035576524\n",
      "\n",
      "Current Iter : 65/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 65 Acc : 0.444200000166893 Test Acc : 0.39162500084377827\n",
      "\n",
      "Current Iter : 66/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 66 Acc : 0.44599999994039535 Test Acc : 0.393375000404194\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Iter : 67/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 67 Acc : 0.44940000063180924 Test Acc : 0.3921250002551824\n",
      "\n",
      "Current Iter : 68/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 68 Acc : 0.4482000005841255 Test Acc : 0.39250000049360095\n",
      "\n",
      "Current Iter : 69/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 69 Acc : 0.4492000004649162 Test Acc : 0.39524999985471365\n",
      "\n",
      "Current Iter : 70/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 70 Acc : 0.4514000003933907 Test Acc : 0.3934999998472631\n",
      "\n",
      "Current Iter : 71/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 71 Acc : 0.4526000002026558 Test Acc : 0.39524999996647237\n",
      "\n",
      "Current Iter : 72/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 72 Acc : 0.454000001013279 Test Acc : 0.3931249995343387\n",
      "\n",
      "Current Iter : 73/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 73 Acc : 0.45519999992847443 Test Acc : 0.39549999963492155\n",
      "\n",
      "Current Iter : 74/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 74 Acc : 0.4568000010251999 Test Acc : 0.39574999989941717\n",
      "\n",
      "Current Iter : 75/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 75 Acc : 0.4578000006079674 Test Acc : 0.3952500000782311\n",
      "\n",
      "Current Iter : 76/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 76 Acc : 0.45920000088214874 Test Acc : 0.39475000033155083\n",
      "\n",
      "Current Iter : 77/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 77 Acc : 0.46200000077486036 Test Acc : 0.395250000692904\n",
      "\n",
      "Current Iter : 78/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 78 Acc : 0.46300000095367433 Test Acc : 0.3950000005401671\n",
      "\n",
      "Current Iter : 79/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 79 Acc : 0.4624000009894371 Test Acc : 0.3983750005438924\n",
      "\n",
      "Current Iter : 80/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 80 Acc : 0.46280000066757204 Test Acc : 0.39875000117346643\n",
      "\n",
      "Current Iter : 81/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 81 Acc : 0.4636000008583069 Test Acc : 0.39650000104680655\n",
      "\n",
      "Current Iter : 82/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 82 Acc : 0.46400000077486037 Test Acc : 0.3991250008530915\n",
      "\n",
      "Current Iter : 83/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 83 Acc : 0.4660000002384186 Test Acc : 0.3978750009089708\n",
      "\n",
      "Current Iter : 84/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 84 Acc : 0.46699999994039537 Test Acc : 0.39962500140070917\n",
      "\n",
      "Current Iter : 85/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 85 Acc : 0.4697999999523163 Test Acc : 0.39975000094622376\n",
      "\n",
      "Current Iter : 86/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 86 Acc : 0.47160000026226045 Test Acc : 0.4006250006146729\n",
      "\n",
      "Current Iter : 87/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 87 Acc : 0.4730000008940697 Test Acc : 0.39687500102445483\n",
      "\n",
      "Current Iter : 88/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 88 Acc : 0.4728000002503395 Test Acc : 0.39612500108778476\n",
      "\n",
      "Current Iter : 89/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 89 Acc : 0.4740000004172325 Test Acc : 0.3945000009611249\n",
      "\n",
      "Current Iter : 90/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 90 Acc : 0.4763999999761581 Test Acc : 0.39537500116974117\n",
      "\n",
      "Current Iter : 91/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 91 Acc : 0.47380000019073487 Test Acc : 0.39775000128895044\n",
      "\n",
      "Current Iter : 92/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 92 Acc : 0.4750000004172325 Test Acc : 0.3930000008456409\n",
      "\n",
      "Current Iter : 93/200 batch : 7980/8000 acc : 0.25\n",
      " Current : 93 Acc : 0.474800000667572 Test Acc : 0.39750000124797225\n",
      "\n",
      "Current Iter : 94/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 94 Acc : 0.4782000003457069 Test Acc : 0.39550000132992863\n",
      "\n",
      "Current Iter : 95/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 95 Acc : 0.4794000002741814 Test Acc : 0.39787500104866924\n",
      "\n",
      "Current Iter : 96/200 batch : 7980/8000 acc : 0.25\n",
      " Current : 96 Acc : 0.4798000001907349 Test Acc : 0.39550000104121863\n",
      "\n",
      "Current Iter : 97/200 batch : 7980/8000 acc : 0.25\n",
      " Current : 97 Acc : 0.47960000044107437 Test Acc : 0.39587500093504785\n",
      "\n",
      "Current Iter : 98/200 batch : 7980/8000 acc : 0.25\n",
      " Current : 98 Acc : 0.4822000008225441 Test Acc : 0.39575000101700425\n",
      "\n",
      "Current Iter : 99/200 batch : 7980/8000 acc : 0.25\n",
      " Current : 99 Acc : 0.48280000025033953 Test Acc : 0.39500000068917873\n",
      "\n",
      "Current Iter : 100/200 batch : 7980/8000 acc : 0.25\n",
      " Current : 100 Acc : 0.4848000003695488 Test Acc : 0.39787500103935597\n",
      "\n",
      "Current Iter : 101/200 batch : 7980/8000 acc : 0.25\n",
      " Current : 101 Acc : 0.4860000008940697 Test Acc : 0.39675000090152024\n",
      "\n",
      "Current Iter : 102/200 batch : 7980/8000 acc : 0.25\n",
      " Current : 102 Acc : 0.4860000004172325 Test Acc : 0.39712500079534946\n",
      "\n",
      "Current Iter : 103/200 batch : 7980/8000 acc : 0.25\n",
      " Current : 103 Acc : 0.48640000027418134 Test Acc : 0.3967500009480864\n",
      "\n",
      "Current Iter : 104/200 batch : 7980/8000 acc : 0.25\n",
      " Current : 104 Acc : 0.48780000060796735 Test Acc : 0.39875000054948034\n",
      "\n",
      "Current Iter : 105/200 batch : 7980/8000 acc : 0.25\n",
      " Current : 105 Acc : 0.49000000023841855 Test Acc : 0.39825000065378846\n",
      "\n",
      "Current Iter : 106/200 batch : 7980/8000 acc : 0.25\n",
      " Current : 106 Acc : 0.4904000006318092 Test Acc : 0.3986250005662441\n",
      "\n",
      "Current Iter : 107/200 batch : 7980/8000 acc : 0.25\n",
      " Current : 107 Acc : 0.4940000006556511 Test Acc : 0.4000000003445894\n",
      "\n",
      "Current Iter : 108/200 batch : 7980/8000 acc : 0.25\n",
      " Current : 108 Acc : 0.491800000846386 Test Acc : 0.3996250008512288\n",
      "\n",
      "Current Iter : 109/200 batch : 7980/8000 acc : 0.25\n",
      " Current : 109 Acc : 0.4908000003695488 Test Acc : 0.39775000083260237\n",
      "\n",
      "Current Iter : 110/200 batch : 7980/8000 acc : 0.25\n",
      " Current : 110 Acc : 0.49220000034570693 Test Acc : 0.39712500050663946\n",
      "\n",
      "Current Iter : 111/200 batch : 7980/8000 acc : 0.25\n",
      " Current : 111 Acc : 0.4936000009179115 Test Acc : 0.40050000060349705\n",
      "\n",
      "Current Iter : 112/200 batch : 7980/8000 acc : 0.25\n",
      " Current : 112 Acc : 0.49420000094175337 Test Acc : 0.4011250009480864\n",
      "\n",
      "Current Iter : 113/200 batch : 7980/8000 acc : 0.25\n",
      " Current : 113 Acc : 0.4944000000357628 Test Acc : 0.39975000055506826\n",
      "\n",
      "Current Iter : 114/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 114 Acc : 0.4962000008225441 Test Acc : 0.40037500075064597\n",
      "\n",
      "Current Iter : 115/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 115 Acc : 0.49500000089406965 Test Acc : 0.4015000010188669\n",
      "\n",
      "Current Iter : 116/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 116 Acc : 0.49700000101327896 Test Acc : 0.400625001238659\n",
      "\n",
      "Current Iter : 117/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 117 Acc : 0.4986000012755394 Test Acc : 0.40387500083073974\n",
      "\n",
      "Current Iter : 118/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 118 Acc : 0.5002000008225441 Test Acc : 0.40375000074505807\n",
      "\n",
      "Current Iter : 119/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 119 Acc : 0.5036000014543534 Test Acc : 0.40162500116974115\n",
      "\n",
      "Current Iter : 120/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 120 Acc : 0.5034000010490417 Test Acc : 0.40637500094249845\n",
      "\n",
      "Current Iter : 121/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 121 Acc : 0.5030000010132789 Test Acc : 0.403375000981614\n",
      "\n",
      "Current Iter : 122/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 122 Acc : 0.5038000017404556 Test Acc : 0.40537500076927246\n",
      "\n",
      "Current Iter : 123/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 123 Acc : 0.5050000011920929 Test Acc : 0.40375000118277965\n",
      "\n",
      "Current Iter : 124/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 124 Acc : 0.5060000013113022 Test Acc : 0.40450000085867943\n",
      "\n",
      "Current Iter : 125/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 125 Acc : 0.5064000014066696 Test Acc : 0.4038750011008233\n",
      "\n",
      "Current Iter : 126/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 126 Acc : 0.5068000013828278 Test Acc : 0.40412500088103115\n",
      "\n",
      "Current Iter : 127/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 127 Acc : 0.5060000023841857 Test Acc : 0.4030000005941838\n",
      "\n",
      "Current Iter : 128/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 128 Acc : 0.5082000014781952 Test Acc : 0.4033750006835908\n",
      "\n",
      "Current Iter : 129/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 129 Acc : 0.5098000019788742 Test Acc : 0.4023750008735806\n",
      "\n",
      "Current Iter : 130/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 130 Acc : 0.5124000004529953 Test Acc : 0.4026250007096678\n",
      "\n",
      "Current Iter : 131/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 131 Acc : 0.515400001168251 Test Acc : 0.4041250011045486\n",
      "\n",
      "Current Iter : 132/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 132 Acc : 0.5144000006914139 Test Acc : 0.4083750011678785\n",
      "\n",
      "Current Iter : 133/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 133 Acc : 0.5162000011205673 Test Acc : 0.4067500015255064\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Iter : 134/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 134 Acc : 0.5160000013113022 Test Acc : 0.40800000159069894\n",
      "\n",
      "Current Iter : 135/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 135 Acc : 0.5182000017166137 Test Acc : 0.40800000057555735\n",
      "\n",
      "Current Iter : 136/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 136 Acc : 0.517400001168251 Test Acc : 0.4076250012870878\n",
      "\n",
      "Current Iter : 137/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 137 Acc : 0.5198000015020371 Test Acc : 0.40550000097602606\n",
      "\n",
      "Current Iter : 138/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 138 Acc : 0.521400001168251 Test Acc : 0.4073750007897615\n",
      "\n",
      "Current Iter : 139/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 139 Acc : 0.5210000020265579 Test Acc : 0.40812500103376803\n",
      "\n",
      "Current Iter : 140/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 140 Acc : 0.5204000022411347 Test Acc : 0.4090000006556511\n",
      "\n",
      "Current Iter : 141/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 141 Acc : 0.5206000019907951 Test Acc : 0.4088750010356307\n",
      "\n",
      "Current Iter : 142/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 142 Acc : 0.5208000018596649 Test Acc : 0.4095000003091991\n",
      "\n",
      "Current Iter : 143/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 143 Acc : 0.5226000016927719 Test Acc : 0.409875000603497\n",
      "\n",
      "Current Iter : 144/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 144 Acc : 0.5208000006079674 Test Acc : 0.4100000009126961\n",
      "\n",
      "Current Iter : 145/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 145 Acc : 0.5232000008225441 Test Acc : 0.4102500009350479\n",
      "\n",
      "Current Iter : 146/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 146 Acc : 0.5226000002026558 Test Acc : 0.4107500013336539\n",
      "\n",
      "Current Iter : 147/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 147 Acc : 0.5264000003933906 Test Acc : 0.41012500105425714\n",
      "\n",
      "Current Iter : 148/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 148 Acc : 0.5258000004291534 Test Acc : 0.4105000016652048\n",
      "\n",
      "Current Iter : 149/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 149 Acc : 0.5258000018596649 Test Acc : 0.41050000144168736\n",
      "\n",
      "Current Iter : 150/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 150 Acc : 0.5280000013113022 Test Acc : 0.40975000146776436\n",
      "\n",
      "Current Iter : 151/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 151 Acc : 0.5286000012159348 Test Acc : 0.41250000171363355\n",
      "\n",
      "Current Iter : 152/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 152 Acc : 0.5292000010609627 Test Acc : 0.41312500175088646\n",
      "\n",
      "Current Iter : 153/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 153 Acc : 0.5286000007390976 Test Acc : 0.41300000140443444\n",
      "\n",
      "Current Iter : 154/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 154 Acc : 0.5308000015020371 Test Acc : 0.4095000012218952\n",
      "\n",
      "Current Iter : 155/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 155 Acc : 0.5332000025510788 Test Acc : 0.411875001359731\n",
      "\n",
      "Current Iter : 156/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 156 Acc : 0.5322000025510788 Test Acc : 0.41300000180490315\n",
      "\n",
      "Current Iter : 157/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 157 Acc : 0.5342000016570091 Test Acc : 0.4126250016968697\n",
      "\n",
      "Current Iter : 158/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 158 Acc : 0.5352000018358231 Test Acc : 0.4130000018514693\n",
      "\n",
      "Current Iter : 159/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 159 Acc : 0.5362000018954277 Test Acc : 0.4135000021662563\n",
      "\n",
      "Current Iter : 160/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 160 Acc : 0.5354000009298324 Test Acc : 0.41462500163353977\n",
      "\n",
      "Current Iter : 161/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 161 Acc : 0.538800001502037 Test Acc : 0.41350000170990825\n",
      "\n",
      "Current Iter : 162/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 162 Acc : 0.5362000013589859 Test Acc : 0.414750002194196\n",
      "\n",
      "Current Iter : 163/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 163 Acc : 0.5382000015974044 Test Acc : 0.4142500022705644\n",
      "\n",
      "Current Iter : 164/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 164 Acc : 0.5402000015974044 Test Acc : 0.4148750019073486\n",
      "\n",
      "Current Iter : 165/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 165 Acc : 0.5394000010490417 Test Acc : 0.4152500022109598\n",
      "\n",
      "Current Iter : 166/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 166 Acc : 0.5396000015139579 Test Acc : 0.4166250022407621\n",
      "\n",
      "Current Iter : 167/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 167 Acc : 0.54200000166893 Test Acc : 0.4155000020097941\n",
      "\n",
      "Current Iter : 168/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 168 Acc : 0.5444000016450882 Test Acc : 0.4160000018496066\n",
      "\n",
      "Current Iter : 169/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 169 Acc : 0.5428000013232231 Test Acc : 0.4107500024698675\n",
      "\n",
      "Current Iter : 170/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 170 Acc : 0.5438000013232231 Test Acc : 0.4118750025797635\n",
      "\n",
      "Current Iter : 171/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 171 Acc : 0.5444000020623208 Test Acc : 0.4116250025294721\n",
      "\n",
      "Current Iter : 172/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 172 Acc : 0.5488000019192696 Test Acc : 0.413875002656132\n",
      "\n",
      "Current Iter : 173/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 173 Acc : 0.5496000016927719 Test Acc : 0.41425000234507026\n",
      "\n",
      "Current Iter : 174/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 174 Acc : 0.5500000014305114 Test Acc : 0.4150000022538006\n",
      "\n",
      "Current Iter : 175/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 175 Acc : 0.5500000019073487 Test Acc : 0.4141250027064234\n",
      "\n",
      "Current Iter : 176/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 176 Acc : 0.5494000021219253 Test Acc : 0.41225000240840015\n",
      "\n",
      "Current Iter : 177/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 177 Acc : 0.5508000020980834 Test Acc : 0.41025000234134495\n",
      "\n",
      "Current Iter : 178/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 178 Acc : 0.550200001358986 Test Acc : 0.4108750026021153\n",
      "\n",
      "Current Iter : 179/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 179 Acc : 0.5512000008821487 Test Acc : 0.41162500272504987\n",
      "\n",
      "Current Iter : 180/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 180 Acc : 0.5514000008106231 Test Acc : 0.4121250029373914\n",
      "\n",
      "Current Iter : 181/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 181 Acc : 0.5524000009298324 Test Acc : 0.41187500254251064\n",
      "\n",
      "Current Iter : 182/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 182 Acc : 0.5536000012159348 Test Acc : 0.4092500021774322\n",
      "\n",
      "Current Iter : 183/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 183 Acc : 0.5532000006437302 Test Acc : 0.4101250020880252\n",
      "\n",
      "Current Iter : 184/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 184 Acc : 0.5544000006914139 Test Acc : 0.41087500234134494\n",
      "\n",
      "Current Iter : 185/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 185 Acc : 0.5542000008821487 Test Acc : 0.4131250023189932\n",
      "\n",
      "Current Iter : 186/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 186 Acc : 0.5568000018000603 Test Acc : 0.4127500023785979\n",
      "\n",
      "Current Iter : 187/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 187 Acc : 0.5616000009179115 Test Acc : 0.41312500247731804\n",
      "\n",
      "Current Iter : 188/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 188 Acc : 0.5596000022292137 Test Acc : 0.41400000230409206\n",
      "\n",
      "Current Iter : 189/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 189 Acc : 0.5598000013828277 Test Acc : 0.4140000025276095\n",
      "\n",
      "Current Iter : 190/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 190 Acc : 0.5592000007033348 Test Acc : 0.4136250025499612\n",
      "\n",
      "Current Iter : 191/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 191 Acc : 0.5614000019431115 Test Acc : 0.4127500028628856\n",
      "\n",
      "Current Iter : 192/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 192 Acc : 0.5584000006318093 Test Acc : 0.41362500202842056\n",
      "\n",
      "Current Iter : 193/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 193 Acc : 0.5618000004887581 Test Acc : 0.4137500017974526\n",
      "\n",
      "Current Iter : 194/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 194 Acc : 0.5628000009059906 Test Acc : 0.4143750021699816\n",
      "\n",
      "Current Iter : 195/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 195 Acc : 0.5620000009536743 Test Acc : 0.41075000223703684\n",
      "\n",
      "Current Iter : 196/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 196 Acc : 0.5620000004768372 Test Acc : 0.4125000018719584\n",
      "\n",
      "Current Iter : 197/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 197 Acc : 0.5624000005722046 Test Acc : 0.4111250021774322\n",
      "\n",
      "Current Iter : 198/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 198 Acc : 0.5640000005960465 Test Acc : 0.4118750020954758\n",
      "\n",
      "Current Iter : 199/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 199 Acc : 0.5648000010251999 Test Acc : 0.41175000187940897\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Normal CNN \n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# create layers\n",
    "l1 = CNN(3,3, 16); \n",
    "l2 = CNN(3,16,16); \n",
    "l3 = CNN(3,16,16); \n",
    "\n",
    "l4 = CNN(3,16,16); \n",
    "l5 = CNN(3,16,16); \n",
    "l6 = CNN(3,16,10); \n",
    "\n",
    "# 2. graph \n",
    "x = tf.placeholder(tf.float32,(batch_size,96,96,3))\n",
    "y = tf.placeholder(tf.float32,(batch_size,10))\n",
    "\n",
    "layer1, layer1a = l1. feedforward(x,stride=2)\n",
    "layer2, layer2a = l2. feedforward(layer1a,stride=2)\n",
    "layer3, layer3a = l3. feedforward(layer2a,stride=2)\n",
    "layer4, layer4a = l4. feedforward(layer3a,stride=2)\n",
    "layer5, layer5a = l5. feedforward(layer4a)\n",
    "layer6, layer6a = l6. feedforward(layer5a)\n",
    "\n",
    "final_layer   = tf.reduce_mean(layer6a,(1,2))\n",
    "final_softmax = tf_softmax(final_layer)\n",
    "cost          = -tf.reduce_mean(y * tf.log(final_softmax + 1e-8))\n",
    "correct_prediction = tf.equal(tf.argmax(final_softmax, 1), tf.argmax(y, 1))\n",
    "accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "gradient = tf.tile((final_softmax-y)[:,None,None,:],[1,6,6,1])/batch_size\n",
    "grad6p,grad6w,grad6_up = l6.backprop(gradient)\n",
    "grad5p,grad5w,grad5_up = l5.backprop(grad6p)\n",
    "grad4p,grad4w,grad4_up = l4.backprop(grad5p,stride=2)\n",
    "grad3p,grad3w,grad3_up = l3.backprop(grad4p,stride=2)\n",
    "grad2p,grad2w,grad2_up = l2.backprop(grad3p,stride=2)\n",
    "grad1p,grad1w,grad1_up = l1.backprop(grad2p,stride=2)\n",
    "\n",
    "gradient_update = grad6_up + grad5_up + grad4_up + grad3_up + grad2_up + grad1_up \n",
    "\n",
    "# train\n",
    "sess.run(tf.global_variables_initializer())\n",
    "avg_acc_train = 0; avg_acc_test  = 0; \n",
    "train_acc = [];test_acc = []\n",
    "for iter in range(num_epoch):\n",
    "\n",
    "    for current_batch_index in range(0,len(train_images),batch_size):\n",
    "        current_data  = train_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = train_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy,gradient_update],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(train_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_train = avg_acc_train + sess_results[0]\n",
    "        \n",
    "    # Get weights\n",
    "    save_to_image(sess.run([l1.getw(),l2.getw(),l3.getw(),l4.getw(),l5.getw(),l6.getw()]),'Normal/weights/')\n",
    "    save_to_image(sess.run([grad1w,grad2w,grad3w,grad4w,grad5w,grad6w],feed_dict={x:current_data,y:current_label}),'Normal/gradientw/')\n",
    "    save_to_image(sess.run([grad1p,grad2p,grad3p,grad4p,grad5p,grad6p],feed_dict={x:current_data,y:current_label}),'Normal/gradientp/')\n",
    "    save_to_image(sess.run([grad1_up,grad2_up,grad3_up,grad4_up,grad5_up,grad6_up],feed_dict={x:current_data,y:current_label}),'Normal/gradient_update/')\n",
    "        \n",
    "    for current_batch_index in range(0,len(test_images), batch_size):\n",
    "        current_data  = test_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = test_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(test_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_test = avg_acc_test + sess_results[0]   \n",
    "        \n",
    "    print(\"\\n Current : \"+ str(iter) + \" Acc : \" + str(avg_acc_train/(len(train_images)/batch_size)) + \" Test Acc : \" + str(avg_acc_test/(len(test_images)/batch_size)) + '\\n')\n",
    "    \n",
    "    # save the training\n",
    "    train_acc.append(avg_acc_train/(len(train_images)/batch_size))\n",
    "    test_acc .append(avg_acc_test / (len(test_images)/batch_size))\n",
    "    avg_acc_train = 0 ; avg_acc_test  = 0\n",
    "    \n",
    "np.save('Normal/train.npy',train_acc)\n",
    "np.save('Normal/test.npy', test_acc)    \n",
    "sess.close()\n",
    "tf.reset_default_graph();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-20T23:57:02.051Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Iter : 0/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 0 Acc : 0.2766000022739172 Test Acc : 0.296625001989305\n",
      "\n",
      "Current Iter : 1/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 1 Acc : 0.35040000222623346 Test Acc : 0.36412500159814953\n",
      "\n",
      "Current Iter : 2/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 2 Acc : 0.386200001552701 Test Acc : 0.38925000132061544\n",
      "\n",
      "Current Iter : 3/200 batch : 7980/8000 acc : 0.55\n",
      " Current : 3 Acc : 0.4256000012159348 Test Acc : 0.4056250012293458\n",
      "\n",
      "Current Iter : 4/200 batch : 7980/8000 acc : 0.55\n",
      " Current : 4 Acc : 0.4468000010251999 Test Acc : 0.401250001527369\n",
      "\n",
      "Current Iter : 5/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 5 Acc : 0.4640000008046627 Test Acc : 0.42287500207312406\n",
      "\n",
      "Current Iter : 6/200 batch : 7980/8000 acc : 0.55\n",
      " Current : 6 Acc : 0.4894000007510185 Test Acc : 0.4295000021252781\n",
      "\n",
      "Current Iter : 7/200 batch : 7980/8000 acc : 0.55\n",
      " Current : 7 Acc : 0.5056000011563301 Test Acc : 0.4326250012777746\n",
      "\n",
      "Current Iter : 8/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 8 Acc : 0.5252000007033348 Test Acc : 0.4442500014975667\n",
      "\n",
      "Current Iter : 9/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 9 Acc : 0.5406000008583068 Test Acc : 0.4570000011473894\n",
      "\n",
      "Current Iter : 10/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 10 Acc : 0.5544000008106231 Test Acc : 0.46700000163167715\n",
      "\n",
      "Current Iter : 11/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 11 Acc : 0.57 Test Acc : 0.46062500153668223\n",
      "\n",
      "Current Iter : 12/200 batch : 7980/8000 acc : 0.55\n",
      " Current : 12 Acc : 0.5814000010490418 Test Acc : 0.4511250007804483\n",
      "\n",
      "Current Iter : 13/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 13 Acc : 0.5918000010251999 Test Acc : 0.4648750017117709\n",
      "\n",
      "Current Iter : 14/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 14 Acc : 0.5972000012397766 Test Acc : 0.4750000018347055\n",
      "\n",
      "Current Iter : 15/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 15 Acc : 0.6050000010728837 Test Acc : 0.48275000154972075\n",
      "\n",
      "Current Iter : 16/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 16 Acc : 0.6114000008106232 Test Acc : 0.48475000135600566\n",
      "\n",
      "Current Iter : 17/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 17 Acc : 0.6192000010013581 Test Acc : 0.48100000105798246\n",
      "\n",
      "Current Iter : 18/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 18 Acc : 0.6230000001192093 Test Acc : 0.4810000013560057\n",
      "\n",
      "Current Iter : 19/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 19 Acc : 0.6349999972581863 Test Acc : 0.48175000105053184\n",
      "\n",
      "Current Iter : 20/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 20 Acc : 0.6411999979019165 Test Acc : 0.48175000194460155\n",
      "\n",
      "Current Iter : 21/200 batch : 7980/8000 acc : 0.55\n",
      " Current : 21 Acc : 0.6503999980688095 Test Acc : 0.47750000055879355\n",
      "\n",
      "Current Iter : 22/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 22 Acc : 0.657599999666214 Test Acc : 0.4742500003799796\n",
      "\n",
      "Current Iter : 23/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 23 Acc : 0.664799998998642 Test Acc : 0.4712500010803342\n",
      "\n",
      "Current Iter : 24/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 24 Acc : 0.6693999999761582 Test Acc : 0.4720000003278255\n",
      "\n",
      "Current Iter : 25/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 25 Acc : 0.671799998998642 Test Acc : 0.47250000059604647\n",
      "\n",
      "Current Iter : 26/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 26 Acc : 0.6793999998569489 Test Acc : 0.4695000010728836\n",
      "\n",
      "Current Iter : 27/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 27 Acc : 0.683399998664856 Test Acc : 0.4632500008866191\n",
      "\n",
      "Current Iter : 28/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 28 Acc : 0.6905999995470047 Test Acc : 0.4692500014975667\n",
      "\n",
      "Current Iter : 29/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 29 Acc : 0.6949999997615814 Test Acc : 0.4636250005662441\n",
      "\n",
      "Current Iter : 30/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 30 Acc : 0.695400000333786 Test Acc : 0.4672500016354024\n",
      "\n",
      "Current Iter : 31/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 31 Acc : 0.7052000008821487 Test Acc : 0.46412500156089664\n",
      "\n",
      "Current Iter : 32/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 32 Acc : 0.711399999499321 Test Acc : 0.46337500220164657\n",
      "\n",
      "Current Iter : 33/200 batch : 7980/8000 acc : 0.55\n",
      " Current : 33 Acc : 0.7170000004768372 Test Acc : 0.46387500189244746\n",
      "\n",
      "Current Iter : 34/200 batch : 7980/8000 acc : 0.55\n",
      " Current : 34 Acc : 0.7205999993085861 Test Acc : 0.46150000248104334\n",
      "\n",
      "Current Iter : 35/200 batch : 7980/8000 acc : 0.55\n",
      " Current : 35 Acc : 0.7260000001192093 Test Acc : 0.4635000016540289\n",
      "\n",
      "Current Iter : 36/200 batch : 7980/8000 acc : 0.55\n",
      " Current : 36 Acc : 0.7329999996423722 Test Acc : 0.46200000166893007\n",
      "\n",
      "Current Iter : 37/200 batch : 7980/8000 acc : 0.55\n",
      " Current : 37 Acc : 0.739599999666214 Test Acc : 0.4660000007972121\n",
      "\n",
      "Current Iter : 38/200 batch : 7980/8000 acc : 0.55\n",
      " Current : 38 Acc : 0.7434000005722046 Test Acc : 0.4678750012628734\n",
      "\n",
      "Current Iter : 39/200 batch : 7980/8000 acc : 0.55\n",
      " Current : 39 Acc : 0.75 Test Acc : 0.4688750021345913\n",
      "\n",
      "Current Iter : 40/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 40 Acc : 0.7517999989986419 Test Acc : 0.4690000018663704\n",
      "\n",
      "Current Iter : 41/200 batch : 7980/8000 acc : 0.55\n",
      " Current : 41 Acc : 0.7560000007152557 Test Acc : 0.46800000151619314\n",
      "\n",
      "Current Iter : 42/200 batch : 7980/8000 acc : 0.55\n",
      " Current : 42 Acc : 0.7664000008106232 Test Acc : 0.47050000181421636\n",
      "\n",
      "Current Iter : 43/200 batch : 7980/8000 acc : 0.65\n",
      " Current : 43 Acc : 0.7712000012397766 Test Acc : 0.46650000227615235\n",
      "\n",
      "Current Iter : 44/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 44 Acc : 0.7808000009059906 Test Acc : 0.4678750024549663\n",
      "\n",
      "Current Iter : 45/200 batch : 7980/8000 acc : 0.55\n",
      " Current : 45 Acc : 0.78300000166893 Test Acc : 0.46787500189617276\n",
      "\n",
      "Current Iter : 46/200 batch : 7980/8000 acc : 0.55\n",
      " Current : 46 Acc : 0.7868000018596649 Test Acc : 0.4643750022444874\n",
      "\n",
      "Current Iter : 47/200 batch : 7980/8000 acc : 0.55\n",
      " Current : 47 Acc : 0.7908000006675721 Test Acc : 0.4620000019762665\n",
      "\n",
      "Current Iter : 48/200 batch : 7980/8000 acc : 0.55\n",
      " Current : 48 Acc : 0.7992000002861023 Test Acc : 0.4607500020135194\n",
      "\n",
      "Current Iter : 49/200 batch : 7980/8000 acc : 0.55\n",
      " Current : 49 Acc : 0.8018000018596649 Test Acc : 0.4615000021643937\n",
      "\n",
      "Current Iter : 50/200 batch : 7980/8000 acc : 0.55\n",
      " Current : 50 Acc : 0.8082000012397766 Test Acc : 0.460875001642853\n",
      "\n",
      "Current Iter : 51/200 batch : 7980/8000 acc : 0.55\n",
      " Current : 51 Acc : 0.8134000022411346 Test Acc : 0.4635000022687018\n",
      "\n",
      "Current Iter : 52/200 batch : 7980/8000 acc : 0.55\n",
      " Current : 52 Acc : 0.81800000166893 Test Acc : 0.4582500012777746\n",
      "\n",
      "Current Iter : 53/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 53 Acc : 0.8204000012874604 Test Acc : 0.4575000007636845\n",
      "\n",
      "Current Iter : 54/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 54 Acc : 0.8216000015735626 Test Acc : 0.45250000117346645\n",
      "\n",
      "Current Iter : 55/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 55 Acc : 0.8258000023365021 Test Acc : 0.45000000240281224\n",
      "\n",
      "Current Iter : 56/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 56 Acc : 0.8320000019073487 Test Acc : 0.45137500097975136\n",
      "\n",
      "Current Iter : 57/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 57 Acc : 0.8386000015735626 Test Acc : 0.44775000100955364\n",
      "\n",
      "Current Iter : 58/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 58 Acc : 0.8405999999046325 Test Acc : 0.4435000012628734\n",
      "\n",
      "Current Iter : 59/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 59 Acc : 0.8410000004768372 Test Acc : 0.4408750011958182\n",
      "\n",
      "Current Iter : 60/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 60 Acc : 0.8475999991893768 Test Acc : 0.4410000009462237\n",
      "\n",
      "Current Iter : 61/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 61 Acc : 0.8477999994754791 Test Acc : 0.4375000006891787\n",
      "\n",
      "Current Iter : 62/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 62 Acc : 0.8543999998569488 Test Acc : 0.4373750011250377\n",
      "\n",
      "Current Iter : 63/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 63 Acc : 0.8564000000953674 Test Acc : 0.44062500149942935\n",
      "\n",
      "Current Iter : 64/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 64 Acc : 0.8551999990940093 Test Acc : 0.43900000023655594\n",
      "\n",
      "Current Iter : 65/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 65 Acc : 0.860599999666214 Test Acc : 0.4338750011380762\n",
      "\n",
      "Current Iter : 66/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 66 Acc : 0.8622000000476837 Test Acc : 0.437125000609085\n",
      "\n",
      "Current Iter : 67/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 67 Acc : 0.8652000010013581 Test Acc : 0.4346250008698553\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Iter : 68/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 68 Acc : 0.867599999666214 Test Acc : 0.4348750016186386\n",
      "\n",
      "Current Iter : 69/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 69 Acc : 0.8715999991893768 Test Acc : 0.4385000020079315\n",
      "\n",
      "Current Iter : 70/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 70 Acc : 0.8749999983310699 Test Acc : 0.4386250011064112\n",
      "\n",
      "Current Iter : 71/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 71 Acc : 0.8789999995231629 Test Acc : 0.4378750023804605\n",
      "\n",
      "Current Iter : 72/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 72 Acc : 0.8789999976158142 Test Acc : 0.4415000015310943\n",
      "\n",
      "Current Iter : 73/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 73 Acc : 0.8839999980926514 Test Acc : 0.43925000162795186\n",
      "\n",
      "Current Iter : 74/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 74 Acc : 0.8843999993801117 Test Acc : 0.439625001642853\n",
      "\n",
      "Current Iter : 75/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 75 Acc : 0.8855999977588653 Test Acc : 0.4322500024549663\n",
      "\n",
      "Current Iter : 76/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 76 Acc : 0.8903999969959259 Test Acc : 0.4378750011883676\n",
      "\n",
      "Current Iter : 77/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 77 Acc : 0.8955999956130981 Test Acc : 0.4362500007264316\n",
      "\n",
      "Current Iter : 78/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 78 Acc : 0.8949999959468842 Test Acc : 0.4370000016875565\n",
      "\n",
      "Current Iter : 79/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 79 Acc : 0.8983999948501586 Test Acc : 0.4353750012256205\n",
      "\n",
      "Current Iter : 80/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 80 Acc : 0.8977999949455261 Test Acc : 0.440250002425164\n",
      "\n",
      "Current Iter : 81/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 81 Acc : 0.905599995136261 Test Acc : 0.4365000013820827\n",
      "\n",
      "Current Iter : 82/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 82 Acc : 0.9099999935626983 Test Acc : 0.4321250011958182\n",
      "\n",
      "Current Iter : 83/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 83 Acc : 0.9119999940395356 Test Acc : 0.4357500013150275\n",
      "\n",
      "Current Iter : 84/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 84 Acc : 0.9195999917984009 Test Acc : 0.4348750018887222\n",
      "\n",
      "Current Iter : 85/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 85 Acc : 0.9159999926090241 Test Acc : 0.43450000116601584\n",
      "\n",
      "Current Iter : 86/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 86 Acc : 0.9177999935150146 Test Acc : 0.4358750007301569\n",
      "\n",
      "Current Iter : 87/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 87 Acc : 0.9169999933242798 Test Acc : 0.4328750006482005\n",
      "\n",
      "Current Iter : 88/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 88 Acc : 0.922799994468689 Test Acc : 0.4290000013820827\n",
      "\n",
      "Current Iter : 89/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 89 Acc : 0.9221999936103821 Test Acc : 0.4323750011436641\n",
      "\n",
      "Current Iter : 90/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 90 Acc : 0.9247999937534332 Test Acc : 0.4280000004731119\n",
      "\n",
      "Current Iter : 91/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 91 Acc : 0.9225999939441681 Test Acc : 0.4300000008568168\n",
      "\n",
      "Current Iter : 92/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 92 Acc : 0.9249999930858612 Test Acc : 0.4351250016503036\n",
      "\n",
      "Current Iter : 93/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 93 Acc : 0.9277999935150146 Test Acc : 0.4321250005997717\n",
      "\n",
      "Current Iter : 94/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 94 Acc : 0.9297999939918518 Test Acc : 0.43587500140070917\n",
      "\n",
      "Current Iter : 95/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 95 Acc : 0.9305999937057495 Test Acc : 0.43437500143423674\n",
      "\n",
      "Current Iter : 96/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 96 Acc : 0.9327999925613404 Test Acc : 0.4332500002533197\n",
      "\n",
      "Current Iter : 97/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 97 Acc : 0.9319999935626984 Test Acc : 0.4315000009536743\n",
      "\n",
      "Current Iter : 98/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 98 Acc : 0.9311999936103821 Test Acc : 0.43575000040233136\n",
      "\n",
      "Current Iter : 99/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 99 Acc : 0.9335999929904938 Test Acc : 0.43450000040233133\n",
      "\n",
      "Current Iter : 100/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 100 Acc : 0.9273999917507172 Test Acc : 0.43437500063329937\n",
      "\n",
      "Current Iter : 101/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 101 Acc : 0.9321999928951263 Test Acc : 0.4347500010207295\n",
      "\n",
      "Current Iter : 102/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 102 Acc : 0.9361999924182892 Test Acc : 0.43625000163912775\n",
      "\n",
      "Current Iter : 103/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 103 Acc : 0.9377999913692474 Test Acc : 0.440750000923872\n",
      "\n",
      "Current Iter : 104/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 104 Acc : 0.9381999912261962 Test Acc : 0.438500001616776\n",
      "\n",
      "Current Iter : 105/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 105 Acc : 0.9431999924182892 Test Acc : 0.4303750011324883\n",
      "\n",
      "Current Iter : 106/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 106 Acc : 0.9459999904632569 Test Acc : 0.4348750014975667\n",
      "\n",
      "Current Iter : 107/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 107 Acc : 0.9477999923229218 Test Acc : 0.4336250013485551\n",
      "\n",
      "Current Iter : 108/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 108 Acc : 0.9487999920845032 Test Acc : 0.43225000143051145\n",
      "\n",
      "Current Iter : 109/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 109 Acc : 0.951199993133545 Test Acc : 0.43362500032410023\n",
      "\n",
      "Current Iter : 110/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 110 Acc : 0.9519999923706055 Test Acc : 0.43412500120699404\n",
      "\n",
      "Current Iter : 111/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 111 Acc : 0.9555999932289123 Test Acc : 0.4303750006482005\n",
      "\n",
      "Current Iter : 112/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 112 Acc : 0.9569999933242798 Test Acc : 0.4302500011958182\n",
      "\n",
      "Current Iter : 113/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 113 Acc : 0.9567999937534333 Test Acc : 0.4275000014528632\n",
      "\n",
      "Current Iter : 114/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 114 Acc : 0.9491999933719635 Test Acc : 0.4275000014342368\n",
      "\n",
      "Current Iter : 115/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 115 Acc : 0.9483999927043915 Test Acc : 0.4213750005885959\n",
      "\n",
      "Current Iter : 116/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 116 Acc : 0.9497999930381775 Test Acc : 0.42925000036135313\n",
      "\n",
      "Current Iter : 117/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 117 Acc : 0.9509999935626984 Test Acc : 0.43262500027194617\n",
      "\n",
      "Current Iter : 118/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 118 Acc : 0.9471999926567077 Test Acc : 0.4322500005364418\n",
      "\n",
      "Current Iter : 119/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 119 Acc : 0.9431999933719635 Test Acc : 0.43187500048428773\n",
      "\n",
      "Current Iter : 120/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 120 Acc : 0.9467999925613403 Test Acc : 0.4331250003352761\n",
      "\n",
      "Current Iter : 121/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 121 Acc : 0.9539999916553498 Test Acc : 0.4286250004917383\n",
      "\n",
      "Current Iter : 122/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 122 Acc : 0.9559999921321869 Test Acc : 0.42875000115483997\n",
      "\n",
      "Current Iter : 123/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 123 Acc : 0.9559999935626984 Test Acc : 0.4262500010430813\n",
      "\n",
      "Current Iter : 124/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 124 Acc : 0.9565999922752381 Test Acc : 0.41900000169873236\n",
      "\n",
      "Current Iter : 125/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 125 Acc : 0.9521999921798706 Test Acc : 0.42225000105798244\n",
      "\n",
      "Current Iter : 126/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 126 Acc : 0.9605999937057496 Test Acc : 0.41687500093132257\n",
      "\n",
      "Current Iter : 127/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 127 Acc : 0.9649999938011169 Test Acc : 0.42150000110268593\n",
      "\n",
      "Current Iter : 128/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 128 Acc : 0.9639999930858612 Test Acc : 0.423750001527369\n",
      "\n",
      "Current Iter : 129/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 129 Acc : 0.9673999934196472 Test Acc : 0.4206250006519258\n",
      "\n",
      "Current Iter : 130/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 130 Acc : 0.9705999937057496 Test Acc : 0.42087500061839817\n",
      "\n",
      "Current Iter : 131/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 131 Acc : 0.9739999945163726 Test Acc : 0.4261250015348196\n",
      "\n",
      "Current Iter : 132/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 132 Acc : 0.9777999954223633 Test Acc : 0.4250000010430813\n",
      "\n",
      "Current Iter : 133/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 133 Acc : 0.9675999939441681 Test Acc : 0.42625000135973096\n",
      "\n",
      "Current Iter : 134/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 134 Acc : 0.9765999948978424 Test Acc : 0.42162500113248824\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Iter : 135/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 135 Acc : 0.9767999951839447 Test Acc : 0.42462500136345627\n",
      "\n",
      "Current Iter : 136/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 136 Acc : 0.9763999950885772 Test Acc : 0.42025000242516397\n",
      "\n",
      "Current Iter : 137/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 137 Acc : 0.975599995136261 Test Acc : 0.42750000167638064\n",
      "\n",
      "Current Iter : 138/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 138 Acc : 0.9785999956130982 Test Acc : 0.423875002078712\n",
      "\n",
      "Current Iter : 139/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 139 Acc : 0.9821999959945679 Test Acc : 0.42925000093877314\n",
      "\n",
      "Current Iter : 140/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 140 Acc : 0.9791999957561492 Test Acc : 0.43075000174343586\n",
      "\n",
      "Current Iter : 141/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 141 Acc : 0.977199994802475 Test Acc : 0.41850000189617276\n",
      "\n",
      "Current Iter : 142/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 142 Acc : 0.9791999950408935 Test Acc : 0.41737500209361317\n",
      "\n",
      "Current Iter : 143/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 143 Acc : 0.9573999927043915 Test Acc : 0.4181250017695129\n",
      "\n",
      "Current Iter : 144/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 144 Acc : 0.9747999951839447 Test Acc : 0.42087500110268594\n",
      "\n",
      "Current Iter : 145/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 145 Acc : 0.9829999959468841 Test Acc : 0.4207500008121133\n",
      "\n",
      "Current Iter : 146/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 146 Acc : 0.9863999967575073 Test Acc : 0.41400000112131236\n",
      "\n",
      "Current Iter : 147/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 147 Acc : 0.9853999965190887 Test Acc : 0.42187500160187485\n",
      "\n",
      "Current Iter : 148/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 148 Acc : 0.990799997806549 Test Acc : 0.41462500078603626\n",
      "\n",
      "Current Iter : 149/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 149 Acc : 0.9913999979496002 Test Acc : 0.41812500144354997\n",
      "\n",
      "Current Iter : 150/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 150 Acc : 0.9873999972343445 Test Acc : 0.4200000011920929\n",
      "\n",
      "Current Iter : 151/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 151 Acc : 0.9835999960899353 Test Acc : 0.4153750003967434\n",
      "\n",
      "Current Iter : 152/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 152 Acc : 0.9465999913215637 Test Acc : 0.41787500130012634\n",
      "\n",
      "Current Iter : 153/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 153 Acc : 0.9641999936103821 Test Acc : 0.4202500013448298\n",
      "\n",
      "Current Iter : 154/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 154 Acc : 0.9845999965667724 Test Acc : 0.42137500070035455\n",
      "\n",
      "Current Iter : 155/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 155 Acc : 0.9889999973773956 Test Acc : 0.4158750008419156\n",
      "\n",
      "Current Iter : 156/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 156 Acc : 0.9935999984741211 Test Acc : 0.4226250014826655\n",
      "\n",
      "Current Iter : 157/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 157 Acc : 0.9939999985694885 Test Acc : 0.41950000120326875\n",
      "\n",
      "Current Iter : 158/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 158 Acc : 0.981599996805191 Test Acc : 0.4205000014603138\n",
      "\n",
      "Current Iter : 159/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 159 Acc : 0.9759999947547913 Test Acc : 0.4211250014975667\n",
      "\n",
      "Current Iter : 160/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 160 Acc : 0.9829999961853028 Test Acc : 0.425500001180917\n",
      "\n",
      "Current Iter : 161/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 161 Acc : 0.9755999946594238 Test Acc : 0.4185000012814999\n",
      "\n",
      "Current Iter : 162/200 batch : 1940/5000 acc : 0.95\r"
     ]
    }
   ],
   "source": [
    "# 2. batch normalization\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# 1. layers\n",
    "l1 = CNN(3,3, 16); l1n = tf_batch_norm_layer(16,(0,1,2))\n",
    "l2 = CNN(3,16,16); l2n = tf_batch_norm_layer(16,(0,1,2))\n",
    "l3 = CNN(3,16,16); l3n = tf_batch_norm_layer(16,(0,1,2))\n",
    "l4 = CNN(3,16,16); l4n = tf_batch_norm_layer(16,(0,1,2))\n",
    "l5 = CNN(3,16,16); l5n = tf_batch_norm_layer(16,(0,1,2))\n",
    "l6 = CNN(3,16,10); \n",
    "\n",
    "# create the graph \n",
    "x = tf.placeholder(tf.float32,(batch_size,96,96,3))\n",
    "y = tf.placeholder(tf.float32,(batch_size,10))\n",
    "is_train = tf.placeholder_with_default(True,())\n",
    "\n",
    "layer1, layer1a = l1. feedforward(x,stride=2)\n",
    "layer1b,update1 = l1n.feedforward(layer1a,is_train)\n",
    "layer2, layer2a = l2. feedforward(layer1b,stride=2)\n",
    "layer2b,update2 = l2n.feedforward(layer2a,is_train)\n",
    "layer3, layer3a = l3. feedforward(layer2b,stride=2)\n",
    "layer3b,update3 = l3n.feedforward(layer3a,is_train)\n",
    "layer4, layer4a = l4. feedforward(layer3b,stride=2)\n",
    "layer4b,update4 = l4n.feedforward(layer4a,is_train)\n",
    "layer5, layer5a = l5. feedforward(layer4b)\n",
    "layer5b,update5 = l5n.feedforward(layer5a,is_train)\n",
    "layer6, layer6a = l6. feedforward(layer5b)\n",
    "\n",
    "final_layer   = tf.reduce_mean(layer6a,(1,2))\n",
    "final_softmax = tf_softmax(final_layer)\n",
    "cost          = -tf.reduce_mean(y * tf.log(final_softmax + 1e-8))\n",
    "correct_prediction = tf.equal(tf.argmax(final_softmax, 1), tf.argmax(y, 1))\n",
    "accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "gradient = tf.tile((final_softmax-y)[:,None,None,:],[1,6,6,1])/batch_size\n",
    "grad6p,grad6w,grad6_up = l6.backprop(gradient)\n",
    "grad5n = l5n.backprop(grad6p)\n",
    "grad5p,grad5w,grad5_up = l5.backprop(grad5n)\n",
    "grad4n = l4n.backprop(grad5p)\n",
    "grad4p,grad4w,grad4_up = l4.backprop(grad4n,stride=2)\n",
    "\n",
    "grad3n = l3n.backprop(grad4p)\n",
    "grad3p,grad3w,grad3_up = l3.backprop(grad3n,stride=2)\n",
    "grad2n = l2n.backprop(grad3p)\n",
    "grad2p,grad2w,grad2_up = l2.backprop(grad2n,stride=2)\n",
    "grad1n = l1n.backprop(grad2p)\n",
    "grad1p,grad1w,grad1_up = l1.backprop(grad1n,stride=2)\n",
    "\n",
    "update_ops  = update1 + update2 + update3 + update4 + update5\n",
    "gradient_update = grad6_up + grad5_up + grad4_up + grad3_up + grad2_up + grad1_up \n",
    "\n",
    "# train\n",
    "sess.run(tf.global_variables_initializer())\n",
    "avg_acc_train = 0; avg_acc_test  = 0; \n",
    "train_acc = []; test_acc = []\n",
    "for iter in range(num_epoch):\n",
    "\n",
    "    for current_batch_index in range(0,len(train_images),batch_size):\n",
    "        current_data  = train_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = train_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy,gradient_update,update_ops],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(train_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_train = avg_acc_train + sess_results[0]\n",
    "        \n",
    "    # Get weights\n",
    "    save_to_image(sess.run([l1.getw(),l2.getw(),l3.getw(),l4.getw(),l5.getw(),l6.getw()]),'batch Norm/weights/')\n",
    "    save_to_image(sess.run([grad1w,grad2w,grad3w,grad4w,grad5w,grad6w],feed_dict={x:current_data,y:current_label}),'batch Norm/gradientw/')\n",
    "    save_to_image(sess.run([grad1p,grad2p,grad3p,grad4p,grad5p,grad6p],feed_dict={x:current_data,y:current_label}),'batch Norm/gradientp/')\n",
    "    save_to_image(sess.run([grad1_up,grad2_up,grad3_up,grad4_up,grad5_up,grad6_up],feed_dict={x:current_data,y:current_label}),'batch Norm/gradient_update/')\n",
    "        \n",
    "    for current_batch_index in range(0,len(test_images), batch_size):\n",
    "        current_data  = test_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = test_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy],feed_dict={x:current_data,y:current_label,is_train:False})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(test_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_test = avg_acc_test + sess_results[0]   \n",
    "        \n",
    "    print(\"\\n Current : \"+ str(iter) + \" Acc : \" + str(avg_acc_train/(len(train_images)/batch_size)) + \" Test Acc : \" + str(avg_acc_test/(len(test_images)/batch_size)) + '\\n')\n",
    "    \n",
    "    # save the training\n",
    "    train_acc.append(avg_acc_train/(len(train_images)/batch_size))\n",
    "    test_acc .append(avg_acc_test / (len(test_images)/batch_size))\n",
    "    avg_acc_train = 0 ; avg_acc_test  = 0\n",
    "   \n",
    "np.save('batch Norm/train.npy',train_acc)\n",
    "np.save('batch Norm/test.npy', test_acc)\n",
    "sess.close()\n",
    "tf.reset_default_graph();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-20T23:57:02.661Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# 3. layer normalization\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# 1. layers\n",
    "l1 = CNN(3,3, 16); l1n = tf_layer_norm_layer(batch_size,(1,2,3))\n",
    "l2 = CNN(3,16,16); l2n = tf_layer_norm_layer(batch_size,(1,2,3))\n",
    "l3 = CNN(3,16,16); l3n = tf_layer_norm_layer(batch_size,(1,2,3))\n",
    "l4 = CNN(3,16,16); l4n = tf_layer_norm_layer(batch_size,(1,2,3))\n",
    "l5 = CNN(3,16,16); l5n = tf_layer_norm_layer(batch_size,(1,2,3))\n",
    "l6 = CNN(3,16,10); \n",
    "\n",
    "# create the graph \n",
    "x = tf.placeholder(tf.float32,(batch_size,96,96,3))\n",
    "y = tf.placeholder(tf.float32,(batch_size,10))\n",
    "is_train = tf.placeholder_with_default(True,())\n",
    "\n",
    "layer1, layer1a = l1. feedforward(x,stride=2)\n",
    "layer1b,update1 = l1n.feedforward(layer1a,is_train)\n",
    "layer2, layer2a = l2. feedforward(layer1b,stride=2)\n",
    "layer2b,update2 = l2n.feedforward(layer2a,is_train)\n",
    "layer3, layer3a = l3. feedforward(layer2b,stride=2)\n",
    "layer3b,update3 = l3n.feedforward(layer3a,is_train)\n",
    "layer4, layer4a = l4. feedforward(layer3b,stride=2)\n",
    "layer4b,update4 = l4n.feedforward(layer4a,is_train)\n",
    "layer5, layer5a = l5. feedforward(layer4b)\n",
    "layer5b,update5 = l5n.feedforward(layer5a,is_train)\n",
    "layer6, layer6a = l6. feedforward(layer5b)\n",
    "\n",
    "final_layer   = tf.reduce_mean(layer6a,(1,2))\n",
    "final_softmax = tf_softmax(final_layer)\n",
    "cost          = -tf.reduce_mean(y * tf.log(final_softmax + 1e-8))\n",
    "correct_prediction = tf.equal(tf.argmax(final_softmax, 1), tf.argmax(y, 1))\n",
    "accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "gradient = tf.tile((final_softmax-y)[:,None,None,:],[1,6,6,1])/batch_size\n",
    "grad6p,grad6w,grad6_up = l6.backprop(gradient)\n",
    "grad5n = l5n.backprop(grad6p)\n",
    "grad5p,grad5w,grad5_up = l5.backprop(grad5n)\n",
    "grad4n = l4n.backprop(grad5p)\n",
    "grad4p,grad4w,grad4_up = l4.backprop(grad4n,stride=2)\n",
    "\n",
    "grad3n = l3n.backprop(grad4p)\n",
    "grad3p,grad3w,grad3_up = l3.backprop(grad3n,stride=2)\n",
    "grad2n = l2n.backprop(grad3p)\n",
    "grad2p,grad2w,grad2_up = l2.backprop(grad2n,stride=2)\n",
    "grad1n = l1n.backprop(grad2p)\n",
    "grad1p,grad1w,grad1_up = l1.backprop(grad1n,stride=2)\n",
    "\n",
    "update_ops  = update1 + update2 + update3 + update4 + update5\n",
    "gradient_update = grad6_up + grad5_up + grad4_up + grad3_up + grad2_up + grad1_up \n",
    "\n",
    "# train\n",
    "sess.run(tf.global_variables_initializer())\n",
    "avg_acc_train = 0; avg_acc_test  = 0; \n",
    "train_acc = []; test_acc = []\n",
    "for iter in range(num_epoch):\n",
    "\n",
    "    for current_batch_index in range(0,len(train_images),batch_size):\n",
    "        current_data  = train_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = train_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy,gradient_update,update_ops],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(train_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_train = avg_acc_train + sess_results[0]\n",
    "        \n",
    "    # Get weights\n",
    "    save_to_image(sess.run([l1.getw(),l2.getw(),l3.getw(),l4.getw(),l5.getw(),l6.getw()]),'Layer Norm/weights/')\n",
    "    save_to_image(sess.run([grad1w,grad2w,grad3w,grad4w,grad5w,grad6w],feed_dict={x:current_data,y:current_label}),'Layer Norm/gradientw/')\n",
    "    save_to_image(sess.run([grad1p,grad2p,grad3p,grad4p,grad5p,grad6p],feed_dict={x:current_data,y:current_label}),'Layer Norm/gradientp/')\n",
    "    save_to_image(sess.run([grad1_up,grad2_up,grad3_up,grad4_up,grad5_up,grad6_up],feed_dict={x:current_data,y:current_label}),'Layer Norm/gradient_update/')\n",
    "        \n",
    "    for current_batch_index in range(0,len(test_images), batch_size):\n",
    "        current_data  = test_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = test_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy],feed_dict={x:current_data,y:current_label,is_train:False})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(test_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_test = avg_acc_test + sess_results[0]   \n",
    "        \n",
    "    print(\"\\n Current : \"+ str(iter) + \" Acc : \" + str(avg_acc_train/(len(train_images)/batch_size)) + \" Test Acc : \" + str(avg_acc_test/(len(test_images)/batch_size)) + '\\n')\n",
    "    \n",
    "    # save the training\n",
    "    train_acc.append(avg_acc_train/(len(train_images)/batch_size))\n",
    "    test_acc .append(avg_acc_test / (len(test_images)/batch_size))\n",
    "    avg_acc_train = 0 ; avg_acc_test  = 0\n",
    "   \n",
    "np.save('Layer Norm/train.npy',train_acc)\n",
    "np.save('Layer Norm/test.npy', test_acc)\n",
    "sess.close()\n",
    "tf.reset_default_graph();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-20T23:57:02.947Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# 4. Instance normalization\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# 1. layers\n",
    "l1 = CNN(3,3, 16); l1n = tf_instance_norm_layer(batch_size,16,(1,2))\n",
    "l2 = CNN(3,16,16); l2n = tf_instance_norm_layer(batch_size,16,(1,2))\n",
    "l3 = CNN(3,16,16); l3n = tf_instance_norm_layer(batch_size,16,(1,2))\n",
    "l4 = CNN(3,16,16); l4n = tf_instance_norm_layer(batch_size,16,(1,2))\n",
    "l5 = CNN(3,16,16); l5n = tf_instance_norm_layer(batch_size,16,(1,2))\n",
    "l6 = CNN(3,16,10); \n",
    "\n",
    "# create the graph \n",
    "x = tf.placeholder(tf.float32,(batch_size,96,96,3))\n",
    "y = tf.placeholder(tf.float32,(batch_size,10))\n",
    "is_train = tf.placeholder_with_default(True,())\n",
    "\n",
    "layer1, layer1a = l1. feedforward(x,stride=2)\n",
    "layer1b,update1 = l1n.feedforward(layer1a,is_train)\n",
    "layer2, layer2a = l2. feedforward(layer1b,stride=2)\n",
    "layer2b,update2 = l2n.feedforward(layer2a,is_train)\n",
    "layer3, layer3a = l3. feedforward(layer2b,stride=2)\n",
    "layer3b,update3 = l3n.feedforward(layer3a,is_train)\n",
    "layer4, layer4a = l4. feedforward(layer3b,stride=2)\n",
    "layer4b,update4 = l4n.feedforward(layer4a,is_train)\n",
    "layer5, layer5a = l5. feedforward(layer4b)\n",
    "layer5b,update5 = l5n.feedforward(layer5a,is_train)\n",
    "layer6, layer6a = l6. feedforward(layer5b)\n",
    "\n",
    "final_layer   = tf.reduce_mean(layer6a,(1,2))\n",
    "final_softmax = tf_softmax(final_layer)\n",
    "cost          = -tf.reduce_mean(y * tf.log(final_softmax + 1e-8))\n",
    "correct_prediction = tf.equal(tf.argmax(final_softmax, 1), tf.argmax(y, 1))\n",
    "accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "gradient = tf.tile((final_softmax-y)[:,None,None,:],[1,6,6,1])/batch_size\n",
    "grad6p,grad6w,grad6_up = l6.backprop(gradient)\n",
    "grad5n = l5n.backprop(grad6p)\n",
    "grad5p,grad5w,grad5_up = l5.backprop(grad5n)\n",
    "grad4n = l4n.backprop(grad5p)\n",
    "grad4p,grad4w,grad4_up = l4.backprop(grad4n,stride=2)\n",
    "\n",
    "grad3n = l3n.backprop(grad4p)\n",
    "grad3p,grad3w,grad3_up = l3.backprop(grad3n,stride=2)\n",
    "grad2n = l2n.backprop(grad3p)\n",
    "grad2p,grad2w,grad2_up = l2.backprop(grad2n,stride=2)\n",
    "grad1n = l1n.backprop(grad2p)\n",
    "grad1p,grad1w,grad1_up = l1.backprop(grad1n,stride=2)\n",
    "\n",
    "update_ops  = update1 + update2 + update3 + update4 + update5\n",
    "gradient_update = grad6_up + grad5_up + grad4_up + grad3_up + grad2_up + grad1_up \n",
    "\n",
    "# train\n",
    "sess.run(tf.global_variables_initializer())\n",
    "avg_acc_train = 0; avg_acc_test  = 0; \n",
    "train_acc = []; test_acc = []\n",
    "for iter in range(num_epoch):\n",
    "\n",
    "    for current_batch_index in range(0,len(train_images),batch_size):\n",
    "        current_data  = train_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = train_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy,gradient_update,update_ops],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(train_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_train = avg_acc_train + sess_results[0]\n",
    "        \n",
    "    # Get weights\n",
    "    save_to_image(sess.run([l1.getw(),l2.getw(),l3.getw(),l4.getw(),l5.getw(),l6.getw()]),'Instace Norm/weights/')\n",
    "    save_to_image(sess.run([grad1w,grad2w,grad3w,grad4w,grad5w,grad6w],feed_dict={x:current_data,y:current_label}),'Instace Norm/gradientw/')\n",
    "    save_to_image(sess.run([grad1p,grad2p,grad3p,grad4p,grad5p,grad6p],feed_dict={x:current_data,y:current_label}),'Instace Norm/gradientp/')\n",
    "    save_to_image(sess.run([grad1_up,grad2_up,grad3_up,grad4_up,grad5_up,grad6_up],feed_dict={x:current_data,y:current_label}),'Instace Norm/gradient_update/')\n",
    "        \n",
    "    for current_batch_index in range(0,len(test_images), batch_size):\n",
    "        current_data  = test_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = test_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy],feed_dict={x:current_data,y:current_label,is_train:False})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(test_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_test = avg_acc_test + sess_results[0]   \n",
    "        \n",
    "    print(\"\\n Current : \"+ str(iter) + \" Acc : \" + str(avg_acc_train/(len(train_images)/batch_size)) + \" Test Acc : \" + str(avg_acc_test/(len(test_images)/batch_size)) + '\\n')\n",
    "    \n",
    "    # save the training\n",
    "    train_acc.append(avg_acc_train/(len(train_images)/batch_size))\n",
    "    test_acc .append(avg_acc_test / (len(test_images)/batch_size))\n",
    "    avg_acc_train = 0 ; avg_acc_test  = 0\n",
    "   \n",
    "np.save('Instace Norm/train.npy',train_acc)\n",
    "np.save('Instace Norm/test.npy', test_acc)\n",
    "sess.close()\n",
    "tf.reset_default_graph();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-20T23:57:03.301Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# 5. box cox \n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# create layers\n",
    "l1 = CNN(3,3, 16); l1n = tf_box_cox()\n",
    "l2 = CNN(3,16,16); l2n = tf_box_cox()\n",
    "l3 = CNN(3,16,16); l3n = tf_box_cox()\n",
    "\n",
    "l4 = CNN(3,16,16); l4n = tf_box_cox()\n",
    "l5 = CNN(3,16,16); l5n = tf_box_cox()\n",
    "l6 = CNN(3,16,10); \n",
    "\n",
    "# 2. graph \n",
    "x = tf.placeholder(tf.float32,(batch_size,96,96,3))\n",
    "y = tf.placeholder(tf.float32,(batch_size,10))\n",
    "\n",
    "layer1, layer1a = l1. feedforward(x,stride=2)\n",
    "layer1a = l1n.feedforward(layer1a)\n",
    "layer2, layer2a = l2. feedforward(layer1a,stride=2)\n",
    "layer2a = l2n.feedforward(layer2a)\n",
    "layer3, layer3a = l3. feedforward(layer2a,stride=2)\n",
    "layer3a = l3n.feedforward(layer3a)\n",
    "layer4, layer4a = l4. feedforward(layer3a,stride=2)\n",
    "layer4a = l4n.feedforward(layer4a)\n",
    "layer5, layer5a = l5. feedforward(layer4a)\n",
    "layer5a = l5n.feedforward(layer5a)\n",
    "layer6, layer6a = l6. feedforward(layer5a)\n",
    "\n",
    "final_layer   = tf.reduce_mean(layer6a,(1,2))\n",
    "final_softmax = tf_softmax(final_layer)\n",
    "cost          = -tf.reduce_mean(y * tf.log(final_softmax + 1e-8))\n",
    "correct_prediction = tf.equal(tf.argmax(final_softmax, 1), tf.argmax(y, 1))\n",
    "accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "gradient = tf.tile((final_softmax-y)[:,None,None,:],[1,6,6,1])/batch_size\n",
    "grad6p,grad6w,grad6_up  = l6.backprop(gradient)\n",
    "grad5n,grad5l,grad5n_up = l5n.backprop(grad6p)\n",
    "grad5p,grad5w,grad5_up  = l5.backprop(grad5n)\n",
    "grad4n,grad4l,grad4n_up = l4n.backprop(grad5p)\n",
    "grad4p,grad4w,grad4_up  = l4.backprop(grad4n,stride=2)\n",
    "\n",
    "grad3n,grad3l,grad3n_up = l3n.backprop(grad4p)\n",
    "grad3p,grad3w,grad3_up  = l3.backprop(grad3n,stride=2)\n",
    "grad2n,grad2l,grad2n_up = l2n.backprop(grad3p)\n",
    "grad2p,grad2w,grad2_up  = l2.backprop(grad2n,stride=2)\n",
    "grad1n,grad1l,grad1n_up = l1n.backprop(grad2p)\n",
    "grad1p,grad1w,grad1_up  = l1.backprop(grad1n,stride=2)\n",
    "\n",
    "gradient_update = grad6_up + grad5n_up + grad5_up + grad4n_up +  grad4_up + grad3n_up + grad3_up + grad2n_up + grad2_up + grad1n_up + grad1_up \n",
    "\n",
    "# train\n",
    "sess.run(tf.global_variables_initializer())\n",
    "avg_acc_train = 0; avg_acc_test  = 0; \n",
    "train_acc = [];test_acc = []\n",
    "for iter in range(num_epoch):\n",
    "\n",
    "    for current_batch_index in range(0,len(train_images),batch_size):\n",
    "        current_data  = train_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = train_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy,gradient_update],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(train_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_train = avg_acc_train + sess_results[0]\n",
    "        \n",
    "    # Get weights\n",
    "    save_to_image(sess.run([l1.getw(),l2.getw(),l3.getw(),l4.getw(),l5.getw(),l6.getw()]),'Box Cox/weights/')\n",
    "    save_to_image(sess.run([grad1w,grad2w,grad3w,grad4w,grad5w,grad6w],feed_dict={x:current_data,y:current_label}),'Box Cox/gradientw/')\n",
    "    save_to_image(sess.run([grad1p,grad2p,grad3p,grad4p,grad5p,grad6p],feed_dict={x:current_data,y:current_label}),'Box Cox/gradientp/')\n",
    "    save_to_image(sess.run([grad1_up,grad2_up,grad3_up,grad4_up,grad5_up,grad6_up],feed_dict={x:current_data,y:current_label}),'Box Cox/gradient_update/')\n",
    "        \n",
    "    for current_batch_index in range(0,len(test_images), batch_size):\n",
    "        current_data  = test_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = test_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(test_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_test = avg_acc_test + sess_results[0]   \n",
    "        \n",
    "    print(\"\\n Current : \"+ str(iter) + \" Acc : \" + str(avg_acc_train/(len(train_images)/batch_size)) + \" Test Acc : \" + str(avg_acc_test/(len(test_images)/batch_size)) + '\\n')\n",
    "    \n",
    "    # save the training\n",
    "    train_acc.append(avg_acc_train/(len(train_images)/batch_size))\n",
    "    test_acc .append(avg_acc_test / (len(test_images)/batch_size))\n",
    "    avg_acc_train = 0 ; avg_acc_test  = 0\n",
    "    \n",
    "np.save('Box Cox/train.npy',train_acc)\n",
    "np.save('Box Cox/test.npy', test_acc)    \n",
    "sess.close()\n",
    "tf.reset_default_graph();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-20T23:57:03.707Z"
    }
   },
   "outputs": [],
   "source": [
    "%%notify\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "1. mttk/STL10. (2018). GitHub. Retrieved 19 December 2018, from https://github.com/mttk/STL10\n",
    "2. [duplicate], H. (2018). How to display multiple images in one figure correctly?. Stack Overflow. Retrieved 19 December 2018, from https://stackoverflow.com/questions/46615554/how-to-display-multiple-images-in-one-figure-correctly\n",
    "3. plot, H. (2010). How to change the font size on a matplotlib plot. Stack Overflow. Retrieved 20 December 2018, from https://stackoverflow.com/questions/3899980/how-to-change-the-font-size-on-a-matplotlib-plot\n",
    "4. ShopRunner/jupyter-notify. (2018). GitHub. Retrieved 20 December 2018, from https://github.com/ShopRunner/jupyter-notify"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
