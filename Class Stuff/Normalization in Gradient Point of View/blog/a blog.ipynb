{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T20:26:58.937224Z",
     "start_time": "2018-12-20T20:26:58.926217Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# import Library and some random image data set\n",
    "import tensorflow as tf\n",
    "import numpy      as np\n",
    "import seaborn    as sns \n",
    "import pandas     as pd\n",
    "import os,sys\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(78); tf.set_random_seed(78)\n",
    "\n",
    "# get some of the STL data set\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from skimage import util \n",
    "from skimage.transform import resize\n",
    "from skimage.io import imread\n",
    "import warnings\n",
    "from numpy import inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T07:57:50.528979Z",
     "start_time": "2018-12-20T07:57:48.468510Z"
    },
    "code_folding": [
     0,
     2,
     29,
     37
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 96, 96, 3) 1.0 0.0\n",
      "(5000, 10) 1.0 0.0\n",
      "(8000, 96, 96, 3) 1.0 0.0\n",
      "(8000, 10) 1.0 0.0\n"
     ]
    }
   ],
   "source": [
    "# read all of the data\n",
    "# https://github.com/mttk/STL10\n",
    "def read_all_images(path_to_data):\n",
    "    \"\"\"\n",
    "    :param path_to_data: the file containing the binary images from the STL-10 dataset\n",
    "    :return: an array containing all the images\n",
    "    \"\"\"\n",
    "\n",
    "    with open(path_to_data, 'rb') as f:\n",
    "        # read whole file in uint8 chunks\n",
    "        everything = np.fromfile(f, dtype=np.uint8)\n",
    "\n",
    "        # We force the data into 3x96x96 chunks, since the\n",
    "        # images are stored in \"column-major order\", meaning\n",
    "        # that \"the first 96*96 values are the red channel,\n",
    "        # the next 96*96 are green, and the last are blue.\"\n",
    "        # The -1 is since the size of the pictures depends\n",
    "        # on the input file, and this way numpy determines\n",
    "        # the size on its own.\n",
    "\n",
    "        images = np.reshape(everything, (-1, 3, 96, 96))\n",
    "\n",
    "        # Now transpose the images into a standard image format\n",
    "        # readable by, for example, matplotlib.imshow\n",
    "        # You might want to comment this line or reverse the shuffle\n",
    "        # if you will use a learning algorithm like CNN, since they like\n",
    "        # their channels separated.\n",
    "        images = np.transpose(images, (0, 3, 2, 1))\n",
    "        return images\n",
    "def read_labels(path_to_labels):\n",
    "    \"\"\"\n",
    "    :param path_to_labels: path to the binary file containing labels from the STL-10 dataset\n",
    "    :return: an array containing the labels\n",
    "    \"\"\"\n",
    "    with open(path_to_labels, 'rb') as f:\n",
    "        labels = np.fromfile(f, dtype=np.uint8)\n",
    "        return labels\n",
    "def show_images(data,row=1,col=1):\n",
    "    fig=plt.figure(figsize=(10,10))\n",
    "    columns = col; rows = row\n",
    "    for i in range(1, columns*rows +1):\n",
    "        fig.add_subplot(rows, columns, i)\n",
    "        plt.imshow(data[i-1])\n",
    "    plt.show()\n",
    "\n",
    "train_images = read_all_images(\"../../../DataSet/STL10/stl10_binary/train_X.bin\") / 255.0\n",
    "train_labels = read_labels    (\"../../../DataSet/STL10/stl10_binary/train_Y.bin\")\n",
    "test_images  = read_all_images(\"../../../DataSet/STL10/stl10_binary/test_X.bin\")  / 255.0\n",
    "test_labels  = read_labels    (\"../../../DataSet/STL10/stl10_binary/test_y.bin\")\n",
    "\n",
    "label_encoder= OneHotEncoder(sparse=False,categories='auto')\n",
    "train_labels = label_encoder.fit_transform(train_labels.reshape((-1,1)))\n",
    "test_labels  = label_encoder.fit_transform(test_labels.reshape((-1,1)))\n",
    "\n",
    "print(train_images.shape,train_images.max(),train_images.min())\n",
    "print(train_labels.shape,train_labels.max(),train_labels.min())\n",
    "print(test_images.shape,test_images.max(),test_images.min())\n",
    "print(test_labels.shape,test_labels.max(),test_labels.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T20:27:01.299540Z",
     "start_time": "2018-12-20T20:27:01.182829Z"
    },
    "code_folding": [
     58,
     99,
     140,
     181
    ]
   },
   "outputs": [],
   "source": [
    "# create the layers\n",
    "def tf_softmax(x): return tf.nn.softmax(x)\n",
    "\n",
    "def tf_elu(x):   return tf.nn.elu(x)\n",
    "def d_tf_elu(x): return tf.cast(tf.greater(x,0),tf.float32)  + (tf_elu(tf.cast(tf.less_equal(x,0),tf.float32) * x) + 1.0)\n",
    "\n",
    "def tf_relu(x):   return tf.nn.relu(x)\n",
    "def d_tf_relu(x): return tf.cast(tf.greater(x,0),tf.float32)\n",
    "\n",
    "def tf_tanh(x):   return tf.nn.tanh(x)\n",
    "def d_tf_tanh(x): return 1 - tf_tanh(x) ** 2\n",
    "\n",
    "def tf_sigmoid(x):   return tf.nn.sigmoid(x)\n",
    "def d_tf_sigmoid(x): return tf_sigmoid(x) * (1.0-tf_sigmoid(x))\n",
    "\n",
    "class CNN():\n",
    "\n",
    "    def __init__(self,k,inc,out, stddev=0.05,which_reg=0,act=tf_relu,d_act=d_tf_relu):\n",
    "        self.w              = tf.Variable(tf.random_normal([k,k,inc,out],stddev=stddev,seed=4,dtype=tf.float32))\n",
    "        self.m,self.v       = tf.Variable(tf.zeros_like(self.w)),tf.Variable(tf.zeros_like(self.w))\n",
    "        self.act,self.d_act = act,d_act\n",
    "        \n",
    "    def getw(self): return self.w\n",
    "\n",
    "    def feedforward(self,input,stride=1,padding='SAME'):\n",
    "        self.input  = input\n",
    "        self.layer  = tf.nn.conv2d(input,self.w,strides=[1,stride,stride,1],padding=padding) \n",
    "        self.layerA = self.act(self.layer)\n",
    "        return [self.layer,self.layerA]\n",
    "    \n",
    "    def backprop(self,gradient,stride=1,padding='SAME'):\n",
    "        grad_part_1 = gradient\n",
    "        grad_part_2 = self.d_act(self.layer)\n",
    "        grad_part_3 = self.input\n",
    "\n",
    "        grad_middle = grad_part_1 * grad_part_2\n",
    "        grad        = tf.nn.conv2d_backprop_filter(input = grad_part_3,filter_sizes = tf.shape(self.w),  out_backprop = grad_middle,strides=[1,stride,stride,1],padding=padding) / batch_size\n",
    "        grad_pass   = tf.nn.conv2d_backprop_input (input_sizes = tf.shape(self.input),filter= self.w,out_backprop = grad_middle,strides=[1,stride,stride,1],padding=padding)\n",
    "\n",
    "        update_w = []\n",
    "        update_w.append(tf.assign( self.m,self.m*beta1 + (1-beta1) * (grad)   ))\n",
    "        update_w.append(tf.assign( self.v,self.v*beta2 + (1-beta2) * (grad ** 2)   ))\n",
    "        m_hat = self.m / (1-beta1) ; v_hat = self.v / (1-beta2)\n",
    "        adam_middle = m_hat * learning_rate/(tf.sqrt(v_hat) + adam_e)\n",
    "        update_w.append(tf.assign(self.w,tf.subtract(self.w,adam_middle  )))\n",
    "        \n",
    "        return grad_pass,grad,update_w\n",
    "    \n",
    "class tf_batch_norm_layer():\n",
    "    \n",
    "    def __init__(self,vector_shape,axis):\n",
    "        self.moving_mean = tf.Variable(tf.zeros(shape=[1,1,1,vector_shape],dtype=tf.float32))\n",
    "        self.moving_vari = tf.Variable(tf.zeros(shape=[1,1,1,vector_shape],dtype=tf.float32))\n",
    "        self.axis        = axis\n",
    "        \n",
    "    def feedforward(self,input,training_phase=True,eps = 1e-8):\n",
    "        self.input = input\n",
    "        self.input_size          = self.input.shape\n",
    "        self.batch,self.h,self.w,self.c = self.input_size[0].value,self.input_size[1].value,self.input_size[2].value,self.input_size[3].value\n",
    "\n",
    "        # Training Moving Average Mean         \n",
    "        def training_fn():\n",
    "            self.mean    = tf.reduce_mean(self.input,axis=self.axis ,keepdims=True)\n",
    "            self.var     = tf.reduce_mean(tf.square(self.input-self.mean),axis=self.axis,keepdims=True)\n",
    "            centered_data= (self.input - self.mean)/tf.sqrt(self.var + eps)\n",
    "            \n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean*0.9 + 0.1 * self.mean ))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari*0.9 + 0.1 * self.var  ))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        # Testing Moving Average Mean        \n",
    "        def  testing_fn():\n",
    "            centered_data   = (self.input - self.moving_mean)/tf.sqrt(self.moving_vari + eps)\n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        self.output,update_variable = tf.cond(training_phase,true_fn=training_fn,false_fn=testing_fn)\n",
    "        return self.output,update_variable\n",
    "    \n",
    "    def backprop(self,grad,eps = 1e-8):\n",
    "        change_parts = 1.0 /(self.batch * self.h * self.w)\n",
    "        grad_sigma   = tf.reduce_sum( grad *  (self.input-self.mean)     ,axis=self.axis,keepdims=True) * -0.5 * (self.var+eps) ** -1.5\n",
    "        grad_mean    = tf.reduce_sum( grad *  (-1./tf.sqrt(self.var+eps)),axis=self.axis,keepdims=True) + grad_sigma * change_parts * 2.0 * tf.reduce_sum((self.input-self.mean),axis=self.axis,keepdims=True) * -1\n",
    "        grad_x       = grad * 1/(tf.sqrt(self.var+eps)) + grad_sigma * change_parts * 2.0 * (self.input-self.mean) + grad_mean * change_parts\n",
    "        return grad_x\n",
    "\n",
    "class tf_layer_norm_layer():\n",
    "    \n",
    "    def __init__(self,vector_shape,axis):\n",
    "        self.moving_mean = tf.Variable(tf.zeros(shape=[vector_shape,1,1,1],dtype=tf.float32))\n",
    "        self.moving_vari = tf.Variable(tf.zeros(shape=[vector_shape,1,1,1],dtype=tf.float32))\n",
    "        self.axis        = axis\n",
    "        \n",
    "    def feedforward(self,input,training_phase=True,eps = 1e-8):\n",
    "        self.input = input\n",
    "        self.input_size          = self.input.shape\n",
    "        self.batch,self.h,self.w,self.c = self.input_size[0].value,self.input_size[1].value,self.input_size[2].value,self.input_size[3].value\n",
    "\n",
    "        # Training Moving Average Mean         \n",
    "        def training_fn():\n",
    "            self.mean    = tf.reduce_mean(self.input,axis=self.axis ,keepdims=True)\n",
    "            self.var     = tf.reduce_mean(tf.square(self.input-self.mean),axis=self.axis,keepdims=True)\n",
    "            centered_data= (self.input - self.mean)/tf.sqrt(self.var + eps)\n",
    "            \n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean*0.9 + 0.1 * self.mean ))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari*0.9 + 0.1 * self.var  ))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        # Testing Moving Average Mean        \n",
    "        def  testing_fn():\n",
    "            centered_data   = (self.input - self.moving_mean)/tf.sqrt(self.moving_vari + eps)\n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        self.output,update_variable = tf.cond(training_phase,true_fn=training_fn,false_fn=testing_fn)\n",
    "        return self.output,update_variable\n",
    "    \n",
    "    def backprop(self,grad,eps = 1e-8):\n",
    "        change_parts = 1.0 /(self.h * self.w * self.c)\n",
    "        grad_sigma   = tf.reduce_sum( grad *  (self.input-self.mean)     ,axis=self.axis,keepdims=True) * -0.5 * (self.var+eps) ** -1.5\n",
    "        grad_mean    = tf.reduce_sum( grad *  (-1./tf.sqrt(self.var+eps)),axis=self.axis,keepdims=True) + grad_sigma * change_parts * 2.0 * tf.reduce_sum((self.input-self.mean),axis=self.axis,keepdims=True) * -1\n",
    "        grad_x       = grad * 1/(tf.sqrt(self.var+eps)) + grad_sigma * change_parts * 2.0 * (self.input-self.mean) + grad_mean * change_parts\n",
    "        return grad_x\n",
    "    \n",
    "class tf_instance_norm_layer():\n",
    "    \n",
    "    def __init__(self,batch_size,vector_shape,axis):\n",
    "        self.moving_mean = tf.Variable(tf.zeros(shape=[batch_size,1,1,vector_shape],dtype=tf.float32))\n",
    "        self.moving_vari = tf.Variable(tf.zeros(shape=[batch_size,1,1,vector_shape],dtype=tf.float32))\n",
    "        self.axis        = axis\n",
    "        \n",
    "    def feedforward(self,input,training_phase=True,eps = 1e-8):\n",
    "        self.input = input\n",
    "        self.input_size          = self.input.shape\n",
    "        self.batch,self.h,self.w,self.c = self.input_size[0].value,self.input_size[1].value,self.input_size[2].value,self.input_size[3].value\n",
    "\n",
    "        # Training Moving Average Mean         \n",
    "        def training_fn():\n",
    "            self.mean    = tf.reduce_mean(self.input,axis=self.axis ,keepdims=True)\n",
    "            self.var     = tf.reduce_mean(tf.square(self.input-self.mean),axis=self.axis,keepdims=True)\n",
    "            centered_data= (self.input - self.mean)/tf.sqrt(self.var + eps)\n",
    "            \n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean*0.9 + 0.1 * self.mean ))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari*0.9 + 0.1 * self.var  ))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        # Testing Moving Average Mean        \n",
    "        def  testing_fn():\n",
    "            centered_data   = (self.input - self.moving_mean)/tf.sqrt(self.moving_vari + eps)\n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        self.output,update_variable = tf.cond(training_phase,true_fn=training_fn,false_fn=testing_fn)\n",
    "        return self.output,update_variable\n",
    "    \n",
    "    def backprop(self,grad,eps = 1e-8):\n",
    "        change_parts = 1.0 /(self.h * self.w)\n",
    "        grad_sigma   = tf.reduce_sum( grad *  (self.input-self.mean)     ,axis=self.axis,keepdims=True) * -0.5 * (self.var+eps) ** -1.5\n",
    "        grad_mean    = tf.reduce_sum( grad *  (-1./tf.sqrt(self.var+eps)),axis=self.axis,keepdims=True) + grad_sigma * change_parts * 2.0 * tf.reduce_sum((self.input-self.mean),axis=self.axis,keepdims=True) * -1\n",
    "        grad_x       = grad * 1/(tf.sqrt(self.var+eps)) + grad_sigma * change_parts * 2.0 * (self.input-self.mean) + grad_mean * change_parts\n",
    "        return grad_x\n",
    "  \n",
    "class tf_box_cox():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.lmbda    = tf.Variable(2.0) \n",
    "        self.m,self.v = tf.Variable(tf.zeros_like(self.lmbda)),tf.Variable(tf.zeros_like(self.lmbda))\n",
    "    def getw(self): return self.lmbda\n",
    "    \n",
    "    def feedforward(self,data):\n",
    "        self.input = data\n",
    "        self.layer = tf.pow((self.input + 1.0),self.lmbda)\n",
    "        return (self.layer - 1.0)/(self.lmbda + 1e-8)\n",
    "    \n",
    "    def backprop(self,grad):\n",
    "        \n",
    "        # Gradient that gets passed along\n",
    "        grad_pass = tf.pow((self.input + 1),self.lmbda-1.0) * grad\n",
    "        \n",
    "        # Grad respect to the lmbda value (not tested!)\n",
    "        grad_lmbda1 =   (self.layer * tf.log(self.input + 1 ))/(self.lmbda + 1e-8)\n",
    "        grad_lmbda2 = - (self.layer - 1)/(self.lmbda ** 2 + 1e-8)\n",
    "        grad_lmbda  = tf.reduce_mean((grad_lmbda1 + grad_lmbda2)*grad)\n",
    "\n",
    "        update_w = []\n",
    "        update_w.append(tf.assign( self.m,self.m*beta1 + (1-beta1) * (grad_lmbda)   ))\n",
    "        update_w.append(tf.assign( self.v,self.v*beta2 + (1-beta2) * (grad_lmbda ** 2)   ))\n",
    "        m_hat = self.m / (1-beta1) ; v_hat = self.v / (1-beta2)\n",
    "        adam_middle = m_hat * learning_rate/(tf.sqrt(v_hat) + adam_e)\n",
    "        update_w.append(tf.assign(self.lmbda,tf.subtract(self.lmbda,adam_middle  )))\n",
    "        \n",
    "        return grad_pass,grad_lmbda,update_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T20:30:55.915949Z",
     "start_time": "2018-12-20T20:30:55.911974Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# set hyper parameter\n",
    "num_epoch = 200; learning_rate = 0.0008; batch_size = 20; beta1,beta2,adam_e = 0.9,0.999,1e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-20T20:30:56.310Z"
    },
    "code_folding": [
     0,
     47,
     54
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Iter : 0/200 batch : 7980/8000 acc : 0.25\n",
      " Current : 0 Acc : 0.11420000231266021 Test Acc : 0.10775000226683915\n",
      "\n",
      "Current Iter : 1/200 batch : 7980/8000 acc : 0.25\n",
      " Current : 1 Acc : 0.1784000030606985 Test Acc : 0.2738750026375055\n",
      "\n",
      "Current Iter : 2/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 2 Acc : 0.27140000301599504 Test Acc : 0.32900000233203175\n",
      "\n",
      "Current Iter : 3/200 batch : 7980/8000 acc : 0.55\n",
      " Current : 3 Acc : 0.3054000016897917 Test Acc : 0.3330000018980354\n",
      "\n",
      "Current Iter : 4/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 4 Acc : 0.3236000025421381 Test Acc : 0.33375000132247806\n",
      "\n",
      "Current Iter : 5/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 5 Acc : 0.33600000163912774 Test Acc : 0.3402500012423843\n",
      "\n",
      "Current Iter : 6/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 6 Acc : 0.3498000012636185 Test Acc : 0.3471250015869737\n",
      "\n",
      "Current Iter : 7/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 7 Acc : 0.3570000014603138 Test Acc : 0.35400000227615236\n",
      "\n",
      "Current Iter : 8/200 batch : 7980/8000 acc : 0.55\n",
      " Current : 8 Acc : 0.36500000074505806 Test Acc : 0.3558750017359853\n",
      "\n",
      "Current Iter : 9/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 9 Acc : 0.37320000171661377 Test Acc : 0.3603750014305115\n",
      "\n",
      "Current Iter : 10/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 10 Acc : 0.3746000015735626 Test Acc : 0.36875000152736903\n",
      "\n",
      "Current Iter : 11/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 11 Acc : 0.3790000012367964 Test Acc : 0.37512500140815974\n",
      "\n",
      "Current Iter : 12/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 12 Acc : 0.38480000145733356 Test Acc : 0.3793750012293458\n",
      "\n",
      "Current Iter : 13/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 13 Acc : 0.39040000121295454 Test Acc : 0.38325000178068874\n",
      "\n",
      "Current Iter : 14/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 14 Acc : 0.39620000126957894 Test Acc : 0.386375001296401\n",
      "\n",
      "Current Iter : 15/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 15 Acc : 0.40300000038743017 Test Acc : 0.38900000128895046\n",
      "\n",
      "Current Iter : 16/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 16 Acc : 0.408600000590086 Test Acc : 0.39250000067055224\n",
      "\n",
      "Current Iter : 17/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 17 Acc : 0.41400000008940696 Test Acc : 0.3956250008568168\n",
      "\n",
      "Current Iter : 18/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 18 Acc : 0.41720000028610227 Test Acc : 0.39875000115484\n",
      "\n",
      "Current Iter : 19/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 19 Acc : 0.4206000004410744 Test Acc : 0.4000000009313226\n",
      "\n",
      "Current Iter : 20/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 20 Acc : 0.42279999995231626 Test Acc : 0.4000000009313226\n",
      "\n",
      "Current Iter : 21/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 21 Acc : 0.4280000004172325 Test Acc : 0.403500000834465\n",
      "\n",
      "Current Iter : 22/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 22 Acc : 0.4326000012159347 Test Acc : 0.4077500004693866\n",
      "\n",
      "Current Iter : 23/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 23 Acc : 0.43500000110268594 Test Acc : 0.4116250009089708\n",
      "\n",
      "Current Iter : 24/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 24 Acc : 0.43940000119805334 Test Acc : 0.41450000017881394\n",
      "\n",
      "Current Iter : 25/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 25 Acc : 0.43880000039935113 Test Acc : 0.4163750007003546\n",
      "\n",
      "Current Iter : 26/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 26 Acc : 0.4418000006377697 Test Acc : 0.4206250001490116\n",
      "\n",
      "Current Iter : 27/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 27 Acc : 0.4456000008881092 Test Acc : 0.4235000003874302\n",
      "\n",
      "Current Iter : 28/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 28 Acc : 0.44760000124573707 Test Acc : 0.4268750006519258\n",
      "\n",
      "Current Iter : 29/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 29 Acc : 0.4504000008404255 Test Acc : 0.4298750006593764\n",
      "\n",
      "Current Iter : 30/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 30 Acc : 0.45440000090003013 Test Acc : 0.43287500102072957\n",
      "\n",
      "Current Iter : 31/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 31 Acc : 0.45920000091195107 Test Acc : 0.4366250006854534\n",
      "\n",
      "Current Iter : 32/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 32 Acc : 0.4602000006437302 Test Acc : 0.438250000923872\n",
      "\n",
      "Current Iter : 33/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 33 Acc : 0.46440000063180925 Test Acc : 0.4402500012889504\n",
      "\n",
      "Current Iter : 34/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 34 Acc : 0.4670000004172325 Test Acc : 0.4416250015422702\n",
      "\n",
      "Current Iter : 35/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 35 Acc : 0.4694000008404255 Test Acc : 0.44412500139325856\n",
      "\n",
      "Current Iter : 36/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 36 Acc : 0.47220000067353246 Test Acc : 0.44612500168383123\n",
      "\n",
      "Current Iter : 37/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 37 Acc : 0.47400000056624414 Test Acc : 0.44812500167638064\n",
      "\n",
      "Current Iter : 38/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 38 Acc : 0.4770000013411045 Test Acc : 0.44912500113248827\n",
      "\n",
      "Current Iter : 39/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 39 Acc : 0.4812000009119511 Test Acc : 0.4500000017136335\n",
      "\n",
      "Current Iter : 40/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 40 Acc : 0.48280000099539755 Test Acc : 0.4508750015869737\n",
      "\n",
      "Current Iter : 41/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 41 Acc : 0.4846000007688999 Test Acc : 0.45200000163167714\n",
      "\n",
      "Current Iter : 42/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 42 Acc : 0.4936000010073185 Test Acc : 0.4568750009685755\n",
      "\n",
      "Current Iter : 43/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 43 Acc : 0.5000000006556511 Test Acc : 0.4595000001788139\n",
      "\n",
      "Current Iter : 44/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 44 Acc : 0.5070000003576278 Test Acc : 0.464125000089407\n",
      "\n",
      "Current Iter : 45/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 45 Acc : 0.5106000007390976 Test Acc : 0.46500000052154067\n",
      "\n",
      "Current Iter : 46/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 46 Acc : 0.5153999996185302 Test Acc : 0.46725000102072956\n",
      "\n",
      "Current Iter : 47/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 47 Acc : 0.519 Test Acc : 0.46862500108778476\n",
      "\n",
      "Current Iter : 48/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 48 Acc : 0.5210000008940697 Test Acc : 0.47112500078976155\n",
      "\n",
      "Current Iter : 49/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 49 Acc : 0.5224000000357628 Test Acc : 0.47187500178813935\n",
      "\n",
      "Current Iter : 50/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 50 Acc : 0.5270000001192093 Test Acc : 0.4731250013411045\n",
      "\n",
      "Current Iter : 51/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 51 Acc : 0.5252000002861023 Test Acc : 0.474750000834465\n",
      "\n",
      "Current Iter : 52/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 52 Acc : 0.5308000000715256 Test Acc : 0.47387500178068875\n",
      "\n",
      "Current Iter : 53/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 53 Acc : 0.5312000014781952 Test Acc : 0.47600000124424696\n",
      "\n",
      "Current Iter : 54/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 54 Acc : 0.5326000015735626 Test Acc : 0.47662500131875274\n",
      "\n",
      "Current Iter : 55/200 batch : 4360/5000 acc : 0.75\r"
     ]
    }
   ],
   "source": [
    "# 1. Normal CNN \n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# create layers\n",
    "l1 = CNN(3,3, 16); \n",
    "l2 = CNN(3,16,16); \n",
    "l3 = CNN(3,16,16); \n",
    "\n",
    "l4 = CNN(3,16,32); \n",
    "l5 = CNN(3,32,32); \n",
    "l6 = CNN(3,32,10); \n",
    "\n",
    "# create the graph \n",
    "x = tf.placeholder(tf.float32,[batch_size,96,96,3])\n",
    "y = tf.placeholder(tf.float32,[batch_size,10])\n",
    "\n",
    "layer1,layer1a = l1.feedforward(x,stride=2)      ;          \n",
    "layer2,layer2a = l2.feedforward(layer1a,stride=2);         \n",
    "layer3,layer3a = l3.feedforward(layer2a,stride=2); \n",
    "\n",
    "layer4,layer4a = l4.feedforward(layer3a,stride=2);          \n",
    "layer5,layer5a = l5.feedforward(layer4a,stride=1);         \n",
    "layer6,layer6a = l6.feedforward(layer5a,stride=1); \n",
    "\n",
    "final_layer   = tf.reduce_mean(layer6a,(1,2))\n",
    "final_softmax = tf_softmax(final_layer)\n",
    "cost          = -tf.reduce_mean(y * tf.log(final_softmax + 1e-8))\n",
    "correct_prediction = tf.equal(tf.argmax(final_softmax, 1), tf.argmax(y, 1))\n",
    "accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "gradient = tf.tile((final_softmax-y)[:,None,None,:],[1,6,6,1])/batch_size\n",
    "grad6p,grad6w,grad6_up = l6.backprop(gradient)\n",
    "grad5p,grad5w,grad5_up = l5.backprop(grad6p)\n",
    "grad4p,grad4w,grad4_up = l4.backprop(grad5p,stride=2)\n",
    "\n",
    "grad3p,grad3w,grad3_up = l3.backprop(grad4p,stride=2)\n",
    "grad2p,grad2w,grad2_up = l2.backprop(grad3p,stride=2)\n",
    "grad1p,grad1w,grad1_up = l1.backprop(grad2p,stride=2)\n",
    "\n",
    "gradient_update = grad6_up + grad5_up + grad4_up + grad3_up + grad2_up + grad1_up \n",
    "\n",
    "# train\n",
    "sess.run(tf.global_variables_initializer())\n",
    "avg_acc_train = 0; avg_acc_test  = 0; \n",
    "train_acc     = [];test_acc = []\n",
    "for iter in range(num_epoch):\n",
    "\n",
    "    for current_batch_index in range(0,len(train_images),batch_size):\n",
    "        current_data  = train_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = train_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy,gradient_update],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(train_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_train = avg_acc_train + sess_results[0]\n",
    "        \n",
    "    for current_batch_index in range(0,len(test_images), batch_size):\n",
    "        current_data  = test_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = test_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(test_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_test = avg_acc_test + sess_results[0]   \n",
    "        \n",
    "    print(\"\\n Current : \"+ str(iter) + \" Acc : \" + str(avg_acc_train/(len(train_images)/batch_size)) + \" Test Acc : \" + str(avg_acc_test/(len(test_images)/batch_size)) + '\\n')\n",
    "    \n",
    "    # save the training\n",
    "    train_acc.append(avg_acc_train/(len(train_images)/batch_size))\n",
    "    test_acc .append(avg_acc_test / (len(test_images)/batch_size))\n",
    "    avg_acc_train = 0 ; avg_acc_test  = 0\n",
    "    \n",
    "sess.close()\n",
    "tf.reset_default_graph();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T06:09:14.239248Z",
     "start_time": "2018-12-20T06:08:54.084190Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T09:22:32.728296Z",
     "start_time": "2018-12-20T09:22:32.507642Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# create layers\n",
    "num_epoch = 150; learning_rate = 0.001; batch_size = 100\n",
    "beta1,beta2,adam_e = 0.9,0.999,1e-8\n",
    "sess = tf.InteractiveSession()\n",
    "l1 = CNN(3,3, 16); l1n = tf_box_cox()\n",
    "l2 = CNN(3,16,16); l2n = tf_box_cox()\n",
    "l3 = CNN(3,16,16); l3n = tf_box_cox()\n",
    "\n",
    "l4 = CNN(3,16,32); l4n = tf_box_cox()\n",
    "l5 = CNN(3,32,32); l5n = tf_box_cox()\n",
    "l6 = CNN(3,32,10); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T09:22:33.995782Z",
     "start_time": "2018-12-20T09:22:33.036092Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# create the graph \n",
    "x = tf.placeholder(tf.float32,[batch_size,96,96,3])\n",
    "y = tf.placeholder(tf.float32,[batch_size,10])\n",
    "\n",
    "layer1,layer1a = l1.feedforward(x,stride=2)      ;          \n",
    "layer1n = l1n.feedforward(layer1a)\n",
    "layer2,layer2a = l2.feedforward(layer1n,stride=2);          \n",
    "layer2n = l2n.feedforward(layer2a)\n",
    "layer3,layer3a = l3.feedforward(layer2n,stride=2); \n",
    "layer3n = l3n.feedforward(layer3a)\n",
    "\n",
    "layer4,layer4a = l4.feedforward(layer3n,stride=2);          \n",
    "layer4n = l4n.feedforward(layer4a)\n",
    "layer5,layer5a = l5.feedforward(layer4n,stride=1);          \n",
    "layer5n = l5n.feedforward(layer5a)\n",
    "layer6,layer6a = l6.feedforward(layer5n,stride=1); \n",
    "\n",
    "final_layer   = tf.reduce_mean(layer6a,(1,2))\n",
    "final_softmax = tf_softmax(final_layer)\n",
    "cost          = -tf.reduce_mean(y * tf.log(final_softmax + 1e-8))\n",
    "auto_train = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "correct_prediction = tf.equal(tf.argmax(final_softmax, 1), tf.argmax(y, 1))\n",
    "accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "gradient = tf.tile((final_softmax-y)[:,None,None,:],[1,6,6,1])/batch_size\n",
    "grad6p,grad6w,grad6_up  = l6.backprop(gradient)\n",
    "grad5n,grad5l,grad5n_up = l5n.backprop(grad6p); \n",
    "grad5p,grad5w,grad5_up  = l5.backprop(grad5n,stride=1)\n",
    "grad4n,grad4l,grad4n_up = l4n.backprop(grad5p); \n",
    "grad4p,grad4w,grad4_up  = l4.backprop(grad4n,stride=2)\n",
    "\n",
    "grad3n,grad3l,grad3n_up = l3n.backprop(grad4p);\n",
    "grad3p,grad3w,grad3_up  = l3.backprop(grad3n,stride=2)\n",
    "grad2n,grad2l,grad2n_up = l2n.backprop(grad3p); \n",
    "grad2p,grad2w,grad2_up  = l2.backprop(grad2n,stride=2)\n",
    "grad1n,grad1l,grad1n_up = l1n.backprop(grad2p); \n",
    "grad1p,grad1w,grad1_up  = l1.backprop(grad1n,stride=2)\n",
    "\n",
    "gradient_update = grad6_up + \\\n",
    "                  grad5n_up + grad5_up + \\\n",
    "                  grad4n_up + grad4_up + \\\n",
    "                  grad3n_up + grad3_up + \\\n",
    "                  grad2n_up + grad2_up + \\\n",
    "                  grad1n_up + grad1_up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T09:31:30.028152Z",
     "start_time": "2018-12-20T09:22:34.190006Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Iter : 0/150 batch : 7900/8000 acc : 0.15\n",
      " Current : 0 Acc : 0.10639999989420175 Test Acc : 0.12137499949894845\n",
      "\n",
      "Current Iter : 1/150 batch : 7900/8000 acc : 0.21\n",
      " Current : 1 Acc : 0.11419999979436397 Test Acc : 0.14175000037066637\n",
      "\n",
      "Current Iter : 2/150 batch : 7900/8000 acc : 0.14\n",
      " Current : 2 Acc : 0.11979999981820583 Test Acc : 0.10049999975599348\n",
      "\n",
      "Current Iter : 3/150 batch : 7900/8000 acc : 0.14\n",
      " Current : 3 Acc : 0.11179999962449073 Test Acc : 0.09999999976716936\n",
      "\n",
      "Current Iter : 4/150 batch : 7900/8000 acc : 0.11\n",
      " Current : 4 Acc : 0.12080000028014183 Test Acc : 0.12437499961815775\n",
      "\n",
      "Current Iter : 5/150 batch : 7900/8000 acc : 0.12\n",
      " Current : 5 Acc : 0.15379999995231627 Test Acc : 0.158500000461936\n",
      "\n",
      "Current Iter : 6/150 batch : 7900/8000 acc : 0.16\n",
      " Current : 6 Acc : 0.17820000007748604 Test Acc : 0.1642500003799796\n",
      "\n",
      "Current Iter : 7/150 batch : 7900/8000 acc : 0.25\n",
      " Current : 7 Acc : 0.2115999987721443 Test Acc : 0.21212499951943756\n",
      "\n",
      "Current Iter : 8/150 batch : 7900/8000 acc : 0.34\n",
      " Current : 8 Acc : 0.23819999873638154 Test Acc : 0.23000000063329934\n",
      "\n",
      "Current Iter : 9/150 batch : 7900/8000 acc : 0.32\n",
      " Current : 9 Acc : 0.247600000500679 Test Acc : 0.25125000029802325\n",
      "\n",
      "Current Iter : 10/150 batch : 7900/8000 acc : 0.33\n",
      " Current : 10 Acc : 0.264399998486042 Test Acc : 0.2599999999627471\n",
      "\n",
      "Current Iter : 11/150 batch : 7900/8000 acc : 0.32\n",
      " Current : 11 Acc : 0.27479999989271164 Test Acc : 0.28787500113248826\n",
      "\n",
      "Current Iter : 12/150 batch : 7900/8000 acc : 0.39\n",
      " Current : 12 Acc : 0.29060000002384184 Test Acc : 0.30837500039488075\n",
      "\n",
      "Current Iter : 13/150 batch : 7900/8000 acc : 0.37\n",
      " Current : 13 Acc : 0.3015999999642372 Test Acc : 0.3076250026002526\n",
      "\n",
      "Current Iter : 14/150 batch : 7900/8000 acc : 0.44\n",
      " Current : 14 Acc : 0.3059999996423721 Test Acc : 0.3325000017881393\n",
      "\n",
      "Current Iter : 15/150 batch : 7900/8000 acc : 0.36\n",
      " Current : 15 Acc : 0.31920000046491626 Test Acc : 0.33862500060349704\n",
      "\n",
      "Current Iter : 16/150 batch : 7900/8000 acc : 0.41\n",
      " Current : 16 Acc : 0.34560000240802763 Test Acc : 0.3528750021010637\n",
      "\n",
      "Current Iter : 17/150 batch : 7900/8000 acc : 0.37\n",
      " Current : 17 Acc : 0.34800000220537186 Test Acc : 0.3527500003576279\n",
      "\n",
      "Current Iter : 18/150 batch : 7900/8000 acc : 0.39\n",
      " Current : 18 Acc : 0.3610000014305115 Test Acc : 0.35099999997764825\n",
      "\n",
      "Current Iter : 19/150 batch : 7900/8000 acc : 0.38\n",
      " Current : 19 Acc : 0.3654000011086464 Test Acc : 0.371000000461936\n",
      "\n",
      "Current Iter : 20/150 batch : 7900/8000 acc : 0.41\n",
      " Current : 20 Acc : 0.36479999899864196 Test Acc : 0.3733750004321337\n",
      "\n",
      "Current Iter : 21/150 batch : 7900/8000 acc : 0.38\n",
      " Current : 21 Acc : 0.3781999999284744 Test Acc : 0.3697499994188547\n",
      "\n",
      "Current Iter : 22/150 batch : 7900/8000 acc : 0.38\n",
      " Current : 22 Acc : 0.37879999935626985 Test Acc : 0.38037499971687794\n",
      "\n",
      "Current Iter : 23/150 batch : 7900/8000 acc : 0.46\n",
      " Current : 23 Acc : 0.3835999995470047 Test Acc : 0.3742500010877848\n",
      "\n",
      "Current Iter : 24/150 batch : 7900/8000 acc : 0.36\n",
      " Current : 24 Acc : 0.38399999976158145 Test Acc : 0.37837500087916853\n",
      "\n",
      "Current Iter : 25/150 batch : 7900/8000 acc : 0.41\n",
      " Current : 25 Acc : 0.3906000000238419 Test Acc : 0.3916249979287386\n",
      "\n",
      "Current Iter : 26/150 batch : 7900/8000 acc : 0.38\n",
      " Current : 26 Acc : 0.4013999980688095 Test Acc : 0.39199999906122684\n",
      "\n",
      "Current Iter : 27/150 batch : 7900/8000 acc : 0.37\n",
      " Current : 27 Acc : 0.4056000018119812 Test Acc : 0.3842499990016222\n",
      "\n",
      "Current Iter : 28/150 batch : 7900/8000 acc : 0.35\n",
      " Current : 28 Acc : 0.4023999977111816 Test Acc : 0.37712499871850014\n",
      "\n",
      "Current Iter : 29/150 batch : 7900/8000 acc : 0.38\n",
      " Current : 29 Acc : 0.40499999940395354 Test Acc : 0.39100000001490115\n",
      "\n",
      "Current Iter : 30/150 batch : 7900/8000 acc : 0.34\n",
      " Current : 30 Acc : 0.40160000026226045 Test Acc : 0.39062499813735485\n",
      "\n",
      "Current Iter : 31/150 batch : 7900/8000 acc : 0.45\n",
      " Current : 31 Acc : 0.4083999985456467 Test Acc : 0.4024999991059303\n",
      "\n",
      "Current Iter : 32/150 batch : 7900/8000 acc : 0.47\n",
      " Current : 32 Acc : 0.42139999866485595 Test Acc : 0.39974999949336054\n",
      "\n",
      "Current Iter : 33/150 batch : 7900/8000 acc : 0.38\n",
      " Current : 33 Acc : 0.4173999959230423 Test Acc : 0.40099999979138373\n",
      "\n",
      "Current Iter : 34/150 batch : 7900/8000 acc : 0.39\n",
      " Current : 34 Acc : 0.42339999675750734 Test Acc : 0.3997499991208315\n",
      "\n",
      "Current Iter : 35/150 batch : 7900/8000 acc : 0.48\n",
      " Current : 35 Acc : 0.42619999647140505 Test Acc : 0.40762499794363977\n",
      "\n",
      "Current Iter : 36/150 batch : 7900/8000 acc : 0.41\n",
      " Current : 36 Acc : 0.42779999852180484 Test Acc : 0.41049999967217443\n",
      "\n",
      "Current Iter : 37/150 batch : 7900/8000 acc : 0.49\n",
      " Current : 37 Acc : 0.43420000016689303 Test Acc : 0.40287499912083147\n",
      "\n",
      "Current Iter : 38/150 batch : 7900/8000 acc : 0.41\n",
      " Current : 38 Acc : 0.434599996805191 Test Acc : 0.41649999767541884\n",
      "\n",
      "Current Iter : 39/150 batch : 7900/8000 acc : 0.41\n",
      " Current : 39 Acc : 0.43919999957084654 Test Acc : 0.4141249984502792\n",
      "\n",
      "Current Iter : 40/150 batch : 7900/8000 acc : 0.36\n",
      " Current : 40 Acc : 0.44099999725818634 Test Acc : 0.39174999929964543\n",
      "\n",
      "Current Iter : 41/150 batch : 7900/8000 acc : 0.41\n",
      " Current : 41 Acc : 0.44059999763965607 Test Acc : 0.4194999985396862\n",
      "\n",
      "Current Iter : 42/150 batch : 7900/8000 acc : 0.43\n",
      " Current : 42 Acc : 0.44019999861717224 Test Acc : 0.41899999864399434\n",
      "\n",
      "Current Iter : 43/150 batch : 7900/8000 acc : 0.41\n",
      " Current : 43 Acc : 0.4511999958753586 Test Acc : 0.41424999833106996\n",
      "\n",
      "Current Iter : 44/150 batch : 7900/8000 acc : 0.42\n",
      " Current : 44 Acc : 0.44839999556541443 Test Acc : 0.42849999815225603\n",
      "\n",
      "Current Iter : 45/150 batch : 7900/8000 acc : 0.44\n",
      " Current : 45 Acc : 0.457799996137619 Test Acc : 0.4251249980181456\n",
      "\n",
      "Current Iter : 46/150 batch : 7900/8000 acc : 0.39\n",
      " Current : 46 Acc : 0.4569999974966049 Test Acc : 0.40474999994039534\n",
      "\n",
      "Current Iter : 47/150 batch : 7900/8000 acc : 0.43\n",
      " Current : 47 Acc : 0.45719999611377715 Test Acc : 0.4339999996125698\n",
      "\n",
      "Current Iter : 48/150 batch : 7900/8000 acc : 0.45\n",
      " Current : 48 Acc : 0.46739999651908876 Test Acc : 0.433874998241663\n",
      "\n",
      "Current Iter : 49/150 batch : 7900/8000 acc : 0.38\n",
      " Current : 49 Acc : 0.4659999978542328 Test Acc : 0.40949999876320364\n",
      "\n",
      "Current Iter : 50/150 batch : 7900/8000 acc : 0.43\n",
      " Current : 50 Acc : 0.4623999935388565 Test Acc : 0.437499999627471\n",
      "\n",
      "Current Iter : 51/150 batch : 7900/8000 acc : 0.44\n",
      " Current : 51 Acc : 0.4641999971866608 Test Acc : 0.4358749981969595\n",
      "\n",
      "Current Iter : 52/150 batch : 7900/8000 acc : 0.42\n",
      " Current : 52 Acc : 0.4739999997615814 Test Acc : 0.4126249995082617\n",
      "\n",
      "Current Iter : 53/150 batch : 7900/8000 acc : 0.43\n",
      " Current : 53 Acc : 0.47019999861717227 Test Acc : 0.4441249962896109\n",
      "\n",
      "Current Iter : 54/150 batch : 7900/8000 acc : 0.45\n",
      " Current : 54 Acc : 0.4809999948740005 Test Acc : 0.44837499894201754\n",
      "\n",
      "Current Iter : 55/150 batch : 7900/8000 acc : 0.44\n",
      " Current : 55 Acc : 0.4781999945640564 Test Acc : 0.4493749957531691\n",
      "\n",
      "Current Iter : 56/150 batch : 7900/8000 acc : 0.38\n",
      " Current : 56 Acc : 0.4693999969959259 Test Acc : 0.3964999996125698\n",
      "\n",
      "Current Iter : 57/150 batch : 7900/8000 acc : 0.46\n",
      " Current : 57 Acc : 0.46819999754428865 Test Acc : 0.4571249958127737\n",
      "\n",
      "Current Iter : 58/150 batch : 7900/8000 acc : 0.45\n",
      " Current : 58 Acc : 0.4881999945640564 Test Acc : 0.4383749973028898\n",
      "\n",
      "Current Iter : 59/150 batch : 7900/8000 acc : 0.49\n",
      " Current : 59 Acc : 0.4795999974012375 Test Acc : 0.4561249990016222\n",
      "\n",
      "Current Iter : 60/150 batch : 7900/8000 acc : 0.48\n",
      " Current : 60 Acc : 0.4865999978780746 Test Acc : 0.45349999628961085\n",
      "\n",
      "Current Iter : 61/150 batch : 7900/8000 acc : 0.47\n",
      " Current : 61 Acc : 0.4955999982357025 Test Acc : 0.4607499983161688\n",
      "\n",
      "Current Iter : 62/150 batch : 7900/8000 acc : 0.52\n",
      " Current : 62 Acc : 0.4979999941587448 Test Acc : 0.46762499660253526\n",
      "\n",
      "Current Iter : 63/150 batch : 7900/8000 acc : 0.44\n",
      " Current : 63 Acc : 0.49679999709129335 Test Acc : 0.44987499825656413\n",
      "\n",
      "Current Iter : 64/150 batch : 7900/8000 acc : 0.58\n",
      " Current : 64 Acc : 0.5038000005483627 Test Acc : 0.4738749984651804\n",
      "\n",
      "Current Iter : 65/150 batch : 7900/8000 acc : 0.47\n",
      " Current : 65 Acc : 0.5165999954938889 Test Acc : 0.457499997317791\n",
      "\n",
      "Current Iter : 66/150 batch : 7900/8000 acc : 0.53\n",
      " Current : 66 Acc : 0.5145999974012375 Test Acc : 0.47487499564886093\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Iter : 67/150 batch : 7900/8000 acc : 0.46\n",
      " Current : 67 Acc : 0.5201999932527542 Test Acc : 0.4617499984800816\n",
      "\n",
      "Current Iter : 68/150 batch : 7900/8000 acc : 0.51\n",
      " Current : 68 Acc : 0.5215999972820282 Test Acc : 0.4598749980330467\n",
      "\n",
      "Current Iter : 69/150 batch : 7900/8000 acc : 0.47\n",
      " Current : 69 Acc : 0.5119999939203262 Test Acc : 0.47162499763071536\n",
      "\n",
      "Current Iter : 70/150 batch : 7900/8000 acc : 0.53\n",
      " Current : 70 Acc : 0.5390000009536743 Test Acc : 0.48337499611079693\n",
      "\n",
      "Current Iter : 71/150 batch : 7900/8000 acc : 0.47\n",
      " Current : 71 Acc : 0.52819999396801 Test Acc : 0.4769999966025352\n",
      "\n",
      "Current Iter : 72/150 batch : 7900/8000 acc : 0.59\n",
      " Current : 72 Acc : 0.5243999963998794 Test Acc : 0.4658749982714653\n",
      "\n",
      "Current Iter : 73/150 batch : 7900/8000 acc : 0.48\n",
      " Current : 73 Acc : 0.5217999970912933 Test Acc : 0.47449999712407587\n",
      "\n",
      "Current Iter : 74/150 batch : 7900/8000 acc : 0.47\n",
      " Current : 74 Acc : 0.5339999961853027 Test Acc : 0.4834999952465296\n",
      "\n",
      "Current Iter : 75/150 batch : 7900/8000 acc : 0.48\n",
      " Current : 75 Acc : 0.5411999970674515 Test Acc : 0.47262499779462813\n",
      "\n",
      "Current Iter : 76/150 batch : 7900/8000 acc : 0.51\n",
      " Current : 76 Acc : 0.5407999980449677 Test Acc : 0.4871249973773956\n",
      "\n",
      "Current Iter : 77/150 batch : 7900/8000 acc : 0.52\n",
      " Current : 77 Acc : 0.5381999921798706 Test Acc : 0.4753749970346689\n",
      "\n",
      "Current Iter : 78/150 batch : 7900/8000 acc : 0.49\n",
      " Current : 78 Acc : 0.5429999983310699 Test Acc : 0.48699999526143073\n",
      "\n",
      "Current Iter : 79/150 batch : 7900/8000 acc : 0.48\n",
      " Current : 79 Acc : 0.5413999962806701 Test Acc : 0.48237499706447123\n",
      "\n",
      "Current Iter : 80/150 batch : 7900/8000 acc : 0.51\n",
      " Current : 80 Acc : 0.5437999951839447 Test Acc : 0.48487499728798866\n",
      "\n",
      "Current Iter : 81/150 batch : 7900/8000 acc : 0.54\n",
      " Current : 81 Acc : 0.5507999956607819 Test Acc : 0.487874997779727\n",
      "\n",
      "Current Iter : 82/150 batch : 7900/8000 acc : 0.48\n",
      " Current : 82 Acc : 0.5541999965906144 Test Acc : 0.4809999968856573\n",
      "\n",
      "Current Iter : 83/150 batch : 7900/8000 acc : 0.51\n",
      " Current : 83 Acc : 0.5444000005722046 Test Acc : 0.4821249969303608\n",
      "\n",
      "Current Iter : 84/150 batch : 7900/8000 acc : 0.57\n",
      " Current : 84 Acc : 0.5549999970197678 Test Acc : 0.4799999985843897\n",
      "\n",
      "Current Iter : 85/150 batch : 7900/8000 acc : 0.51\n",
      " Current : 85 Acc : 0.5598000013828277 Test Acc : 0.48849999718368053\n",
      "\n",
      "Current Iter : 86/150 batch : 7900/8000 acc : 0.51\n",
      " Current : 86 Acc : 0.5555999928712845 Test Acc : 0.48524999991059303\n",
      "\n",
      "Current Iter : 87/150 batch : 7900/8000 acc : 0.51\n",
      " Current : 87 Acc : 0.5653999948501587 Test Acc : 0.4876249980181456\n",
      "\n",
      "Current Iter : 88/150 batch : 7900/8000 acc : 0.52\n",
      " Current : 88 Acc : 0.5612000000476837 Test Acc : 0.4732499971985817\n",
      "\n",
      "Current Iter : 89/150 batch : 7900/8000 acc : 0.49\n",
      " Current : 89 Acc : 0.5449999994039536 Test Acc : 0.4792499952018261\n",
      "\n",
      "Current Iter : 90/150 batch : 7900/8000 acc : 0.51\n",
      " Current : 90 Acc : 0.5703999990224838 Test Acc : 0.49199999794363974\n",
      "\n",
      "Current Iter : 91/150 batch : 7900/8000 acc : 0.52\n",
      " Current : 91 Acc : 0.570799998641014 Test Acc : 0.48574999868869784\n",
      "\n",
      "Current Iter : 92/150 batch : 7900/8000 acc : 0.49\n",
      " Current : 92 Acc : 0.5691999942064285 Test Acc : 0.48999999798834326\n",
      "\n",
      "Current Iter : 93/150 batch : 7900/8000 acc : 0.52\n",
      " Current : 93 Acc : 0.5703999984264374 Test Acc : 0.47312499918043616\n",
      "\n",
      "Current Iter : 94/150 batch : 7900/8000 acc : 0.52\n",
      " Current : 94 Acc : 0.5733999985456467 Test Acc : 0.4919999986886978\n",
      "\n",
      "Current Iter : 95/150 batch : 7900/8000 acc : 0.54\n",
      " Current : 95 Acc : 0.5731999963521958 Test Acc : 0.48162499740719794\n",
      "\n",
      "Current Iter : 96/150 batch : 7900/8000 acc : 0.52\n",
      " Current : 96 Acc : 0.5568000006675721 Test Acc : 0.46924999728798866\n",
      "\n",
      "Current Iter : 97/150 batch : 7900/8000 acc : 0.49\n",
      " Current : 97 Acc : 0.5734000009298325 Test Acc : 0.48274999782443045\n",
      "\n",
      "Current Iter : 98/150 batch : 7900/8000 acc : 0.44\n",
      " Current : 98 Acc : 0.5765999984741211 Test Acc : 0.47887499667704103\n",
      "\n",
      "Current Iter : 99/150 batch : 7900/8000 acc : 0.49\n",
      " Current : 99 Acc : 0.5715999984741211 Test Acc : 0.4903749950230122\n",
      "\n",
      "Current Iter : 100/150 batch : 7900/8000 acc : 0.49\n",
      " Current : 100 Acc : 0.5863999950885773 Test Acc : 0.4821249973028898\n",
      "\n",
      "Current Iter : 101/150 batch : 7900/8000 acc : 0.59\n",
      " Current : 101 Acc : 0.5683999973535537 Test Acc : 0.4699999962002039\n",
      "\n",
      "Current Iter : 102/150 batch : 7900/8000 acc : 0.49\n",
      " Current : 102 Acc : 0.575599998831749 Test Acc : 0.48137499764561653\n",
      "\n",
      "Current Iter : 103/150 batch : 7900/8000 acc : 0.45\n",
      " Current : 103 Acc : 0.5838000023365021 Test Acc : 0.47737499736249445\n",
      "\n",
      "Current Iter : 104/150 batch : 7900/8000 acc : 0.47\n",
      " Current : 104 Acc : 0.5897999995946884 Test Acc : 0.49262499548494815\n",
      "\n",
      "Current Iter : 105/150 batch : 7900/8000 acc : 0.45\n",
      " Current : 105 Acc : 0.5983999991416931 Test Acc : 0.48549999557435514\n",
      "\n",
      "Current Iter : 106/150 batch : 7900/8000 acc : 0.49\n",
      " Current : 106 Acc : 0.5851999986171722 Test Acc : 0.48862499855458735\n",
      "\n",
      "Current Iter : 107/150 batch : 7900/8000 acc : 0.59\n",
      " Current : 107 Acc : 0.5925999987125397 Test Acc : 0.48874999657273294\n",
      "\n",
      "Current Iter : 108/150 batch : 7900/8000 acc : 0.45\n",
      " Current : 108 Acc : 0.5943999975919724 Test Acc : 0.4902499958872795\n",
      "\n",
      "Current Iter : 109/150 batch : 7900/8000 acc : 0.54\n",
      " Current : 109 Acc : 0.5686000019311905 Test Acc : 0.4824999958276749\n",
      "\n",
      "Current Iter : 110/150 batch : 7900/8000 acc : 0.48\n",
      " Current : 110 Acc : 0.5891999983787537 Test Acc : 0.4802499979734421\n",
      "\n",
      "Current Iter : 111/150 batch : 7900/8000 acc : 0.48\n",
      " Current : 111 Acc : 0.5793999952077865 Test Acc : 0.48074999675154684\n",
      "\n",
      "Current Iter : 112/150 batch : 7900/8000 acc : 0.49\n",
      " Current : 112 Acc : 0.5890000021457672 Test Acc : 0.48624999560415744\n",
      "\n",
      "Current Iter : 113/150 batch : 7900/8000 acc : 0.46\n",
      " Current : 113 Acc : 0.5952000027894974 Test Acc : 0.48787499628961084\n",
      "\n",
      "Current Iter : 114/150 batch : 7900/8000 acc : 0.47\n",
      " Current : 114 Acc : 0.5959999984502793 Test Acc : 0.49337499737739565\n",
      "\n",
      "Current Iter : 115/150 batch : 7900/8000 acc : 0.47\n",
      " Current : 115 Acc : 0.5979999995231629 Test Acc : 0.4941249959170818\n",
      "\n",
      "Current Iter : 116/150 batch : 7900/8000 acc : 0.46\n",
      " Current : 116 Acc : 0.5955999970436097 Test Acc : 0.48987499698996545\n",
      "\n",
      "Current Iter : 117/150 batch : 7900/8000 acc : 0.46\n",
      " Current : 117 Acc : 0.588000003695488 Test Acc : 0.4984999965876341\n",
      "\n",
      "Current Iter : 118/150 batch : 7900/8000 acc : 0.45\n",
      " Current : 118 Acc : 0.5839999961853027 Test Acc : 0.4912499975413084\n",
      "\n",
      "Current Iter : 119/150 batch : 7900/8000 acc : 0.48\n",
      " Current : 119 Acc : 0.5943999969959259 Test Acc : 0.5022499959915876\n",
      "\n",
      "Current Iter : 120/150 batch : 7900/8000 acc : 0.45\n",
      " Current : 120 Acc : 0.5897999966144561 Test Acc : 0.5049999944865704\n",
      "\n",
      "Current Iter : 121/150 batch : 7900/8000 acc : 0.53\n",
      " Current : 121 Acc : 0.603199999332428 Test Acc : 0.502999996393919\n",
      "\n",
      "Current Iter : 122/150 batch : 7900/8000 acc : 0.48\n",
      " Current : 122 Acc : 0.594799998998642 Test Acc : 0.5062499947845935\n",
      "\n",
      "Current Iter : 123/150 batch : 7900/8000 acc : 0.47\n",
      " Current : 123 Acc : 0.6085999983549119 Test Acc : 0.5098749969154597\n",
      "\n",
      "Current Iter : 124/150 batch : 7900/8000 acc : 0.49\n",
      " Current : 124 Acc : 0.6056000030040741 Test Acc : 0.5058749947696924\n",
      "\n",
      "Current Iter : 125/150 batch : 7900/8000 acc : 0.52\n",
      " Current : 125 Acc : 0.5893999969959259 Test Acc : 0.49824999608099463\n",
      "\n",
      "Current Iter : 126/150 batch : 7900/8000 acc : 0.46\n",
      " Current : 126 Acc : 0.5943999999761581 Test Acc : 0.4941249966621399\n",
      "\n",
      "Current Iter : 127/150 batch : 7900/8000 acc : 0.56\n",
      " Current : 127 Acc : 0.6127999997138978 Test Acc : 0.5047499965876341\n",
      "\n",
      "Current Iter : 128/150 batch : 7900/8000 acc : 0.49\n",
      " Current : 128 Acc : 0.6193999975919724 Test Acc : 0.5081249963492155\n",
      "\n",
      "Current Iter : 129/150 batch : 7900/8000 acc : 0.47\n",
      " Current : 129 Acc : 0.6269999980926514 Test Acc : 0.5084999971091747\n",
      "\n",
      "Current Iter : 130/150 batch : 7900/8000 acc : 0.48\n",
      " Current : 130 Acc : 0.6263999962806701 Test Acc : 0.5142499972134829\n",
      "\n",
      "Current Iter : 131/150 batch : 7900/8000 acc : 0.47\n",
      " Current : 131 Acc : 0.6095999962091446 Test Acc : 0.5048749949783087\n",
      "\n",
      "Current Iter : 132/150 batch : 7900/8000 acc : 0.47\n",
      " Current : 132 Acc : 0.6069999980926514 Test Acc : 0.482499997317791\n",
      "\n",
      "Current Iter : 133/150 batch : 7900/8000 acc : 0.48\n",
      " Current : 133 Acc : 0.5990000033378601 Test Acc : 0.5041249975562095\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Iter : 134/150 batch : 7900/8000 acc : 0.42\n",
      " Current : 134 Acc : 0.589400002360344 Test Acc : 0.49337499365210535\n",
      "\n",
      "Current Iter : 135/150 batch : 7900/8000 acc : 0.51\n",
      " Current : 135 Acc : 0.6179999995231629 Test Acc : 0.5069999977946281\n",
      "\n",
      "Current Iter : 136/150 batch : 7900/8000 acc : 0.47\n",
      " Current : 136 Acc : 0.6142000031471252 Test Acc : 0.49812499769032004\n",
      "\n",
      "Current Iter : 137/150 batch : 7900/8000 acc : 0.48\n",
      " Current : 137 Acc : 0.6310000014305115 Test Acc : 0.49799999557435515\n",
      "\n",
      "Current Iter : 138/150 batch : 7900/8000 acc : 0.49\n",
      " Current : 138 Acc : 0.6242000019550323 Test Acc : 0.5007499985396862\n",
      "\n",
      "Current Iter : 139/150 batch : 7900/8000 acc : 0.47\n",
      " Current : 139 Acc : 0.6029999947547913 Test Acc : 0.4994999963790178\n",
      "\n",
      "Current Iter : 140/150 batch : 7900/8000 acc : 0.45\n",
      " Current : 140 Acc : 0.6160000038146972 Test Acc : 0.49149999506771563\n",
      "\n",
      "Current Iter : 141/150 batch : 7900/8000 acc : 0.48\n",
      " Current : 141 Acc : 0.6243999981880188 Test Acc : 0.5137499962002039\n",
      "\n",
      "Current Iter : 142/150 batch : 7900/8000 acc : 0.47\n",
      " Current : 142 Acc : 0.6331999981403351 Test Acc : 0.4983749981969595\n",
      "\n",
      "Current Iter : 143/150 batch : 7900/8000 acc : 0.45\n",
      " Current : 143 Acc : 0.6225999975204468 Test Acc : 0.4977499932050705\n",
      "\n",
      "Current Iter : 144/150 batch : 7900/8000 acc : 0.58\n",
      " Current : 144 Acc : 0.6149999976158143 Test Acc : 0.5018749974668026\n",
      "\n",
      "Current Iter : 145/150 batch : 7900/8000 acc : 0.51\n",
      " Current : 145 Acc : 0.6067999958992004 Test Acc : 0.5007499981671572\n",
      "\n",
      "Current Iter : 146/150 batch : 7900/8000 acc : 0.44\n",
      " Current : 146 Acc : 0.6162000024318695 Test Acc : 0.48599999621510503\n",
      "\n",
      "Current Iter : 147/150 batch : 7900/8000 acc : 0.47\n",
      " Current : 147 Acc : 0.6169999992847442 Test Acc : 0.5013749964535237\n",
      "\n",
      "Current Iter : 148/150 batch : 7900/8000 acc : 0.49\n",
      " Current : 148 Acc : 0.6493999999761582 Test Acc : 0.5061249982565641\n",
      "\n",
      "Current Iter : 149/150 batch : 7900/8000 acc : 0.43\n",
      " Current : 149 Acc : 0.6492000019550324 Test Acc : 0.5071249973028898\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "sess.run(tf.global_variables_initializer())\n",
    "avg_acc_train = 0; avg_acc_test  = 0; \n",
    "train_acc     = [];test_acc = []\n",
    "for iter in range(num_epoch):\n",
    "\n",
    "    for current_batch_index in range(0,len(train_images),batch_size):\n",
    "        current_data  = train_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = train_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy,gradient_update],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(train_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_train = avg_acc_train + sess_results[0]\n",
    "        \n",
    "    for current_batch_index in range(0,len(test_images), batch_size):\n",
    "        current_data  = test_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = test_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(test_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_test = avg_acc_test + sess_results[0]   \n",
    "        \n",
    "    print(\"\\n Current : \"+ str(iter) + \" Acc : \" + str(avg_acc_train/(len(train_images)/batch_size)) + \" Test Acc : \" + str(avg_acc_test/(len(test_images)/batch_size)) + '\\n')\n",
    "    \n",
    "    # save the training\n",
    "    train_acc.append(avg_acc_train/(len(train_images)/batch_size))\n",
    "    test_acc .append(avg_acc_test/(len(test_images)/batch_size)  )\n",
    "    \n",
    "    avg_acc_train = 0 ; avg_acc_test  = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T19:30:41.693057Z",
     "start_time": "2018-12-20T19:30:41.571383Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T07:39:51.379493Z",
     "start_time": "2018-12-20T07:39:51.079050Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T07:43:08.033249Z",
     "start_time": "2018-12-20T07:43:06.719715Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T20:25:08.945691Z",
     "start_time": "2018-12-20T20:25:08.918763Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T20:25:16.316488Z",
     "start_time": "2018-12-20T20:25:16.066594Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T07:57:11.376556Z",
     "start_time": "2018-12-20T07:57:11.370596Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "1. mttk/STL10. (2018). GitHub. Retrieved 19 December 2018, from https://github.com/mttk/STL10\n",
    "2. [duplicate], H. (2018). How to display multiple images in one figure correctly?. Stack Overflow. Retrieved 19 December 2018, from https://stackoverflow.com/questions/46615554/how-to-display-multiple-images-in-one-figure-correctly"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
