{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T07:57:42.564610Z",
     "start_time": "2018-12-20T07:57:39.629309Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# import Library and some random image data set\n",
    "import tensorflow as tf\n",
    "import numpy      as np\n",
    "import seaborn    as sns \n",
    "import pandas     as pd\n",
    "import os,sys\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(78); tf.set_random_seed(78)\n",
    "\n",
    "# get some of the STL data set\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from skimage import util \n",
    "from skimage.transform import resize\n",
    "from skimage.io import imread\n",
    "import warnings\n",
    "from numpy import inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T07:57:50.528979Z",
     "start_time": "2018-12-20T07:57:48.468510Z"
    },
    "code_folding": [
     0,
     2,
     29,
     37
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 96, 96, 3) 1.0 0.0\n",
      "(5000, 10) 1.0 0.0\n",
      "(8000, 96, 96, 3) 1.0 0.0\n",
      "(8000, 10) 1.0 0.0\n"
     ]
    }
   ],
   "source": [
    "# read all of the data\n",
    "# https://github.com/mttk/STL10\n",
    "def read_all_images(path_to_data):\n",
    "    \"\"\"\n",
    "    :param path_to_data: the file containing the binary images from the STL-10 dataset\n",
    "    :return: an array containing all the images\n",
    "    \"\"\"\n",
    "\n",
    "    with open(path_to_data, 'rb') as f:\n",
    "        # read whole file in uint8 chunks\n",
    "        everything = np.fromfile(f, dtype=np.uint8)\n",
    "\n",
    "        # We force the data into 3x96x96 chunks, since the\n",
    "        # images are stored in \"column-major order\", meaning\n",
    "        # that \"the first 96*96 values are the red channel,\n",
    "        # the next 96*96 are green, and the last are blue.\"\n",
    "        # The -1 is since the size of the pictures depends\n",
    "        # on the input file, and this way numpy determines\n",
    "        # the size on its own.\n",
    "\n",
    "        images = np.reshape(everything, (-1, 3, 96, 96))\n",
    "\n",
    "        # Now transpose the images into a standard image format\n",
    "        # readable by, for example, matplotlib.imshow\n",
    "        # You might want to comment this line or reverse the shuffle\n",
    "        # if you will use a learning algorithm like CNN, since they like\n",
    "        # their channels separated.\n",
    "        images = np.transpose(images, (0, 3, 2, 1))\n",
    "        return images\n",
    "def read_labels(path_to_labels):\n",
    "    \"\"\"\n",
    "    :param path_to_labels: path to the binary file containing labels from the STL-10 dataset\n",
    "    :return: an array containing the labels\n",
    "    \"\"\"\n",
    "    with open(path_to_labels, 'rb') as f:\n",
    "        labels = np.fromfile(f, dtype=np.uint8)\n",
    "        return labels\n",
    "def show_images(data,row=1,col=1):\n",
    "    fig=plt.figure(figsize=(10,10))\n",
    "    columns = col; rows = row\n",
    "    for i in range(1, columns*rows +1):\n",
    "        fig.add_subplot(rows, columns, i)\n",
    "        plt.imshow(data[i-1])\n",
    "    plt.show()\n",
    "\n",
    "train_images = read_all_images(\"../../../DataSet/STL10/stl10_binary/train_X.bin\") / 255.0\n",
    "train_labels = read_labels    (\"../../../DataSet/STL10/stl10_binary/train_Y.bin\")\n",
    "test_images  = read_all_images(\"../../../DataSet/STL10/stl10_binary/test_X.bin\")  / 255.0\n",
    "test_labels  = read_labels    (\"../../../DataSet/STL10/stl10_binary/test_y.bin\")\n",
    "\n",
    "label_encoder= OneHotEncoder(sparse=False,categories='auto')\n",
    "train_labels = label_encoder.fit_transform(train_labels.reshape((-1,1)))\n",
    "test_labels  = label_encoder.fit_transform(test_labels.reshape((-1,1)))\n",
    "\n",
    "print(train_images.shape,train_images.max(),train_images.min())\n",
    "print(train_labels.shape,train_labels.max(),train_labels.min())\n",
    "print(test_images.shape,test_images.max(),test_images.min())\n",
    "print(test_labels.shape,test_labels.max(),test_labels.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T08:30:04.030406Z",
     "start_time": "2018-12-20T08:30:03.960359Z"
    },
    "code_folding": [
     58,
     99,
     140
    ]
   },
   "outputs": [],
   "source": [
    "# create the layers\n",
    "def tf_softmax(x): return tf.nn.softmax(x)\n",
    "\n",
    "def tf_elu(x):   return tf.nn.elu(x)\n",
    "def d_tf_elu(x): return tf.cast(tf.greater(x,0),tf.float32)  + (tf_elu(tf.cast(tf.less_equal(x,0),tf.float32) * x) + 1.0)\n",
    "\n",
    "def tf_relu(x):   return tf.nn.relu(x)\n",
    "def d_tf_relu(x): return tf.cast(tf.greater(x,0),tf.float32)\n",
    "\n",
    "def tf_tanh(x):   return tf.nn.tanh(x)\n",
    "def d_tf_tanh(x): return 1 - tf_tanh(x) ** 2\n",
    "\n",
    "def tf_sigmoid(x):   return tf.nn.sigmoid(x)\n",
    "def d_tf_sigmoid(x): return tf_sigmoid(x) * (1.0-tf_sigmoid(x))\n",
    "\n",
    "class CNN():\n",
    "\n",
    "    def __init__(self,k,inc,out, stddev=0.05,which_reg=0,act=tf_relu,d_act=d_tf_relu):\n",
    "        self.w          = tf.Variable(tf.random_normal([k,k,inc,out],stddev=stddev,seed=4,dtype=tf.float32))\n",
    "        self.m,self.v   = tf.Variable(tf.zeros_like(self.w)),tf.Variable(tf.zeros_like(self.w))\n",
    "        self.act,self.d_act = act,d_act\n",
    "        self.which_reg  = which_reg\n",
    "        \n",
    "    def getw(self): return self.w\n",
    "\n",
    "    def feedforward(self,input,stride=1,padding='SAME'):\n",
    "        self.input  = input\n",
    "        self.layer  = tf.nn.conv2d(input,self.w,strides=[1,stride,stride,1],padding=padding) \n",
    "        self.layerA = self.act(self.layer)\n",
    "        return [self.layer,self.layerA]\n",
    "    \n",
    "    def backprop(self,gradient,stride=1,padding='SAME'):\n",
    "        grad_part_1 = gradient\n",
    "        grad_part_2 = self.d_act(self.layer)\n",
    "        grad_part_3 = self.input\n",
    "\n",
    "        grad_middle = grad_part_1 * grad_part_2\n",
    "        grad        = tf.nn.conv2d_backprop_filter(input = grad_part_3,filter_sizes = tf.shape(self.w),  out_backprop = grad_middle,strides=[1,stride,stride,1],padding=padding) / batch_size\n",
    "        grad_pass   = tf.nn.conv2d_backprop_input (input_sizes = tf.shape(self.input),filter= self.w,out_backprop = grad_middle,strides=[1,stride,stride,1],padding=padding)\n",
    "\n",
    "        if self.which_reg == 0:   grad = grad\n",
    "        if self.which_reg == 0.5: grad = grad + lamda * (tf.sqrt(tf.abs(self.w))) * (1.0/tf.sqrt(tf.abs(self.w)+ 10e-5)) * tf.sign(self.w)\n",
    "        if self.which_reg == 1:   grad = grad + lamda * tf.sign(self.w)\n",
    "        if self.which_reg == 1.5: grad = grad + lamda * 1.0/(tf.sqrt(tf.square(self.w) + 10e-5)) * self.w\n",
    "        if self.which_reg == 2:   grad = grad + lamda * (1.0/tf.sqrt(tf.square(tf.abs(self.w))+ 10e-5)) * tf.abs(self.w) * tf.sign(self.w)\n",
    "        if self.which_reg == 2.5: grad = grad + lamda * 2.0 * self.w\n",
    "        if self.which_reg == 3:   grad = grad + lamda * tf.pow(tf.pow(tf.abs(self.w),3)+ 10e-5,-0.66) * tf.pow(tf.abs(self.w),2) * tf.sign(self.w)\n",
    "        if self.which_reg == 4:   grad = grad + lamda * tf.pow(tf.pow(tf.abs(self.w),4)+ 10e-5,-0.75) * tf.pow(tf.abs(self.w),3) * tf.sign(self.w)\n",
    "\n",
    "        update_w = []\n",
    "        update_w.append(tf.assign( self.m,self.m*beta1 + (1-beta1) * (grad)   ))\n",
    "        update_w.append(tf.assign( self.v,self.v*beta2 + (1-beta2) * (grad ** 2)   ))\n",
    "        m_hat = self.m / (1-beta1) ; v_hat = self.v / (1-beta2)\n",
    "        adam_middle = m_hat * learning_rate/(tf.sqrt(v_hat) + adam_e)\n",
    "        update_w.append(tf.assign(self.w,tf.subtract(self.w,adam_middle  )))\n",
    "        \n",
    "        return grad_pass,grad,update_w\n",
    "    \n",
    "class tf_batch_norm_layer():\n",
    "    \n",
    "    def __init__(self,vector_shape,axis):\n",
    "        self.moving_mean = tf.Variable(tf.zeros(shape=[1,1,1,vector_shape],dtype=tf.float32))\n",
    "        self.moving_vari = tf.Variable(tf.zeros(shape=[1,1,1,vector_shape],dtype=tf.float32))\n",
    "        self.axis        = axis\n",
    "        \n",
    "    def feedforward(self,input,training_phase=True,eps = 1e-8):\n",
    "        self.input = input\n",
    "        self.input_size          = self.input.shape\n",
    "        self.batch,self.h,self.w,self.c = self.input_size[0].value,self.input_size[1].value,self.input_size[2].value,self.input_size[3].value\n",
    "\n",
    "        # Training Moving Average Mean         \n",
    "        def training_fn():\n",
    "            self.mean    = tf.reduce_mean(self.input,axis=self.axis ,keepdims=True)\n",
    "            self.var     = tf.reduce_mean(tf.square(self.input-self.mean),axis=self.axis,keepdims=True)\n",
    "            centered_data= (self.input - self.mean)/tf.sqrt(self.var + eps)\n",
    "            \n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean*0.9 + 0.1 * self.mean ))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari*0.9 + 0.1 * self.var  ))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        # Testing Moving Average Mean        \n",
    "        def  testing_fn():\n",
    "            centered_data   = (self.input - self.moving_mean)/tf.sqrt(self.moving_vari + eps)\n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        self.output,update_variable = tf.cond(training_phase,true_fn=training_fn,false_fn=testing_fn)\n",
    "        return self.output,update_variable\n",
    "    \n",
    "    def backprop(self,grad,eps = 1e-8):\n",
    "        change_parts = 1.0 /(self.batch * self.h * self.w)\n",
    "        grad_sigma   = tf.reduce_sum( grad *  (self.input-self.mean)     ,axis=self.axis,keepdims=True) * -0.5 * (self.var+eps) ** -1.5\n",
    "        grad_mean    = tf.reduce_sum( grad *  (-1./tf.sqrt(self.var+eps)),axis=self.axis,keepdims=True) + grad_sigma * change_parts * 2.0 * tf.reduce_sum((self.input-self.mean),axis=self.axis,keepdims=True) * -1\n",
    "        grad_x       = grad * 1/(tf.sqrt(self.var+eps)) + grad_sigma * change_parts * 2.0 * (self.input-self.mean) + grad_mean * change_parts\n",
    "        return grad_x\n",
    "\n",
    "class tf_layer_norm_layer():\n",
    "    \n",
    "    def __init__(self,vector_shape,axis):\n",
    "        self.moving_mean = tf.Variable(tf.zeros(shape=[vector_shape,1,1,1],dtype=tf.float32))\n",
    "        self.moving_vari = tf.Variable(tf.zeros(shape=[vector_shape,1,1,1],dtype=tf.float32))\n",
    "        self.axis        = axis\n",
    "        \n",
    "    def feedforward(self,input,training_phase=True,eps = 1e-8):\n",
    "        self.input = input\n",
    "        self.input_size          = self.input.shape\n",
    "        self.batch,self.h,self.w,self.c = self.input_size[0].value,self.input_size[1].value,self.input_size[2].value,self.input_size[3].value\n",
    "\n",
    "        # Training Moving Average Mean         \n",
    "        def training_fn():\n",
    "            self.mean    = tf.reduce_mean(self.input,axis=self.axis ,keepdims=True)\n",
    "            self.var     = tf.reduce_mean(tf.square(self.input-self.mean),axis=self.axis,keepdims=True)\n",
    "            centered_data= (self.input - self.mean)/tf.sqrt(self.var + eps)\n",
    "            \n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean*0.9 + 0.1 * self.mean ))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari*0.9 + 0.1 * self.var  ))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        # Testing Moving Average Mean        \n",
    "        def  testing_fn():\n",
    "            centered_data   = (self.input - self.moving_mean)/tf.sqrt(self.moving_vari + eps)\n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        self.output,update_variable = tf.cond(training_phase,true_fn=training_fn,false_fn=testing_fn)\n",
    "        return self.output,update_variable\n",
    "    \n",
    "    def backprop(self,grad,eps = 1e-8):\n",
    "        change_parts = 1.0 /(self.h * self.w * self.c)\n",
    "        grad_sigma   = tf.reduce_sum( grad *  (self.input-self.mean)     ,axis=self.axis,keepdims=True) * -0.5 * (self.var+eps) ** -1.5\n",
    "        grad_mean    = tf.reduce_sum( grad *  (-1./tf.sqrt(self.var+eps)),axis=self.axis,keepdims=True) + grad_sigma * change_parts * 2.0 * tf.reduce_sum((self.input-self.mean),axis=self.axis,keepdims=True) * -1\n",
    "        grad_x       = grad * 1/(tf.sqrt(self.var+eps)) + grad_sigma * change_parts * 2.0 * (self.input-self.mean) + grad_mean * change_parts\n",
    "        return grad_x\n",
    "    \n",
    "class tf_instance_norm_layer():\n",
    "    \n",
    "    def __init__(self,batch_size,vector_shape,axis):\n",
    "        self.moving_mean = tf.Variable(tf.zeros(shape=[batch_size,1,1,vector_shape],dtype=tf.float32))\n",
    "        self.moving_vari = tf.Variable(tf.zeros(shape=[batch_size,1,1,vector_shape],dtype=tf.float32))\n",
    "        self.axis        = axis\n",
    "        \n",
    "    def feedforward(self,input,training_phase=True,eps = 1e-8):\n",
    "        self.input = input\n",
    "        self.input_size          = self.input.shape\n",
    "        self.batch,self.h,self.w,self.c = self.input_size[0].value,self.input_size[1].value,self.input_size[2].value,self.input_size[3].value\n",
    "\n",
    "        # Training Moving Average Mean         \n",
    "        def training_fn():\n",
    "            self.mean    = tf.reduce_mean(self.input,axis=self.axis ,keepdims=True)\n",
    "            self.var     = tf.reduce_mean(tf.square(self.input-self.mean),axis=self.axis,keepdims=True)\n",
    "            centered_data= (self.input - self.mean)/tf.sqrt(self.var + eps)\n",
    "            \n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean*0.9 + 0.1 * self.mean ))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari*0.9 + 0.1 * self.var  ))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        # Testing Moving Average Mean        \n",
    "        def  testing_fn():\n",
    "            centered_data   = (self.input - self.moving_mean)/tf.sqrt(self.moving_vari + eps)\n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        self.output,update_variable = tf.cond(training_phase,true_fn=training_fn,false_fn=testing_fn)\n",
    "        return self.output,update_variable\n",
    "    \n",
    "    def backprop(self,grad,eps = 1e-8):\n",
    "        change_parts = 1.0 /(self.h * self.w)\n",
    "        grad_sigma   = tf.reduce_sum( grad *  (self.input-self.mean)     ,axis=self.axis,keepdims=True) * -0.5 * (self.var+eps) ** -1.5\n",
    "        grad_mean    = tf.reduce_sum( grad *  (-1./tf.sqrt(self.var+eps)),axis=self.axis,keepdims=True) + grad_sigma * change_parts * 2.0 * tf.reduce_sum((self.input-self.mean),axis=self.axis,keepdims=True) * -1\n",
    "        grad_x       = grad * 1/(tf.sqrt(self.var+eps)) + grad_sigma * change_parts * 2.0 * (self.input-self.mean) + grad_mean * change_parts\n",
    "        return grad_x\n",
    "  \n",
    "class tf_box_cox():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.lmbda    = tf.Variable(2.0) \n",
    "        self.m,self.v = tf.Variable(tf.zeros_like(self.lmbda)),tf.Variable(tf.zeros_like(self.lmbda))\n",
    "    def getw(self): return self.lmbda\n",
    "    \n",
    "    def feedforward(self,data):\n",
    "        self.input = data\n",
    "        self.layer = tf.pow((self.input + 1.0),self.lmbda)\n",
    "        return (self.layer - 1.0)/(self.lmbda + 1e-8)\n",
    "    \n",
    "    def backprop(self,grad):\n",
    "        \n",
    "        # Gradient that gets passed along\n",
    "        grad_pass = tf.pow((self.input + 1),self.lmbda-1.0) * grad\n",
    "        \n",
    "        # Grad respect to the lmbda value (not tested!)\n",
    "        grad_lmbda1 =   (self.layer * tf.log(self.input + 1 ))/(self.lmbda + 1e-8)\n",
    "        grad_lmbda2 = - (self.layer - 1)/(self.lmbda ** 2 + 1e-8)\n",
    "        grad_lmbda  = tf.reduce_mean((grad_lmbda1 + grad_lmbda2)*grad)\n",
    "\n",
    "        update_w = []\n",
    "        update_w.append(tf.assign( self.m,self.m*beta1 + (1-beta1) * (grad_lmbda)   ))\n",
    "        update_w.append(tf.assign( self.v,self.v*beta2 + (1-beta2) * (grad_lmbda ** 2)   ))\n",
    "        m_hat = self.m / (1-beta1) ; v_hat = self.v / (1-beta2)\n",
    "        adam_middle = m_hat * learning_rate/(tf.sqrt(v_hat) + adam_e)\n",
    "        update_w.append(tf.assign(self.lmbda,tf.subtract(self.lmbda,adam_middle  )))\n",
    "        \n",
    "        return grad_pass,grad_lmbda,update_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T06:21:04.871403Z",
     "start_time": "2018-12-20T06:21:04.769676Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# hyper parameter\n",
    "num_epoch = 300; learning_rate = 0.0008; batch_size = 20\n",
    "beta1,beta2,adam_e = 0.9,0.999,1e-9\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T06:07:37.013560Z",
     "start_time": "2018-12-20T06:07:36.830997Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# create layers\n",
    "l1 = CNN(3,3, 16); \n",
    "l2 = CNN(3,16,16); \n",
    "l3 = CNN(3,16,16); \n",
    "\n",
    "l4 = CNN(3,16,32); \n",
    "l5 = CNN(3,32,32); \n",
    "l6 = CNN(3,32,10); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T06:08:47.504461Z",
     "start_time": "2018-12-20T06:08:47.299795Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# create the graph \n",
    "x = tf.placeholder(tf.float32,[batch_size,96,96,3])\n",
    "y = tf.placeholder(tf.float32,[batch_size,10])\n",
    "\n",
    "layer1,layer1a = l1.feedforward(x,stride=2)      ;          \n",
    "layer2,layer2a = l2.feedforward(layer1a,stride=2);         \n",
    "layer3,layer3a = l3.feedforward(layer2a,stride=2); \n",
    "\n",
    "layer4,layer4a = l4.feedforward(layer3a,stride=2);          \n",
    "layer5,layer5a = l5.feedforward(layer4a,stride=1);         \n",
    "layer6,layer6a = l6.feedforward(layer5a,stride=1); \n",
    "\n",
    "final_layer   = tf.reduce_mean(layer6a,(1,2))\n",
    "final_softmax = tf_softmax(final_layer)\n",
    "cost          = -tf.reduce_mean(y * tf.log(final_softmax + 1e-8))\n",
    "correct_prediction = tf.equal(tf.argmax(final_softmax, 1), tf.argmax(y, 1))\n",
    "accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "gradient = tf.tile((final_softmax-y)[:,None,None,:],[1,6,6,1])/batch_size\n",
    "grad6w,grad6p,grad6_up = l6.backprop(gradient)\n",
    "grad5w,grad5p,grad5_up = l5.backprop(grad6p)\n",
    "grad4w,grad4p,grad4_up = l4.backprop(grad5p,stride=2)\n",
    "\n",
    "grad3w,grad3p,grad3_up = l3.backprop(grad4p,stride=2)\n",
    "grad2w,grad2p,grad2_up = l2.backprop(grad3p,stride=2)\n",
    "grad1w,grad1p,grad1_up = l1.backprop(grad2p,stride=2)\n",
    "\n",
    "gradient_update = grad6_up + \\\n",
    "                   grad5_up + \\\n",
    "                   grad4_up + \\\n",
    "                   grad3_up + \\\n",
    "                   grad2_up + \\\n",
    "                   grad1_up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T06:09:14.239248Z",
     "start_time": "2018-12-20T06:08:54.084190Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Iter : 0/300 batch : 7980/8000 acc : 0.35\n",
      " Current : 0 Acc : 0.12840000288188458 Test Acc : 0.21200000332668423\n",
      "\n",
      "Current Iter : 1/300 batch : 7980/8000 acc : 0.45\n",
      " Current : 1 Acc : 0.22900000382959843 Test Acc : 0.25975000286474825\n",
      "\n",
      "Current Iter : 2/300 batch : 7980/8000 acc : 0.55\n",
      " Current : 2 Acc : 0.2868000023066998 Test Acc : 0.3093750014156103\n",
      "\n",
      "Current Iter : 3/300 batch : 900/8000 acc : 0.455\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-fbb428fac211>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mcurrent_data\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mtest_images\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcurrent_batch_index\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mcurrent_batch_index\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mcurrent_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcurrent_batch_index\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mcurrent_batch_index\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0msess_results\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mcurrent_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mcurrent_label\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Current Iter : '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/'\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_epoch\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;33m+\u001b[0m \u001b[1;34m' batch : '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_batch_index\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/'\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' acc : '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'\\r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0mavg_acc_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mavg_acc_test\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msess_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1332\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train\n",
    "sess.run(tf.global_variables_initializer())\n",
    "avg_acc_train = 0; avg_acc_test  = 0; \n",
    "train_acc     = [];test_acc = []\n",
    "for iter in range(num_epoch):\n",
    "\n",
    "    for current_batch_index in range(0,len(train_images),batch_size):\n",
    "        current_data  = train_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = train_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy,gradient_update],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(train_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_train = avg_acc_train + sess_results[0]\n",
    "        \n",
    "    for current_batch_index in range(0,len(test_images), batch_size):\n",
    "        current_data  = test_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = test_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(test_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_test = avg_acc_test + sess_results[0]   \n",
    "        \n",
    "    print(\"\\n Current : \"+ str(iter) + \" Acc : \" + str(avg_acc_train/(len(train_images)/batch_size)) + \" Test Acc : \" + str(avg_acc_test/(len(test_images)/batch_size)) + '\\n')\n",
    "    \n",
    "    # save the training\n",
    "    train_acc.append(avg_acc_train/(len(train_images)/batch_size))\n",
    "    test_acc .append(avg_acc_test/(len(test_images)/batch_size)  )\n",
    "    \n",
    "    \n",
    "    avg_acc_train = 0 ; avg_acc_test  = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T08:30:06.162676Z",
     "start_time": "2018-12-20T08:30:05.913117Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# create layers\n",
    "num_epoch = 150; learning_rate = 0.001; batch_size = 50\n",
    "beta1,beta2,adam_e = 0.9,0.999,1e-8\n",
    "tf.reset_default_graph(); sess.close()\n",
    "sess = tf.InteractiveSession()\n",
    "l1 = CNN(3,3, 16); l1n = tf_box_cox()\n",
    "l2 = CNN(3,16,16); l2n = tf_box_cox()\n",
    "l3 = CNN(3,16,16); l3n = tf_box_cox()\n",
    "\n",
    "l4 = CNN(3,16,32); l4n = tf_box_cox()\n",
    "l5 = CNN(3,32,32); l5n = tf_box_cox()\n",
    "l6 = CNN(3,32,10); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T08:30:07.254768Z",
     "start_time": "2018-12-20T08:30:06.356929Z"
    }
   },
   "outputs": [],
   "source": [
    "# create the graph \n",
    "x = tf.placeholder(tf.float32,[batch_size,96,96,3])\n",
    "y = tf.placeholder(tf.float32,[batch_size,10])\n",
    "\n",
    "layer1,layer1a = l1.feedforward(x,stride=2)      ;          \n",
    "layer1n = l1n.feedforward(layer1a)\n",
    "layer2,layer2a = l2.feedforward(layer1n,stride=2);          \n",
    "layer2n = l2n.feedforward(layer2a)\n",
    "layer3,layer3a = l3.feedforward(layer2n,stride=2); \n",
    "layer3n = l3n.feedforward(layer3a)\n",
    "\n",
    "layer4,layer4a = l4.feedforward(layer3n,stride=2);          \n",
    "layer4n = l4n.feedforward(layer4a)\n",
    "layer5,layer5a = l5.feedforward(layer4n,stride=1);          \n",
    "layer5n = l5n.feedforward(layer5a)\n",
    "layer6,layer6a = l6.feedforward(layer5n,stride=1); \n",
    "\n",
    "final_layer   = tf.reduce_mean(layer6a,(1,2))\n",
    "final_softmax = tf_softmax(final_layer)\n",
    "cost          = -tf.reduce_mean(y * tf.log(final_softmax + 1e-8))\n",
    "auto_train = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "correct_prediction = tf.equal(tf.argmax(final_softmax, 1), tf.argmax(y, 1))\n",
    "accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "gradient = tf.tile((final_softmax-y)[:,None,None,:],[1,6,6,1])/batch_size\n",
    "grad6p,grad6w,grad6_up  = l6.backprop(gradient)\n",
    "grad5n,grad5l,grad5n_up = l5n.backprop(grad6p); \n",
    "grad5p,grad5w,grad5_up  = l5.backprop(grad5n)\n",
    "grad4n,grad4l,grad4n_up = l4n.backprop(grad5p); \n",
    "grad4p,grad4w,grad4_up  = l4.backprop(grad4n,stride=2)\n",
    "\n",
    "grad3n,grad3l,grad3n_up = l3n.backprop(grad4p);\n",
    "grad3p,grad3w,grad3_up  = l3.backprop(grad3n,stride=2)\n",
    "grad2n,grad2l,grad2n_up = l2n.backprop(grad3p); \n",
    "grad2p,grad2w,grad2_up  = l2.backprop(grad2n,stride=2)\n",
    "grad1n,grad1l,grad1n_up = l1n.backprop(grad2p); \n",
    "grad1p,grad1w,grad1_up  = l1.backprop(grad1n,stride=2)\n",
    "\n",
    "gradient_update = grad6_up + \\\n",
    "                  grad5n_up + grad5_up + \\\n",
    "                  grad4n_up + grad4_up + \\\n",
    "                  grad3n_up + grad3_up + \\\n",
    "                  grad2n_up + grad2_up + \\\n",
    "                  grad1n_up + grad1_up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T08:51:27.749600Z",
     "start_time": "2018-12-20T08:41:13.374760Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Iter : 0/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 0 Acc : 0.7214000010490418 Test Acc : 0.4846250001341105\n",
      "\n",
      "Current Iter : 1/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 1 Acc : 0.7100000017881394 Test Acc : 0.4939999973401427\n",
      "\n",
      "Current Iter : 2/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 2 Acc : 0.7267999994754791 Test Acc : 0.4888749985024333\n",
      "\n",
      "Current Iter : 3/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 3 Acc : 0.7335999995470047 Test Acc : 0.5029999984428286\n",
      "\n",
      "Current Iter : 4/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 4 Acc : 0.7430000013113022 Test Acc : 0.49649999774992465\n",
      "\n",
      "Current Iter : 5/150 batch : 7950/8000 acc : 0.46\n",
      " Current : 5 Acc : 0.7493999993801117 Test Acc : 0.49587499909102917\n",
      "\n",
      "Current Iter : 6/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 6 Acc : 0.747000002861023 Test Acc : 0.49162499960511924\n",
      "\n",
      "Current Iter : 7/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 7 Acc : 0.7547999984025955 Test Acc : 0.49449999909847975\n",
      "\n",
      "Current Iter : 8/150 batch : 7950/8000 acc : 0.44\n",
      " Current : 8 Acc : 0.7440000003576279 Test Acc : 0.4883749969303608\n",
      "\n",
      "Current Iter : 9/150 batch : 7950/8000 acc : 0.54\n",
      " Current : 9 Acc : 0.7382000017166138 Test Acc : 0.48787499871104956\n",
      "\n",
      "Current Iter : 10/150 batch : 7950/8000 acc : 0.46\n",
      " Current : 10 Acc : 0.7699999982118606 Test Acc : 0.4929999992251396\n",
      "\n",
      "Current Iter : 11/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 11 Acc : 0.7701999992132187 Test Acc : 0.49175000097602606\n",
      "\n",
      "Current Iter : 12/150 batch : 7950/8000 acc : 0.56\n",
      " Current : 12 Acc : 0.7769999980926514 Test Acc : 0.4893750011920929\n",
      "\n",
      "Current Iter : 13/150 batch : 7950/8000 acc : 0.56\n",
      " Current : 13 Acc : 0.7793999940156937 Test Acc : 0.49175000097602606\n",
      "\n",
      "Current Iter : 14/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 14 Acc : 0.7759999948740005 Test Acc : 0.48637499921023847\n",
      "\n",
      "Current Iter : 15/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 15 Acc : 0.7300000035762787 Test Acc : 0.48962499778717755\n",
      "\n",
      "Current Iter : 16/150 batch : 7950/8000 acc : 0.46\n",
      " Current : 16 Acc : 0.7757999974489213 Test Acc : 0.4919999998062849\n",
      "\n",
      "Current Iter : 17/150 batch : 7950/8000 acc : 0.46\n",
      " Current : 17 Acc : 0.7701999986171723 Test Acc : 0.4873749978840351\n",
      "\n",
      "Current Iter : 18/150 batch : 7950/8000 acc : 0.46\n",
      " Current : 18 Acc : 0.7823999994993209 Test Acc : 0.48799999952316286\n",
      "\n",
      "Current Iter : 19/150 batch : 7950/8000 acc : 0.44\n",
      " Current : 19 Acc : 0.7697999989986419 Test Acc : 0.4811249991878867\n",
      "\n",
      "Current Iter : 20/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 20 Acc : 0.7515999984741211 Test Acc : 0.4789999986067414\n",
      "\n",
      "Current Iter : 21/150 batch : 7950/8000 acc : 0.58\n",
      " Current : 21 Acc : 0.7653999996185302 Test Acc : 0.4737499995157123\n",
      "\n",
      "Current Iter : 22/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 22 Acc : 0.7655999994277954 Test Acc : 0.48174999821931125\n",
      "\n",
      "Current Iter : 23/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 23 Acc : 0.784599996805191 Test Acc : 0.4913749979808927\n",
      "\n",
      "Current Iter : 24/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 24 Acc : 0.7948000013828278 Test Acc : 0.4933750005438924\n",
      "\n",
      "Current Iter : 25/150 batch : 7950/8000 acc : 0.54\n",
      " Current : 25 Acc : 0.7967999982833862 Test Acc : 0.47837500013411044\n",
      "\n",
      "Current Iter : 26/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 26 Acc : 0.7843999987840653 Test Acc : 0.4882499996572733\n",
      "\n",
      "Current Iter : 27/150 batch : 7950/8000 acc : 0.46\n",
      " Current : 27 Acc : 0.7781999981403351 Test Acc : 0.4942500011995435\n",
      "\n",
      "Current Iter : 28/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 28 Acc : 0.7187999993562698 Test Acc : 0.48262499906122686\n",
      "\n",
      "Current Iter : 29/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 29 Acc : 0.7761999952793122 Test Acc : 0.4948749981820583\n",
      "\n",
      "Current Iter : 30/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 30 Acc : 0.7941999948024749 Test Acc : 0.49499999824911356\n",
      "\n",
      "Current Iter : 31/150 batch : 7950/8000 acc : 0.58\n",
      " Current : 31 Acc : 0.8015999919176102 Test Acc : 0.4851249985396862\n",
      "\n",
      "Current Iter : 32/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 32 Acc : 0.8037999957799912 Test Acc : 0.4899999972432852\n",
      "\n",
      "Current Iter : 33/150 batch : 7950/8000 acc : 0.54\n",
      " Current : 33 Acc : 0.7981999933719635 Test Acc : 0.4902499983087182\n",
      "\n",
      "Current Iter : 34/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 34 Acc : 0.8017999941110611 Test Acc : 0.49499999936670064\n",
      "\n",
      "Current Iter : 35/150 batch : 7950/8000 acc : 0.44\n",
      " Current : 35 Acc : 0.7759999972581864 Test Acc : 0.4763749990612268\n",
      "\n",
      "Current Iter : 36/150 batch : 7950/8000 acc : 0.44\n",
      " Current : 36 Acc : 0.7885999941825866 Test Acc : 0.5013749985024333\n",
      "\n",
      "Current Iter : 37/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 37 Acc : 0.8089999943971634 Test Acc : 0.49574999753385784\n",
      "\n",
      "Current Iter : 38/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 38 Acc : 0.8157999938726426 Test Acc : 0.4917499987408519\n",
      "\n",
      "Current Iter : 39/150 batch : 7950/8000 acc : 0.54\n",
      " Current : 39 Acc : 0.8209999948740005 Test Acc : 0.4932499995455146\n",
      "\n",
      "Current Iter : 40/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 40 Acc : 0.8233999902009964 Test Acc : 0.4943749995902181\n",
      "\n",
      "Current Iter : 41/150 batch : 7950/8000 acc : 0.54\n",
      " Current : 41 Acc : 0.8319999945163726 Test Acc : 0.49262499921023845\n",
      "\n",
      "Current Iter : 42/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 42 Acc : 0.8249999946355819 Test Acc : 0.4917499987408519\n",
      "\n",
      "Current Iter : 43/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 43 Acc : 0.7488000011444091 Test Acc : 0.46862500011920927\n",
      "\n",
      "Current Iter : 44/150 batch : 7950/8000 acc : 0.54\n",
      " Current : 44 Acc : 0.7763999980688095 Test Acc : 0.47987499851733445\n",
      "\n",
      "Current Iter : 45/150 batch : 7950/8000 acc : 0.54\n",
      " Current : 45 Acc : 0.7662000012397766 Test Acc : 0.48337499927729366\n",
      "\n",
      "Current Iter : 46/150 batch : 7950/8000 acc : 0.54\n",
      " Current : 46 Acc : 0.8129999935626984 Test Acc : 0.4789999989792705\n",
      "\n",
      "Current Iter : 47/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 47 Acc : 0.8233999937772751 Test Acc : 0.4892500001937151\n",
      "\n",
      "Current Iter : 48/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 48 Acc : 0.8353999960422516 Test Acc : 0.4877499990165234\n",
      "\n",
      "Current Iter : 49/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 49 Acc : 0.8359999984502793 Test Acc : 0.48499999977648256\n",
      "\n",
      "Current Iter : 50/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 50 Acc : 0.8329999965429306 Test Acc : 0.479250000230968\n",
      "\n",
      "Current Iter : 51/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 51 Acc : 0.8307999956607819 Test Acc : 0.48149999864399434\n",
      "\n",
      "Current Iter : 52/150 batch : 7950/8000 acc : 0.54\n",
      " Current : 52 Acc : 0.8097999924421311 Test Acc : 0.4879999980330467\n",
      "\n",
      "Current Iter : 53/150 batch : 7950/8000 acc : 0.54\n",
      " Current : 53 Acc : 0.8009999948740005 Test Acc : 0.4553749982267618\n",
      "\n",
      "Current Iter : 54/150 batch : 7950/8000 acc : 0.54\n",
      " Current : 54 Acc : 0.8269999951124192 Test Acc : 0.47450000047683716\n",
      "\n",
      "Current Iter : 55/150 batch : 7950/8000 acc : 0.54\n",
      " Current : 55 Acc : 0.8283999961614609 Test Acc : 0.4753750007599592\n",
      "\n",
      "Current Iter : 56/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 56 Acc : 0.8317999935150147 Test Acc : 0.46250000037252903\n",
      "\n",
      "Current Iter : 57/150 batch : 7950/8000 acc : 0.54\n",
      " Current : 57 Acc : 0.8057999962568283 Test Acc : 0.46037499960511924\n",
      "\n",
      "Current Iter : 58/150 batch : 7950/8000 acc : 0.58\n",
      " Current : 58 Acc : 0.7566000014543534 Test Acc : 0.4887500010430813\n",
      "\n",
      "Current Iter : 59/150 batch : 7950/8000 acc : 0.56\n",
      " Current : 59 Acc : 0.7881999963521957 Test Acc : 0.46324999798089267\n",
      "\n",
      "Current Iter : 60/150 batch : 7950/8000 acc : 0.56\n",
      " Current : 60 Acc : 0.8203999930620194 Test Acc : 0.4713749993592501\n",
      "\n",
      "Current Iter : 61/150 batch : 7950/8000 acc : 0.58\n",
      " Current : 61 Acc : 0.8343999922275543 Test Acc : 0.46700000017881393\n",
      "\n",
      "Current Iter : 62/150 batch : 7950/8000 acc : 0.56\n",
      " Current : 62 Acc : 0.838399994969368 Test Acc : 0.4676249992102385\n",
      "\n",
      "Current Iter : 63/150 batch : 7950/8000 acc : 0.56\n",
      " Current : 63 Acc : 0.8219999921321869 Test Acc : 0.458874998241663\n",
      "\n",
      "Current Iter : 64/150 batch : 7950/8000 acc : 0.54\n",
      " Current : 64 Acc : 0.8373999983072281 Test Acc : 0.4852499984204769\n",
      "\n",
      "Current Iter : 65/150 batch : 7950/8000 acc : 0.54\n",
      " Current : 65 Acc : 0.8447999954223633 Test Acc : 0.4827499985694885\n",
      "\n",
      "Current Iter : 66/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 66 Acc : 0.8451999944448471 Test Acc : 0.478499998152256\n",
      "\n",
      "Current Iter : 67/150 batch : 7950/8000 acc : 0.56\n",
      " Current : 67 Acc : 0.8145999944210053 Test Acc : 0.47887499779462817\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Iter : 68/150 batch : 7950/8000 acc : 0.56\n",
      " Current : 68 Acc : 0.812199993133545 Test Acc : 0.4727499997243285\n",
      "\n",
      "Current Iter : 69/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 69 Acc : 0.8233999979496002 Test Acc : 0.47812499944120646\n",
      "\n",
      "Current Iter : 70/150 batch : 7950/8000 acc : 0.56\n",
      " Current : 70 Acc : 0.8415999966859817 Test Acc : 0.4749999977648258\n",
      "\n",
      "Current Iter : 71/150 batch : 7950/8000 acc : 0.54\n",
      " Current : 71 Acc : 0.8601999944448471 Test Acc : 0.4839999994263053\n",
      "\n",
      "Current Iter : 72/150 batch : 7950/8000 acc : 0.54\n",
      " Current : 72 Acc : 0.8643999987840653 Test Acc : 0.484124999307096\n",
      "\n",
      "Current Iter : 73/150 batch : 7950/8000 acc : 0.56\n",
      " Current : 73 Acc : 0.86719999730587 Test Acc : 0.4828749982640147\n",
      "\n",
      "Current Iter : 74/150 batch : 7950/8000 acc : 0.54\n",
      " Current : 74 Acc : 0.8529999959468841 Test Acc : 0.4842499982565641\n",
      "\n",
      "Current Iter : 75/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 75 Acc : 0.8219999945163727 Test Acc : 0.46562499925494194\n",
      "\n",
      "Current Iter : 76/150 batch : 7950/8000 acc : 0.54\n",
      " Current : 76 Acc : 0.8367999923229218 Test Acc : 0.4739999996498227\n",
      "\n",
      "Current Iter : 77/150 batch : 7950/8000 acc : 0.54\n",
      " Current : 77 Acc : 0.8589999949932099 Test Acc : 0.4776249997317791\n",
      "\n",
      "Current Iter : 78/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 78 Acc : 0.8621999961137772 Test Acc : 0.47699999827891587\n",
      "\n",
      "Current Iter : 79/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 79 Acc : 0.8679999965429306 Test Acc : 0.46762499772012234\n",
      "\n",
      "Current Iter : 80/150 batch : 7950/8000 acc : 0.54\n",
      " Current : 80 Acc : 0.8281999951601029 Test Acc : 0.4855000000447035\n",
      "\n",
      "Current Iter : 81/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 81 Acc : 0.8319999945163726 Test Acc : 0.4708749996498227\n",
      "\n",
      "Current Iter : 82/150 batch : 7950/8000 acc : 0.56\n",
      " Current : 82 Acc : 0.860999995470047 Test Acc : 0.47287499755620954\n",
      "\n",
      "Current Iter : 83/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 83 Acc : 0.8553999954462052 Test Acc : 0.47062499839812516\n",
      "\n",
      "Current Iter : 84/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 84 Acc : 0.8677999937534332 Test Acc : 0.4713749988004565\n",
      "\n",
      "Current Iter : 85/150 batch : 7950/8000 acc : 0.58\n",
      " Current : 85 Acc : 0.8717999929189681 Test Acc : 0.4674999972805381\n",
      "\n",
      "Current Iter : 86/150 batch : 7950/8000 acc : 0.58\n",
      " Current : 86 Acc : 0.8785999941825867 Test Acc : 0.46387499924749137\n",
      "\n",
      "Current Iter : 87/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 87 Acc : 0.8711999952793121 Test Acc : 0.45737499818205835\n",
      "\n",
      "Current Iter : 88/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 88 Acc : 0.8493999975919724 Test Acc : 0.45024999789893627\n",
      "\n",
      "Current Iter : 89/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 89 Acc : 0.7582000002264977 Test Acc : 0.47774999868124723\n",
      "\n",
      "Current Iter : 90/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 90 Acc : 0.8605999952554703 Test Acc : 0.48212499991059304\n",
      "\n",
      "Current Iter : 91/150 batch : 7950/8000 acc : 0.54\n",
      " Current : 91 Acc : 0.8757999962568284 Test Acc : 0.47962499782443047\n",
      "\n",
      "Current Iter : 92/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 92 Acc : 0.885999995470047 Test Acc : 0.47999999932944776\n",
      "\n",
      "Current Iter : 93/150 batch : 7950/8000 acc : 0.54\n",
      " Current : 93 Acc : 0.8883999967575074 Test Acc : 0.47862499989569185\n",
      "\n",
      "Current Iter : 94/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 94 Acc : 0.8891999953985215 Test Acc : 0.47925000078976154\n",
      "\n",
      "Current Iter : 95/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 95 Acc : 0.8927999955415725 Test Acc : 0.47837499920278787\n",
      "\n",
      "Current Iter : 96/150 batch : 7950/8000 acc : 0.34\n",
      " Current : 96 Acc : 0.6205999985337257 Test Acc : 0.40387499989010395\n",
      "\n",
      "Current Iter : 97/150 batch : 7950/8000 acc : 0.46\n",
      " Current : 97 Acc : 0.669000001847744 Test Acc : 0.46912499945610764\n",
      "\n",
      "Current Iter : 98/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 98 Acc : 0.8037999957799912 Test Acc : 0.4848749989643693\n",
      "\n",
      "Current Iter : 99/150 batch : 7950/8000 acc : 0.56\n",
      " Current : 99 Acc : 0.8595999950170516 Test Acc : 0.48312499802559616\n",
      "\n",
      "Current Iter : 100/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 100 Acc : 0.8791999977827072 Test Acc : 0.4838749999180436\n",
      "\n",
      "Current Iter : 101/150 batch : 7950/8000 acc : 0.56\n",
      " Current : 101 Acc : 0.8875999939441681 Test Acc : 0.474499998614192\n",
      "\n",
      "Current Iter : 102/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 102 Acc : 0.8863999927043915 Test Acc : 0.4797499990090728\n",
      "\n",
      "Current Iter : 103/150 batch : 7950/8000 acc : 0.58\n",
      " Current : 103 Acc : 0.8947999954223633 Test Acc : 0.4807499997317791\n",
      "\n",
      "Current Iter : 104/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 104 Acc : 0.8097999995946884 Test Acc : 0.48912499845027924\n",
      "\n",
      "Current Iter : 105/150 batch : 7950/8000 acc : 0.54\n",
      " Current : 105 Acc : 0.8553999942541123 Test Acc : 0.4808749997988343\n",
      "\n",
      "Current Iter : 106/150 batch : 7950/8000 acc : 0.58\n",
      " Current : 106 Acc : 0.8893999946117401 Test Acc : 0.4778749983757734\n",
      "\n",
      "Current Iter : 107/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 107 Acc : 0.8943999940156937 Test Acc : 0.4781249977648258\n",
      "\n",
      "Current Iter : 108/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 108 Acc : 0.8993999969959259 Test Acc : 0.47299999725073577\n",
      "\n",
      "Current Iter : 109/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 109 Acc : 0.8981999969482422 Test Acc : 0.47125000022351743\n",
      "\n",
      "Current Iter : 110/150 batch : 7950/8000 acc : 0.56\n",
      " Current : 110 Acc : 0.8967999964952469 Test Acc : 0.4784999992698431\n",
      "\n",
      "Current Iter : 111/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 111 Acc : 0.8801999974250794 Test Acc : 0.45487500037997963\n",
      "\n",
      "Current Iter : 112/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 112 Acc : 0.8613999968767166 Test Acc : 0.4808749990537763\n",
      "\n",
      "Current Iter : 113/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 113 Acc : 0.8869999933242798 Test Acc : 0.48237499836832287\n",
      "\n",
      "Current Iter : 114/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 114 Acc : 0.844199994802475 Test Acc : 0.4711249995976686\n",
      "\n",
      "Current Iter : 115/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 115 Acc : 0.8541999959945679 Test Acc : 0.4813749993219972\n",
      "\n",
      "Current Iter : 116/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 116 Acc : 0.890399996638298 Test Acc : 0.48237499967217445\n",
      "\n",
      "Current Iter : 117/150 batch : 7950/8000 acc : 0.58\n",
      " Current : 117 Acc : 0.8915999948978424 Test Acc : 0.48574999924749135\n",
      "\n",
      "Current Iter : 118/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 118 Acc : 0.8957999950647354 Test Acc : 0.480124999396503\n",
      "\n",
      "Current Iter : 119/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 119 Acc : 0.9023999941349029 Test Acc : 0.4749999990686774\n",
      "\n",
      "Current Iter : 120/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 120 Acc : 0.9055999964475632 Test Acc : 0.47262499760836363\n",
      "\n",
      "Current Iter : 121/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 121 Acc : 0.84519999563694 Test Acc : 0.48099999893456696\n",
      "\n",
      "Current Iter : 122/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 122 Acc : 0.8739999938011169 Test Acc : 0.46562499925494194\n",
      "\n",
      "Current Iter : 123/150 batch : 7950/8000 acc : 0.46\n",
      " Current : 123 Acc : 0.879399995803833 Test Acc : 0.476499998383224\n",
      "\n",
      "Current Iter : 124/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 124 Acc : 0.8921999979019165 Test Acc : 0.48487499784678223\n",
      "\n",
      "Current Iter : 125/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 125 Acc : 0.8957999950647354 Test Acc : 0.4766249978914857\n",
      "\n",
      "Current Iter : 126/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 126 Acc : 0.8787999933958054 Test Acc : 0.48049999978393315\n",
      "\n",
      "Current Iter : 127/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 127 Acc : 0.8799999940395355 Test Acc : 0.4759999975562096\n",
      "\n",
      "Current Iter : 128/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 128 Acc : 0.8981999963521957 Test Acc : 0.4712499979883432\n",
      "\n",
      "Current Iter : 129/150 batch : 7950/8000 acc : 0.56\n",
      " Current : 129 Acc : 0.89319999396801 Test Acc : 0.4747499978169799\n",
      "\n",
      "Current Iter : 130/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 130 Acc : 0.8905999940633774 Test Acc : 0.46949999686330557\n",
      "\n",
      "Current Iter : 131/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 131 Acc : 0.8603999918699264 Test Acc : 0.48112499974668027\n",
      "\n",
      "Current Iter : 132/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 132 Acc : 0.8907999956607818 Test Acc : 0.477874998934567\n",
      "\n",
      "Current Iter : 133/150 batch : 7950/8000 acc : 0.56\n",
      " Current : 133 Acc : 0.8897999954223633 Test Acc : 0.47999999709427354\n",
      "\n",
      "Current Iter : 134/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 134 Acc : 0.8891999977827072 Test Acc : 0.48037499766796826\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Iter : 135/150 batch : 7950/8000 acc : 0.46\n",
      " Current : 135 Acc : 0.8897999930381775 Test Acc : 0.4744999984279275\n",
      "\n",
      "Current Iter : 136/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 136 Acc : 0.8615999954938889 Test Acc : 0.4624999988824129\n",
      "\n",
      "Current Iter : 137/150 batch : 7950/8000 acc : 0.54\n",
      " Current : 137 Acc : 0.8771999931335449 Test Acc : 0.4774999998509884\n",
      "\n",
      "Current Iter : 138/150 batch : 7950/8000 acc : 0.54\n",
      " Current : 138 Acc : 0.8989999949932098 Test Acc : 0.4794999986886978\n",
      "\n",
      "Current Iter : 139/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 139 Acc : 0.9037999951839447 Test Acc : 0.4808749994263053\n",
      "\n",
      "Current Iter : 140/150 batch : 7950/8000 acc : 0.56\n",
      " Current : 140 Acc : 0.9085999965667725 Test Acc : 0.47774999849498273\n",
      "\n",
      "Current Iter : 141/150 batch : 7950/8000 acc : 0.56\n",
      " Current : 141 Acc : 0.907599995136261 Test Acc : 0.48299999851733444\n",
      "\n",
      "Current Iter : 142/150 batch : 7950/8000 acc : 0.58\n",
      " Current : 142 Acc : 0.8921999955177307 Test Acc : 0.4753749996423721\n",
      "\n",
      "Current Iter : 143/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 143 Acc : 0.8653999954462052 Test Acc : 0.46937499940395355\n",
      "\n",
      "Current Iter : 144/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 144 Acc : 0.8761999970674514 Test Acc : 0.4839999992400408\n",
      "\n",
      "Current Iter : 145/150 batch : 7950/8000 acc : 0.54\n",
      " Current : 145 Acc : 0.898799996972084 Test Acc : 0.47112499866634605\n",
      "\n",
      "Current Iter : 146/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 146 Acc : 0.8499999910593032 Test Acc : 0.4839999981224537\n",
      "\n",
      "Current Iter : 147/150 batch : 7950/8000 acc : 0.44\n",
      " Current : 147 Acc : 0.8979999953508377 Test Acc : 0.4733749993145466\n",
      "\n",
      "Current Iter : 148/150 batch : 7950/8000 acc : 0.42\n",
      " Current : 148 Acc : 0.8913999968767166 Test Acc : 0.4719999982044101\n",
      "\n",
      "Current Iter : 149/150 batch : 7950/8000 acc : 0.46\n",
      " Current : 149 Acc : 0.8959999948740005 Test Acc : 0.46674999967217445\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "# sess.run(tf.global_variables_initializer())\n",
    "avg_acc_train = 0; avg_acc_test  = 0; \n",
    "train_acc     = [];test_acc = []\n",
    "for iter in range(num_epoch):\n",
    "\n",
    "    for current_batch_index in range(0,len(train_images),batch_size):\n",
    "        current_data  = train_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = train_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy,gradient_update],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(train_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_train = avg_acc_train + sess_results[0]\n",
    "        \n",
    "    for current_batch_index in range(0,len(test_images), batch_size):\n",
    "        current_data  = test_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = test_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(test_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_test = avg_acc_test + sess_results[0]   \n",
    "        \n",
    "    print(\"\\n Current : \"+ str(iter) + \" Acc : \" + str(avg_acc_train/(len(train_images)/batch_size)) + \" Test Acc : \" + str(avg_acc_test/(len(test_images)/batch_size)) + '\\n')\n",
    "    \n",
    "    # save the training\n",
    "    train_acc.append(avg_acc_train/(len(train_images)/batch_size))\n",
    "    test_acc .append(avg_acc_test/(len(test_images)/batch_size)  )\n",
    "    \n",
    "    avg_acc_train = 0 ; avg_acc_test  = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T08:40:26.607818Z",
     "start_time": "2018-12-20T08:40:26.584879Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.621524, 1.5086932, 1.4740642, 1.7049636, 2.0535245]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run([l1n.getw(),l2n.getw(),l3n.getw(),l4n.getw(),l5n.getw()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T07:39:51.379493Z",
     "start_time": "2018-12-20T07:39:51.079050Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T07:43:08.033249Z",
     "start_time": "2018-12-20T07:43:06.719715Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T07:57:07.576480Z",
     "start_time": "2018-12-20T07:51:47.829257Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T07:51:37.093685Z",
     "start_time": "2018-12-20T07:51:37.016902Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T07:57:11.376556Z",
     "start_time": "2018-12-20T07:57:11.370596Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "1. mttk/STL10. (2018). GitHub. Retrieved 19 December 2018, from https://github.com/mttk/STL10\n",
    "2. [duplicate], H. (2018). How to display multiple images in one figure correctly?. Stack Overflow. Retrieved 19 December 2018, from https://stackoverflow.com/questions/46615554/how-to-display-multiple-images-in-one-figure-correctly"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
