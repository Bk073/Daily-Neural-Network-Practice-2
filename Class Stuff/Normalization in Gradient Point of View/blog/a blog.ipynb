{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T06:25:48.211729Z",
     "start_time": "2018-12-20T06:25:44.682125Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# import Library and some random image data set\n",
    "import tensorflow as tf\n",
    "import numpy      as np\n",
    "import seaborn    as sns \n",
    "import pandas     as pd\n",
    "import os,sys\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(78); tf.set_random_seed(78)\n",
    "\n",
    "# get some of the STL data set\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from skimage import util \n",
    "from skimage.transform import resize\n",
    "from skimage.io import imread\n",
    "import warnings\n",
    "from numpy import inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T06:25:53.354955Z",
     "start_time": "2018-12-20T06:25:50.789791Z"
    },
    "code_folding": [
     0,
     2,
     29,
     37
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 96, 96, 3) 1.0 0.0\n",
      "(5000, 10) 1.0 0.0\n",
      "(8000, 96, 96, 3) 1.0 0.0\n",
      "(8000, 10) 1.0 0.0\n"
     ]
    }
   ],
   "source": [
    "# read all of the data\n",
    "# https://github.com/mttk/STL10\n",
    "def read_all_images(path_to_data):\n",
    "    \"\"\"\n",
    "    :param path_to_data: the file containing the binary images from the STL-10 dataset\n",
    "    :return: an array containing all the images\n",
    "    \"\"\"\n",
    "\n",
    "    with open(path_to_data, 'rb') as f:\n",
    "        # read whole file in uint8 chunks\n",
    "        everything = np.fromfile(f, dtype=np.uint8)\n",
    "\n",
    "        # We force the data into 3x96x96 chunks, since the\n",
    "        # images are stored in \"column-major order\", meaning\n",
    "        # that \"the first 96*96 values are the red channel,\n",
    "        # the next 96*96 are green, and the last are blue.\"\n",
    "        # The -1 is since the size of the pictures depends\n",
    "        # on the input file, and this way numpy determines\n",
    "        # the size on its own.\n",
    "\n",
    "        images = np.reshape(everything, (-1, 3, 96, 96))\n",
    "\n",
    "        # Now transpose the images into a standard image format\n",
    "        # readable by, for example, matplotlib.imshow\n",
    "        # You might want to comment this line or reverse the shuffle\n",
    "        # if you will use a learning algorithm like CNN, since they like\n",
    "        # their channels separated.\n",
    "        images = np.transpose(images, (0, 3, 2, 1))\n",
    "        return images\n",
    "def read_labels(path_to_labels):\n",
    "    \"\"\"\n",
    "    :param path_to_labels: path to the binary file containing labels from the STL-10 dataset\n",
    "    :return: an array containing the labels\n",
    "    \"\"\"\n",
    "    with open(path_to_labels, 'rb') as f:\n",
    "        labels = np.fromfile(f, dtype=np.uint8)\n",
    "        return labels\n",
    "def show_images(data,row=1,col=1):\n",
    "    fig=plt.figure(figsize=(10,10))\n",
    "    columns = col; rows = row\n",
    "    for i in range(1, columns*rows +1):\n",
    "        fig.add_subplot(rows, columns, i)\n",
    "        plt.imshow(data[i-1])\n",
    "    plt.show()\n",
    "\n",
    "train_images = read_all_images(\"../../../DataSet/STL10/stl10_binary/train_X.bin\") / 255.0\n",
    "train_labels = read_labels    (\"../../../DataSet/STL10/stl10_binary/train_Y.bin\")\n",
    "test_images  = read_all_images(\"../../../DataSet/STL10/stl10_binary/test_X.bin\")  / 255.0\n",
    "test_labels  = read_labels    (\"../../../DataSet/STL10/stl10_binary/test_y.bin\")\n",
    "\n",
    "label_encoder= OneHotEncoder(sparse=False,categories='auto')\n",
    "train_labels = label_encoder.fit_transform(train_labels.reshape((-1,1)))\n",
    "test_labels  = label_encoder.fit_transform(test_labels.reshape((-1,1)))\n",
    "\n",
    "print(train_images.shape,train_images.max(),train_images.min())\n",
    "print(train_labels.shape,train_labels.max(),train_labels.min())\n",
    "print(test_images.shape,test_images.max(),test_images.min())\n",
    "print(test_labels.shape,test_labels.max(),test_labels.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T07:01:13.292261Z",
     "start_time": "2018-12-20T07:01:13.233418Z"
    },
    "code_folding": [
     58,
     99,
     140
    ]
   },
   "outputs": [],
   "source": [
    "# create the layers\n",
    "def tf_softmax(x): return tf.nn.softmax(x)\n",
    "\n",
    "def tf_elu(x):   return tf.nn.elu(x)\n",
    "def d_tf_elu(x): return tf.cast(tf.greater(x,0),tf.float32)  + (tf_elu(tf.cast(tf.less_equal(x,0),tf.float32) * x) + 1.0)\n",
    "\n",
    "def tf_relu(x):   return tf.nn.relu(x)\n",
    "def d_tf_relu(x): return tf.cast(tf.greater(x,0),tf.float32)\n",
    "\n",
    "def tf_tanh(x):   return tf.nn.tanh(x)\n",
    "def d_tf_tanh(x): return 1 - tf_tanh(x) ** 2\n",
    "\n",
    "def tf_sigmoid(x):   return tf.nn.sigmoid(x)\n",
    "def d_tf_sigmoid(x): return tf_sigmoid(x) * (1.0-tf_sigmoid(x))\n",
    "\n",
    "class CNN():\n",
    "\n",
    "    def __init__(self,k,inc,out, stddev=0.05,which_reg=0,act=tf_relu,d_act=d_tf_relu):\n",
    "        self.w          = tf.Variable(tf.random_normal([k,k,inc,out],stddev=stddev,seed=2,dtype=tf.float32))\n",
    "        self.m,self.v   = tf.Variable(tf.zeros_like(self.w)),tf.Variable(tf.zeros_like(self.w))\n",
    "        self.act,self.d_act = act,d_act\n",
    "        self.which_reg  = which_reg\n",
    "        \n",
    "    def getw(self): return self.w\n",
    "\n",
    "    def feedforward(self,input,stride=1,padding='SAME'):\n",
    "        self.input  = input\n",
    "        self.layer  = tf.nn.conv2d(input,self.w,strides=[1,stride,stride,1],padding=padding) \n",
    "        self.layerA = self.act(self.layer)\n",
    "        return [self.layer,self.layerA]\n",
    "    \n",
    "    def backprop(self,gradient,stride=1,padding='SAME'):\n",
    "        grad_part_1 = gradient\n",
    "        grad_part_2 = self.d_act(self.layer)\n",
    "        grad_part_3 = self.input\n",
    "\n",
    "        grad_middle = grad_part_1 * grad_part_2\n",
    "        grad        = tf.nn.conv2d_backprop_filter(input = grad_part_3,filter_sizes = tf.shape(self.w),  out_backprop = grad_middle,strides=[1,stride,stride,1],padding=padding) / batch_size\n",
    "        grad_pass   = tf.nn.conv2d_backprop_input (input_sizes = tf.shape(self.input),filter= self.w,out_backprop = grad_middle,strides=[1,stride,stride,1],padding=padding)\n",
    "\n",
    "        if self.which_reg == 0:   grad = grad\n",
    "        if self.which_reg == 0.5: grad = grad + lamda * (tf.sqrt(tf.abs(self.w))) * (1.0/tf.sqrt(tf.abs(self.w)+ 10e-5)) * tf.sign(self.w)\n",
    "        if self.which_reg == 1:   grad = grad + lamda * tf.sign(self.w)\n",
    "        if self.which_reg == 1.5: grad = grad + lamda * 1.0/(tf.sqrt(tf.square(self.w) + 10e-5)) * self.w\n",
    "        if self.which_reg == 2:   grad = grad + lamda * (1.0/tf.sqrt(tf.square(tf.abs(self.w))+ 10e-5)) * tf.abs(self.w) * tf.sign(self.w)\n",
    "        if self.which_reg == 2.5: grad = grad + lamda * 2.0 * self.w\n",
    "        if self.which_reg == 3:   grad = grad + lamda * tf.pow(tf.pow(tf.abs(self.w),3)+ 10e-5,-0.66) * tf.pow(tf.abs(self.w),2) * tf.sign(self.w)\n",
    "        if self.which_reg == 4:   grad = grad + lamda * tf.pow(tf.pow(tf.abs(self.w),4)+ 10e-5,-0.75) * tf.pow(tf.abs(self.w),3) * tf.sign(self.w)\n",
    "\n",
    "        update_w = []\n",
    "        update_w.append(tf.assign( self.m,self.m*beta1 + (1-beta1) * (grad)   ))\n",
    "        update_w.append(tf.assign( self.v,self.v*beta2 + (1-beta2) * (grad ** 2)   ))\n",
    "        m_hat = self.m / (1-beta1) ; v_hat = self.v / (1-beta2)\n",
    "        adam_middle = m_hat * learning_rate/(tf.sqrt(v_hat) + adam_e)\n",
    "        update_w.append(tf.assign(self.w,tf.subtract(self.w,adam_middle  )))\n",
    "        \n",
    "        return grad_pass,grad,update_w\n",
    "    \n",
    "class tf_batch_norm_layer():\n",
    "    \n",
    "    def __init__(self,vector_shape,axis):\n",
    "        self.moving_mean = tf.Variable(tf.zeros(shape=[1,1,1,vector_shape],dtype=tf.float32))\n",
    "        self.moving_vari = tf.Variable(tf.zeros(shape=[1,1,1,vector_shape],dtype=tf.float32))\n",
    "        self.axis        = axis\n",
    "        \n",
    "    def feedforward(self,input,training_phase=True,eps = 1e-8):\n",
    "        self.input = input\n",
    "        self.input_size          = self.input.shape\n",
    "        self.batch,self.h,self.w,self.c = self.input_size[0].value,self.input_size[1].value,self.input_size[2].value,self.input_size[3].value\n",
    "\n",
    "        # Training Moving Average Mean         \n",
    "        def training_fn():\n",
    "            self.mean    = tf.reduce_mean(self.input,axis=self.axis ,keepdims=True)\n",
    "            self.var     = tf.reduce_mean(tf.square(self.input-self.mean),axis=self.axis,keepdims=True)\n",
    "            centered_data= (self.input - self.mean)/tf.sqrt(self.var + eps)\n",
    "            \n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean*0.9 + 0.1 * self.mean ))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari*0.9 + 0.1 * self.var  ))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        # Testing Moving Average Mean        \n",
    "        def  testing_fn():\n",
    "            centered_data   = (self.input - self.moving_mean)/tf.sqrt(self.moving_vari + eps)\n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        self.output,update_variable = tf.cond(training_phase,true_fn=training_fn,false_fn=testing_fn)\n",
    "        return self.output,update_variable\n",
    "    \n",
    "    def backprop(self,grad,eps = 1e-8):\n",
    "        change_parts = 1.0 /(self.batch * self.h * self.w)\n",
    "        grad_sigma   = tf.reduce_sum( grad *  (self.input-self.mean)     ,axis=self.axis,keepdims=True) * -0.5 * (self.var+eps) ** -1.5\n",
    "        grad_mean    = tf.reduce_sum( grad *  (-1./tf.sqrt(self.var+eps)),axis=self.axis,keepdims=True) + grad_sigma * change_parts * 2.0 * tf.reduce_sum((self.input-self.mean),axis=self.axis,keepdims=True) * -1\n",
    "        grad_x       = grad * 1/(tf.sqrt(self.var+eps)) + grad_sigma * change_parts * 2.0 * (self.input-self.mean) + grad_mean * change_parts\n",
    "        return grad_x\n",
    "\n",
    "class tf_layer_norm_layer():\n",
    "    \n",
    "    def __init__(self,vector_shape,axis):\n",
    "        self.moving_mean = tf.Variable(tf.zeros(shape=[vector_shape,1,1,1],dtype=tf.float32))\n",
    "        self.moving_vari = tf.Variable(tf.zeros(shape=[vector_shape,1,1,1],dtype=tf.float32))\n",
    "        self.axis        = axis\n",
    "        \n",
    "    def feedforward(self,input,training_phase=True,eps = 1e-8):\n",
    "        self.input = input\n",
    "        self.input_size          = self.input.shape\n",
    "        self.batch,self.h,self.w,self.c = self.input_size[0].value,self.input_size[1].value,self.input_size[2].value,self.input_size[3].value\n",
    "\n",
    "        # Training Moving Average Mean         \n",
    "        def training_fn():\n",
    "            self.mean    = tf.reduce_mean(self.input,axis=self.axis ,keepdims=True)\n",
    "            self.var     = tf.reduce_mean(tf.square(self.input-self.mean),axis=self.axis,keepdims=True)\n",
    "            centered_data= (self.input - self.mean)/tf.sqrt(self.var + eps)\n",
    "            \n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean*0.9 + 0.1 * self.mean ))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari*0.9 + 0.1 * self.var  ))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        # Testing Moving Average Mean        \n",
    "        def  testing_fn():\n",
    "            centered_data   = (self.input - self.moving_mean)/tf.sqrt(self.moving_vari + eps)\n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        self.output,update_variable = tf.cond(training_phase,true_fn=training_fn,false_fn=testing_fn)\n",
    "        return self.output,update_variable\n",
    "    \n",
    "    def backprop(self,grad,eps = 1e-8):\n",
    "        change_parts = 1.0 /(self.h * self.w * self.c)\n",
    "        grad_sigma   = tf.reduce_sum( grad *  (self.input-self.mean)     ,axis=self.axis,keepdims=True) * -0.5 * (self.var+eps) ** -1.5\n",
    "        grad_mean    = tf.reduce_sum( grad *  (-1./tf.sqrt(self.var+eps)),axis=self.axis,keepdims=True) + grad_sigma * change_parts * 2.0 * tf.reduce_sum((self.input-self.mean),axis=self.axis,keepdims=True) * -1\n",
    "        grad_x       = grad * 1/(tf.sqrt(self.var+eps)) + grad_sigma * change_parts * 2.0 * (self.input-self.mean) + grad_mean * change_parts\n",
    "        return grad_x\n",
    "    \n",
    "class tf_instance_norm_layer():\n",
    "    \n",
    "    def __init__(self,batch_size,vector_shape,axis):\n",
    "        self.moving_mean = tf.Variable(tf.zeros(shape=[batch_size,1,1,vector_shape],dtype=tf.float32))\n",
    "        self.moving_vari = tf.Variable(tf.zeros(shape=[batch_size,1,1,vector_shape],dtype=tf.float32))\n",
    "        self.axis        = axis\n",
    "        \n",
    "    def feedforward(self,input,training_phase=True,eps = 1e-8):\n",
    "        self.input = input\n",
    "        self.input_size          = self.input.shape\n",
    "        self.batch,self.h,self.w,self.c = self.input_size[0].value,self.input_size[1].value,self.input_size[2].value,self.input_size[3].value\n",
    "\n",
    "        # Training Moving Average Mean         \n",
    "        def training_fn():\n",
    "            self.mean    = tf.reduce_mean(self.input,axis=self.axis ,keepdims=True)\n",
    "            self.var     = tf.reduce_mean(tf.square(self.input-self.mean),axis=self.axis,keepdims=True)\n",
    "            centered_data= (self.input - self.mean)/tf.sqrt(self.var + eps)\n",
    "            \n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean*0.9 + 0.1 * self.mean ))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari*0.9 + 0.1 * self.var  ))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        # Testing Moving Average Mean        \n",
    "        def  testing_fn():\n",
    "            centered_data   = (self.input - self.moving_mean)/tf.sqrt(self.moving_vari + eps)\n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        self.output,update_variable = tf.cond(training_phase,true_fn=training_fn,false_fn=testing_fn)\n",
    "        return self.output,update_variable\n",
    "    \n",
    "    def backprop(self,grad,eps = 1e-8):\n",
    "        change_parts = 1.0 /(self.h * self.w)\n",
    "        grad_sigma   = tf.reduce_sum( grad *  (self.input-self.mean)     ,axis=self.axis,keepdims=True) * -0.5 * (self.var+eps) ** -1.5\n",
    "        grad_mean    = tf.reduce_sum( grad *  (-1./tf.sqrt(self.var+eps)),axis=self.axis,keepdims=True) + grad_sigma * change_parts * 2.0 * tf.reduce_sum((self.input-self.mean),axis=self.axis,keepdims=True) * -1\n",
    "        grad_x       = grad * 1/(tf.sqrt(self.var+eps)) + grad_sigma * change_parts * 2.0 * (self.input-self.mean) + grad_mean * change_parts\n",
    "        return grad_x\n",
    "  \n",
    "class tf_box_cox():\n",
    "    \n",
    "    def __init__(self,channel):\n",
    "#         self.lmbda    = tf.Variable(tf.ones([1,1,1,channel],tf.float32)* 2.0) \n",
    "        self.lmbda    = tf.Variable(2.0) \n",
    "        self.m,self.v = tf.Variable(tf.zeros_like(self.lmbda)),tf.Variable(tf.zeros_like(self.lmbda))\n",
    "    def getw(self): return self.lmbda\n",
    "    \n",
    "    def feedforward(self,data):\n",
    "        self.input = data\n",
    "        self.layer = tf.pow((self.input + 1.0),self.lmbda)\n",
    "        return (self.layer - 1.0)/(self.lmbda + 1e-8)\n",
    "    \n",
    "    def backprop(self,grad):\n",
    "        \n",
    "        # Gradient that gets passed along\n",
    "        grad_pass = tf.pow((self.input + 1),self.lmbda-1.0) * grad\n",
    "        \n",
    "        # Grad respect to the lmbda value (not tested!)\n",
    "        grad_lmbda1 =   (self.layer * tf.log(self.input + 1 ))/(self.lmbda + 1e-8)\n",
    "        grad_lmbda2 = - (self.layer - 1)/(self.lmbda ** 2 + 1e-8)\n",
    "        grad_lmbda  = tf.reduce_mean((grad_lmbda1 + grad_lmbda2)*grad)\n",
    "\n",
    "        update_w = []\n",
    "        update_w.append(tf.assign( self.m,self.m*beta1 + (1-beta1) * (grad_lmbda)   ))\n",
    "        update_w.append(tf.assign( self.v,self.v*beta2 + (1-beta2) * (grad_lmbda ** 2)   ))\n",
    "        m_hat = self.m / (1-beta1) ; v_hat = self.v / (1-beta2)\n",
    "        adam_middle = m_hat * learning_rate/(tf.sqrt(v_hat) + adam_e)\n",
    "        update_w.append(tf.assign(self.lmbda,tf.subtract(self.lmbda,adam_middle  )))\n",
    "        \n",
    "        return grad_pass,grad_lmbda,update_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T06:21:04.871403Z",
     "start_time": "2018-12-20T06:21:04.769676Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# hyper parameter\n",
    "num_epoch = 300; learning_rate = 0.0008; batch_size = 20\n",
    "beta1,beta2,adam_e = 0.9,0.999,1e-9\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T06:07:37.013560Z",
     "start_time": "2018-12-20T06:07:36.830997Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# create layers\n",
    "l1 = CNN(3,3, 16); \n",
    "l2 = CNN(3,16,16); \n",
    "l3 = CNN(3,16,16); \n",
    "\n",
    "l4 = CNN(3,16,32); \n",
    "l5 = CNN(3,32,32); \n",
    "l6 = CNN(3,32,10); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T06:08:47.504461Z",
     "start_time": "2018-12-20T06:08:47.299795Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# create the graph \n",
    "x = tf.placeholder(tf.float32,[batch_size,96,96,3])\n",
    "y = tf.placeholder(tf.float32,[batch_size,10])\n",
    "\n",
    "layer1,layer1a = l1.feedforward(x,stride=2)      ;          \n",
    "layer2,layer2a = l2.feedforward(layer1a,stride=2);         \n",
    "layer3,layer3a = l3.feedforward(layer2a,stride=2); \n",
    "\n",
    "layer4,layer4a = l4.feedforward(layer3a,stride=2);          \n",
    "layer5,layer5a = l5.feedforward(layer4a,stride=1);         \n",
    "layer6,layer6a = l6.feedforward(layer5a,stride=1); \n",
    "\n",
    "final_layer   = tf.reduce_mean(layer6a,(1,2))\n",
    "final_softmax = tf_softmax(final_layer)\n",
    "cost          = -tf.reduce_mean(y * tf.log(final_softmax + 1e-8))\n",
    "correct_prediction = tf.equal(tf.argmax(final_softmax, 1), tf.argmax(y, 1))\n",
    "accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "gradient = tf.tile((final_softmax-y)[:,None,None,:],[1,6,6,1])/batch_size\n",
    "grad6w,grad6p,grad6_up = l6.backprop(gradient)\n",
    "grad5w,grad5p,grad5_up = l5.backprop(grad6p)\n",
    "grad4w,grad4p,grad4_up = l4.backprop(grad5p,stride=2)\n",
    "\n",
    "grad3w,grad3p,grad3_up = l3.backprop(grad4p,stride=2)\n",
    "grad2w,grad2p,grad2_up = l2.backprop(grad3p,stride=2)\n",
    "grad1w,grad1p,grad1_up = l1.backprop(grad2p,stride=2)\n",
    "\n",
    "gradient_update = grad6_up + \\\n",
    "                   grad5_up + \\\n",
    "                   grad4_up + \\\n",
    "                   grad3_up + \\\n",
    "                   grad2_up + \\\n",
    "                   grad1_up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T06:09:14.239248Z",
     "start_time": "2018-12-20T06:08:54.084190Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Iter : 0/300 batch : 7980/8000 acc : 0.35\n",
      " Current : 0 Acc : 0.12840000288188458 Test Acc : 0.21200000332668423\n",
      "\n",
      "Current Iter : 1/300 batch : 7980/8000 acc : 0.45\n",
      " Current : 1 Acc : 0.22900000382959843 Test Acc : 0.25975000286474825\n",
      "\n",
      "Current Iter : 2/300 batch : 7980/8000 acc : 0.55\n",
      " Current : 2 Acc : 0.2868000023066998 Test Acc : 0.3093750014156103\n",
      "\n",
      "Current Iter : 3/300 batch : 900/8000 acc : 0.455\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-fbb428fac211>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mcurrent_data\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mtest_images\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcurrent_batch_index\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mcurrent_batch_index\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mcurrent_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcurrent_batch_index\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mcurrent_batch_index\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0msess_results\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mcurrent_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mcurrent_label\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Current Iter : '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/'\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_epoch\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;33m+\u001b[0m \u001b[1;34m' batch : '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_batch_index\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/'\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' acc : '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'\\r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0mavg_acc_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mavg_acc_test\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msess_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1332\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train\n",
    "sess.run(tf.global_variables_initializer())\n",
    "avg_acc_train = 0; avg_acc_test  = 0; \n",
    "train_acc     = [];test_acc = []\n",
    "for iter in range(num_epoch):\n",
    "\n",
    "    for current_batch_index in range(0,len(train_images),batch_size):\n",
    "        current_data  = train_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = train_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy,gradient_update],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(train_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_train = avg_acc_train + sess_results[0]\n",
    "        \n",
    "    for current_batch_index in range(0,len(test_images), batch_size):\n",
    "        current_data  = test_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = test_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(test_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_test = avg_acc_test + sess_results[0]   \n",
    "        \n",
    "    print(\"\\n Current : \"+ str(iter) + \" Acc : \" + str(avg_acc_train/(len(train_images)/batch_size)) + \" Test Acc : \" + str(avg_acc_test/(len(test_images)/batch_size)) + '\\n')\n",
    "    \n",
    "    # save the training\n",
    "    train_acc.append(avg_acc_train/(len(train_images)/batch_size))\n",
    "    test_acc .append(avg_acc_test/(len(test_images)/batch_size)  )\n",
    "    \n",
    "    \n",
    "    avg_acc_train = 0 ; avg_acc_test  = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T07:01:16.001266Z",
     "start_time": "2018-12-20T07:01:15.770860Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# create layers\n",
    "num_epoch = 300; learning_rate = 0.001; batch_size = 50\n",
    "beta1,beta2,adam_e = 0.9,0.999,1e-8\n",
    "tf.reset_default_graph(); sess.close()\n",
    "sess = tf.InteractiveSession()\n",
    "l1 = CNN(3,3, 16); l1n = tf_box_cox(16)\n",
    "l2 = CNN(3,16,16); l2n = tf_box_cox(16)\n",
    "l3 = CNN(3,16,16); l3n = tf_box_cox(16)\n",
    "\n",
    "l4 = CNN(3,16,32); l4n = tf_box_cox(32)\n",
    "l5 = CNN(3,32,32); l5n = tf_box_cox(32)\n",
    "l6 = CNN(3,32,10); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T07:01:17.794757Z",
     "start_time": "2018-12-20T07:01:16.817374Z"
    }
   },
   "outputs": [],
   "source": [
    "# create the graph \n",
    "x = tf.placeholder(tf.float32,[batch_size,96,96,3])\n",
    "y = tf.placeholder(tf.float32,[batch_size,10])\n",
    "\n",
    "layer1,layer1a = l1.feedforward(x,stride=2)      ;          \n",
    "layer1n = l1n.feedforward(layer1a)\n",
    "layer2,layer2a = l2.feedforward(layer1n,stride=2);          \n",
    "layer2n = l2n.feedforward(layer2a)\n",
    "layer3,layer3a = l3.feedforward(layer2n,stride=2); \n",
    "layer3n = l3n.feedforward(layer3a)\n",
    "\n",
    "layer4,layer4a = l4.feedforward(layer3n,stride=2);          \n",
    "layer4n = l4n.feedforward(layer4a)\n",
    "layer5,layer5a = l5.feedforward(layer4n,stride=1);          \n",
    "layer5n = l5n.feedforward(layer5a)\n",
    "layer6,layer6a = l6.feedforward(layer5n,stride=1); \n",
    "\n",
    "final_layer   = tf.reduce_mean(layer6a,(1,2))\n",
    "final_softmax = tf_softmax(final_layer)\n",
    "cost          = -tf.reduce_mean(y * tf.log(final_softmax + 1e-8))\n",
    "auto_train = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "correct_prediction = tf.equal(tf.argmax(final_softmax, 1), tf.argmax(y, 1))\n",
    "accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "gradient = tf.tile((final_softmax-y)[:,None,None,:],[1,6,6,1])/batch_size\n",
    "grad6p,grad6w,grad6_up  = l6.backprop(gradient)\n",
    "grad5n,grad5l,grad5n_up = l5n.backprop(grad6p); \n",
    "grad5p,grad5w,grad5_up  = l5.backprop(grad5n)\n",
    "grad4n,grad4l,grad4n_up = l4n.backprop(grad5p); \n",
    "grad4p,grad4w,grad4_up  = l4.backprop(grad4n,stride=2)\n",
    "\n",
    "grad3n,grad3l,grad3n_up = l3n.backprop(grad4p);\n",
    "grad3p,grad3w,grad3_up  = l3.backprop(grad3n,stride=2)\n",
    "grad2n,grad2l,grad2n_up = l2n.backprop(grad3p); \n",
    "grad2p,grad2w,grad2_up  = l2.backprop(grad2n,stride=2)\n",
    "grad1n,grad1l,grad1n_up = l1n.backprop(grad2p); \n",
    "grad1p,grad1w,grad1_up  = l1.backprop(grad1n,stride=2)\n",
    "\n",
    "gradient_update = grad6_up + \\\n",
    "                  grad5n_up + grad5_up + \\\n",
    "                  grad4n_up + grad4_up + \\\n",
    "                  grad3n_up + grad3_up + \\\n",
    "                  grad2n_up + grad2_up + \\\n",
    "                  grad1n_up + grad1_up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-20T07:01:17.211Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Iter : 0/300 batch : 7950/8000 acc : 0.12\n",
      " Current : 0 Acc : 0.11699999947100878 Test Acc : 0.1017499994719401\n",
      "\n",
      "[2.0165515, 2.0159009, 2.0178163, 2.013778, 2.0124717]\n",
      "Current Iter : 1/300 batch : 7950/8000 acc : 0.22\n",
      " Current : 1 Acc : 0.16460000015795231 Test Acc : 0.19212500003632157\n",
      "\n",
      "[2.0115786, 2.007327, 2.0083373, 2.002538, 2.003792]\n",
      "Current Iter : 2/300 batch : 7950/8000 acc : 0.36\n",
      " Current : 2 Acc : 0.18100000005215405 Test Acc : 0.20362499984912574\n",
      "\n",
      "[2.0125089, 2.0097117, 2.0068207, 1.9978787, 2.0019345]\n",
      "Current Iter : 3/300 batch : 7950/8000 acc : 0.32\n",
      " Current : 3 Acc : 0.23759999841451646 Test Acc : 0.2661250001285225\n",
      "\n",
      "[2.0073712, 2.0031164, 1.9957869, 1.9832923, 1.9901755]\n",
      "Current Iter : 4/300 batch : 7950/8000 acc : 0.34\n",
      " Current : 4 Acc : 0.28460000075399877 Test Acc : 0.2960000013001263\n",
      "\n",
      "[1.9993737, 1.99514, 1.9873927, 1.972314, 1.981532]\n",
      "Current Iter : 5/300 batch : 7950/8000 acc : 0.38\n",
      " Current : 5 Acc : 0.32060000121593474 Test Acc : 0.3305000017862767\n",
      "\n",
      "[1.9923248, 1.9883082, 1.9781576, 1.9612578, 1.9739286]\n",
      "Current Iter : 6/300 batch : 7950/8000 acc : 0.36\n",
      " Current : 6 Acc : 0.34000000268220903 Test Acc : 0.32012500055134296\n",
      "\n",
      "[1.9840997, 1.9806728, 1.9684274, 1.950742, 1.9668269]\n",
      "Current Iter : 7/300 batch : 7950/8000 acc : 0.48\n",
      " Current : 7 Acc : 0.33620000168681147 Test Acc : 0.33250000085681675\n",
      "\n",
      "[1.9775453, 1.9743624, 1.959206, 1.9403881, 1.9589248]\n",
      "Current Iter : 8/300 batch : 7950/8000 acc : 0.34\n",
      " Current : 8 Acc : 0.3438000024855137 Test Acc : 0.35750000057742\n",
      "\n",
      "[1.9718701, 1.9687462, 1.9509078, 1.9313378, 1.9519461]\n",
      "Current Iter : 9/300 batch : 7950/8000 acc : 0.46\n",
      " Current : 9 Acc : 0.36520000115036966 Test Acc : 0.36500000115484\n",
      "\n",
      "[1.9645737, 1.9617218, 1.9415044, 1.9223721, 1.9461112]\n",
      "Current Iter : 10/300 batch : 7950/8000 acc : 0.48\n",
      " Current : 10 Acc : 0.3594000005722046 Test Acc : 0.3679999993182719\n",
      "\n",
      "[1.9595385, 1.9567801, 1.9347514, 1.9156543, 1.9412445]\n",
      "Current Iter : 11/300 batch : 7950/8000 acc : 0.42\n",
      " Current : 11 Acc : 0.38199999928474426 Test Acc : 0.3898750014603138\n",
      "\n",
      "[1.9541899, 1.9513248, 1.9274344, 1.909198, 1.9379486]\n",
      "Current Iter : 12/300 batch : 7950/8000 acc : 0.46\n",
      " Current : 12 Acc : 0.38359999999403954 Test Acc : 0.3818750003352761\n",
      "\n",
      "[1.9499472, 1.9469645, 1.9217356, 1.903797, 1.9349376]\n",
      "Current Iter : 13/300 batch : 7950/8000 acc : 0.38\n",
      " Current : 13 Acc : 0.37800000071525575 Test Acc : 0.3820000022649765\n",
      "\n",
      "[1.9459766, 1.9428889, 1.9166768, 1.8989937, 1.9325117]\n",
      "Current Iter : 14/300 batch : 7950/8000 acc : 0.42\n",
      " Current : 14 Acc : 0.3998000007867813 Test Acc : 0.4000000010244548\n",
      "\n",
      "[1.9415036, 1.9381955, 1.9107432, 1.8938664, 1.930449]\n",
      "Current Iter : 15/300 batch : 7950/8000 acc : 0.48\n",
      " Current : 15 Acc : 0.3967999994754791 Test Acc : 0.3898750000633299\n",
      "\n",
      "[1.9376304, 1.9342365, 1.9060785, 1.8895379, 1.9285321]\n",
      "Current Iter : 16/300 batch : 7950/8000 acc : 0.48\n",
      " Current : 16 Acc : 0.4091999995708466 Test Acc : 0.39887499986216424\n",
      "\n",
      "[1.9333463, 1.9298229, 1.900947, 1.8848032, 1.9264476]\n",
      "Current Iter : 17/300 batch : 7950/8000 acc : 0.42\n",
      " Current : 17 Acc : 0.4175999990105629 Test Acc : 0.40512499986216427\n",
      "\n",
      "[1.9292299, 1.9256227, 1.8961902, 1.8804669, 1.9248201]\n",
      "Current Iter : 18/300 batch : 7950/8000 acc : 0.42\n",
      " Current : 18 Acc : 0.4165999981760979 Test Acc : 0.4118749998509884\n",
      "\n",
      "[1.9250833, 1.9213544, 1.891431, 1.8761325, 1.9231918]\n",
      "Current Iter : 19/300 batch : 7950/8000 acc : 0.44\n",
      " Current : 19 Acc : 0.4219999983906746 Test Acc : 0.4151249980553985\n",
      "\n",
      "[1.920977, 1.917047, 1.8867157, 1.8718461, 1.9217353]\n",
      "Current Iter : 20/300 batch : 7950/8000 acc : 0.44\n",
      " Current : 20 Acc : 0.4281999984383583 Test Acc : 0.4224999999627471\n",
      "\n",
      "[1.917089, 1.9128757, 1.8822023, 1.8677474, 1.9205624]\n",
      "Current Iter : 21/300 batch : 7950/8000 acc : 0.38\n",
      " Current : 21 Acc : 0.42979999840259553 Test Acc : 0.4284999994561076\n",
      "\n",
      "[1.9134027, 1.9088023, 1.8779428, 1.8638581, 1.9197206]\n",
      "Current Iter : 22/300 batch : 7950/8000 acc : 0.38\n",
      " Current : 22 Acc : 0.4385999977588654 Test Acc : 0.4324999997392297\n",
      "\n",
      "[1.9095533, 1.904584, 1.8735753, 1.8598343, 1.9189622]\n",
      "Current Iter : 23/300 batch : 7950/8000 acc : 0.44\n",
      " Current : 23 Acc : 0.44280000001192094 Test Acc : 0.44237499982118605\n",
      "\n",
      "[1.9056021, 1.9002231, 1.8691893, 1.8557576, 1.9182822]\n",
      "Current Iter : 24/300 batch : 7950/8000 acc : 0.38\n",
      " Current : 24 Acc : 0.4453999987244606 Test Acc : 0.4441250000149012\n",
      "\n",
      "[1.9015099, 1.8956782, 1.864746, 1.8515977, 1.9176395]\n",
      "Current Iter : 25/300 batch : 7950/8000 acc : 0.44\n",
      " Current : 25 Acc : 0.4451999977231026 Test Acc : 0.444374999217689\n",
      "\n",
      "[1.8976128, 1.8916247, 1.8605926, 1.8472455, 1.9168416]\n",
      "Current Iter : 26/300 batch : 7950/8000 acc : 0.48\n",
      " Current : 26 Acc : 0.44619999676942823 Test Acc : 0.44662499856203797\n",
      "\n",
      "[1.8938966, 1.88752, 1.8563893, 1.84307, 1.9168605]\n",
      "Current Iter : 27/300 batch : 7950/8000 acc : 0.52\n",
      " Current : 27 Acc : 0.4549999967217445 Test Acc : 0.4507499985396862\n",
      "\n",
      "[1.8901784, 1.8832616, 1.8523293, 1.8390785, 1.9168504]\n",
      "Current Iter : 28/300 batch : 7950/8000 acc : 0.44\n",
      " Current : 28 Acc : 0.46359999865293505 Test Acc : 0.45224999971687796\n",
      "\n",
      "[1.8862222, 1.8787398, 1.8481511, 1.8350153, 1.9167473]\n",
      "Current Iter : 29/300 batch : 7950/8000 acc : 0.56\n",
      " Current : 29 Acc : 0.46219999819993973 Test Acc : 0.4599999990314245\n",
      "\n",
      "[1.8822627, 1.8746325, 1.8440318, 1.8305911, 1.9166632]\n",
      "Current Iter : 30/300 batch : 7950/8000 acc : 0.52\n",
      " Current : 30 Acc : 0.4639999979734421 Test Acc : 0.45762499812990426\n",
      "\n",
      "[1.8792112, 1.8711634, 1.8412001, 1.8273814, 1.9168488]\n",
      "Current Iter : 31/300 batch : 7950/8000 acc : 0.52\n",
      " Current : 31 Acc : 0.47159999758005144 Test Acc : 0.46212499868124723\n",
      "\n",
      "[1.8755745, 1.8669906, 1.8372773, 1.8235236, 1.9173359]\n",
      "Current Iter : 32/300 batch : 7950/8000 acc : 0.52\n",
      " Current : 32 Acc : 0.4827999997138977 Test Acc : 0.46374999955296514\n",
      "\n",
      "[1.8718767, 1.8627387, 1.8335648, 1.8198544, 1.9179115]\n",
      "Current Iter : 33/300 batch : 7950/8000 acc : 0.48\n",
      " Current : 33 Acc : 0.4814000001549721 Test Acc : 0.4593749985098839\n",
      "\n",
      "[1.8679615, 1.8589178, 1.8296485, 1.8151866, 1.9174182]\n",
      "Current Iter : 34/300 batch : 7950/8000 acc : 0.54\n",
      " Current : 34 Acc : 0.47439999878406525 Test Acc : 0.4742499990388751\n",
      "\n",
      "[1.8651167, 1.855708, 1.8265191, 1.8119074, 1.9182683]\n",
      "Current Iter : 35/300 batch : 7950/8000 acc : 0.52\n",
      " Current : 35 Acc : 0.48759999841451646 Test Acc : 0.4759999983012676\n",
      "\n",
      "[1.861664, 1.8518896, 1.8226063, 1.8078885, 1.918794]\n",
      "Current Iter : 36/300 batch : 7950/8000 acc : 0.48\n",
      " Current : 36 Acc : 0.4987999990582466 Test Acc : 0.47512499764561655\n",
      "\n",
      "[1.8580843, 1.847927, 1.8190683, 1.8043562, 1.9198681]\n",
      "Current Iter : 37/300 batch : 7950/8000 acc : 0.52\n",
      " Current : 37 Acc : 0.4948000001907349 Test Acc : 0.45037500038743017\n",
      "\n",
      "[1.8542136, 1.8437462, 1.81574, 1.8008233, 1.9202036]\n",
      "Current Iter : 38/300 batch : 7950/8000 acc : 0.54\n",
      " Current : 38 Acc : 0.4971999981999397 Test Acc : 0.4511249982751906\n",
      "\n",
      "[1.8514, 1.8406056, 1.8128439, 1.7975075, 1.9212195]\n",
      "Current Iter : 39/300 batch : 7950/8000 acc : 0.56\n",
      " Current : 39 Acc : 0.49839999914169314 Test Acc : 0.4513749979436398\n",
      "\n",
      "[1.848324, 1.8371775, 1.8095695, 1.7941062, 1.9220405]\n",
      "Current Iter : 40/300 batch : 7950/8000 acc : 0.52\n",
      " Current : 40 Acc : 0.5042000004649162 Test Acc : 0.4532499987632036\n",
      "\n",
      "[1.845069, 1.8335974, 1.8062611, 1.7905699, 1.9228671]\n",
      "Current Iter : 41/300 batch : 7950/8000 acc : 0.54\n",
      " Current : 41 Acc : 0.5102000007033348 Test Acc : 0.45699999909847977\n",
      "\n",
      "[1.8415921, 1.82984, 1.8028625, 1.7870424, 1.9236416]\n",
      "Current Iter : 42/300 batch : 7950/8000 acc : 0.54\n",
      " Current : 42 Acc : 0.5137999993562699 Test Acc : 0.46174999978393316\n",
      "\n",
      "[1.8382443, 1.8261737, 1.7995512, 1.783614, 1.9247057]\n",
      "Current Iter : 43/300 batch : 7950/8000 acc : 0.58\n",
      " Current : 43 Acc : 0.5167999985814095 Test Acc : 0.4621249994263053\n",
      "\n",
      "[1.8344084, 1.8220705, 1.7959465, 1.7798758, 1.9253732]\n",
      "Current Iter : 44/300 batch : 7950/8000 acc : 0.58\n",
      " Current : 44 Acc : 0.5213999992609024 Test Acc : 0.46337499748915434\n",
      "\n",
      "[1.830786, 1.8181481, 1.7926133, 1.7764245, 1.9264425]\n",
      "Current Iter : 45/300 batch : 7950/8000 acc : 0.54\n",
      " Current : 45 Acc : 0.525799999833107 Test Acc : 0.46949999760836364\n",
      "\n",
      "[1.827045, 1.8140901, 1.789282, 1.7728721, 1.9275374]\n",
      "Current Iter : 46/300 batch : 7950/8000 acc : 0.56\n",
      " Current : 46 Acc : 0.5359999999403954 Test Acc : 0.4834999987855554\n",
      "\n",
      "[1.8232274, 1.8099966, 1.7860377, 1.7692736, 1.9286011]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Iter : 47/300 batch : 7950/8000 acc : 0.56\n",
      " Current : 47 Acc : 0.5305999979376793 Test Acc : 0.45999999884516\n",
      "\n",
      "[1.8187954, 1.8052235, 1.7820826, 1.7651002, 1.9288472]\n",
      "Current Iter : 48/300 batch : 7950/8000 acc : 0.52\n",
      " Current : 48 Acc : 0.5340000009536743 Test Acc : 0.4694999983534217\n",
      "\n",
      "[1.8164417, 1.8026444, 1.7805457, 1.7633833, 1.9310234]\n",
      "Current Iter : 49/300 batch : 7950/8000 acc : 0.54\n",
      " Current : 49 Acc : 0.5390000003576279 Test Acc : 0.47787499874830247\n",
      "\n",
      "[1.8128847, 1.7985148, 1.777365, 1.7601672, 1.9321266]\n",
      "Current Iter : 50/300 batch : 7950/8000 acc : 0.58\n",
      " Current : 50 Acc : 0.5432000014185906 Test Acc : 0.48312499895691874\n",
      "\n",
      "[1.8092679, 1.7945374, 1.7744691, 1.7571436, 1.9336115]\n",
      "Current Iter : 51/300 batch : 7950/8000 acc : 0.56\n",
      " Current : 51 Acc : 0.5403999987244607 Test Acc : 0.4936249999329448\n",
      "\n",
      "[1.8060899, 1.7909585, 1.7720557, 1.7542216, 1.9354073]\n",
      "Current Iter : 52/300 batch : 7950/8000 acc : 0.56\n",
      " Current : 52 Acc : 0.5390000009536743 Test Acc : 0.4756249993108213\n",
      "\n",
      "[1.8028411, 1.7872274, 1.7691623, 1.7506489, 1.9361395]\n",
      "Current Iter : 53/300 batch : 7950/8000 acc : 0.58\n",
      " Current : 53 Acc : 0.5536000022292137 Test Acc : 0.49562499970197677\n",
      "\n",
      "[1.799099, 1.7829226, 1.7657357, 1.7471247, 1.9370604]\n",
      "Current Iter : 54/300 batch : 7950/8000 acc : 0.64\n",
      " Current : 54 Acc : 0.5580000033974648 Test Acc : 0.5092499999329447\n",
      "\n",
      "[1.7954558, 1.7792276, 1.7629944, 1.7444727, 1.9388332]\n",
      "Current Iter : 55/300 batch : 7950/8000 acc : 0.62\n",
      " Current : 55 Acc : 0.5587999996542931 Test Acc : 0.49637500159442427\n",
      "\n",
      "[1.7909123, 1.7744114, 1.7596543, 1.7415406, 1.9404218]\n",
      "Current Iter : 56/300 batch : 7950/8000 acc : 0.54\n",
      " Current : 56 Acc : 0.5623999997973442 Test Acc : 0.4974999986588955\n",
      "\n",
      "[1.7863039, 1.7698382, 1.7562759, 1.737955, 1.9415377]\n",
      "Current Iter : 57/300 batch : 7950/8000 acc : 0.54\n",
      " Current : 57 Acc : 0.5620000022649765 Test Acc : 0.4799999972805381\n",
      "\n",
      "[1.7818388, 1.7655324, 1.7535331, 1.7354472, 1.9428853]\n",
      "Current Iter : 58/300 batch : 7950/8000 acc : 0.56\n",
      " Current : 58 Acc : 0.5626000040769576 Test Acc : 0.4993749996647239\n",
      "\n",
      "[1.7783638, 1.7615036, 1.7504975, 1.7321723, 1.9440323]\n",
      "Current Iter : 59/300 batch : 7950/8000 acc : 0.58\n",
      " Current : 59 Acc : 0.5678000023961067 Test Acc : 0.5033749990165234\n",
      "\n",
      "[1.7751755, 1.7577696, 1.7487409, 1.7306657, 1.946028]\n",
      "Current Iter : 60/300 batch : 7950/8000 acc : 0.58\n",
      " Current : 60 Acc : 0.5722000005841256 Test Acc : 0.506500000320375\n",
      "\n",
      "[1.7717446, 1.7537041, 1.7467359, 1.7290864, 1.9482362]\n",
      "Current Iter : 61/300 batch : 7950/8000 acc : 0.66\n",
      " Current : 61 Acc : 0.577200002670288 Test Acc : 0.5101249976083636\n",
      "\n",
      "[1.7684418, 1.7498968, 1.7444104, 1.7277621, 1.9508361]\n",
      "Current Iter : 62/300 batch : 7950/8000 acc : 0.56\n",
      " Current : 62 Acc : 0.5722000014781952 Test Acc : 0.49600000102072955\n",
      "\n",
      "[1.7634596, 1.74502, 1.7412639, 1.7244318, 1.9510709]\n",
      "Current Iter : 63/300 batch : 7950/8000 acc : 0.52\n",
      " Current : 63 Acc : 0.5654000028967857 Test Acc : 0.46599999740719794\n",
      "\n",
      "[1.7601725, 1.7409246, 1.7385032, 1.7224264, 1.9527991]\n",
      "Current Iter : 64/300 batch : 7950/8000 acc : 0.54\n",
      " Current : 64 Acc : 0.5712000045180321 Test Acc : 0.495749999396503\n",
      "\n",
      "[1.7587289, 1.7389531, 1.7372535, 1.721893, 1.9554961]\n",
      "Current Iter : 65/300 batch : 7950/8000 acc : 0.52\n",
      " Current : 65 Acc : 0.5760000029206276 Test Acc : 0.4928749995306134\n",
      "\n",
      "[1.7550124, 1.7346656, 1.7342, 1.7192043, 1.957179]\n",
      "Current Iter : 66/300 batch : 7950/8000 acc : 0.52\n",
      " Current : 66 Acc : 0.5738000047206878 Test Acc : 0.48287499882280827\n",
      "\n",
      "[1.7515018, 1.7307824, 1.7318481, 1.7168611, 1.958799]\n",
      "Current Iter : 67/300 batch : 3300/5000 acc : 0.58\r"
     ]
    }
   ],
   "source": [
    "# train\n",
    "sess.run(tf.global_variables_initializer())\n",
    "avg_acc_train = 0; avg_acc_test  = 0; \n",
    "train_acc     = [];test_acc = []\n",
    "for iter in range(num_epoch):\n",
    "\n",
    "    for current_batch_index in range(0,len(train_images),batch_size):\n",
    "        current_data  = train_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = train_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy,gradient_update],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(train_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_train = avg_acc_train + sess_results[0]\n",
    "        \n",
    "    for current_batch_index in range(0,len(test_images), batch_size):\n",
    "        current_data  = test_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = test_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(test_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_test = avg_acc_test + sess_results[0]   \n",
    "        \n",
    "    print(\"\\n Current : \"+ str(iter) + \" Acc : \" + str(avg_acc_train/(len(train_images)/batch_size)) + \" Test Acc : \" + str(avg_acc_test/(len(test_images)/batch_size)) + '\\n')\n",
    "    \n",
    "    # save the training\n",
    "    train_acc.append(avg_acc_train/(len(train_images)/batch_size))\n",
    "    test_acc .append(avg_acc_test/(len(test_images)/batch_size)  )\n",
    "    \n",
    "    avg_acc_train = 0 ; avg_acc_test  = 0\n",
    "    print(sess.run([l1n.getw(),l2n.getw(),l3n.getw(),l4n.getw(),l5n.getw()]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T06:52:08.136334Z",
     "start_time": "2018-12-20T06:52:08.051288Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.489955e-10 0.0 2.489955e-10 2.489955e-10\n",
      "-0.00010212531 0.0011886288 0.007663564 -0.008750875\n",
      "1.0 7.627299e-08 1.0000063 1.0\n"
     ]
    }
   ],
   "source": [
    "grad_temp = np.asarray(sess.run([grad5l],feed_dict={x:current_data,y:current_label}))\n",
    "print(\n",
    "    grad_temp.mean(),\n",
    "    grad_temp.std(),\n",
    "    grad_temp.max(),\n",
    "    grad_temp.min()\n",
    ")\n",
    "\n",
    "grad_temp = np.asarray(sess.run([grad6p],feed_dict={x:current_data,y:current_label}))\n",
    "print(\n",
    "    grad_temp.mean(),\n",
    "    grad_temp.std(),\n",
    "    grad_temp.max(),\n",
    "    grad_temp.min()\n",
    ")\n",
    "\n",
    "grad_temp = np.asarray(sess.run([grad5n],feed_dict={x:current_data,y:current_label}))\n",
    "print(\n",
    "    grad_temp.mean(),\n",
    "    grad_temp.std(),\n",
    "    grad_temp.max(),\n",
    "    grad_temp.min()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "1. mttk/STL10. (2018). GitHub. Retrieved 19 December 2018, from https://github.com/mttk/STL10\n",
    "2. [duplicate], H. (2018). How to display multiple images in one figure correctly?. Stack Overflow. Retrieved 19 December 2018, from https://stackoverflow.com/questions/46615554/how-to-display-multiple-images-in-one-figure-correctly"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
