{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T07:57:42.564610Z",
     "start_time": "2018-12-20T07:57:39.629309Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# import Library and some random image data set\n",
    "import tensorflow as tf\n",
    "import numpy      as np\n",
    "import seaborn    as sns \n",
    "import pandas     as pd\n",
    "import os,sys\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(78); tf.set_random_seed(78)\n",
    "\n",
    "# get some of the STL data set\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from skimage import util \n",
    "from skimage.transform import resize\n",
    "from skimage.io import imread\n",
    "import warnings\n",
    "from numpy import inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T07:57:50.528979Z",
     "start_time": "2018-12-20T07:57:48.468510Z"
    },
    "code_folding": [
     0,
     2,
     29,
     37
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 96, 96, 3) 1.0 0.0\n",
      "(5000, 10) 1.0 0.0\n",
      "(8000, 96, 96, 3) 1.0 0.0\n",
      "(8000, 10) 1.0 0.0\n"
     ]
    }
   ],
   "source": [
    "# read all of the data\n",
    "# https://github.com/mttk/STL10\n",
    "def read_all_images(path_to_data):\n",
    "    \"\"\"\n",
    "    :param path_to_data: the file containing the binary images from the STL-10 dataset\n",
    "    :return: an array containing all the images\n",
    "    \"\"\"\n",
    "\n",
    "    with open(path_to_data, 'rb') as f:\n",
    "        # read whole file in uint8 chunks\n",
    "        everything = np.fromfile(f, dtype=np.uint8)\n",
    "\n",
    "        # We force the data into 3x96x96 chunks, since the\n",
    "        # images are stored in \"column-major order\", meaning\n",
    "        # that \"the first 96*96 values are the red channel,\n",
    "        # the next 96*96 are green, and the last are blue.\"\n",
    "        # The -1 is since the size of the pictures depends\n",
    "        # on the input file, and this way numpy determines\n",
    "        # the size on its own.\n",
    "\n",
    "        images = np.reshape(everything, (-1, 3, 96, 96))\n",
    "\n",
    "        # Now transpose the images into a standard image format\n",
    "        # readable by, for example, matplotlib.imshow\n",
    "        # You might want to comment this line or reverse the shuffle\n",
    "        # if you will use a learning algorithm like CNN, since they like\n",
    "        # their channels separated.\n",
    "        images = np.transpose(images, (0, 3, 2, 1))\n",
    "        return images\n",
    "def read_labels(path_to_labels):\n",
    "    \"\"\"\n",
    "    :param path_to_labels: path to the binary file containing labels from the STL-10 dataset\n",
    "    :return: an array containing the labels\n",
    "    \"\"\"\n",
    "    with open(path_to_labels, 'rb') as f:\n",
    "        labels = np.fromfile(f, dtype=np.uint8)\n",
    "        return labels\n",
    "def show_images(data,row=1,col=1):\n",
    "    fig=plt.figure(figsize=(10,10))\n",
    "    columns = col; rows = row\n",
    "    for i in range(1, columns*rows +1):\n",
    "        fig.add_subplot(rows, columns, i)\n",
    "        plt.imshow(data[i-1])\n",
    "    plt.show()\n",
    "\n",
    "train_images = read_all_images(\"../../../DataSet/STL10/stl10_binary/train_X.bin\") / 255.0\n",
    "train_labels = read_labels    (\"../../../DataSet/STL10/stl10_binary/train_Y.bin\")\n",
    "test_images  = read_all_images(\"../../../DataSet/STL10/stl10_binary/test_X.bin\")  / 255.0\n",
    "test_labels  = read_labels    (\"../../../DataSet/STL10/stl10_binary/test_y.bin\")\n",
    "\n",
    "label_encoder= OneHotEncoder(sparse=False,categories='auto')\n",
    "train_labels = label_encoder.fit_transform(train_labels.reshape((-1,1)))\n",
    "test_labels  = label_encoder.fit_transform(test_labels.reshape((-1,1)))\n",
    "\n",
    "print(train_images.shape,train_images.max(),train_images.min())\n",
    "print(train_labels.shape,train_labels.max(),train_labels.min())\n",
    "print(test_images.shape,test_images.max(),test_images.min())\n",
    "print(test_labels.shape,test_labels.max(),test_labels.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T07:57:50.712525Z",
     "start_time": "2018-12-20T07:57:50.650679Z"
    },
    "code_folding": [
     15,
     58,
     99,
     140
    ]
   },
   "outputs": [],
   "source": [
    "# create the layers\n",
    "def tf_softmax(x): return tf.nn.softmax(x)\n",
    "\n",
    "def tf_elu(x):   return tf.nn.elu(x)\n",
    "def d_tf_elu(x): return tf.cast(tf.greater(x,0),tf.float32)  + (tf_elu(tf.cast(tf.less_equal(x,0),tf.float32) * x) + 1.0)\n",
    "\n",
    "def tf_relu(x):   return tf.nn.relu(x)\n",
    "def d_tf_relu(x): return tf.cast(tf.greater(x,0),tf.float32)\n",
    "\n",
    "def tf_tanh(x):   return tf.nn.tanh(x)\n",
    "def d_tf_tanh(x): return 1 - tf_tanh(x) ** 2\n",
    "\n",
    "def tf_sigmoid(x):   return tf.nn.sigmoid(x)\n",
    "def d_tf_sigmoid(x): return tf_sigmoid(x) * (1.0-tf_sigmoid(x))\n",
    "\n",
    "class CNN():\n",
    "\n",
    "    def __init__(self,k,inc,out, stddev=0.05,which_reg=0,act=tf_relu,d_act=d_tf_relu):\n",
    "        self.w          = tf.Variable(tf.random_normal([k,k,inc,out],stddev=stddev,seed=2,dtype=tf.float32))\n",
    "        self.m,self.v   = tf.Variable(tf.zeros_like(self.w)),tf.Variable(tf.zeros_like(self.w))\n",
    "        self.act,self.d_act = act,d_act\n",
    "        self.which_reg  = which_reg\n",
    "        \n",
    "    def getw(self): return self.w\n",
    "\n",
    "    def feedforward(self,input,stride=1,padding='SAME'):\n",
    "        self.input  = input\n",
    "        self.layer  = tf.nn.conv2d(input,self.w,strides=[1,stride,stride,1],padding=padding) \n",
    "        self.layerA = self.act(self.layer)\n",
    "        return [self.layer,self.layerA]\n",
    "    \n",
    "    def backprop(self,gradient,stride=1,padding='SAME'):\n",
    "        grad_part_1 = gradient\n",
    "        grad_part_2 = self.d_act(self.layer)\n",
    "        grad_part_3 = self.input\n",
    "\n",
    "        grad_middle = grad_part_1 * grad_part_2\n",
    "        grad        = tf.nn.conv2d_backprop_filter(input = grad_part_3,filter_sizes = tf.shape(self.w),  out_backprop = grad_middle,strides=[1,stride,stride,1],padding=padding) / batch_size\n",
    "        grad_pass   = tf.nn.conv2d_backprop_input (input_sizes = tf.shape(self.input),filter= self.w,out_backprop = grad_middle,strides=[1,stride,stride,1],padding=padding)\n",
    "\n",
    "        if self.which_reg == 0:   grad = grad\n",
    "        if self.which_reg == 0.5: grad = grad + lamda * (tf.sqrt(tf.abs(self.w))) * (1.0/tf.sqrt(tf.abs(self.w)+ 10e-5)) * tf.sign(self.w)\n",
    "        if self.which_reg == 1:   grad = grad + lamda * tf.sign(self.w)\n",
    "        if self.which_reg == 1.5: grad = grad + lamda * 1.0/(tf.sqrt(tf.square(self.w) + 10e-5)) * self.w\n",
    "        if self.which_reg == 2:   grad = grad + lamda * (1.0/tf.sqrt(tf.square(tf.abs(self.w))+ 10e-5)) * tf.abs(self.w) * tf.sign(self.w)\n",
    "        if self.which_reg == 2.5: grad = grad + lamda * 2.0 * self.w\n",
    "        if self.which_reg == 3:   grad = grad + lamda * tf.pow(tf.pow(tf.abs(self.w),3)+ 10e-5,-0.66) * tf.pow(tf.abs(self.w),2) * tf.sign(self.w)\n",
    "        if self.which_reg == 4:   grad = grad + lamda * tf.pow(tf.pow(tf.abs(self.w),4)+ 10e-5,-0.75) * tf.pow(tf.abs(self.w),3) * tf.sign(self.w)\n",
    "\n",
    "        update_w = []\n",
    "        update_w.append(tf.assign( self.m,self.m*beta1 + (1-beta1) * (grad)   ))\n",
    "        update_w.append(tf.assign( self.v,self.v*beta2 + (1-beta2) * (grad ** 2)   ))\n",
    "        m_hat = self.m / (1-beta1) ; v_hat = self.v / (1-beta2)\n",
    "        adam_middle = m_hat * learning_rate/(tf.sqrt(v_hat) + adam_e)\n",
    "        update_w.append(tf.assign(self.w,tf.subtract(self.w,adam_middle  )))\n",
    "        \n",
    "        return grad_pass,grad,update_w\n",
    "    \n",
    "class tf_batch_norm_layer():\n",
    "    \n",
    "    def __init__(self,vector_shape,axis):\n",
    "        self.moving_mean = tf.Variable(tf.zeros(shape=[1,1,1,vector_shape],dtype=tf.float32))\n",
    "        self.moving_vari = tf.Variable(tf.zeros(shape=[1,1,1,vector_shape],dtype=tf.float32))\n",
    "        self.axis        = axis\n",
    "        \n",
    "    def feedforward(self,input,training_phase=True,eps = 1e-8):\n",
    "        self.input = input\n",
    "        self.input_size          = self.input.shape\n",
    "        self.batch,self.h,self.w,self.c = self.input_size[0].value,self.input_size[1].value,self.input_size[2].value,self.input_size[3].value\n",
    "\n",
    "        # Training Moving Average Mean         \n",
    "        def training_fn():\n",
    "            self.mean    = tf.reduce_mean(self.input,axis=self.axis ,keepdims=True)\n",
    "            self.var     = tf.reduce_mean(tf.square(self.input-self.mean),axis=self.axis,keepdims=True)\n",
    "            centered_data= (self.input - self.mean)/tf.sqrt(self.var + eps)\n",
    "            \n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean*0.9 + 0.1 * self.mean ))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari*0.9 + 0.1 * self.var  ))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        # Testing Moving Average Mean        \n",
    "        def  testing_fn():\n",
    "            centered_data   = (self.input - self.moving_mean)/tf.sqrt(self.moving_vari + eps)\n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        self.output,update_variable = tf.cond(training_phase,true_fn=training_fn,false_fn=testing_fn)\n",
    "        return self.output,update_variable\n",
    "    \n",
    "    def backprop(self,grad,eps = 1e-8):\n",
    "        change_parts = 1.0 /(self.batch * self.h * self.w)\n",
    "        grad_sigma   = tf.reduce_sum( grad *  (self.input-self.mean)     ,axis=self.axis,keepdims=True) * -0.5 * (self.var+eps) ** -1.5\n",
    "        grad_mean    = tf.reduce_sum( grad *  (-1./tf.sqrt(self.var+eps)),axis=self.axis,keepdims=True) + grad_sigma * change_parts * 2.0 * tf.reduce_sum((self.input-self.mean),axis=self.axis,keepdims=True) * -1\n",
    "        grad_x       = grad * 1/(tf.sqrt(self.var+eps)) + grad_sigma * change_parts * 2.0 * (self.input-self.mean) + grad_mean * change_parts\n",
    "        return grad_x\n",
    "\n",
    "class tf_layer_norm_layer():\n",
    "    \n",
    "    def __init__(self,vector_shape,axis):\n",
    "        self.moving_mean = tf.Variable(tf.zeros(shape=[vector_shape,1,1,1],dtype=tf.float32))\n",
    "        self.moving_vari = tf.Variable(tf.zeros(shape=[vector_shape,1,1,1],dtype=tf.float32))\n",
    "        self.axis        = axis\n",
    "        \n",
    "    def feedforward(self,input,training_phase=True,eps = 1e-8):\n",
    "        self.input = input\n",
    "        self.input_size          = self.input.shape\n",
    "        self.batch,self.h,self.w,self.c = self.input_size[0].value,self.input_size[1].value,self.input_size[2].value,self.input_size[3].value\n",
    "\n",
    "        # Training Moving Average Mean         \n",
    "        def training_fn():\n",
    "            self.mean    = tf.reduce_mean(self.input,axis=self.axis ,keepdims=True)\n",
    "            self.var     = tf.reduce_mean(tf.square(self.input-self.mean),axis=self.axis,keepdims=True)\n",
    "            centered_data= (self.input - self.mean)/tf.sqrt(self.var + eps)\n",
    "            \n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean*0.9 + 0.1 * self.mean ))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari*0.9 + 0.1 * self.var  ))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        # Testing Moving Average Mean        \n",
    "        def  testing_fn():\n",
    "            centered_data   = (self.input - self.moving_mean)/tf.sqrt(self.moving_vari + eps)\n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        self.output,update_variable = tf.cond(training_phase,true_fn=training_fn,false_fn=testing_fn)\n",
    "        return self.output,update_variable\n",
    "    \n",
    "    def backprop(self,grad,eps = 1e-8):\n",
    "        change_parts = 1.0 /(self.h * self.w * self.c)\n",
    "        grad_sigma   = tf.reduce_sum( grad *  (self.input-self.mean)     ,axis=self.axis,keepdims=True) * -0.5 * (self.var+eps) ** -1.5\n",
    "        grad_mean    = tf.reduce_sum( grad *  (-1./tf.sqrt(self.var+eps)),axis=self.axis,keepdims=True) + grad_sigma * change_parts * 2.0 * tf.reduce_sum((self.input-self.mean),axis=self.axis,keepdims=True) * -1\n",
    "        grad_x       = grad * 1/(tf.sqrt(self.var+eps)) + grad_sigma * change_parts * 2.0 * (self.input-self.mean) + grad_mean * change_parts\n",
    "        return grad_x\n",
    "    \n",
    "class tf_instance_norm_layer():\n",
    "    \n",
    "    def __init__(self,batch_size,vector_shape,axis):\n",
    "        self.moving_mean = tf.Variable(tf.zeros(shape=[batch_size,1,1,vector_shape],dtype=tf.float32))\n",
    "        self.moving_vari = tf.Variable(tf.zeros(shape=[batch_size,1,1,vector_shape],dtype=tf.float32))\n",
    "        self.axis        = axis\n",
    "        \n",
    "    def feedforward(self,input,training_phase=True,eps = 1e-8):\n",
    "        self.input = input\n",
    "        self.input_size          = self.input.shape\n",
    "        self.batch,self.h,self.w,self.c = self.input_size[0].value,self.input_size[1].value,self.input_size[2].value,self.input_size[3].value\n",
    "\n",
    "        # Training Moving Average Mean         \n",
    "        def training_fn():\n",
    "            self.mean    = tf.reduce_mean(self.input,axis=self.axis ,keepdims=True)\n",
    "            self.var     = tf.reduce_mean(tf.square(self.input-self.mean),axis=self.axis,keepdims=True)\n",
    "            centered_data= (self.input - self.mean)/tf.sqrt(self.var + eps)\n",
    "            \n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean*0.9 + 0.1 * self.mean ))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari*0.9 + 0.1 * self.var  ))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        # Testing Moving Average Mean        \n",
    "        def  testing_fn():\n",
    "            centered_data   = (self.input - self.moving_mean)/tf.sqrt(self.moving_vari + eps)\n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        self.output,update_variable = tf.cond(training_phase,true_fn=training_fn,false_fn=testing_fn)\n",
    "        return self.output,update_variable\n",
    "    \n",
    "    def backprop(self,grad,eps = 1e-8):\n",
    "        change_parts = 1.0 /(self.h * self.w)\n",
    "        grad_sigma   = tf.reduce_sum( grad *  (self.input-self.mean)     ,axis=self.axis,keepdims=True) * -0.5 * (self.var+eps) ** -1.5\n",
    "        grad_mean    = tf.reduce_sum( grad *  (-1./tf.sqrt(self.var+eps)),axis=self.axis,keepdims=True) + grad_sigma * change_parts * 2.0 * tf.reduce_sum((self.input-self.mean),axis=self.axis,keepdims=True) * -1\n",
    "        grad_x       = grad * 1/(tf.sqrt(self.var+eps)) + grad_sigma * change_parts * 2.0 * (self.input-self.mean) + grad_mean * change_parts\n",
    "        return grad_x\n",
    "  \n",
    "class tf_box_cox():\n",
    "    \n",
    "    def __init__(self,channel):\n",
    "        self.lmbda    = tf.Variable(2.0) \n",
    "        self.m,self.v = tf.Variable(tf.zeros_like(self.lmbda)),tf.Variable(tf.zeros_like(self.lmbda))\n",
    "    def getw(self): return self.lmbda\n",
    "    \n",
    "    def feedforward(self,data):\n",
    "        self.input = data\n",
    "        self.layer = tf.pow((self.input + 1.0),self.lmbda)\n",
    "        return (self.layer - 1.0)/(self.lmbda + 1e-8)\n",
    "    \n",
    "    def backprop(self,grad):\n",
    "        \n",
    "        # Gradient that gets passed along\n",
    "        grad_pass = tf.pow((self.input + 1),self.lmbda-1.0) * grad\n",
    "        \n",
    "        # Grad respect to the lmbda value (not tested!)\n",
    "        grad_lmbda1 =   (self.layer * tf.log(self.input + 1 ))/(self.lmbda + 1e-8)\n",
    "        grad_lmbda2 = - (self.layer - 1)/(self.lmbda ** 2 + 1e-8)\n",
    "        grad_lmbda  = tf.reduce_mean((grad_lmbda1 + grad_lmbda2)*grad)\n",
    "\n",
    "        update_w = []\n",
    "        update_w.append(tf.assign( self.m,self.m*beta1 + (1-beta1) * (grad_lmbda)   ))\n",
    "        update_w.append(tf.assign( self.v,self.v*beta2 + (1-beta2) * (grad_lmbda ** 2)   ))\n",
    "        m_hat = self.m / (1-beta1) ; v_hat = self.v / (1-beta2)\n",
    "        adam_middle = m_hat * learning_rate/(tf.sqrt(v_hat) + adam_e)\n",
    "        update_w.append(tf.assign(self.lmbda,tf.subtract(self.lmbda,adam_middle  )))\n",
    "        \n",
    "        return grad_pass,grad_lmbda,update_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T06:21:04.871403Z",
     "start_time": "2018-12-20T06:21:04.769676Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# hyper parameter\n",
    "num_epoch = 300; learning_rate = 0.0008; batch_size = 20\n",
    "beta1,beta2,adam_e = 0.9,0.999,1e-9\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T06:07:37.013560Z",
     "start_time": "2018-12-20T06:07:36.830997Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# create layers\n",
    "l1 = CNN(3,3, 16); \n",
    "l2 = CNN(3,16,16); \n",
    "l3 = CNN(3,16,16); \n",
    "\n",
    "l4 = CNN(3,16,32); \n",
    "l5 = CNN(3,32,32); \n",
    "l6 = CNN(3,32,10); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T06:08:47.504461Z",
     "start_time": "2018-12-20T06:08:47.299795Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# create the graph \n",
    "x = tf.placeholder(tf.float32,[batch_size,96,96,3])\n",
    "y = tf.placeholder(tf.float32,[batch_size,10])\n",
    "\n",
    "layer1,layer1a = l1.feedforward(x,stride=2)      ;          \n",
    "layer2,layer2a = l2.feedforward(layer1a,stride=2);         \n",
    "layer3,layer3a = l3.feedforward(layer2a,stride=2); \n",
    "\n",
    "layer4,layer4a = l4.feedforward(layer3a,stride=2);          \n",
    "layer5,layer5a = l5.feedforward(layer4a,stride=1);         \n",
    "layer6,layer6a = l6.feedforward(layer5a,stride=1); \n",
    "\n",
    "final_layer   = tf.reduce_mean(layer6a,(1,2))\n",
    "final_softmax = tf_softmax(final_layer)\n",
    "cost          = -tf.reduce_mean(y * tf.log(final_softmax + 1e-8))\n",
    "correct_prediction = tf.equal(tf.argmax(final_softmax, 1), tf.argmax(y, 1))\n",
    "accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "gradient = tf.tile((final_softmax-y)[:,None,None,:],[1,6,6,1])/batch_size\n",
    "grad6w,grad6p,grad6_up = l6.backprop(gradient)\n",
    "grad5w,grad5p,grad5_up = l5.backprop(grad6p)\n",
    "grad4w,grad4p,grad4_up = l4.backprop(grad5p,stride=2)\n",
    "\n",
    "grad3w,grad3p,grad3_up = l3.backprop(grad4p,stride=2)\n",
    "grad2w,grad2p,grad2_up = l2.backprop(grad3p,stride=2)\n",
    "grad1w,grad1p,grad1_up = l1.backprop(grad2p,stride=2)\n",
    "\n",
    "gradient_update = grad6_up + \\\n",
    "                   grad5_up + \\\n",
    "                   grad4_up + \\\n",
    "                   grad3_up + \\\n",
    "                   grad2_up + \\\n",
    "                   grad1_up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T06:09:14.239248Z",
     "start_time": "2018-12-20T06:08:54.084190Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Iter : 0/300 batch : 7980/8000 acc : 0.35\n",
      " Current : 0 Acc : 0.12840000288188458 Test Acc : 0.21200000332668423\n",
      "\n",
      "Current Iter : 1/300 batch : 7980/8000 acc : 0.45\n",
      " Current : 1 Acc : 0.22900000382959843 Test Acc : 0.25975000286474825\n",
      "\n",
      "Current Iter : 2/300 batch : 7980/8000 acc : 0.55\n",
      " Current : 2 Acc : 0.2868000023066998 Test Acc : 0.3093750014156103\n",
      "\n",
      "Current Iter : 3/300 batch : 900/8000 acc : 0.455\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-fbb428fac211>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mcurrent_data\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mtest_images\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcurrent_batch_index\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mcurrent_batch_index\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mcurrent_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcurrent_batch_index\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mcurrent_batch_index\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0msess_results\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mcurrent_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mcurrent_label\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Current Iter : '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/'\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_epoch\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;33m+\u001b[0m \u001b[1;34m' batch : '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_batch_index\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/'\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' acc : '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'\\r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0mavg_acc_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mavg_acc_test\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msess_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1332\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train\n",
    "sess.run(tf.global_variables_initializer())\n",
    "avg_acc_train = 0; avg_acc_test  = 0; \n",
    "train_acc     = [];test_acc = []\n",
    "for iter in range(num_epoch):\n",
    "\n",
    "    for current_batch_index in range(0,len(train_images),batch_size):\n",
    "        current_data  = train_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = train_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy,gradient_update],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(train_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_train = avg_acc_train + sess_results[0]\n",
    "        \n",
    "    for current_batch_index in range(0,len(test_images), batch_size):\n",
    "        current_data  = test_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = test_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(test_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_test = avg_acc_test + sess_results[0]   \n",
    "        \n",
    "    print(\"\\n Current : \"+ str(iter) + \" Acc : \" + str(avg_acc_train/(len(train_images)/batch_size)) + \" Test Acc : \" + str(avg_acc_test/(len(test_images)/batch_size)) + '\\n')\n",
    "    \n",
    "    # save the training\n",
    "    train_acc.append(avg_acc_train/(len(train_images)/batch_size))\n",
    "    test_acc .append(avg_acc_test/(len(test_images)/batch_size)  )\n",
    "    \n",
    "    \n",
    "    avg_acc_train = 0 ; avg_acc_test  = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T07:58:04.238600Z",
     "start_time": "2018-12-20T07:58:03.253590Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# create layers\n",
    "num_epoch = 150; learning_rate = 0.001; batch_size = 50\n",
    "beta1,beta2,adam_e = 0.9,0.999,1e-8\n",
    "# tf.reset_default_graph(); sess.close()\n",
    "sess = tf.InteractiveSession()\n",
    "l1 = CNN(3,3, 16); l1n = tf_box_cox(16)\n",
    "l2 = CNN(3,16,16); l2n = tf_box_cox(16)\n",
    "l3 = CNN(3,16,16); l3n = tf_box_cox(16)\n",
    "\n",
    "l4 = CNN(3,16,32); l4n = tf_box_cox(32)\n",
    "l5 = CNN(3,32,32); l5n = tf_box_cox(32)\n",
    "l6 = CNN(3,32,10); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T07:58:05.755803Z",
     "start_time": "2018-12-20T07:58:04.833032Z"
    }
   },
   "outputs": [],
   "source": [
    "# create the graph \n",
    "x = tf.placeholder(tf.float32,[batch_size,96,96,3])\n",
    "y = tf.placeholder(tf.float32,[batch_size,10])\n",
    "\n",
    "layer1,layer1a = l1.feedforward(x,stride=2)      ;          \n",
    "layer1n = l1n.feedforward(layer1a)\n",
    "layer2,layer2a = l2.feedforward(layer1n,stride=2);          \n",
    "layer2n = l2n.feedforward(layer2a)\n",
    "layer3,layer3a = l3.feedforward(layer2n,stride=2); \n",
    "layer3n = l3n.feedforward(layer3a)\n",
    "\n",
    "layer4,layer4a = l4.feedforward(layer3n,stride=2);          \n",
    "layer4n = l4n.feedforward(layer4a)\n",
    "layer5,layer5a = l5.feedforward(layer4n,stride=1);          \n",
    "layer5n = l5n.feedforward(layer5a)\n",
    "layer6,layer6a = l6.feedforward(layer5n,stride=1); \n",
    "\n",
    "final_layer   = tf.reduce_mean(layer6a,(1,2))\n",
    "final_softmax = tf_softmax(final_layer)\n",
    "cost          = -tf.reduce_mean(y * tf.log(final_softmax + 1e-8))\n",
    "auto_train = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "correct_prediction = tf.equal(tf.argmax(final_softmax, 1), tf.argmax(y, 1))\n",
    "accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "gradient = tf.tile((final_softmax-y)[:,None,None,:],[1,6,6,1])/batch_size\n",
    "grad6p,grad6w,grad6_up  = l6.backprop(gradient)\n",
    "grad5n,grad5l,grad5n_up = l5n.backprop(grad6p); \n",
    "grad5p,grad5w,grad5_up  = l5.backprop(grad5n)\n",
    "grad4n,grad4l,grad4n_up = l4n.backprop(grad5p); \n",
    "grad4p,grad4w,grad4_up  = l4.backprop(grad4n,stride=2)\n",
    "\n",
    "grad3n,grad3l,grad3n_up = l3n.backprop(grad4p);\n",
    "grad3p,grad3w,grad3_up  = l3.backprop(grad3n,stride=2)\n",
    "grad2n,grad2l,grad2n_up = l2n.backprop(grad3p); \n",
    "grad2p,grad2w,grad2_up  = l2.backprop(grad2n,stride=2)\n",
    "grad1n,grad1l,grad1n_up = l1n.backprop(grad2p); \n",
    "grad1p,grad1w,grad1_up  = l1.backprop(grad1n,stride=2)\n",
    "\n",
    "gradient_update = grad6_up + \\\n",
    "                  grad5n_up + grad5_up + \\\n",
    "                  grad4n_up + grad4_up + \\\n",
    "                  grad3n_up + grad3_up + \\\n",
    "                  grad2n_up + grad2_up + \\\n",
    "                  grad1n_up + grad1_up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-20T07:58:06.362Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Iter : 0/150 batch : 7950/8000 acc : 0.14\n",
      " Current : 0 Acc : 0.11519999988377094 Test Acc : 0.1536250002682209\n",
      "\n",
      "Current Iter : 1/150 batch : 7950/8000 acc : 0.16\n",
      " Current : 1 Acc : 0.13579999953508376 Test Acc : 0.11912499913014471\n",
      "\n",
      "Current Iter : 2/150 batch : 7950/8000 acc : 0.24\n",
      " Current : 2 Acc : 0.18959999959915877 Test Acc : 0.22787500023841858\n",
      "\n",
      "Current Iter : 3/150 batch : 7950/8000 acc : 0.28\n",
      " Current : 3 Acc : 0.2246000000089407 Test Acc : 0.21637499970383942\n",
      "\n",
      "Current Iter : 4/150 batch : 7950/8000 acc : 0.32\n",
      " Current : 4 Acc : 0.25980000153183935 Test Acc : 0.2677499999292195\n",
      "\n",
      "Current Iter : 5/150 batch : 7950/8000 acc : 0.34\n",
      " Current : 5 Acc : 0.28740000069141386 Test Acc : 0.29887500014156104\n",
      "\n",
      "Current Iter : 6/150 batch : 7950/8000 acc : 0.36\n",
      " Current : 6 Acc : 0.30079999938607216 Test Acc : 0.3226250006817281\n",
      "\n",
      "Current Iter : 7/150 batch : 7950/8000 acc : 0.44\n",
      " Current : 7 Acc : 0.35100000083446503 Test Acc : 0.3398750016465783\n",
      "\n",
      "Current Iter : 8/150 batch : 7950/8000 acc : 0.36\n",
      " Current : 8 Acc : 0.3658000002801418 Test Acc : 0.37462499979883435\n",
      "\n",
      "Current Iter : 9/150 batch : 7950/8000 acc : 0.42\n",
      " Current : 9 Acc : 0.36940000057220457 Test Acc : 0.3793749989941716\n",
      "\n",
      "Current Iter : 10/150 batch : 7950/8000 acc : 0.38\n",
      " Current : 10 Acc : 0.3796000005304813 Test Acc : 0.38600000012665986\n",
      "\n",
      "Current Iter : 11/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 11 Acc : 0.39379999876022337 Test Acc : 0.3865000009536743\n",
      "\n",
      "Current Iter : 12/150 batch : 7950/8000 acc : 0.46\n",
      " Current : 12 Acc : 0.40019999906420706 Test Acc : 0.3943750005215406\n",
      "\n",
      "Current Iter : 13/150 batch : 7950/8000 acc : 0.42\n",
      " Current : 13 Acc : 0.40599999979138374 Test Acc : 0.39700000109151007\n",
      "\n",
      "Current Iter : 14/150 batch : 7950/8000 acc : 0.42\n",
      " Current : 14 Acc : 0.4165999999642372 Test Acc : 0.39937500059604647\n",
      "\n",
      "Current Iter : 15/150 batch : 7950/8000 acc : 0.42\n",
      " Current : 15 Acc : 0.4209999988973141 Test Acc : 0.4053749998100102\n",
      "\n",
      "Current Iter : 16/150 batch : 7950/8000 acc : 0.42\n",
      " Current : 16 Acc : 0.42299999922513964 Test Acc : 0.40837499853223563\n",
      "\n",
      "Current Iter : 17/150 batch : 7950/8000 acc : 0.42\n",
      " Current : 17 Acc : 0.43159999907016755 Test Acc : 0.41375000057742\n",
      "\n",
      "Current Iter : 18/150 batch : 7950/8000 acc : 0.46\n",
      " Current : 18 Acc : 0.4387999990582466 Test Acc : 0.40225000120699406\n",
      "\n",
      "Current Iter : 19/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 19 Acc : 0.43919999986886976 Test Acc : 0.4183749994263053\n",
      "\n",
      "Current Iter : 20/150 batch : 7950/8000 acc : 0.44\n",
      " Current : 20 Acc : 0.4433999997377396 Test Acc : 0.4013749993406236\n",
      "\n",
      "Current Iter : 21/150 batch : 7950/8000 acc : 0.44\n",
      " Current : 21 Acc : 0.44659999817609786 Test Acc : 0.4107500000856817\n",
      "\n",
      "Current Iter : 22/150 batch : 7950/8000 acc : 0.46\n",
      " Current : 22 Acc : 0.4528000006079674 Test Acc : 0.41799999950453637\n",
      "\n",
      "Current Iter : 23/150 batch : 7950/8000 acc : 0.46\n",
      " Current : 23 Acc : 0.45799999833106997 Test Acc : 0.4195000000298023\n",
      "\n",
      "Current Iter : 24/150 batch : 7950/8000 acc : 0.44\n",
      " Current : 24 Acc : 0.46259999960660936 Test Acc : 0.41275000032037495\n",
      "\n",
      "Current Iter : 25/150 batch : 7950/8000 acc : 0.46\n",
      " Current : 25 Acc : 0.46859999805688857 Test Acc : 0.4133750002831221\n",
      "\n",
      "Current Iter : 26/150 batch : 7950/8000 acc : 0.56\n",
      " Current : 26 Acc : 0.4718000015616417 Test Acc : 0.42337500043213366\n",
      "\n",
      "Current Iter : 27/150 batch : 7950/8000 acc : 0.54\n",
      " Current : 27 Acc : 0.4757999986410141 Test Acc : 0.40924999937415124\n",
      "\n",
      "Current Iter : 28/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 28 Acc : 0.47720000058412554 Test Acc : 0.41424999907612803\n",
      "\n",
      "Current Iter : 29/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 29 Acc : 0.48459999948740007 Test Acc : 0.4208749998360872\n",
      "\n",
      "Current Iter : 30/150 batch : 7950/8000 acc : 0.46\n",
      " Current : 30 Acc : 0.4702000007033348 Test Acc : 0.4316249996423721\n",
      "\n",
      "Current Iter : 31/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 31 Acc : 0.47979999989271166 Test Acc : 0.4368749991059303\n",
      "\n",
      "Current Iter : 32/150 batch : 7950/8000 acc : 0.54\n",
      " Current : 32 Acc : 0.4923999983072281 Test Acc : 0.4372499991208315\n",
      "\n",
      "Current Iter : 33/150 batch : 7950/8000 acc : 0.54\n",
      " Current : 33 Acc : 0.4948000001907349 Test Acc : 0.43787499852478506\n",
      "\n",
      "Current Iter : 34/150 batch : 7950/8000 acc : 0.54\n",
      " Current : 34 Acc : 0.497399999499321 Test Acc : 0.4406249994412065\n",
      "\n",
      "Current Iter : 35/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 35 Acc : 0.4961999985575676 Test Acc : 0.43949999921023847\n",
      "\n",
      "Current Iter : 36/150 batch : 7950/8000 acc : 0.54\n",
      " Current : 36 Acc : 0.5021999973058701 Test Acc : 0.4441250001080334\n",
      "\n",
      "Current Iter : 37/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 37 Acc : 0.5062000000476837 Test Acc : 0.44149999832734466\n",
      "\n",
      "Current Iter : 38/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 38 Acc : 0.5095999997854233 Test Acc : 0.4422499985434115\n",
      "\n",
      "Current Iter : 39/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 39 Acc : 0.5176000002026558 Test Acc : 0.443499998934567\n",
      "\n",
      "Current Iter : 40/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 40 Acc : 0.5203999975323677 Test Acc : 0.4432499984279275\n",
      "\n",
      "Current Iter : 41/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 41 Acc : 0.5225999990105629 Test Acc : 0.44574999921023845\n",
      "\n",
      "Current Iter : 42/150 batch : 7950/8000 acc : 0.56\n",
      " Current : 42 Acc : 0.5315999990701675 Test Acc : 0.44924999941140414\n",
      "\n",
      "Current Iter : 43/150 batch : 7950/8000 acc : 0.54\n",
      " Current : 43 Acc : 0.5370000004768372 Test Acc : 0.4511249991133809\n",
      "\n",
      "Current Iter : 44/150 batch : 7950/8000 acc : 0.54\n",
      " Current : 44 Acc : 0.5406000003218651 Test Acc : 0.44587499760091304\n",
      "\n",
      "Current Iter : 45/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 45 Acc : 0.5406000018119812 Test Acc : 0.43112499862909315\n",
      "\n",
      "Current Iter : 46/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 46 Acc : 0.5333999991416931 Test Acc : 0.43549999929964545\n",
      "\n",
      "Current Iter : 47/150 batch : 7950/8000 acc : 0.56\n",
      " Current : 47 Acc : 0.5391999986767769 Test Acc : 0.4416249994188547\n",
      "\n",
      "Current Iter : 48/150 batch : 7950/8000 acc : 0.56\n",
      " Current : 48 Acc : 0.5317999988794326 Test Acc : 0.4362500002607703\n",
      "\n",
      "Current Iter : 49/150 batch : 7950/8000 acc : 0.54\n",
      " Current : 49 Acc : 0.5338000020384789 Test Acc : 0.4613749973475933\n",
      "\n",
      "Current Iter : 50/150 batch : 7950/8000 acc : 0.56\n",
      " Current : 50 Acc : 0.5453999987244607 Test Acc : 0.46862499620765447\n",
      "\n",
      "Current Iter : 51/150 batch : 7950/8000 acc : 0.56\n",
      " Current : 51 Acc : 0.5486000013351441 Test Acc : 0.46199999898672106\n",
      "\n",
      "Current Iter : 52/150 batch : 7950/8000 acc : 0.56\n",
      " Current : 52 Acc : 0.5488000029325485 Test Acc : 0.44724999777972696\n",
      "\n",
      "Current Iter : 53/150 batch : 7950/8000 acc : 0.56\n",
      " Current : 53 Acc : 0.5610000005364418 Test Acc : 0.4621249983087182\n",
      "\n",
      "Current Iter : 54/150 batch : 7950/8000 acc : 0.58\n",
      " Current : 54 Acc : 0.5638000032305718 Test Acc : 0.4619999995455146\n",
      "\n",
      "Current Iter : 55/150 batch : 7950/8000 acc : 0.58\n",
      " Current : 55 Acc : 0.5672000017762184 Test Acc : 0.4541249996051192\n",
      "\n",
      "Current Iter : 56/150 batch : 7950/8000 acc : 0.58\n",
      " Current : 56 Acc : 0.5692000025510788 Test Acc : 0.46274999994784594\n",
      "\n",
      "Current Iter : 57/150 batch : 7950/8000 acc : 0.54\n",
      " Current : 57 Acc : 0.5746000015735626 Test Acc : 0.4607499990612268\n",
      "\n",
      "Current Iter : 58/150 batch : 7950/8000 acc : 0.56\n",
      " Current : 58 Acc : 0.5748000022768974 Test Acc : 0.4663750005885959\n",
      "\n",
      "Current Iter : 59/150 batch : 7950/8000 acc : 0.58\n",
      " Current : 59 Acc : 0.5782000029087067 Test Acc : 0.4638749988749623\n",
      "\n",
      "Current Iter : 60/150 batch : 7950/8000 acc : 0.58\n",
      " Current : 60 Acc : 0.5758000034093856 Test Acc : 0.4622499981895089\n",
      "\n",
      "Current Iter : 61/150 batch : 7950/8000 acc : 0.58\n",
      " Current : 61 Acc : 0.5756000000238418 Test Acc : 0.46537499893456696\n",
      "\n",
      "Current Iter : 62/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 62 Acc : 0.5906000044941903 Test Acc : 0.465999998152256\n",
      "\n",
      "Current Iter : 63/150 batch : 7950/8000 acc : 0.54\n",
      " Current : 63 Acc : 0.5922000032663345 Test Acc : 0.4813749983906746\n",
      "\n",
      "Current Iter : 64/150 batch : 7950/8000 acc : 0.54\n",
      " Current : 64 Acc : 0.5932000011205674 Test Acc : 0.4808749988675117\n",
      "\n",
      "Current Iter : 65/150 batch : 7950/8000 acc : 0.54\n",
      " Current : 65 Acc : 0.6020000010728837 Test Acc : 0.47699999921023845\n",
      "\n",
      "Current Iter : 66/150 batch : 7950/8000 acc : 0.58\n",
      " Current : 66 Acc : 0.6016000005602836 Test Acc : 0.4794999983161688\n",
      "\n",
      "Current Iter : 67/150 batch : 7950/8000 acc : 0.54\n",
      " Current : 67 Acc : 0.5954000025987625 Test Acc : 0.47037499975413083\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Iter : 68/150 batch : 7950/8000 acc : 0.58\n",
      " Current : 68 Acc : 0.6014000031352044 Test Acc : 0.4743749985471368\n",
      "\n",
      "Current Iter : 69/150 batch : 7950/8000 acc : 0.56\n",
      " Current : 69 Acc : 0.6022000032663345 Test Acc : 0.4452499979175627\n",
      "\n",
      "Current Iter : 70/150 batch : 7950/8000 acc : 0.56\n",
      " Current : 70 Acc : 0.598800003528595 Test Acc : 0.45949999932199714\n",
      "\n",
      "Current Iter : 71/150 batch : 7950/8000 acc : 0.54\n",
      " Current : 71 Acc : 0.5994000035524368 Test Acc : 0.48125\n",
      "\n",
      "Current Iter : 72/150 batch : 7950/8000 acc : 0.46\n",
      " Current : 72 Acc : 0.5988000044226647 Test Acc : 0.47962499894201754\n",
      "\n",
      "Current Iter : 73/150 batch : 7950/8000 acc : 0.56\n",
      " Current : 73 Acc : 0.5958000034093857 Test Acc : 0.4742499994114041\n",
      "\n",
      "Current Iter : 74/150 batch : 7950/8000 acc : 0.56\n",
      " Current : 74 Acc : 0.6136000049114227 Test Acc : 0.4542499979957938\n",
      "\n",
      "Current Iter : 75/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 75 Acc : 0.6262000054121017 Test Acc : 0.4508749990724027\n",
      "\n",
      "Current Iter : 76/150 batch : 7950/8000 acc : 0.54\n",
      " Current : 76 Acc : 0.6312000042200089 Test Acc : 0.4579999984242022\n",
      "\n",
      "Current Iter : 77/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 77 Acc : 0.6390000027418137 Test Acc : 0.46462499909102917\n",
      "\n",
      "Current Iter : 78/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 78 Acc : 0.6410000061988831 Test Acc : 0.4624999988824129\n",
      "\n",
      "Current Iter : 79/150 batch : 7950/8000 acc : 0.46\n",
      " Current : 79 Acc : 0.640000005364418 Test Acc : 0.4728749990463257\n",
      "\n",
      "Current Iter : 80/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 80 Acc : 0.6190000006556511 Test Acc : 0.46374999787658455\n",
      "\n",
      "Current Iter : 81/150 batch : 7950/8000 acc : 0.46\n",
      " Current : 81 Acc : 0.6302000045776367 Test Acc : 0.45849999859929086\n",
      "\n",
      "Current Iter : 82/150 batch : 7950/8000 acc : 0.46\n",
      " Current : 82 Acc : 0.630800005197525 Test Acc : 0.4629999980330467\n",
      "\n",
      "Current Iter : 83/150 batch : 7950/8000 acc : 0.58\n",
      " Current : 83 Acc : 0.6166000041365624 Test Acc : 0.4539999976754189\n",
      "\n",
      "Current Iter : 84/150 batch : 7950/8000 acc : 0.46\n",
      " Current : 84 Acc : 0.5956000038981437 Test Acc : 0.46537499837577345\n",
      "\n",
      "Current Iter : 85/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 85 Acc : 0.6376000049710274 Test Acc : 0.4699999995529652\n",
      "\n",
      "Current Iter : 86/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 86 Acc : 0.6392000076174736 Test Acc : 0.4716249980032444\n",
      "\n",
      "Current Iter : 87/150 batch : 7950/8000 acc : 0.54\n",
      " Current : 87 Acc : 0.6408000046014786 Test Acc : 0.47412499971687794\n",
      "\n",
      "Current Iter : 88/150 batch : 7950/8000 acc : 0.56\n",
      " Current : 88 Acc : 0.6452000051736831 Test Acc : 0.4808749983087182\n",
      "\n",
      "Current Iter : 89/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 89 Acc : 0.6568000072240829 Test Acc : 0.48237499725073574\n",
      "\n",
      "Current Iter : 90/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 90 Acc : 0.661600006222725 Test Acc : 0.48099999893456696\n",
      "\n",
      "Current Iter : 91/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 91 Acc : 0.6590000027418137 Test Acc : 0.48612500037997963\n",
      "\n",
      "Current Iter : 92/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 92 Acc : 0.6698000013828278 Test Acc : 0.4842499986290932\n",
      "\n",
      "Current Iter : 93/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 93 Acc : 0.6762000030279159 Test Acc : 0.4876250009983778\n",
      "\n",
      "Current Iter : 94/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 94 Acc : 0.6686000037193298 Test Acc : 0.48712500035762785\n",
      "\n",
      "Current Iter : 95/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 95 Acc : 0.6686000084877014 Test Acc : 0.48199999928474424\n",
      "\n",
      "Current Iter : 96/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 96 Acc : 0.6714000087976456 Test Acc : 0.48524999916553496\n",
      "\n",
      "Current Iter : 97/150 batch : 7950/8000 acc : 0.54\n",
      " Current : 97 Acc : 0.6844000047445298 Test Acc : 0.48137499783188104\n",
      "\n",
      "Current Iter : 98/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 98 Acc : 0.6770000070333481 Test Acc : 0.48437499795109035\n",
      "\n",
      "Current Iter : 99/150 batch : 7950/8000 acc : 0.44\n",
      " Current : 99 Acc : 0.6802000057697296 Test Acc : 0.47699999827891587\n",
      "\n",
      "Current Iter : 100/150 batch : 7950/8000 acc : 0.58\n",
      " Current : 100 Acc : 0.6848000049591064 Test Acc : 0.4788749998435378\n",
      "\n",
      "Current Iter : 101/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 101 Acc : 0.668400005698204 Test Acc : 0.4747499980032444\n",
      "\n",
      "Current Iter : 102/150 batch : 7950/8000 acc : 0.46\n",
      " Current : 102 Acc : 0.6662000030279159 Test Acc : 0.4654999993741512\n",
      "\n",
      "Current Iter : 103/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 103 Acc : 0.6840000039339066 Test Acc : 0.4697499992325902\n",
      "\n",
      "Current Iter : 104/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 104 Acc : 0.6948000031709671 Test Acc : 0.4842499990016222\n",
      "\n",
      "Current Iter : 105/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 105 Acc : 0.7012000048160553 Test Acc : 0.4867499992251396\n",
      "\n",
      "Current Iter : 106/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 106 Acc : 0.7056000047922134 Test Acc : 0.4822499997913837\n",
      "\n",
      "Current Iter : 107/150 batch : 7950/8000 acc : 0.56\n",
      " Current : 107 Acc : 0.6928000050783157 Test Acc : 0.4759999977424741\n",
      "\n",
      "Current Iter : 108/150 batch : 7950/8000 acc : 0.46\n",
      " Current : 108 Acc : 0.6888000041246414 Test Acc : 0.4582499984651804\n",
      "\n",
      "Current Iter : 109/150 batch : 7950/8000 acc : 0.64\n",
      " Current : 109 Acc : 0.6937999999523163 Test Acc : 0.475999996997416\n",
      "\n",
      "Current Iter : 110/150 batch : 7950/8000 acc : 0.56\n",
      " Current : 110 Acc : 0.7096000027656555 Test Acc : 0.4827499991282821\n",
      "\n",
      "Current Iter : 111/150 batch : 7950/8000 acc : 0.56\n",
      " Current : 111 Acc : 0.7196000027656555 Test Acc : 0.48287499863654376\n",
      "\n",
      "Current Iter : 112/150 batch : 7950/8000 acc : 0.54\n",
      " Current : 112 Acc : 0.717200003862381 Test Acc : 0.4748749991878867\n",
      "\n",
      "Current Iter : 113/150 batch : 1100/8000 acc : 0.48\r"
     ]
    }
   ],
   "source": [
    "# train\n",
    "sess.run(tf.global_variables_initializer())\n",
    "avg_acc_train = 0; avg_acc_test  = 0; \n",
    "train_acc     = [];test_acc = []\n",
    "for iter in range(num_epoch):\n",
    "\n",
    "    for current_batch_index in range(0,len(train_images),batch_size):\n",
    "        current_data  = train_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = train_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy,gradient_update],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(train_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_train = avg_acc_train + sess_results[0]\n",
    "        \n",
    "    for current_batch_index in range(0,len(test_images), batch_size):\n",
    "        current_data  = test_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = test_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(test_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_test = avg_acc_test + sess_results[0]   \n",
    "        \n",
    "    print(\"\\n Current : \"+ str(iter) + \" Acc : \" + str(avg_acc_train/(len(train_images)/batch_size)) + \" Test Acc : \" + str(avg_acc_test/(len(test_images)/batch_size)) + '\\n')\n",
    "    \n",
    "    # save the training\n",
    "    train_acc.append(avg_acc_train/(len(train_images)/batch_size))\n",
    "    test_acc .append(avg_acc_test/(len(test_images)/batch_size)  )\n",
    "    \n",
    "    avg_acc_train = 0 ; avg_acc_test  = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-20T07:58:09.471Z"
    }
   },
   "outputs": [],
   "source": [
    "print(sess.run([l1n.getw(),l2n.getw(),l3n.getw(),l4n.getw(),l5n.getw()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T07:39:51.379493Z",
     "start_time": "2018-12-20T07:39:51.079050Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T07:43:08.033249Z",
     "start_time": "2018-12-20T07:43:06.719715Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T07:57:07.576480Z",
     "start_time": "2018-12-20T07:51:47.829257Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T07:51:37.093685Z",
     "start_time": "2018-12-20T07:51:37.016902Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T07:57:11.376556Z",
     "start_time": "2018-12-20T07:57:11.370596Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "1. mttk/STL10. (2018). GitHub. Retrieved 19 December 2018, from https://github.com/mttk/STL10\n",
    "2. [duplicate], H. (2018). How to display multiple images in one figure correctly?. Stack Overflow. Retrieved 19 December 2018, from https://stackoverflow.com/questions/46615554/how-to-display-multiple-images-in-one-figure-correctly"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
