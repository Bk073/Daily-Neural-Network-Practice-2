{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T00:09:30.510300Z",
     "start_time": "2018-12-20T00:09:30.504310Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# import Library and some random image data set\n",
    "import tensorflow as tf\n",
    "import numpy      as np\n",
    "import seaborn    as sns \n",
    "import pandas     as pd\n",
    "import os,sys\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(78); tf.set_random_seed(78)\n",
    "\n",
    "# get some of the STL data set\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from skimage import util \n",
    "from skimage.transform import resize\n",
    "from skimage.io import imread\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from numpy import inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T00:09:32.436144Z",
     "start_time": "2018-12-20T00:09:30.532236Z"
    },
    "code_folding": [
     2,
     29,
     37
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 96, 96, 3) 1.0 0.0\n",
      "(5000, 10) 1.0 0.0\n",
      "(8000, 96, 96, 3) 1.0 0.0\n",
      "(8000, 10) 1.0 0.0\n"
     ]
    }
   ],
   "source": [
    "# read all of the data\n",
    "# https://github.com/mttk/STL10\n",
    "def read_all_images(path_to_data):\n",
    "    \"\"\"\n",
    "    :param path_to_data: the file containing the binary images from the STL-10 dataset\n",
    "    :return: an array containing all the images\n",
    "    \"\"\"\n",
    "\n",
    "    with open(path_to_data, 'rb') as f:\n",
    "        # read whole file in uint8 chunks\n",
    "        everything = np.fromfile(f, dtype=np.uint8)\n",
    "\n",
    "        # We force the data into 3x96x96 chunks, since the\n",
    "        # images are stored in \"column-major order\", meaning\n",
    "        # that \"the first 96*96 values are the red channel,\n",
    "        # the next 96*96 are green, and the last are blue.\"\n",
    "        # The -1 is since the size of the pictures depends\n",
    "        # on the input file, and this way numpy determines\n",
    "        # the size on its own.\n",
    "\n",
    "        images = np.reshape(everything, (-1, 3, 96, 96))\n",
    "\n",
    "        # Now transpose the images into a standard image format\n",
    "        # readable by, for example, matplotlib.imshow\n",
    "        # You might want to comment this line or reverse the shuffle\n",
    "        # if you will use a learning algorithm like CNN, since they like\n",
    "        # their channels separated.\n",
    "        images = np.transpose(images, (0, 3, 2, 1))\n",
    "        return images\n",
    "def read_labels(path_to_labels):\n",
    "    \"\"\"\n",
    "    :param path_to_labels: path to the binary file containing labels from the STL-10 dataset\n",
    "    :return: an array containing the labels\n",
    "    \"\"\"\n",
    "    with open(path_to_labels, 'rb') as f:\n",
    "        labels = np.fromfile(f, dtype=np.uint8)\n",
    "        return labels\n",
    "def show_images(data,row=1,col=1):\n",
    "    fig=plt.figure(figsize=(10,10))\n",
    "    columns = col; rows = row\n",
    "    for i in range(1, columns*rows +1):\n",
    "        fig.add_subplot(rows, columns, i)\n",
    "        plt.imshow(data[i-1])\n",
    "    plt.show()\n",
    "\n",
    "train_images = read_all_images(\"../../../DataSet/STL10/stl10_binary/train_X.bin\") / 255.0\n",
    "train_labels = read_labels    (\"../../../DataSet/STL10/stl10_binary/train_Y.bin\")\n",
    "test_images  = read_all_images(\"../../../DataSet/STL10/stl10_binary/test_X.bin\")  / 255.0\n",
    "test_labels  = read_labels    (\"../../../DataSet/STL10/stl10_binary/test_y.bin\")\n",
    "\n",
    "label_encoder= OneHotEncoder(sparse=False,categories='auto')\n",
    "train_labels = label_encoder.fit_transform(train_labels.reshape((-1,1)))\n",
    "test_labels  = label_encoder.fit_transform(test_labels.reshape((-1,1)))\n",
    "\n",
    "print(train_images.shape,train_images.max(),train_images.min())\n",
    "print(train_labels.shape,train_labels.max(),train_labels.min())\n",
    "print(test_images.shape,test_images.max(),test_images.min())\n",
    "print(test_labels.shape,test_labels.max(),test_labels.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T03:54:18.060221Z",
     "start_time": "2018-12-20T03:54:17.987402Z"
    },
    "code_folding": [
     59,
     100,
     141
    ]
   },
   "outputs": [],
   "source": [
    "# create the layers\n",
    "def tf_softmax(x): return tf.nn.softmax(x)\n",
    "\n",
    "def tf_elu(x):   return tf.nn.elu(x)\n",
    "def d_tf_elu(x): return tf.cast(tf.greater(x,0),tf.float32)  + (tf_elu(tf.cast(tf.less_equal(x,0),tf.float32) * x) + 1.0)\n",
    "\n",
    "def tf_relu(x):   return tf.nn.relu(x)\n",
    "def d_tf_relu(x): return tf.cast(tf.greater(x,0),tf.float32)\n",
    "\n",
    "def tf_tanh(x):   return tf.nn.tanh(x)\n",
    "def d_tf_tanh(x): return 1 - tf_tanh(x) ** 2\n",
    "\n",
    "def tf_sigmoid(x):   return tf.nn.sigmoid(x)\n",
    "def d_tf_sigmoid(x): return tf_sigmoid(x) * (1.0-tf_sigmoid(x))\n",
    "\n",
    "class CNN():\n",
    "\n",
    "    def __init__(self,k,inc,out, stddev=0.05,which_reg=0,act=tf_relu,d_act=d_tf_relu):\n",
    "        self.w          = tf.Variable(tf.random_normal([k,k,inc,out],stddev=stddev,seed=2,dtype=tf.float32))\n",
    "        self.m,self.v   = tf.Variable(tf.zeros_like(self.w)),tf.Variable(tf.zeros_like(self.w))\n",
    "        self.act,self.d_act = act,d_act\n",
    "        self.which_reg  = which_reg\n",
    "        \n",
    "    def getw(self): return self.w\n",
    "\n",
    "    def feedforward(self,input,stride=1,padding='SAME'):\n",
    "        self.input  = input\n",
    "        self.layer  = tf.nn.conv2d(input,self.w,strides=[1,stride,stride,1],padding=padding) \n",
    "        self.layerA = self.act(self.layer)\n",
    "        return [self.layer,self.layerA]\n",
    "    \n",
    "    def backprop(self,gradient,stride=1,padding='SAME'):\n",
    "        grad_part_1 = gradient\n",
    "        grad_part_2 = self.d_act(self.layer)\n",
    "        grad_part_3 = self.input\n",
    "\n",
    "        grad_middle = grad_part_1 * grad_part_2\n",
    "        grad        = tf.nn.conv2d_backprop_filter(input = grad_part_3,filter_sizes = tf.shape(self.w),  out_backprop = grad_middle,strides=[1,stride,stride,1],padding=padding) / batch_size\n",
    "        grad_pass   = tf.nn.conv2d_backprop_input (input_sizes = tf.shape(self.input),filter= self.w,out_backprop = grad_middle,strides=[1,stride,stride,1],padding=padding)\n",
    "\n",
    "        if self.which_reg == 0:   grad = grad\n",
    "        if self.which_reg == 0.5: grad = grad + lamda * (tf.sqrt(tf.abs(self.w))) * (1.0/tf.sqrt(tf.abs(self.w)+ 10e-5)) * tf.sign(self.w)\n",
    "        if self.which_reg == 1:   grad = grad + lamda * tf.sign(self.w)\n",
    "        if self.which_reg == 1.5: grad = grad + lamda * 1.0/(tf.sqrt(tf.square(self.w) + 10e-5)) * self.w\n",
    "        if self.which_reg == 2:   grad = grad + lamda * (1.0/tf.sqrt(tf.square(tf.abs(self.w))+ 10e-5)) * tf.abs(self.w) * tf.sign(self.w)\n",
    "        if self.which_reg == 2.5: grad = grad + lamda * 2.0 * self.w\n",
    "        if self.which_reg == 3:   grad = grad + lamda * tf.pow(tf.pow(tf.abs(self.w),3)+ 10e-5,-0.66) * tf.pow(tf.abs(self.w),2) * tf.sign(self.w)\n",
    "        if self.which_reg == 4:   grad = grad + lamda * tf.pow(tf.pow(tf.abs(self.w),4)+ 10e-5,-0.75) * tf.pow(tf.abs(self.w),3) * tf.sign(self.w)\n",
    "\n",
    "        update_w = []\n",
    "        \n",
    "        update_w.append(tf.assign( self.m,self.m*beta1 + (1-beta1) * (grad)   ))\n",
    "        update_w.append(tf.assign( self.v,self.v*beta2 + (1-beta2) * (grad ** 2)   ))\n",
    "        m_hat = self.m / (1-beta1) ; v_hat = self.v / (1-beta2)\n",
    "        adam_middle = m_hat * learning_rate/(tf.sqrt(v_hat) + adam_e)\n",
    "        update_w.append(tf.assign(self.w,tf.subtract(self.w,adam_middle  )))\n",
    "        \n",
    "        return grad,grad_pass,update_w\n",
    "    \n",
    "class tf_batch_norm_layer():\n",
    "    \n",
    "    def __init__(self,vector_shape,axis):\n",
    "        self.moving_mean = tf.Variable(tf.zeros(shape=[1,1,1,vector_shape],dtype=tf.float32))\n",
    "        self.moving_vari = tf.Variable(tf.zeros(shape=[1,1,1,vector_shape],dtype=tf.float32))\n",
    "        self.axis        = axis\n",
    "        \n",
    "    def feedforward(self,input,training_phase=True,eps = 1e-8):\n",
    "        self.input = input\n",
    "        self.input_size          = self.input.shape\n",
    "        self.batch,self.h,self.w,self.c = self.input_size[0].value,self.input_size[1].value,self.input_size[2].value,self.input_size[3].value\n",
    "\n",
    "        # Training Moving Average Mean         \n",
    "        def training_fn():\n",
    "            self.mean    = tf.reduce_mean(self.input,axis=self.axis ,keepdims=True)\n",
    "            self.var     = tf.reduce_mean(tf.square(self.input-self.mean),axis=self.axis,keepdims=True)\n",
    "            centered_data= (self.input - self.mean)/tf.sqrt(self.var + eps)\n",
    "            \n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean*0.9 + 0.1 * self.mean ))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari*0.9 + 0.1 * self.var  ))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        # Testing Moving Average Mean        \n",
    "        def  testing_fn():\n",
    "            centered_data   = (self.input - self.moving_mean)/tf.sqrt(self.moving_vari + eps)\n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        self.output,update_variable = tf.cond(training_phase,true_fn=training_fn,false_fn=testing_fn)\n",
    "        return self.output,update_variable\n",
    "    \n",
    "    def backprop(self,grad,eps = 1e-8):\n",
    "        change_parts = 1.0 /(self.batch * self.h * self.w)\n",
    "        grad_sigma   = tf.reduce_sum( grad *  (self.input-self.mean)     ,axis=self.axis,keepdims=True) * -0.5 * (self.var+eps) ** -1.5\n",
    "        grad_mean    = tf.reduce_sum( grad *  (-1./tf.sqrt(self.var+eps)),axis=self.axis,keepdims=True) + grad_sigma * change_parts * 2.0 * tf.reduce_sum((self.input-self.mean),axis=self.axis,keepdims=True) * -1\n",
    "        grad_x       = grad * 1/(tf.sqrt(self.var+eps)) + grad_sigma * change_parts * 2.0 * (self.input-self.mean) + grad_mean * change_parts\n",
    "        return grad_x\n",
    "\n",
    "class tf_layer_norm_layer():\n",
    "    \n",
    "    def __init__(self,vector_shape,axis):\n",
    "        self.moving_mean = tf.Variable(tf.zeros(shape=[vector_shape,1,1,1],dtype=tf.float32))\n",
    "        self.moving_vari = tf.Variable(tf.zeros(shape=[vector_shape,1,1,1],dtype=tf.float32))\n",
    "        self.axis        = axis\n",
    "        \n",
    "    def feedforward(self,input,training_phase=True,eps = 1e-8):\n",
    "        self.input = input\n",
    "        self.input_size          = self.input.shape\n",
    "        self.batch,self.h,self.w,self.c = self.input_size[0].value,self.input_size[1].value,self.input_size[2].value,self.input_size[3].value\n",
    "\n",
    "        # Training Moving Average Mean         \n",
    "        def training_fn():\n",
    "            self.mean    = tf.reduce_mean(self.input,axis=self.axis ,keepdims=True)\n",
    "            self.var     = tf.reduce_mean(tf.square(self.input-self.mean),axis=self.axis,keepdims=True)\n",
    "            centered_data= (self.input - self.mean)/tf.sqrt(self.var + eps)\n",
    "            \n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean*0.9 + 0.1 * self.mean ))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari*0.9 + 0.1 * self.var  ))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        # Testing Moving Average Mean        \n",
    "        def  testing_fn():\n",
    "            centered_data   = (self.input - self.moving_mean)/tf.sqrt(self.moving_vari + eps)\n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        self.output,update_variable = tf.cond(training_phase,true_fn=training_fn,false_fn=testing_fn)\n",
    "        return self.output,update_variable\n",
    "    \n",
    "    def backprop(self,grad,eps = 1e-8):\n",
    "        change_parts = 1.0 /(self.h * self.w * self.c)\n",
    "        grad_sigma   = tf.reduce_sum( grad *  (self.input-self.mean)     ,axis=self.axis,keepdims=True) * -0.5 * (self.var+eps) ** -1.5\n",
    "        grad_mean    = tf.reduce_sum( grad *  (-1./tf.sqrt(self.var+eps)),axis=self.axis,keepdims=True) + grad_sigma * change_parts * 2.0 * tf.reduce_sum((self.input-self.mean),axis=self.axis,keepdims=True) * -1\n",
    "        grad_x       = grad * 1/(tf.sqrt(self.var+eps)) + grad_sigma * change_parts * 2.0 * (self.input-self.mean) + grad_mean * change_parts\n",
    "        return grad_x\n",
    "    \n",
    "class tf_instance_norm_layer():\n",
    "    \n",
    "    def __init__(self,batch_size,vector_shape,axis):\n",
    "        self.moving_mean = tf.Variable(tf.zeros(shape=[batch_size,1,1,vector_shape],dtype=tf.float32))\n",
    "        self.moving_vari = tf.Variable(tf.zeros(shape=[batch_size,1,1,vector_shape],dtype=tf.float32))\n",
    "        self.axis        = axis\n",
    "        \n",
    "    def feedforward(self,input,training_phase=True,eps = 1e-8):\n",
    "        self.input = input\n",
    "        self.input_size          = self.input.shape\n",
    "        self.batch,self.h,self.w,self.c = self.input_size[0].value,self.input_size[1].value,self.input_size[2].value,self.input_size[3].value\n",
    "\n",
    "        # Training Moving Average Mean         \n",
    "        def training_fn():\n",
    "            self.mean    = tf.reduce_mean(self.input,axis=self.axis ,keepdims=True)\n",
    "            self.var     = tf.reduce_mean(tf.square(self.input-self.mean),axis=self.axis,keepdims=True)\n",
    "            centered_data= (self.input - self.mean)/tf.sqrt(self.var + eps)\n",
    "            \n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean*0.9 + 0.1 * self.mean ))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari*0.9 + 0.1 * self.var  ))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        # Testing Moving Average Mean        \n",
    "        def  testing_fn():\n",
    "            centered_data   = (self.input - self.moving_mean)/tf.sqrt(self.moving_vari + eps)\n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        self.output,update_variable = tf.cond(training_phase,true_fn=training_fn,false_fn=testing_fn)\n",
    "        return self.output,update_variable\n",
    "    \n",
    "    def backprop(self,grad,eps = 1e-8):\n",
    "        change_parts = 1.0 /(self.h * self.w)\n",
    "        grad_sigma   = tf.reduce_sum( grad *  (self.input-self.mean)     ,axis=self.axis,keepdims=True) * -0.5 * (self.var+eps) ** -1.5\n",
    "        grad_mean    = tf.reduce_sum( grad *  (-1./tf.sqrt(self.var+eps)),axis=self.axis,keepdims=True) + grad_sigma * change_parts * 2.0 * tf.reduce_sum((self.input-self.mean),axis=self.axis,keepdims=True) * -1\n",
    "        grad_x       = grad * 1/(tf.sqrt(self.var+eps)) + grad_sigma * change_parts * 2.0 * (self.input-self.mean) + grad_mean * change_parts\n",
    "        return grad_x\n",
    "  \n",
    "class tf_box_cox():\n",
    "    \n",
    "    def __init__(self,channel):\n",
    "        self.lmbda    = tf.Variable(tf.ones([1,1,1,channel],tf.float32)* 2.0) \n",
    "        self.m,self.v = tf.Variable(tf.zeros_like(self.lmbda)),tf.Variable(tf.zeros_like(self.lmbda))\n",
    "    \n",
    "    def feedforward(self,data):\n",
    "        self.input = data\n",
    "        self.layer = (tf.pow((self.input + 1.0),self.lmbda) - 1.0)/self.lmbda\n",
    "        return self.layer \n",
    "    \n",
    "    def backprop(self,grad):\n",
    "        grad_pass = tf.pow((self.input + 1),self.lmbda-1.0)\n",
    "        \n",
    "        # Grad respect to the lmbda value (not tested!)\n",
    "        grad_lmbda = tf.pow((self.input+1),self.lmbda) * (tf.log(self.input+1)*self.lmbda -1) + 1\n",
    "        grad_lmbda = grad_lmbda / (self.lmbda ** 2)\n",
    "        grad_lmbda = tf.reduce_mean(grad_lmbda,(0,1,2),True)\n",
    "\n",
    "        update_w = []\n",
    "        update_w.append(tf.assign( self.m,self.m*beta1 + (1-beta1) * (grad_lmbda)   ))\n",
    "        update_w.append(tf.assign( self.v,self.v*beta2 + (1-beta2) * (grad_lmbda ** 2)   ))\n",
    "        m_hat = self.m / (1-beta1) ; v_hat = self.v / (1-beta2)\n",
    "        adam_middle = m_hat * learning_rate/(tf.sqrt(v_hat) + adam_e)\n",
    "        update_w.append(tf.assign(self.lmbda,tf.subtract(self.lmbda,adam_middle  )))\n",
    "        return grad_pass,update_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T03:54:18.646681Z",
     "start_time": "2018-12-20T03:54:18.575868Z"
    }
   },
   "outputs": [],
   "source": [
    "# hyper parameter\n",
    "num_epoch = 300; learning_rate = 0.0008; batch_size = 20\n",
    "beta1,beta2,adam_e = 0.9,0.999,1e-8\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T03:54:19.274836Z",
     "start_time": "2018-12-20T03:54:19.076320Z"
    }
   },
   "outputs": [],
   "source": [
    "# create layers\n",
    "l1 = CNN(3,3, 16); l1n = tf_box_cox(16)\n",
    "l2 = CNN(3,16,16); l2n = tf_box_cox(16)\n",
    "l3 = CNN(3,16,16); l3n = tf_box_cox(16)\n",
    "\n",
    "l4 = CNN(3,16,32); l4n = tf_box_cox(32)\n",
    "l5 = CNN(3,32,32); l5n = tf_box_cox(32)\n",
    "l6 = CNN(3,32,10); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T03:54:21.111733Z",
     "start_time": "2018-12-20T03:54:19.625465Z"
    }
   },
   "outputs": [],
   "source": [
    "# create the graph \n",
    "x = tf.placeholder(tf.float32,[batch_size,96,96,3])\n",
    "y = tf.placeholder(tf.float32,[batch_size,10])\n",
    "\n",
    "layer1,layer1a = l1.feedforward(x,stride=2)      ;          layer1n = l1n.feedforward(layer1a)\n",
    "layer2,layer2a = l2.feedforward(layer1n,stride=2);          layer2n = l2n.feedforward(layer2a)\n",
    "layer3,layer3a = l3.feedforward(layer2n,stride=2); layer3n = l3n.feedforward(layer3a)\n",
    "\n",
    "layer4,layer4a = l4.feedforward(layer3n,stride=2);          layer4n = l4n.feedforward(layer4a)\n",
    "layer5,layer5a = l5.feedforward(layer4n,stride=1);          layer5n = l5n.feedforward(layer5a)\n",
    "layer6,layer6a = l6.feedforward(layer5n,stride=1); \n",
    "\n",
    "final_layer   = tf.reduce_mean(layer6a,(1,2))\n",
    "final_softmax = tf_softmax(final_layer)\n",
    "cost = -tf.reduce_mean(y * tf.log(final_softmax + 1e-8))\n",
    "auto_train = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "correct_prediction = tf.equal(tf.argmax(final_softmax, 1), tf.argmax(y, 1))\n",
    "accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "grad6w,grad6p,grad6_up = l6.backprop((final_softmax-y)[:,None,None,:],stride=1)\n",
    "grad5n,grad5n_up = l5n.backprop(grad6p); grad5w,grad5p,grad5_up = l5.backprop(grad5n)\n",
    "grad4n,grad4n_up = l4n.backprop(grad5p); grad4w,grad4p,grad4_up = l4.backprop(grad4n,stride=2)\n",
    "\n",
    "grad3n,grad3n_up = l3n.backprop(grad4p); grad3w,grad3p,grad3_up = l3.backprop(grad3n,stride=2)\n",
    "grad2n,grad2n_up = l2n.backprop(grad3p); grad2w,grad2p,grad2_up = l2.backprop(grad2n,stride=2)\n",
    "grad1n,grad1n_up = l1n.backprop(grad2p); grad1w,grad1p,grad1_up = l1.backprop(grad1n,stride=2)\n",
    "\n",
    "gradient_update = grad6_up + \\\n",
    "                  grad5n_up + grad5_up + \\\n",
    "                  grad4n_up + grad4_up + \\\n",
    "                  grad3n_up + grad3_up + \\\n",
    "                  grad2n_up + grad2_up + \\\n",
    "                  grad1n_up + grad1_up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-20T03:54:20.570Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Iter : 0/300 batch : 7980/8000 acc : 0.15\n",
      " Current : 0 Acc : 0.13180000242590903 Test Acc : 0.16400000316090882\n",
      "\n",
      "Current Iter : 1/300 batch : 7980/8000 acc : 0.25\n",
      " Current : 1 Acc : 0.23960000255703925 Test Acc : 0.2818750022165477\n",
      "\n",
      "Current Iter : 2/300 batch : 7980/8000 acc : 0.35\n",
      " Current : 2 Acc : 0.3150000025779009 Test Acc : 0.326625002482906\n",
      "\n",
      "Current Iter : 3/300 batch : 7980/8000 acc : 0.35\n",
      " Current : 3 Acc : 0.3498000023365021 Test Acc : 0.3735000015422702\n",
      "\n",
      "Current Iter : 4/300 batch : 7980/8000 acc : 0.45\n",
      " Current : 4 Acc : 0.37400000101327896 Test Acc : 0.387750001642853\n",
      "\n",
      "Current Iter : 5/300 batch : 7980/8000 acc : 0.35\n",
      " Current : 5 Acc : 0.39000000175833704 Test Acc : 0.39950000090524557\n",
      "\n",
      "Current Iter : 6/300 batch : 7980/8000 acc : 0.35\n",
      " Current : 6 Acc : 0.4092000009268522 Test Acc : 0.4062500010430813\n",
      "\n",
      "Current Iter : 7/300 batch : 7980/8000 acc : 0.45\n",
      " Current : 7 Acc : 0.4138000015318394 Test Acc : 0.4142500007338822\n",
      "\n",
      "Current Iter : 8/300 batch : 7980/8000 acc : 0.45\n",
      " Current : 8 Acc : 0.4288000008761883 Test Acc : 0.4213750005699694\n",
      "\n",
      "Current Iter : 9/300 batch : 7980/8000 acc : 0.45\n",
      " Current : 9 Acc : 0.44020000046491625 Test Acc : 0.42862500121816993\n",
      "\n",
      "Current Iter : 10/300 batch : 7980/8000 acc : 0.45\n",
      " Current : 10 Acc : 0.45340000158548355 Test Acc : 0.431750001180917\n",
      "\n",
      "Current Iter : 11/300 batch : 7980/8000 acc : 0.45\n",
      " Current : 11 Acc : 0.4660000021457672 Test Acc : 0.4338750005699694\n",
      "\n",
      "Current Iter : 12/300 batch : 7980/8000 acc : 0.55\n",
      " Current : 12 Acc : 0.47620000219345093 Test Acc : 0.4447500008158386\n",
      "\n",
      "Current Iter : 13/300 batch : 7980/8000 acc : 0.55\n",
      " Current : 13 Acc : 0.4856000008583069 Test Acc : 0.4507500009611249\n",
      "\n",
      "Current Iter : 14/300 batch : 7980/8000 acc : 0.55\n",
      " Current : 14 Acc : 0.4944000019431114 Test Acc : 0.4543750026449561\n",
      "\n",
      "Current Iter : 15/300 batch : 7980/8000 acc : 0.55\n",
      " Current : 15 Acc : 0.505800000846386 Test Acc : 0.4621250015124679\n",
      "\n",
      "Current Iter : 16/300 batch : 7980/8000 acc : 0.55\n",
      " Current : 16 Acc : 0.5182000021338463 Test Acc : 0.4647500011138618\n",
      "\n",
      "Current Iter : 17/300 batch : 7980/8000 acc : 0.55\n",
      " Current : 17 Acc : 0.5294000007510186 Test Acc : 0.4642500014975667\n",
      "\n",
      "Current Iter : 18/300 batch : 7980/8000 acc : 0.55\n",
      " Current : 18 Acc : 0.5416000008583068 Test Acc : 0.4661250015348196\n",
      "\n",
      "Current Iter : 19/300 batch : 7980/8000 acc : 0.55\n",
      " Current : 19 Acc : 0.554400001078844 Test Acc : 0.475375001616776\n",
      "\n",
      "Current Iter : 20/300 batch : 7980/8000 acc : 0.45\n",
      " Current : 20 Acc : 0.560400000333786 Test Acc : 0.4718750020116568\n",
      "\n",
      "Current Iter : 21/300 batch : 7980/8000 acc : 0.65\n",
      " Current : 21 Acc : 0.5756000018417835 Test Acc : 0.4868750023469329\n",
      "\n",
      "Current Iter : 22/300 batch : 7980/8000 acc : 0.55\n",
      " Current : 22 Acc : 0.5896000001430511 Test Acc : 0.49600000116974113\n",
      "\n",
      "Current Iter : 23/300 batch : 7980/8000 acc : 0.55\n",
      " Current : 23 Acc : 0.5972000000476837 Test Acc : 0.4983750017732382\n",
      "\n",
      "Current Iter : 24/300 batch : 7980/8000 acc : 0.55\n",
      " Current : 24 Acc : 0.6066000005602836 Test Acc : 0.49662500239908697\n",
      "\n",
      "Current Iter : 25/300 batch : 7980/8000 acc : 0.65\n",
      " Current : 25 Acc : 0.6068000000715256 Test Acc : 0.492125001475215\n",
      "\n",
      "Current Iter : 26/300 batch : 7980/8000 acc : 0.55\n",
      " Current : 26 Acc : 0.6020000008940697 Test Acc : 0.498250002078712\n",
      "\n",
      "Current Iter : 27/300 batch : 7980/8000 acc : 0.65\n",
      " Current : 27 Acc : 0.6194000002741814 Test Acc : 0.4990000008419156\n",
      "\n",
      "Current Iter : 28/300 batch : 7980/8000 acc : 0.65\n",
      " Current : 28 Acc : 0.6382000008821488 Test Acc : 0.49712500162422657\n",
      "\n",
      "Current Iter : 29/300 batch : 7980/8000 acc : 0.65\n",
      " Current : 29 Acc : 0.6406000007390976 Test Acc : 0.49487500201910734\n",
      "\n",
      "Current Iter : 30/300 batch : 7980/8000 acc : 0.55\n",
      " Current : 30 Acc : 0.640400001168251 Test Acc : 0.4975000006705523\n",
      "\n",
      "Current Iter : 31/300 batch : 7980/8000 acc : 0.55\n",
      " Current : 31 Acc : 0.6541999992132187 Test Acc : 0.5023750011995435\n",
      "\n",
      "Current Iter : 32/300 batch : 7980/8000 acc : 0.55\n",
      " Current : 32 Acc : 0.6646000018119812 Test Acc : 0.49937500078231095\n",
      "\n",
      "Current Iter : 33/300 batch : 7980/8000 acc : 0.55\n",
      " Current : 33 Acc : 0.6714000018835068 Test Acc : 0.49112500056624414\n",
      "\n",
      "Current Iter : 34/300 batch : 7980/8000 acc : 0.55\n",
      " Current : 34 Acc : 0.6754000004529953 Test Acc : 0.49187500193715095\n",
      "\n",
      "Current Iter : 35/300 batch : 7980/8000 acc : 0.45\n",
      " Current : 35 Acc : 0.663 Test Acc : 0.5001250002533197\n",
      "\n",
      "Current Iter : 36/300 batch : 7980/8000 acc : 0.55\n",
      " Current : 36 Acc : 0.6896000009775162 Test Acc : 0.500375001616776\n",
      "\n",
      "Current Iter : 37/300 batch : 7980/8000 acc : 0.55\n",
      " Current : 37 Acc : 0.701400000333786 Test Acc : 0.49575000151991844\n",
      "\n",
      "Current Iter : 38/300 batch : 7980/8000 acc : 0.65\n",
      " Current : 38 Acc : 0.7026000009775162 Test Acc : 0.4915000008419156\n",
      "\n",
      "Current Iter : 39/300 batch : 7980/8000 acc : 0.65\n",
      " Current : 39 Acc : 0.7154000010490418 Test Acc : 0.48775000099092725\n",
      "\n",
      "Current Iter : 40/300 batch : 7980/8000 acc : 0.55\n",
      " Current : 40 Acc : 0.7233999986648559 Test Acc : 0.49025000117719175\n",
      "\n",
      "Current Iter : 41/300 batch : 7980/8000 acc : 0.55\n",
      " Current : 41 Acc : 0.7125999993085861 Test Acc : 0.48337500136345624\n",
      "\n",
      "Current Iter : 42/300 batch : 7980/8000 acc : 0.55\n",
      " Current : 42 Acc : 0.7065999995470047 Test Acc : 0.46462500166147946\n",
      "\n",
      "Current Iter : 43/300 batch : 7980/8000 acc : 0.55\n",
      " Current : 43 Acc : 0.7163999989032745 Test Acc : 0.4930000002682209\n",
      "\n",
      "Current Iter : 44/300 batch : 7980/8000 acc : 0.55\n",
      " Current : 44 Acc : 0.7421999982595444 Test Acc : 0.473750002104789\n",
      "\n",
      "Current Iter : 45/300 batch : 7980/8000 acc : 0.55\n",
      " Current : 45 Acc : 0.7278000013828277 Test Acc : 0.49200000025331975\n",
      "\n",
      "Current Iter : 46/300 batch : 7980/8000 acc : 0.45\n",
      " Current : 46 Acc : 0.7336000006198883 Test Acc : 0.478250000923872\n",
      "\n",
      "Current Iter : 47/300 batch : 7980/8000 acc : 0.45\n",
      " Current : 47 Acc : 0.7464000002145768 Test Acc : 0.48200000047683716\n",
      "\n",
      "Current Iter : 48/300 batch : 7980/8000 acc : 0.55\n",
      " Current : 48 Acc : 0.7448000011444091 Test Acc : 0.4840000013634562\n",
      "\n",
      "Current Iter : 49/300 batch : 7980/8000 acc : 0.55\n",
      " Current : 49 Acc : 0.737400000333786 Test Acc : 0.4910000015422702\n",
      "\n",
      "Current Iter : 50/300 batch : 7980/8000 acc : 0.65\n",
      " Current : 50 Acc : 0.7532000002861023 Test Acc : 0.4861250017769635\n",
      "\n",
      "Current Iter : 51/300 batch : 7980/8000 acc : 0.55\n",
      " Current : 51 Acc : 0.7689999995231629 Test Acc : 0.47075000159442426\n",
      "\n",
      "Current Iter : 52/300 batch : 7980/8000 acc : 0.55\n",
      " Current : 52 Acc : 0.7492000002861023 Test Acc : 0.48325000165030363\n",
      "\n",
      "Current Iter : 53/300 batch : 7980/8000 acc : 0.55\n",
      " Current : 53 Acc : 0.7540000007152557 Test Acc : 0.48687500137835743\n",
      "\n",
      "Current Iter : 54/300 batch : 7980/8000 acc : 0.55\n",
      " Current : 54 Acc : 0.7864000005722046 Test Acc : 0.48262500068172814\n",
      "\n",
      "Current Iter : 55/300 batch : 7980/8000 acc : 0.55\n",
      " Current : 55 Acc : 0.7998000004291534 Test Acc : 0.47175000173971054\n",
      "\n",
      "Current Iter : 56/300 batch : 7980/8000 acc : 0.55\n",
      " Current : 56 Acc : 0.7722000000476837 Test Acc : 0.47737500205636024\n",
      "\n",
      "Current Iter : 57/300 batch : 7980/8000 acc : 0.65\n",
      " Current : 57 Acc : 0.7878000004291534 Test Acc : 0.4788750010542572\n",
      "\n",
      "Current Iter : 58/300 batch : 7980/8000 acc : 0.65\n",
      " Current : 58 Acc : 0.772 Test Acc : 0.47525000214576724\n",
      "\n",
      "Current Iter : 59/300 batch : 7980/8000 acc : 0.55\n",
      " Current : 59 Acc : 0.7951999990940094 Test Acc : 0.47525000166147946\n",
      "\n",
      "Current Iter : 60/300 batch : 7980/8000 acc : 0.55\n",
      " Current : 60 Acc : 0.8143999996185303 Test Acc : 0.4683750015497208\n",
      "\n",
      "Current Iter : 61/300 batch : 7980/8000 acc : 0.55\n",
      " Current : 61 Acc : 0.8212000005245209 Test Acc : 0.4695000003278256\n",
      "\n",
      "Current Iter : 62/300 batch : 7980/8000 acc : 0.65\n",
      " Current : 62 Acc : 0.8141999996900559 Test Acc : 0.4763750011473894\n",
      "\n",
      "Current Iter : 63/300 batch : 7980/8000 acc : 0.55\n",
      " Current : 63 Acc : 0.8291999995708466 Test Acc : 0.479000001065433\n",
      "\n",
      "Current Iter : 64/300 batch : 7980/8000 acc : 0.55\n",
      " Current : 64 Acc : 0.8324000008106232 Test Acc : 0.47175000119954347\n",
      "\n",
      "Current Iter : 65/300 batch : 7980/8000 acc : 0.55\n",
      " Current : 65 Acc : 0.8365999984741211 Test Acc : 0.47362500090152027\n",
      "\n",
      "Current Iter : 66/300 batch : 7980/8000 acc : 0.55\n",
      " Current : 66 Acc : 0.8397999987602234 Test Acc : 0.478250000551343\n",
      "\n",
      "Current Iter : 67/300 batch : 7980/8000 acc : 0.65\n",
      " Current : 67 Acc : 0.8187999994754791 Test Acc : 0.47025000128895045\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Iter : 68/300 batch : 7980/8000 acc : 0.55\n",
      " Current : 68 Acc : 0.8599999976158142 Test Acc : 0.4711250007525086\n",
      "\n",
      "Current Iter : 69/300 batch : 7980/8000 acc : 0.65\n",
      " Current : 69 Acc : 0.832 Test Acc : 0.4516250018402934\n",
      "\n",
      "Current Iter : 70/300 batch : 7980/8000 acc : 0.55\n",
      " Current : 70 Acc : 0.8591999983787537 Test Acc : 0.4663750008866191\n",
      "\n",
      "Current Iter : 71/300 batch : 7980/8000 acc : 0.55\n",
      " Current : 71 Acc : 0.8651999990940094 Test Acc : 0.4621250024810433\n",
      "\n",
      "Current Iter : 72/300 batch : 7980/8000 acc : 0.55\n",
      " Current : 72 Acc : 0.8559999990463257 Test Acc : 0.4672500018030405\n",
      "\n",
      "Current Iter : 73/300 batch : 7980/8000 acc : 0.55\n",
      " Current : 73 Acc : 0.86399999833107 Test Acc : 0.46137500155717137\n",
      "\n",
      "Current Iter : 74/300 batch : 7980/8000 acc : 0.55\n",
      " Current : 74 Acc : 0.8712000000476837 Test Acc : 0.4715000016614795\n",
      "\n",
      "Current Iter : 75/300 batch : 7980/8000 acc : 0.55\n",
      " Current : 75 Acc : 0.8961999945640564 Test Acc : 0.4717500016279519\n",
      "\n",
      "Current Iter : 76/300 batch : 7980/8000 acc : 0.55\n",
      " Current : 76 Acc : 0.8883999967575074 Test Acc : 0.47275000117719174\n",
      "\n",
      "Current Iter : 77/300 batch : 7980/8000 acc : 0.55\n",
      " Current : 77 Acc : 0.8423999981880188 Test Acc : 0.4697500015422702\n",
      "\n",
      "Current Iter : 78/300 batch : 3720/8000 acc : 0.65\r"
     ]
    }
   ],
   "source": [
    "# train\n",
    "sess.run(tf.global_variables_initializer())\n",
    "avg_acc_train = 0; avg_acc_test  = 0; \n",
    "train_acc     = [];test_acc = []\n",
    "for iter in range(num_epoch):\n",
    "\n",
    "    for current_batch_index in range(0,len(train_images),batch_size):\n",
    "        current_data  = train_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = train_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy,auto_train],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(train_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_train = avg_acc_train + sess_results[0]\n",
    "        \n",
    "    for current_batch_index in range(0,len(test_images), batch_size):\n",
    "        current_data  = test_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = test_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(test_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_test = avg_acc_test + sess_results[0]   \n",
    "        \n",
    "    print(\"\\n Current : \"+ str(iter) + \" Acc : \" + str(avg_acc_train/(len(train_images)/batch_size)) + \" Test Acc : \" + str(avg_acc_test/(len(test_images)/batch_size)) + '\\n')\n",
    "    \n",
    "    # save the training\n",
    "    train_acc.append(avg_acc_train/(len(train_images)/batch_size))\n",
    "    test_acc .append(avg_acc_test/(len(test_images)/batch_size)  )\n",
    "    \n",
    "    \n",
    "    avg_acc_train = 0 ; avg_acc_test  = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "1. mttk/STL10. (2018). GitHub. Retrieved 19 December 2018, from https://github.com/mttk/STL10\n",
    "2. [duplicate], H. (2018). How to display multiple images in one figure correctly?. Stack Overflow. Retrieved 19 December 2018, from https://stackoverflow.com/questions/46615554/how-to-display-multiple-images-in-one-figure-correctly"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
