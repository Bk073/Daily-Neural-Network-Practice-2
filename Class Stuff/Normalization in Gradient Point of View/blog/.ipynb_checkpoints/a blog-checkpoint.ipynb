{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T21:01:19.410253Z",
     "start_time": "2018-12-20T21:01:19.405232Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# import Library and some random image data set\n",
    "import tensorflow as tf\n",
    "import numpy      as np\n",
    "import seaborn    as sns \n",
    "import pandas     as pd\n",
    "import os,sys\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(78); tf.set_random_seed(78)\n",
    "\n",
    "# get some of the STL data set\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from skimage import util \n",
    "from skimage.transform import resize\n",
    "from skimage.io import imread\n",
    "import warnings\n",
    "from numpy import inf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 35})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T21:00:18.567808Z",
     "start_time": "2018-12-20T21:00:16.491815Z"
    },
    "code_folding": [
     0,
     2,
     29,
     37
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 96, 96, 3) 1.0 0.0\n",
      "(5000, 10) 1.0 0.0\n",
      "(8000, 96, 96, 3) 1.0 0.0\n",
      "(8000, 10) 1.0 0.0\n"
     ]
    }
   ],
   "source": [
    "# read all of the data\n",
    "# https://github.com/mttk/STL10\n",
    "def read_all_images(path_to_data):\n",
    "    \"\"\"\n",
    "    :param path_to_data: the file containing the binary images from the STL-10 dataset\n",
    "    :return: an array containing all the images\n",
    "    \"\"\"\n",
    "\n",
    "    with open(path_to_data, 'rb') as f:\n",
    "        # read whole file in uint8 chunks\n",
    "        everything = np.fromfile(f, dtype=np.uint8)\n",
    "\n",
    "        # We force the data into 3x96x96 chunks, since the\n",
    "        # images are stored in \"column-major order\", meaning\n",
    "        # that \"the first 96*96 values are the red channel,\n",
    "        # the next 96*96 are green, and the last are blue.\"\n",
    "        # The -1 is since the size of the pictures depends\n",
    "        # on the input file, and this way numpy determines\n",
    "        # the size on its own.\n",
    "\n",
    "        images = np.reshape(everything, (-1, 3, 96, 96))\n",
    "\n",
    "        # Now transpose the images into a standard image format\n",
    "        # readable by, for example, matplotlib.imshow\n",
    "        # You might want to comment this line or reverse the shuffle\n",
    "        # if you will use a learning algorithm like CNN, since they like\n",
    "        # their channels separated.\n",
    "        images = np.transpose(images, (0, 3, 2, 1))\n",
    "        return images\n",
    "def read_labels(path_to_labels):\n",
    "    \"\"\"\n",
    "    :param path_to_labels: path to the binary file containing labels from the STL-10 dataset\n",
    "    :return: an array containing the labels\n",
    "    \"\"\"\n",
    "    with open(path_to_labels, 'rb') as f:\n",
    "        labels = np.fromfile(f, dtype=np.uint8)\n",
    "        return labels\n",
    "def show_images(data,row=1,col=1):\n",
    "    fig=plt.figure(figsize=(10,10))\n",
    "    columns = col; rows = row\n",
    "    for i in range(1, columns*rows +1):\n",
    "        fig.add_subplot(rows, columns, i)\n",
    "        plt.imshow(data[i-1])\n",
    "    plt.show()\n",
    "\n",
    "train_images = read_all_images(\"../../../DataSet/STL10/stl10_binary/train_X.bin\") / 255.0\n",
    "train_labels = read_labels    (\"../../../DataSet/STL10/stl10_binary/train_Y.bin\")\n",
    "test_images  = read_all_images(\"../../../DataSet/STL10/stl10_binary/test_X.bin\")  / 255.0\n",
    "test_labels  = read_labels    (\"../../../DataSet/STL10/stl10_binary/test_y.bin\")\n",
    "\n",
    "label_encoder= OneHotEncoder(sparse=False,categories='auto')\n",
    "train_labels = label_encoder.fit_transform(train_labels.reshape((-1,1)))\n",
    "test_labels  = label_encoder.fit_transform(test_labels.reshape((-1,1)))\n",
    "\n",
    "print(train_images.shape,train_images.max(),train_images.min())\n",
    "print(train_labels.shape,train_labels.max(),train_labels.min())\n",
    "print(test_images.shape,test_images.max(),test_images.min())\n",
    "print(test_labels.shape,test_labels.max(),test_labels.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T21:02:00.227446Z",
     "start_time": "2018-12-20T21:02:00.144670Z"
    },
    "code_folding": [
     15,
     89,
     130,
     171,
     202
    ]
   },
   "outputs": [],
   "source": [
    "# create the layers\n",
    "def tf_softmax(x): return tf.nn.softmax(x)\n",
    "\n",
    "def tf_elu(x):   return tf.nn.elu(x)\n",
    "def d_tf_elu(x): return tf.cast(tf.greater(x,0),tf.float32)  + (tf_elu(tf.cast(tf.less_equal(x,0),tf.float32) * x) + 1.0)\n",
    "\n",
    "def tf_relu(x):   return tf.nn.relu(x)\n",
    "def d_tf_relu(x): return tf.cast(tf.greater(x,0),tf.float32)\n",
    "\n",
    "def tf_tanh(x):   return tf.nn.tanh(x)\n",
    "def d_tf_tanh(x): return 1 - tf_tanh(x) ** 2\n",
    "\n",
    "def tf_sigmoid(x):   return tf.nn.sigmoid(x)\n",
    "def d_tf_sigmoid(x): return tf_sigmoid(x) * (1.0-tf_sigmoid(x))\n",
    "\n",
    "class CNN():\n",
    "\n",
    "    def __init__(self,k,inc,out, stddev=0.05,which_reg=0,act=tf_relu,d_act=d_tf_relu):\n",
    "        self.w              = tf.Variable(tf.random_normal([k,k,inc,out],stddev=stddev,seed=4,dtype=tf.float32))\n",
    "        self.m,self.v       = tf.Variable(tf.zeros_like(self.w)),tf.Variable(tf.zeros_like(self.w))\n",
    "        self.act,self.d_act = act,d_act\n",
    "        \n",
    "    def getw(self): return self.w\n",
    "\n",
    "    def feedforward(self,input,stride=1,padding='SAME'):\n",
    "        self.input  = input\n",
    "        self.layer  = tf.nn.conv2d(input,self.w,strides=[1,stride,stride,1],padding=padding) \n",
    "        self.layerA = self.act(self.layer)\n",
    "        return self.layer,self.layerA\n",
    "    \n",
    "    def backprop(self,gradient,stride=1,padding='SAME'):\n",
    "        grad_part_1 = gradient\n",
    "        grad_part_2 = self.d_act(self.layer)\n",
    "        grad_part_3 = self.input\n",
    "\n",
    "        grad_middle = grad_part_1 * grad_part_2\n",
    "        grad        = tf.nn.conv2d_backprop_filter(input = grad_part_3,filter_sizes = tf.shape(self.w),  out_backprop = grad_middle,strides=[1,stride,stride,1],padding=padding) / batch_size\n",
    "        grad_pass   = tf.nn.conv2d_backprop_input (input_sizes = tf.shape(self.input),filter= self.w,out_backprop = grad_middle,strides=[1,stride,stride,1],padding=padding)\n",
    "\n",
    "        update_w = []\n",
    "        update_w.append(tf.assign( self.m,self.m*beta1 + (1-beta1) * (grad)   ))\n",
    "        update_w.append(tf.assign( self.v,self.v*beta2 + (1-beta2) * (grad ** 2)   ))\n",
    "        m_hat = self.m / (1-beta1) ; v_hat = self.v / (1-beta2)\n",
    "        adam_middle = m_hat * learning_rate/(tf.sqrt(v_hat) + adam_e)\n",
    "        update_w.append(tf.assign(self.w,tf.subtract(self.w,adam_middle  )))\n",
    "        \n",
    "        return grad_pass,grad,update_w\n",
    "    \n",
    "class tf_batch_norm_layer():\n",
    "    \n",
    "    def __init__(self,vector_shape,axis):\n",
    "        self.moving_mean = tf.Variable(tf.zeros(shape=[1,1,1,vector_shape],dtype=tf.float32))\n",
    "        self.moving_vari = tf.Variable(tf.zeros(shape=[1,1,1,vector_shape],dtype=tf.float32))\n",
    "        self.axis        = axis\n",
    "        \n",
    "    def feedforward(self,input,training_phase=True,eps = 1e-8):\n",
    "        self.input = input\n",
    "        self.input_size          = self.input.shape\n",
    "        self.batch,self.h,self.w,self.c = self.input_size[0].value,self.input_size[1].value,self.input_size[2].value,self.input_size[3].value\n",
    "\n",
    "        # Training Moving Average Mean         \n",
    "        def training_fn():\n",
    "            self.mean    = tf.reduce_mean(self.input,axis=self.axis ,keepdims=True)\n",
    "            self.var     = tf.reduce_mean(tf.square(self.input-self.mean),axis=self.axis,keepdims=True)\n",
    "            centered_data= (self.input - self.mean)/tf.sqrt(self.var + eps)\n",
    "            \n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean*0.9 + 0.1 * self.mean ))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari*0.9 + 0.1 * self.var  ))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        # Testing Moving Average Mean        \n",
    "        def  testing_fn():\n",
    "            centered_data   = (self.input - self.moving_mean)/tf.sqrt(self.moving_vari + eps)\n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        self.output,update_variable = tf.cond(training_phase,true_fn=training_fn,false_fn=testing_fn)\n",
    "        return self.output,update_variable\n",
    "    \n",
    "    def backprop(self,grad,eps = 1e-8):\n",
    "        change_parts = 1.0 /(self.batch * self.h * self.w)\n",
    "        grad_sigma   = tf.reduce_sum( grad *  (self.input-self.mean)     ,axis=self.axis,keepdims=True) * -0.5 * (self.var+eps) ** -1.5\n",
    "        grad_mean    = tf.reduce_sum( grad *  (-1./tf.sqrt(self.var+eps)),axis=self.axis,keepdims=True) + grad_sigma * change_parts * 2.0 * tf.reduce_sum((self.input-self.mean),axis=self.axis,keepdims=True) * -1\n",
    "        grad_x       = grad * 1/(tf.sqrt(self.var+eps)) + grad_sigma * change_parts * 2.0 * (self.input-self.mean) + grad_mean * change_parts\n",
    "        return grad_x\n",
    "\n",
    "class tf_layer_norm_layer():\n",
    "    \n",
    "    def __init__(self,vector_shape,axis):\n",
    "        self.moving_mean = tf.Variable(tf.zeros(shape=[vector_shape,1,1,1],dtype=tf.float32))\n",
    "        self.moving_vari = tf.Variable(tf.zeros(shape=[vector_shape,1,1,1],dtype=tf.float32))\n",
    "        self.axis        = axis\n",
    "        \n",
    "    def feedforward(self,input,training_phase=True,eps = 1e-8):\n",
    "        self.input = input\n",
    "        self.input_size          = self.input.shape\n",
    "        self.batch,self.h,self.w,self.c = self.input_size[0].value,self.input_size[1].value,self.input_size[2].value,self.input_size[3].value\n",
    "\n",
    "        # Training Moving Average Mean         \n",
    "        def training_fn():\n",
    "            self.mean    = tf.reduce_mean(self.input,axis=self.axis ,keepdims=True)\n",
    "            self.var     = tf.reduce_mean(tf.square(self.input-self.mean),axis=self.axis,keepdims=True)\n",
    "            centered_data= (self.input - self.mean)/tf.sqrt(self.var + eps)\n",
    "            \n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean*0.9 + 0.1 * self.mean ))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari*0.9 + 0.1 * self.var  ))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        # Testing Moving Average Mean        \n",
    "        def  testing_fn():\n",
    "            centered_data   = (self.input - self.moving_mean)/tf.sqrt(self.moving_vari + eps)\n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        self.output,update_variable = tf.cond(training_phase,true_fn=training_fn,false_fn=testing_fn)\n",
    "        return self.output,update_variable\n",
    "    \n",
    "    def backprop(self,grad,eps = 1e-8):\n",
    "        change_parts = 1.0 /(self.h * self.w * self.c)\n",
    "        grad_sigma   = tf.reduce_sum( grad *  (self.input-self.mean)     ,axis=self.axis,keepdims=True) * -0.5 * (self.var+eps) ** -1.5\n",
    "        grad_mean    = tf.reduce_sum( grad *  (-1./tf.sqrt(self.var+eps)),axis=self.axis,keepdims=True) + grad_sigma * change_parts * 2.0 * tf.reduce_sum((self.input-self.mean),axis=self.axis,keepdims=True) * -1\n",
    "        grad_x       = grad * 1/(tf.sqrt(self.var+eps)) + grad_sigma * change_parts * 2.0 * (self.input-self.mean) + grad_mean * change_parts\n",
    "        return grad_x\n",
    "    \n",
    "class tf_instance_norm_layer():\n",
    "    \n",
    "    def __init__(self,batch_size,vector_shape,axis):\n",
    "        self.moving_mean = tf.Variable(tf.zeros(shape=[batch_size,1,1,vector_shape],dtype=tf.float32))\n",
    "        self.moving_vari = tf.Variable(tf.zeros(shape=[batch_size,1,1,vector_shape],dtype=tf.float32))\n",
    "        self.axis        = axis\n",
    "        \n",
    "    def feedforward(self,input,training_phase=True,eps = 1e-8):\n",
    "        self.input = input\n",
    "        self.input_size          = self.input.shape\n",
    "        self.batch,self.h,self.w,self.c = self.input_size[0].value,self.input_size[1].value,self.input_size[2].value,self.input_size[3].value\n",
    "\n",
    "        # Training Moving Average Mean         \n",
    "        def training_fn():\n",
    "            self.mean    = tf.reduce_mean(self.input,axis=self.axis ,keepdims=True)\n",
    "            self.var     = tf.reduce_mean(tf.square(self.input-self.mean),axis=self.axis,keepdims=True)\n",
    "            centered_data= (self.input - self.mean)/tf.sqrt(self.var + eps)\n",
    "            \n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean*0.9 + 0.1 * self.mean ))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari*0.9 + 0.1 * self.var  ))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        # Testing Moving Average Mean        \n",
    "        def  testing_fn():\n",
    "            centered_data   = (self.input - self.moving_mean)/tf.sqrt(self.moving_vari + eps)\n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        self.output,update_variable = tf.cond(training_phase,true_fn=training_fn,false_fn=testing_fn)\n",
    "        return self.output,update_variable\n",
    "    \n",
    "    def backprop(self,grad,eps = 1e-8):\n",
    "        change_parts = 1.0 /(self.h * self.w)\n",
    "        grad_sigma   = tf.reduce_sum( grad *  (self.input-self.mean)     ,axis=self.axis,keepdims=True) * -0.5 * (self.var+eps) ** -1.5\n",
    "        grad_mean    = tf.reduce_sum( grad *  (-1./tf.sqrt(self.var+eps)),axis=self.axis,keepdims=True) + grad_sigma * change_parts * 2.0 * tf.reduce_sum((self.input-self.mean),axis=self.axis,keepdims=True) * -1\n",
    "        grad_x       = grad * 1/(tf.sqrt(self.var+eps)) + grad_sigma * change_parts * 2.0 * (self.input-self.mean) + grad_mean * change_parts\n",
    "        return grad_x\n",
    "  \n",
    "class tf_box_cox():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.lmbda    = tf.Variable(2.0) \n",
    "        self.m,self.v = tf.Variable(tf.zeros_like(self.lmbda)),tf.Variable(tf.zeros_like(self.lmbda))\n",
    "    def getw(self): return self.lmbda\n",
    "    \n",
    "    def feedforward(self,data):\n",
    "        self.input = data\n",
    "        self.layer = tf.pow((self.input + 1.0),self.lmbda)\n",
    "        return (self.layer - 1.0)/(self.lmbda + 1e-8)\n",
    "    \n",
    "    def backprop(self,grad):\n",
    "        \n",
    "        # Gradient that gets passed along\n",
    "        grad_pass = tf.pow((self.input + 1),self.lmbda-1.0) * grad\n",
    "        \n",
    "        # Grad respect to the lmbda value (not tested!)\n",
    "        grad_lmbda1 =   (self.layer * tf.log(self.input + 1 ))/(self.lmbda + 1e-8)\n",
    "        grad_lmbda2 = - (self.layer - 1)/(self.lmbda ** 2 + 1e-8)\n",
    "        grad_lmbda  = tf.reduce_mean((grad_lmbda1 + grad_lmbda2)*grad)\n",
    "\n",
    "        update_w = []\n",
    "        update_w.append(tf.assign( self.m,self.m*beta1 + (1-beta1) * (grad_lmbda)   ))\n",
    "        update_w.append(tf.assign( self.v,self.v*beta2 + (1-beta2) * (grad_lmbda ** 2)   ))\n",
    "        m_hat = self.m / (1-beta1) ; v_hat = self.v / (1-beta2)\n",
    "        adam_middle = m_hat * learning_rate/(tf.sqrt(v_hat) + adam_e)\n",
    "        update_w.append(tf.assign(self.lmbda,tf.subtract(self.lmbda,adam_middle  )))\n",
    "        \n",
    "        return grad_pass,grad_lmbda,update_w\n",
    "    \n",
    "def save_to_image(data,name):\n",
    "    l1g,l2g,l3g,l4g,l5g,l6g = data\n",
    "    l1g,l2g,l3g,l4g,l5g,l6g = np.asarray(l1g),np.asarray(l2g),np.asarray(l3g),np.asarray(l4g),np.asarray(l5g),np.asarray(l6g)\n",
    "    plt.figure(figsize=(25,15))\n",
    "    plt.suptitle('Current Iter : ' + str(iter))\n",
    "    plt.subplot(231); plt.hist(l1g.ravel(),50); plt.title('layer 1')\n",
    "    plt.subplot(232); plt.hist(l2g.ravel(),50); plt.title('layer 2')\n",
    "    plt.subplot(233); plt.hist(l3g.ravel(),50); plt.title('layer 3')\n",
    "    plt.subplot(234); plt.hist(l4g.ravel(),50); plt.title('layer 4')\n",
    "    plt.subplot(235); plt.hist(l5g.ravel(),50); plt.title('layer 5')\n",
    "    plt.subplot(236); plt.hist(l6g.ravel(),50); plt.title('layer 6')\n",
    "    plt.savefig(name + str(iter)+'.png')\n",
    "    plt.tight_layout()\n",
    "    plt.close('all')     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T21:49:36.669478Z",
     "start_time": "2018-12-20T21:49:36.663494Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# set hyper parameter\n",
    "num_epoch = 200; learning_rate = 0.0008; batch_size = 20; beta1,beta2,adam_e = 0.9,0.999,1e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T21:49:35.661396Z",
     "start_time": "2018-12-20T21:19:25.928419Z"
    },
    "code_folding": [
     0,
     46,
     59
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Iter : 0/200 batch : 7980/8000 acc : 0.25\n",
      " Current : 0 Acc : 0.10820000241696835 Test Acc : 0.17150000302121043\n",
      "\n",
      "Current Iter : 1/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 1 Acc : 0.17420000280439854 Test Acc : 0.24750000319443644\n",
      "\n",
      "Current Iter : 2/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 2 Acc : 0.22760000275075434 Test Acc : 0.28162500261329115\n",
      "\n",
      "Current Iter : 3/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 3 Acc : 0.26160000275075435 Test Acc : 0.2953750019427389\n",
      "\n",
      "Current Iter : 4/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 4 Acc : 0.2766000027805567 Test Acc : 0.30250000215135514\n",
      "\n",
      "Current Iter : 5/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 5 Acc : 0.29040000224113466 Test Acc : 0.3280000017210841\n",
      "\n",
      "Current Iter : 6/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 6 Acc : 0.3174000021219254 Test Acc : 0.3348750017769635\n",
      "\n",
      "Current Iter : 7/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 7 Acc : 0.3352000017762184 Test Acc : 0.34287500208243726\n",
      "\n",
      "Current Iter : 8/200 batch : 7980/8000 acc : 0.25\n",
      " Current : 8 Acc : 0.3446000021249056 Test Acc : 0.3446250020340085\n",
      "\n",
      "Current Iter : 9/200 batch : 7980/8000 acc : 0.25\n",
      " Current : 9 Acc : 0.35319999954104425 Test Acc : 0.352875001821667\n",
      "\n",
      "Current Iter : 10/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 10 Acc : 0.3571999998986721 Test Acc : 0.3576250019669533\n",
      "\n",
      "Current Iter : 11/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 11 Acc : 0.3621999996453524 Test Acc : 0.3651250016503036\n",
      "\n",
      "Current Iter : 12/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 12 Acc : 0.37080000016093256 Test Acc : 0.367125001559034\n",
      "\n",
      "Current Iter : 13/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 13 Acc : 0.3726000011265278 Test Acc : 0.37100000145845113\n",
      "\n",
      "Current Iter : 14/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 14 Acc : 0.37700000128149985 Test Acc : 0.3745000007655472\n",
      "\n",
      "Current Iter : 15/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 15 Acc : 0.3828000011146069 Test Acc : 0.3771250011771917\n",
      "\n",
      "Current Iter : 16/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 16 Acc : 0.38360000106692316 Test Acc : 0.3797500009648502\n",
      "\n",
      "Current Iter : 17/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 17 Acc : 0.38700000140070917 Test Acc : 0.3823750014230609\n",
      "\n",
      "Current Iter : 18/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 18 Acc : 0.39340000170469286 Test Acc : 0.38462500140070915\n",
      "\n",
      "Current Iter : 19/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 19 Acc : 0.3974000008702278 Test Acc : 0.38825000083073974\n",
      "\n",
      "Current Iter : 20/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 20 Acc : 0.3982000013589859 Test Acc : 0.3902500014193356\n",
      "\n",
      "Current Iter : 21/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 21 Acc : 0.4028000012636185 Test Acc : 0.3912500010803342\n",
      "\n",
      "Current Iter : 22/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 22 Acc : 0.40680000150203705 Test Acc : 0.39487500112503765\n",
      "\n",
      "Current Iter : 23/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 23 Acc : 0.4082000009417534 Test Acc : 0.3976250010728836\n",
      "\n",
      "Current Iter : 24/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 24 Acc : 0.4132000012993813 Test Acc : 0.3975000010803342\n",
      "\n",
      "Current Iter : 25/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 25 Acc : 0.4124000009298325 Test Acc : 0.4006250010058284\n",
      "\n",
      "Current Iter : 26/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 26 Acc : 0.41660000157356264 Test Acc : 0.40175000090152024\n",
      "\n",
      "Current Iter : 27/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 27 Acc : 0.4212000018954277 Test Acc : 0.40525000045076015\n",
      "\n",
      "Current Iter : 28/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 28 Acc : 0.4202000014781952 Test Acc : 0.4073750005103648\n",
      "\n",
      "Current Iter : 29/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 29 Acc : 0.4224000009894371 Test Acc : 0.41012500097975135\n",
      "\n",
      "Current Iter : 30/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 30 Acc : 0.42720000094175337 Test Acc : 0.4118750006519258\n",
      "\n",
      "Current Iter : 31/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 31 Acc : 0.43020000112056733 Test Acc : 0.41412500085309145\n",
      "\n",
      "Current Iter : 32/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 32 Acc : 0.4290000022649765 Test Acc : 0.4156250006891787\n",
      "\n",
      "Current Iter : 33/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 33 Acc : 0.43480000174045563 Test Acc : 0.41887500079348683\n",
      "\n",
      "Current Iter : 34/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 34 Acc : 0.4342000014781952 Test Acc : 0.42100000055506825\n",
      "\n",
      "Current Iter : 35/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 35 Acc : 0.4392000015377998 Test Acc : 0.4226250003837049\n",
      "\n",
      "Current Iter : 36/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 36 Acc : 0.4482000007033348 Test Acc : 0.4387500012293458\n",
      "\n",
      "Current Iter : 37/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 37 Acc : 0.46440000158548356 Test Acc : 0.4402500008791685\n",
      "\n",
      "Current Iter : 38/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 38 Acc : 0.46980000120401383 Test Acc : 0.4425000011920929\n",
      "\n",
      "Current Iter : 39/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 39 Acc : 0.4778000012636185 Test Acc : 0.4465000008791685\n",
      "\n",
      "Current Iter : 40/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 40 Acc : 0.47840000194311144 Test Acc : 0.44712500093504787\n",
      "\n",
      "Current Iter : 41/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 41 Acc : 0.48160000163316724 Test Acc : 0.45225000124424697\n",
      "\n",
      "Current Iter : 42/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 42 Acc : 0.48540000170469283 Test Acc : 0.45562500137835743\n",
      "\n",
      "Current Iter : 43/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 43 Acc : 0.48600000208616256 Test Acc : 0.4545000013895333\n",
      "\n",
      "Current Iter : 44/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 44 Acc : 0.4878000020980835 Test Acc : 0.46050000116229056\n",
      "\n",
      "Current Iter : 45/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 45 Acc : 0.48900000232458113 Test Acc : 0.46100000143051145\n",
      "\n",
      "Current Iter : 46/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 46 Acc : 0.4906000028252602 Test Acc : 0.4630000011995435\n",
      "\n",
      "Current Iter : 47/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 47 Acc : 0.49220000207424164 Test Acc : 0.4620000011473894\n",
      "\n",
      "Current Iter : 48/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 48 Acc : 0.49480000138282776 Test Acc : 0.46700000036507844\n",
      "\n",
      "Current Iter : 49/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 49 Acc : 0.5006000016331673 Test Acc : 0.4680000006407499\n",
      "\n",
      "Current Iter : 50/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 50 Acc : 0.49980000185966494 Test Acc : 0.4682500005885959\n",
      "\n",
      "Current Iter : 51/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 51 Acc : 0.5048000020980835 Test Acc : 0.47162500116974115\n",
      "\n",
      "Current Iter : 52/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 52 Acc : 0.5038000020980835 Test Acc : 0.4743750014156103\n",
      "\n",
      "Current Iter : 53/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 53 Acc : 0.5044000024795532 Test Acc : 0.47500000178813934\n",
      "\n",
      "Current Iter : 54/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 54 Acc : 0.5080000027418137 Test Acc : 0.47762500181794165\n",
      "\n",
      "Current Iter : 55/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 55 Acc : 0.5062000030279159 Test Acc : 0.47937500178813935\n",
      "\n",
      "Current Iter : 56/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 56 Acc : 0.5044000015258789 Test Acc : 0.48037500109523534\n",
      "\n",
      "Current Iter : 57/200 batch : 7980/8000 acc : 0.25\n",
      " Current : 57 Acc : 0.5078000012636185 Test Acc : 0.4798750012740493\n",
      "\n",
      "Current Iter : 58/200 batch : 7980/8000 acc : 0.25\n",
      " Current : 58 Acc : 0.508600000500679 Test Acc : 0.47937500055879356\n",
      "\n",
      "Current Iter : 59/200 batch : 7980/8000 acc : 0.25\n",
      " Current : 59 Acc : 0.5092000006437302 Test Acc : 0.47775000039488075\n",
      "\n",
      "Current Iter : 60/200 batch : 7980/8000 acc : 0.25\n",
      " Current : 60 Acc : 0.5120000004768371 Test Acc : 0.4791250005736947\n",
      "\n",
      "Current Iter : 61/200 batch : 7980/8000 acc : 0.25\n",
      " Current : 61 Acc : 0.5124000001549721 Test Acc : 0.47987500071525574\n",
      "\n",
      "Current Iter : 62/200 batch : 7980/8000 acc : 0.25\n",
      " Current : 62 Acc : 0.5162000007033348 Test Acc : 0.483125\n",
      "\n",
      "Current Iter : 63/200 batch : 7980/8000 acc : 0.25\n",
      " Current : 63 Acc : 0.5162000008225441 Test Acc : 0.4838750000298023\n",
      "\n",
      "Current Iter : 64/200 batch : 7980/8000 acc : 0.25\n",
      " Current : 64 Acc : 0.5188000007271767 Test Acc : 0.48512499980628493\n",
      "\n",
      "Current Iter : 65/200 batch : 7980/8000 acc : 0.25\n",
      " Current : 65 Acc : 0.5196000002026558 Test Acc : 0.4856250000745058\n",
      "\n",
      "Current Iter : 66/200 batch : 7980/8000 acc : 0.25\n",
      " Current : 66 Acc : 0.5222000007033348 Test Acc : 0.4891250002384186\n",
      "\n",
      "Current Iter : 67/200 batch : 7980/8000 acc : 0.25\n",
      " Current : 67 Acc : 0.525200001001358 Test Acc : 0.4905000002682209\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Iter : 68/200 batch : 7980/8000 acc : 0.25\n",
      " Current : 68 Acc : 0.5258000010251999 Test Acc : 0.4910000006109476\n",
      "\n",
      "Current Iter : 69/200 batch : 7980/8000 acc : 0.25\n",
      " Current : 69 Acc : 0.5268000018596649 Test Acc : 0.49149999998509886\n",
      "\n",
      "Current Iter : 70/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 70 Acc : 0.5314000012874603 Test Acc : 0.4916250002011657\n",
      "\n",
      "Current Iter : 71/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 71 Acc : 0.5352000008821487 Test Acc : 0.4928750002011657\n",
      "\n",
      "Current Iter : 72/200 batch : 7980/8000 acc : 0.25\n",
      " Current : 72 Acc : 0.5358000005483627 Test Acc : 0.4956249999254942\n",
      "\n",
      "Current Iter : 73/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 73 Acc : 0.5368000009059906 Test Acc : 0.4951250002160668\n",
      "\n",
      "Current Iter : 74/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 74 Acc : 0.5396000001430511 Test Acc : 0.4948750001937151\n",
      "\n",
      "Current Iter : 75/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 75 Acc : 0.5392000013589859 Test Acc : 0.49875000096857547\n",
      "\n",
      "Current Iter : 76/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 76 Acc : 0.5426000009775162 Test Acc : 0.49862500071525573\n",
      "\n",
      "Current Iter : 77/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 77 Acc : 0.5458000013828278 Test Acc : 0.5001250007003546\n",
      "\n",
      "Current Iter : 78/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 78 Acc : 0.5456000012159348 Test Acc : 0.5021250016614794\n",
      "\n",
      "Current Iter : 79/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 79 Acc : 0.5466000015735626 Test Acc : 0.5015000016614795\n",
      "\n",
      "Current Iter : 80/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 80 Acc : 0.5488000020980836 Test Acc : 0.502000001333654\n",
      "\n",
      "Current Iter : 81/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 81 Acc : 0.5506000012159348 Test Acc : 0.5032500005885958\n",
      "\n",
      "Current Iter : 82/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 82 Acc : 0.5542000018358231 Test Acc : 0.503374999947846\n",
      "\n",
      "Current Iter : 83/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 83 Acc : 0.5524000010490417 Test Acc : 0.5027500003576278\n",
      "\n",
      "Current Iter : 84/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 84 Acc : 0.5534000009298324 Test Acc : 0.5037500009685755\n",
      "\n",
      "Current Iter : 85/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 85 Acc : 0.5552000018358231 Test Acc : 0.5060000012814999\n",
      "\n",
      "Current Iter : 86/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 86 Acc : 0.5574000014066696 Test Acc : 0.506500000655651\n",
      "\n",
      "Current Iter : 87/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 87 Acc : 0.5604000010490418 Test Acc : 0.5068750005215407\n",
      "\n",
      "Current Iter : 88/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 88 Acc : 0.5602000008821487 Test Acc : 0.504750000834465\n",
      "\n",
      "Current Iter : 89/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 89 Acc : 0.5606000000238418 Test Acc : 0.505750000923872\n",
      "\n",
      "Current Iter : 90/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 90 Acc : 0.5650000010728836 Test Acc : 0.5065000005811453\n",
      "\n",
      "Current Iter : 91/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 91 Acc : 0.563600000500679 Test Acc : 0.5080000008270145\n",
      "\n",
      "Current Iter : 92/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 92 Acc : 0.5667999995946884 Test Acc : 0.5072500010207296\n",
      "\n",
      "Current Iter : 93/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 93 Acc : 0.5681999992132187 Test Acc : 0.5067500003054738\n",
      "\n",
      "Current Iter : 94/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 94 Acc : 0.5687999993562698 Test Acc : 0.5070000007376074\n",
      "\n",
      "Current Iter : 95/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 95 Acc : 0.5711999983787537 Test Acc : 0.5072500006482005\n",
      "\n",
      "Current Iter : 96/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 96 Acc : 0.573799999833107 Test Acc : 0.5103750007972121\n",
      "\n",
      "Current Iter : 97/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 97 Acc : 0.5759999980926513 Test Acc : 0.5091250004991889\n",
      "\n",
      "Current Iter : 98/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 98 Acc : 0.5753999990224838 Test Acc : 0.5088750002905726\n",
      "\n",
      "Current Iter : 99/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 99 Acc : 0.5778000000715255 Test Acc : 0.5083750008419156\n",
      "\n",
      "Current Iter : 100/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 100 Acc : 0.5816000000238418 Test Acc : 0.509125000871718\n",
      "\n",
      "Current Iter : 101/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 101 Acc : 0.5809999986886978 Test Acc : 0.5097500003129244\n",
      "\n",
      "Current Iter : 102/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 102 Acc : 0.5862000002861023 Test Acc : 0.508500000871718\n",
      "\n",
      "Current Iter : 103/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 103 Acc : 0.5856000003814698 Test Acc : 0.5111250009387731\n",
      "\n",
      "Current Iter : 104/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 104 Acc : 0.5866000003814698 Test Acc : 0.5131250006705522\n",
      "\n",
      "Current Iter : 105/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 105 Acc : 0.5884000004529953 Test Acc : 0.5131250002980232\n",
      "\n",
      "Current Iter : 106/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 106 Acc : 0.5886000009775162 Test Acc : 0.5131250004470348\n",
      "\n",
      "Current Iter : 107/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 107 Acc : 0.5900000003576279 Test Acc : 0.5123750007152558\n",
      "\n",
      "Current Iter : 108/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 108 Acc : 0.5914000002145767 Test Acc : 0.514250001013279\n",
      "\n",
      "Current Iter : 109/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 109 Acc : 0.5934000000953674 Test Acc : 0.5145000006258488\n",
      "\n",
      "Current Iter : 110/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 110 Acc : 0.5936000012159347 Test Acc : 0.5151250005513429\n",
      "\n",
      "Current Iter : 111/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 111 Acc : 0.5958000004291535 Test Acc : 0.515625000745058\n",
      "\n",
      "Current Iter : 112/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 112 Acc : 0.5970000005960464 Test Acc : 0.517000001296401\n",
      "\n",
      "Current Iter : 113/200 batch : 7980/8000 acc : 0.55\n",
      " Current : 113 Acc : 0.5964000014066696 Test Acc : 0.5176250008493661\n",
      "\n",
      "Current Iter : 114/200 batch : 7980/8000 acc : 0.55\n",
      " Current : 114 Acc : 0.5998000001907349 Test Acc : 0.5173750007152558\n",
      "\n",
      "Current Iter : 115/200 batch : 7980/8000 acc : 0.55\n",
      " Current : 115 Acc : 0.6008000012636184 Test Acc : 0.5173750004172325\n",
      "\n",
      "Current Iter : 116/200 batch : 7980/8000 acc : 0.55\n",
      " Current : 116 Acc : 0.5992000002861023 Test Acc : 0.5181250009685755\n",
      "\n",
      "Current Iter : 117/200 batch : 7980/8000 acc : 0.55\n",
      " Current : 117 Acc : 0.6008000005483627 Test Acc : 0.5202500015497208\n",
      "\n",
      "Current Iter : 118/200 batch : 7980/8000 acc : 0.55\n",
      " Current : 118 Acc : 0.5994000005722045 Test Acc : 0.5203750013560057\n",
      "\n",
      "Current Iter : 119/200 batch : 7980/8000 acc : 0.55\n",
      " Current : 119 Acc : 0.6012000007629394 Test Acc : 0.5203750015050173\n",
      "\n",
      "Current Iter : 120/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 120 Acc : 0.603200001835823 Test Acc : 0.5232500015199184\n",
      "\n",
      "Current Iter : 121/200 batch : 7980/8000 acc : 0.55\n",
      " Current : 121 Acc : 0.6040000007152557 Test Acc : 0.5243750016391278\n",
      "\n",
      "Current Iter : 122/200 batch : 7980/8000 acc : 0.55\n",
      " Current : 122 Acc : 0.6044000014066696 Test Acc : 0.5236250019073486\n",
      "\n",
      "Current Iter : 123/200 batch : 7980/8000 acc : 0.55\n",
      " Current : 123 Acc : 0.607600000500679 Test Acc : 0.5228750013560056\n",
      "\n",
      "Current Iter : 124/200 batch : 7980/8000 acc : 0.55\n",
      " Current : 124 Acc : 0.6072000007629395 Test Acc : 0.5252500009164214\n",
      "\n",
      "Current Iter : 125/200 batch : 7980/8000 acc : 0.55\n",
      " Current : 125 Acc : 0.6108000003099442 Test Acc : 0.5243750011175871\n",
      "\n",
      "Current Iter : 126/200 batch : 7980/8000 acc : 0.55\n",
      " Current : 126 Acc : 0.6116000000238418 Test Acc : 0.5247500014305114\n",
      "\n",
      "Current Iter : 127/200 batch : 7980/8000 acc : 0.55\n",
      " Current : 127 Acc : 0.609600000500679 Test Acc : 0.5245000011473894\n",
      "\n",
      "Current Iter : 128/200 batch : 7980/8000 acc : 0.55\n",
      " Current : 128 Acc : 0.611399999499321 Test Acc : 0.5252500009536744\n",
      "\n",
      "Current Iter : 129/200 batch : 7980/8000 acc : 0.55\n",
      " Current : 129 Acc : 0.6119999986886978 Test Acc : 0.524875001385808\n",
      "\n",
      "Current Iter : 130/200 batch : 7980/8000 acc : 0.55\n",
      " Current : 130 Acc : 0.6147999991178512 Test Acc : 0.526750001385808\n",
      "\n",
      "Current Iter : 131/200 batch : 7980/8000 acc : 0.55\n",
      " Current : 131 Acc : 0.6157999995946885 Test Acc : 0.5278750013560056\n",
      "\n",
      "Current Iter : 132/200 batch : 7980/8000 acc : 0.55\n",
      " Current : 132 Acc : 0.6173999987840653 Test Acc : 0.528375001475215\n",
      "\n",
      "Current Iter : 133/200 batch : 7980/8000 acc : 0.55\n",
      " Current : 133 Acc : 0.617399999499321 Test Acc : 0.5283750014007091\n",
      "\n",
      "Current Iter : 134/200 batch : 7980/8000 acc : 0.55\n",
      " Current : 134 Acc : 0.6199999991655349 Test Acc : 0.5281250009685755\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Iter : 135/200 batch : 7980/8000 acc : 0.55\n",
      " Current : 135 Acc : 0.6223999996185303 Test Acc : 0.5276250011846423\n",
      "\n",
      "Current Iter : 136/200 batch : 7980/8000 acc : 0.55\n",
      " Current : 136 Acc : 0.6236000002622605 Test Acc : 0.5272500010952353\n",
      "\n",
      "Current Iter : 137/200 batch : 7980/8000 acc : 0.55\n",
      " Current : 137 Acc : 0.6237999994754791 Test Acc : 0.5281250010803342\n",
      "\n",
      "Current Iter : 138/200 batch : 7980/8000 acc : 0.55\n",
      " Current : 138 Acc : 0.6251999995708466 Test Acc : 0.52750000115484\n",
      "\n",
      "Current Iter : 139/200 batch : 7980/8000 acc : 0.55\n",
      " Current : 139 Acc : 0.6261999998092651 Test Acc : 0.52812500115484\n",
      "\n",
      "Current Iter : 140/200 batch : 7980/8000 acc : 0.55\n",
      " Current : 140 Acc : 0.6255999991893768 Test Acc : 0.5285000010207296\n",
      "\n",
      "Current Iter : 141/200 batch : 7980/8000 acc : 0.55\n",
      " Current : 141 Acc : 0.6283999992609024 Test Acc : 0.5295000013336539\n",
      "\n",
      "Current Iter : 142/200 batch : 7980/8000 acc : 0.65\n",
      " Current : 142 Acc : 0.6303999989032746 Test Acc : 0.5280000016465783\n",
      "\n",
      "Current Iter : 143/200 batch : 7980/8000 acc : 0.65\n",
      " Current : 143 Acc : 0.6310000003576278 Test Acc : 0.5268750016018748\n",
      "\n",
      "Current Iter : 144/200 batch : 7980/8000 acc : 0.65\n",
      " Current : 144 Acc : 0.6299999992847443 Test Acc : 0.527625001333654\n",
      "\n",
      "Current Iter : 145/200 batch : 7980/8000 acc : 0.65\n",
      " Current : 145 Acc : 0.6325999999046326 Test Acc : 0.5283750015869737\n",
      "\n",
      "Current Iter : 146/200 batch : 7980/8000 acc : 0.65\n",
      " Current : 146 Acc : 0.6323999991416931 Test Acc : 0.529125001616776\n",
      "\n",
      "Current Iter : 147/200 batch : 7980/8000 acc : 0.65\n",
      " Current : 147 Acc : 0.6334000000953675 Test Acc : 0.5295000017061829\n",
      "\n",
      "Current Iter : 148/200 batch : 7980/8000 acc : 0.55\n",
      " Current : 148 Acc : 0.6349999997615814 Test Acc : 0.5292500017210842\n",
      "\n",
      "Current Iter : 149/200 batch : 7980/8000 acc : 0.65\n",
      " Current : 149 Acc : 0.6355999997854233 Test Acc : 0.5287500012293458\n",
      "\n",
      "Current Iter : 150/200 batch : 7980/8000 acc : 0.65\n",
      " Current : 150 Acc : 0.6361999998092651 Test Acc : 0.5297500013187527\n",
      "\n",
      "Current Iter : 151/200 batch : 7980/8000 acc : 0.65\n",
      " Current : 151 Acc : 0.6372000004053116 Test Acc : 0.5305000017955899\n",
      "\n",
      "Current Iter : 152/200 batch : 7980/8000 acc : 0.65\n",
      " Current : 152 Acc : 0.6377999997138977 Test Acc : 0.5301250017806888\n",
      "\n",
      "Current Iter : 153/200 batch : 7980/8000 acc : 0.65\n",
      " Current : 153 Acc : 0.6402000004053116 Test Acc : 0.5313750018551946\n",
      "\n",
      "Current Iter : 154/200 batch : 7980/8000 acc : 0.65\n",
      " Current : 154 Acc : 0.6376000002622605 Test Acc : 0.5312500020489097\n",
      "\n",
      "Current Iter : 155/200 batch : 7980/8000 acc : 0.55\n",
      " Current : 155 Acc : 0.638200000166893 Test Acc : 0.5318750025704503\n",
      "\n",
      "Current Iter : 156/200 batch : 7980/8000 acc : 0.55\n",
      " Current : 156 Acc : 0.6387999997138977 Test Acc : 0.5320000018924474\n",
      "\n",
      "Current Iter : 157/200 batch : 7980/8000 acc : 0.55\n",
      " Current : 157 Acc : 0.637999999165535 Test Acc : 0.5330000018328428\n",
      "\n",
      "Current Iter : 158/200 batch : 7980/8000 acc : 0.55\n",
      " Current : 158 Acc : 0.6403999990224838 Test Acc : 0.5327500019222497\n",
      "\n",
      "Current Iter : 159/200 batch : 7980/8000 acc : 0.55\n",
      " Current : 159 Acc : 0.6431999996900558 Test Acc : 0.5337500021606684\n",
      "\n",
      "Current Iter : 160/200 batch : 7980/8000 acc : 0.55\n",
      " Current : 160 Acc : 0.6435999985933304 Test Acc : 0.5342500016093255\n",
      "\n",
      "Current Iter : 161/200 batch : 7980/8000 acc : 0.65\n",
      " Current : 161 Acc : 0.6475999994277954 Test Acc : 0.5328750012442469\n",
      "\n",
      "Current Iter : 162/200 batch : 7980/8000 acc : 0.65\n",
      " Current : 162 Acc : 0.6485999991893768 Test Acc : 0.5332500012218953\n",
      "\n",
      "Current Iter : 163/200 batch : 7980/8000 acc : 0.65\n",
      " Current : 163 Acc : 0.648999999165535 Test Acc : 0.5323750014230609\n",
      "\n",
      "Current Iter : 164/200 batch : 7980/8000 acc : 0.65\n",
      " Current : 164 Acc : 0.651199999332428 Test Acc : 0.5321250016987323\n",
      "\n",
      "Current Iter : 165/200 batch : 7980/8000 acc : 0.65\n",
      " Current : 165 Acc : 0.6525999999046326 Test Acc : 0.5326250012218953\n",
      "\n",
      "Current Iter : 166/200 batch : 7980/8000 acc : 0.65\n",
      " Current : 166 Acc : 0.6531999990940094 Test Acc : 0.5330000015720725\n",
      "\n",
      "Current Iter : 167/200 batch : 7980/8000 acc : 0.65\n",
      " Current : 167 Acc : 0.6561999998092651 Test Acc : 0.5315000015124679\n",
      "\n",
      "Current Iter : 168/200 batch : 7980/8000 acc : 0.65\n",
      " Current : 168 Acc : 0.6571999998092651 Test Acc : 0.5327500024437904\n",
      "\n",
      "Current Iter : 169/200 batch : 7980/8000 acc : 0.65\n",
      " Current : 169 Acc : 0.6589999994039536 Test Acc : 0.5335000022128225\n",
      "\n",
      "Current Iter : 170/200 batch : 7980/8000 acc : 0.65\n",
      " Current : 170 Acc : 0.6625999993085862 Test Acc : 0.5337500020489097\n",
      "\n",
      "Current Iter : 171/200 batch : 7980/8000 acc : 0.65\n",
      " Current : 171 Acc : 0.6613999994993209 Test Acc : 0.5343750020489096\n",
      "\n",
      "Current Iter : 172/200 batch : 7980/8000 acc : 0.65\n",
      " Current : 172 Acc : 0.6655999995470047 Test Acc : 0.5352500023320317\n",
      "\n",
      "Current Iter : 173/200 batch : 7980/8000 acc : 0.65\n",
      " Current : 173 Acc : 0.6643999987840652 Test Acc : 0.5351250023767352\n",
      "\n",
      "Current Iter : 174/200 batch : 7980/8000 acc : 0.65\n",
      " Current : 174 Acc : 0.6641999988555908 Test Acc : 0.5356250027194619\n",
      "\n",
      "Current Iter : 175/200 batch : 7980/8000 acc : 0.65\n",
      " Current : 175 Acc : 0.6671999995708465 Test Acc : 0.5342500021681189\n",
      "\n",
      "Current Iter : 176/200 batch : 7980/8000 acc : 0.65\n",
      " Current : 176 Acc : 0.6669999985694886 Test Acc : 0.534875002540648\n",
      "\n",
      "Current Iter : 177/200 batch : 7980/8000 acc : 0.65\n",
      " Current : 177 Acc : 0.6679999992847443 Test Acc : 0.5341250025853514\n",
      "\n",
      "Current Iter : 178/200 batch : 7980/8000 acc : 0.65\n",
      " Current : 178 Acc : 0.6705999999046326 Test Acc : 0.5357500026747585\n",
      "\n",
      "Current Iter : 179/200 batch : 7980/8000 acc : 0.65\n",
      " Current : 179 Acc : 0.6669999988079071 Test Acc : 0.5351250018551945\n",
      "\n",
      "Current Iter : 180/200 batch : 7980/8000 acc : 0.65\n",
      " Current : 180 Acc : 0.6689999990463257 Test Acc : 0.5365000025555492\n",
      "\n",
      "Current Iter : 181/200 batch : 7980/8000 acc : 0.65\n",
      " Current : 181 Acc : 0.6703999997377396 Test Acc : 0.5375000031664967\n",
      "\n",
      "Current Iter : 182/200 batch : 7980/8000 acc : 0.65\n",
      " Current : 182 Acc : 0.6697999997138977 Test Acc : 0.5373750024661422\n",
      "\n",
      "Current Iter : 183/200 batch : 7980/8000 acc : 0.65\n",
      " Current : 183 Acc : 0.6703999989032745 Test Acc : 0.5373750038817525\n",
      "\n",
      "Current Iter : 184/200 batch : 7980/8000 acc : 0.65\n",
      " Current : 184 Acc : 0.6741999983787537 Test Acc : 0.5361250028386713\n",
      "\n",
      "Current Iter : 185/200 batch : 7980/8000 acc : 0.65\n",
      " Current : 185 Acc : 0.6759999995231628 Test Acc : 0.5367500029876828\n",
      "\n",
      "Current Iter : 186/200 batch : 7980/8000 acc : 0.65\n",
      " Current : 186 Acc : 0.6733999991416931 Test Acc : 0.5377500034496188\n",
      "\n",
      "Current Iter : 187/200 batch : 7980/8000 acc : 0.65\n",
      " Current : 187 Acc : 0.6767999987602233 Test Acc : 0.537875003144145\n",
      "\n",
      "Current Iter : 188/200 batch : 7980/8000 acc : 0.65\n",
      " Current : 188 Acc : 0.675799998998642 Test Acc : 0.5375000036880374\n",
      "\n",
      "Current Iter : 189/200 batch : 7980/8000 acc : 0.65\n",
      " Current : 189 Acc : 0.6763999977111816 Test Acc : 0.5373750024661422\n",
      "\n",
      "Current Iter : 190/200 batch : 7980/8000 acc : 0.65\n",
      " Current : 190 Acc : 0.6769999988079071 Test Acc : 0.5375000026449561\n",
      "\n",
      "Current Iter : 191/200 batch : 7980/8000 acc : 0.65\n",
      " Current : 191 Acc : 0.6785999989509582 Test Acc : 0.5380000032484531\n",
      "\n",
      "Current Iter : 192/200 batch : 7980/8000 acc : 0.65\n",
      " Current : 192 Acc : 0.6797999987602233 Test Acc : 0.537500002682209\n",
      "\n",
      "Current Iter : 193/200 batch : 7980/8000 acc : 0.65\n",
      " Current : 193 Acc : 0.6805999987125396 Test Acc : 0.5376250033080577\n",
      "\n",
      "Current Iter : 194/200 batch : 7980/8000 acc : 0.55\n",
      " Current : 194 Acc : 0.6817999985218048 Test Acc : 0.5380000030994415\n",
      "\n",
      "Current Iter : 195/200 batch : 7980/8000 acc : 0.65\n",
      " Current : 195 Acc : 0.6835999994277954 Test Acc : 0.5391250036656856\n",
      "\n",
      "Current Iter : 196/200 batch : 7980/8000 acc : 0.55\n",
      " Current : 196 Acc : 0.6857999992370606 Test Acc : 0.5387500029057264\n",
      "\n",
      "Current Iter : 197/200 batch : 7980/8000 acc : 0.55\n",
      " Current : 197 Acc : 0.6853999991416931 Test Acc : 0.5370000029355287\n",
      "\n",
      "Current Iter : 198/200 batch : 7980/8000 acc : 0.55\n",
      " Current : 198 Acc : 0.6823999984264374 Test Acc : 0.5383750024437904\n",
      "\n",
      "Current Iter : 199/200 batch : 7980/8000 acc : 0.65\n",
      " Current : 199 Acc : 0.6847999987602233 Test Acc : 0.5395000026375055\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Normal CNN \n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# create layers\n",
    "l1 = CNN(3,3, 16); \n",
    "l2 = CNN(3,16,16); \n",
    "l3 = CNN(3,16,16); \n",
    "\n",
    "l4 = CNN(3,16,16); \n",
    "l5 = CNN(3,16,16); \n",
    "l6 = CNN(3,16,10); \n",
    "\n",
    "# 2. graph \n",
    "x = tf.placeholder(tf.float32,(batch_size,96,96,3))\n",
    "y = tf.placeholder(tf.float32,(batch_size,10))\n",
    "\n",
    "layer1, layer1a = l1. feedforward(x,stride=2)\n",
    "layer2, layer2a = l2. feedforward(layer1a,stride=2)\n",
    "layer3, layer3a = l3. feedforward(layer2a,stride=2)\n",
    "layer4, layer4a = l4. feedforward(layer3a,stride=2)\n",
    "layer5, layer5a = l5. feedforward(layer4a)\n",
    "layer6, layer6a = l6. feedforward(layer5a)\n",
    "\n",
    "final_layer   = tf.reduce_mean(layer6a,(1,2))\n",
    "final_softmax = tf_softmax(final_layer)\n",
    "cost          = -tf.reduce_mean(y * tf.log(final_softmax + 1e-8))\n",
    "correct_prediction = tf.equal(tf.argmax(final_softmax, 1), tf.argmax(y, 1))\n",
    "accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "gradient = tf.tile((final_softmax-y)[:,None,None,:],[1,6,6,1])/batch_size\n",
    "grad6p,grad6w,grad6_up = l6.backprop(gradient)\n",
    "grad5p,grad5w,grad5_up = l5.backprop(grad5n)\n",
    "grad4p,grad4w,grad4_up = l4.backprop(grad4n,stride=2)\n",
    "\n",
    "grad3p,grad3w,grad3_up = l3.backprop(grad3n,stride=2)\n",
    "grad2p,grad2w,grad2_up = l2.backprop(grad2n,stride=2)\n",
    "grad1p,grad1w,grad1_up = l1.backprop(grad1n,stride=2)\n",
    "\n",
    "gradient_update = grad6_up + grad5_up + grad4_up + grad3_up + grad2_up + grad1_up \n",
    "\n",
    "# train\n",
    "sess.run(tf.global_variables_initializer())\n",
    "avg_acc_train = 0; avg_acc_test  = 0; \n",
    "normal_train_acc = [];normal_test_acc = []\n",
    "for iter in range(num_epoch):\n",
    "\n",
    "    for current_batch_index in range(0,len(train_images),batch_size):\n",
    "        current_data  = train_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = train_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy,gradient_update],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(train_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_train = avg_acc_train + sess_results[0]\n",
    "        \n",
    "    # Get weights\n",
    "    save_to_image(sess.run([l1.getw(),l2.getw(),l3.getw(),l4.getw(),l5.getw(),l6.getw()]),'Normal/weights/')\n",
    "    save_to_image(sess.run([grad1w,grad2w,grad3w,grad4w,grad5w,grad6w],feed_dict={x:current_data,y:current_label}),'Normal/gradientw/')\n",
    "    save_to_image(sess.run([grad1p,grad2p,grad3p,grad4p,grad5p,grad6p],feed_dict={x:current_data,y:current_label}),'Normal/gradientp/')\n",
    "    save_to_image(sess.run([grad1_up,grad2_up,grad3_up,grad4_up,grad5_up,grad6_up],feed_dict={x:current_data,y:current_label}),'Normal/gradient_update/')\n",
    "        \n",
    "    for current_batch_index in range(0,len(test_images), batch_size):\n",
    "        current_data  = test_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = test_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(test_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_test = avg_acc_test + sess_results[0]   \n",
    "        \n",
    "    print(\"\\n Current : \"+ str(iter) + \" Acc : \" + str(avg_acc_train/(len(train_images)/batch_size)) + \" Test Acc : \" + str(avg_acc_test/(len(test_images)/batch_size)) + '\\n')\n",
    "    \n",
    "    # save the training\n",
    "    normal_train_acc.append(avg_acc_train/(len(train_images)/batch_size))\n",
    "    normal_test_acc .append(avg_acc_test / (len(test_images)/batch_size))\n",
    "    avg_acc_train = 0 ; avg_acc_test  = 0\n",
    "    \n",
    "sess.close()\n",
    "tf.reset_default_graph();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T22:31:33.592073Z",
     "start_time": "2018-12-20T22:25:19.220955Z"
    },
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Iter : 0/200 batch : 7980/8000 acc : 0.25\n",
      " Current : 0 Acc : 0.24980000315606593 Test Acc : 0.2650000027846545\n",
      "\n",
      "Current Iter : 1/200 batch : 7980/8000 acc : 0.25\n",
      " Current : 1 Acc : 0.28280000257492066 Test Acc : 0.26287500286474824\n",
      "\n",
      "Current Iter : 2/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 2 Acc : 0.27560000309348104 Test Acc : 0.26987500257790087\n",
      "\n",
      "Current Iter : 3/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 3 Acc : 0.28700000254809854 Test Acc : 0.2802500026114285\n",
      "\n",
      "Current Iter : 4/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 4 Acc : 0.30280000264942647 Test Acc : 0.2811250024847686\n",
      "\n",
      "Current Iter : 5/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 5 Acc : 0.30480000253021716 Test Acc : 0.2870000022370368\n",
      "\n",
      "Current Iter : 6/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 6 Acc : 0.31660000199079513 Test Acc : 0.29200000257231296\n",
      "\n",
      "Current Iter : 7/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 7 Acc : 0.31740000122785567 Test Acc : 0.29512500232085587\n",
      "\n",
      "Current Iter : 8/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 8 Acc : 0.321400001347065 Test Acc : 0.29950000293552875\n",
      "\n",
      "Current Iter : 9/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 9 Acc : 0.31960000163316726 Test Acc : 0.2986250023543835\n",
      "\n",
      "Current Iter : 10/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 10 Acc : 0.3224000015258789 Test Acc : 0.3036250024195761\n",
      "\n",
      "Current Iter : 11/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 11 Acc : 0.32540000200271607 Test Acc : 0.30575000228360294\n",
      "\n",
      "Current Iter : 12/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 12 Acc : 0.3272000021934509 Test Acc : 0.31087500220164654\n",
      "\n",
      "Current Iter : 13/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 13 Acc : 0.3302000019848347 Test Acc : 0.31362500227987766\n",
      "\n",
      "Current Iter : 14/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 14 Acc : 0.33080000191926956 Test Acc : 0.31750000212341545\n",
      "\n",
      "Current Iter : 15/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 15 Acc : 0.3332000017464161 Test Acc : 0.3175000024959445\n",
      "\n",
      "Current Iter : 16/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 16 Acc : 0.33780000200867655 Test Acc : 0.32000000233761966\n",
      "\n",
      "Current Iter : 17/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 17 Acc : 0.3376000006198883 Test Acc : 0.3228750019613653\n",
      "\n",
      "Current Iter : 18/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 18 Acc : 0.34380000066757205 Test Acc : 0.32450000217184427\n",
      "\n",
      "Current Iter : 19/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 19 Acc : 0.34860000091791155 Test Acc : 0.32475000151433053\n",
      "\n",
      "Current Iter : 20/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 20 Acc : 0.34520000115036964 Test Acc : 0.32625000215135513\n",
      "\n",
      "Current Iter : 21/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 21 Acc : 0.35380000084638596 Test Acc : 0.32825000196695325\n",
      "\n",
      "Current Iter : 22/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 22 Acc : 0.35160000087320803 Test Acc : 0.3286250028759241\n",
      "\n",
      "Current Iter : 23/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 23 Acc : 0.3524000006765127 Test Acc : 0.3307500020880252\n",
      "\n",
      "Current Iter : 24/200 batch : 7980/8000 acc : 0.55\n",
      " Current : 24 Acc : 0.3550000010728836 Test Acc : 0.33225000208243727\n",
      "\n",
      "Current Iter : 25/200 batch : 7980/8000 acc : 0.55\n",
      " Current : 25 Acc : 0.35540000104904174 Test Acc : 0.33462500214576724\n",
      "\n",
      "Current Iter : 26/200 batch : 7980/8000 acc : 0.55\n",
      " Current : 26 Acc : 0.35620000058412554 Test Acc : 0.3356250024586916\n",
      "\n",
      "Current Iter : 27/200 batch : 7980/8000 acc : 0.55\n",
      " Current : 27 Acc : 0.35680000060796735 Test Acc : 0.3372500016540289\n",
      "\n",
      "Current Iter : 28/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 28 Acc : 0.362200001001358 Test Acc : 0.34100000190548596\n",
      "\n",
      "Current Iter : 29/200 batch : 7980/8000 acc : 0.55\n",
      " Current : 29 Acc : 0.3606000007688999 Test Acc : 0.3400000020302832\n",
      "\n",
      "Current Iter : 30/200 batch : 7980/8000 acc : 0.55\n",
      " Current : 30 Acc : 0.365200001001358 Test Acc : 0.34087500190362335\n",
      "\n",
      "Current Iter : 31/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 31 Acc : 0.36600000101327895 Test Acc : 0.34350000173784795\n",
      "\n",
      "Current Iter : 32/200 batch : 7980/8000 acc : 0.55\n",
      " Current : 32 Acc : 0.36900000113248826 Test Acc : 0.3425000017415732\n",
      "\n",
      "Current Iter : 33/200 batch : 7980/8000 acc : 0.55\n",
      " Current : 33 Acc : 0.37300000137090683 Test Acc : 0.3440000013727695\n",
      "\n",
      "Current Iter : 34/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 34 Acc : 0.3736000013947487 Test Acc : 0.3461250014323741\n",
      "\n",
      "Current Iter : 35/200 batch : 7980/8000 acc : 0.55\n",
      " Current : 35 Acc : 0.3766000010073185 Test Acc : 0.34662500164471566\n",
      "\n",
      "Current Iter : 36/200 batch : 7980/8000 acc : 0.45\n",
      " Current : 36 Acc : 0.3784000011086464 Test Acc : 0.3453750020079315\n",
      "\n",
      "Current Iter : 37/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 37 Acc : 0.3784000013768673 Test Acc : 0.34912500222213566\n",
      "\n",
      "Current Iter : 38/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 38 Acc : 0.3804000013768673 Test Acc : 0.34962500250898304\n",
      "\n",
      "Current Iter : 39/200 batch : 7980/8000 acc : 0.35\n",
      " Current : 39 Acc : 0.3834000013768673 Test Acc : 0.34950000206939874\n",
      "\n",
      "Current Iter : 40/200 batch : 4980/5000 acc : 0.35\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-5a5afdf11ea0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;31m# Get weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[0msave_to_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ml2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ml3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ml4\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ml5\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ml6\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'batch Norm/weights/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0msave_to_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgrad1w\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgrad2w\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgrad3w\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgrad4w\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgrad5w\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgrad6w\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mcurrent_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mcurrent_label\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'batch Norm/gradientw/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[0msave_to_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgrad1p\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgrad2p\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgrad3p\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgrad4p\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgrad5p\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgrad6p\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mcurrent_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mcurrent_label\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'batch Norm/gradientp/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[0msave_to_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgrad1_up\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgrad2_up\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgrad3_up\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgrad4_up\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgrad5_up\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgrad6_up\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mcurrent_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mcurrent_label\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'batch Norm/gradient_update/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-06b50a768fa7>\u001b[0m in \u001b[0;36msave_to_image\u001b[1;34m(data, name)\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msuptitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Current Iter : '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m231\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml1g\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'layer 1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m232\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml2g\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'layer 2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m233\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml3g\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'layer 3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m234\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml4g\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'layer 4'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36msubplot\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m     \u001b[0mfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgcf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1103\u001b[1;33m     \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1104\u001b[0m     \u001b[0mbbox\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbbox\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1105\u001b[0m     \u001b[0mbyebye\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\matplotlib\\figure.py\u001b[0m in \u001b[0;36madd_subplot\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1368\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_axstack\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1370\u001b[1;33m             \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubplot_class_factory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprojection_class\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1371\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_axstack\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1372\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msca\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\matplotlib\\axes\\_subplots.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, fig, *args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[1;31m# _axes_class is set in the subplot_class_factory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_axes_class\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigbox\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;31m# add a layout box to this, for both the full axis, and the poss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;31m# of the axis.  We need both because the axes may become smaller\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, fig, rect, facecolor, frameon, sharex, sharey, label, xscale, yscale, **kwargs)\u001b[0m\n\u001b[0;32m    481\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_rasterization_zorder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    482\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_connected\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m  \u001b[1;31m# a dict from events to (id, func)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 483\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcla\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    484\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    485\u001b[0m         \u001b[1;31m# funcs used to format x and y - fall back on major formatters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36mcla\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1038\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontainers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1039\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Disable grid on init to use rcParameter\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m         self.grid(self._gridOn, which=rcParams['axes.grid.which'],\n\u001b[0;32m   1042\u001b[0m                   axis=rcParams['axes.grid.axis'])\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36mgrid\u001b[1;34m(self, b, which, axis, **kwargs)\u001b[0m\n\u001b[0;32m   2721\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxaxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwhich\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwhich\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2722\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'y'\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'both'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2723\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0myaxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwhich\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwhich\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2724\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2725\u001b[0m     def ticklabel_format(self, *, axis='both', style='', scilimits=None,\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\matplotlib\\axis.py\u001b[0m in \u001b[0;36mgrid\u001b[1;34m(self, b, which, **kwargs)\u001b[0m\n\u001b[0;32m   1455\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gridOnMajor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1456\u001b[0m             self.set_tick_params(which='major', gridOn=self._gridOnMajor,\n\u001b[1;32m-> 1457\u001b[1;33m                                  **gridkw)\n\u001b[0m\u001b[0;32m   1458\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstale\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1459\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\matplotlib\\axis.py\u001b[0m in \u001b[0;36mset_tick_params\u001b[1;34m(self, which, reset, **kw)\u001b[0m\n\u001b[0;32m    860\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mwhich\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'major'\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mwhich\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'both'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 862\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mtick\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmajorTicks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    863\u001b[0m                     \u001b[0mtick\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_major_tick_kw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    864\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mwhich\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'minor'\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mwhich\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'both'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\matplotlib\\axis.py\u001b[0m in \u001b[0;36m__get__\u001b[1;34m(self, instance, cls)\u001b[0m\n\u001b[0;32m    675\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_major\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    676\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmajorTicks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 677\u001b[1;33m                 \u001b[0mtick\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmajor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    678\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmajorTicks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtick\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    679\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmajorTicks\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\matplotlib\\axis.py\u001b[0m in \u001b[0;36m_get_tick\u001b[1;34m(self, major)\u001b[0m\n\u001b[0;32m   2192\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2193\u001b[0m             \u001b[0mtick_kw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_minor_tick_kw\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2194\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mYTick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmajor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmajor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mtick_kw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2196\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\matplotlib\\axis.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, axes, loc, label, size, width, color, tickdir, pad, labelsize, labelcolor, zorder, gridOn, tick1On, tick2On, label1On, label2On, major, labelrotation, grid_color, grid_linestyle, grid_linewidth, grid_alpha, **kw)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtick1line\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tick1line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtick2line\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tick2line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgridline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_gridline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\matplotlib\\axis.py\u001b[0m in \u001b[0;36m_get_tick2line\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    607\u001b[0m                           \u001b[0mmarkersize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    608\u001b[0m                           \u001b[0mmarkeredgewidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_width\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 609\u001b[1;33m                           zorder=self._zorder)\n\u001b[0m\u001b[0;32m    610\u001b[0m         \u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_yaxis_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwhich\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'tick2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    611\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_artist_props\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\matplotlib\\lines.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, xdata, ydata, linewidth, linestyle, color, marker, markersize, markeredgewidth, markeredgecolor, markerfacecolor, markerfacecoloralt, fillstyle, antialiased, dash_capstyle, solid_capstyle, dash_joinstyle, solid_joinstyle, pickradius, drawstyle, markevery, **kwargs)\u001b[0m\n\u001b[0;32m    384\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_color\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_color\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 386\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_marker\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMarkerStyle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmarker\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfillstyle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    387\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_markevery\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\matplotlib\\markers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, marker, fillstyle)\u001b[0m\n\u001b[0;32m    248\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_marker_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_fillstyle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfillstyle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 250\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_marker\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmarker\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_recache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\matplotlib\\markers.py\u001b[0m in \u001b[0;36mset_marker\u001b[1;34m(self, marker)\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_marker\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmarker\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 323\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_recache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\matplotlib\\markers.py\u001b[0m in \u001b[0;36m_recache\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_capstyle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'butt'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 263\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_marker_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__bool__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\matplotlib\\markers.py\u001b[0m in \u001b[0;36m_set_tickright\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    763\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_set_tickright\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 765\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transform\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAffine2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    766\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_snap_threshold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    767\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\matplotlib\\transforms.py\u001b[0m in \u001b[0;36mscale\u001b[1;34m(self, sx, sy)\u001b[0m\n\u001b[0;32m   2040\u001b[0m         scale_mtx = np.array(\n\u001b[0;32m   2041\u001b[0m             [[sx, 0.0, 0.0], [0.0, sy, 0.0], [0.0, 0.0, 1.0]], float)\n\u001b[1;32m-> 2042\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mtx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscale_mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mtx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2043\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minvalidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2044\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x1080 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2. batch normalization\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# 1. layers\n",
    "l1 = CNN(3,3, 16); l1n = tf_batch_norm_layer(16,(0,1,2))\n",
    "l2 = CNN(3,16,16); l2n = tf_batch_norm_layer(16,(0,1,2))\n",
    "l3 = CNN(3,16,16); l3n = tf_batch_norm_layer(16,(0,1,2))\n",
    "l4 = CNN(3,16,16); l4n = tf_batch_norm_layer(16,(0,1,2))\n",
    "l5 = CNN(3,16,16); l5n = tf_batch_norm_layer(16,(0,1,2))\n",
    "l6 = CNN(3,16,10); \n",
    "\n",
    "# create the graph \n",
    "x = tf.placeholder(tf.float32,(batch_size,96,96,3))\n",
    "y = tf.placeholder(tf.float32,(batch_size,10))\n",
    "is_train = tf.placeholder_with_default(True,())\n",
    "\n",
    "layer1, layer1a = l1. feedforward(x,stride=2)\n",
    "layer1b,update1 = l1n.feedforward(layer1a,is_train)\n",
    "layer2, layer2a = l2. feedforward(layer1b,stride=2)\n",
    "layer2b,update2 = l2n.feedforward(layer2a,is_train)\n",
    "layer3, layer3a = l3. feedforward(layer2b,stride=2)\n",
    "layer3b,update3 = l3n.feedforward(layer3a,is_train)\n",
    "layer4, layer4a = l4. feedforward(layer3b,stride=2)\n",
    "layer4b,update4 = l4n.feedforward(layer4a,is_train)\n",
    "layer5, layer5a = l5. feedforward(layer4b)\n",
    "layer5b,update5 = l5n.feedforward(layer5a,is_train)\n",
    "layer6, layer6a = l6. feedforward(layer5b)\n",
    "\n",
    "final_layer   = tf.reduce_mean(layer6a,(1,2))\n",
    "final_softmax = tf_softmax(final_layer)\n",
    "cost          = -tf.reduce_mean(y * tf.log(final_softmax + 1e-8))\n",
    "correct_prediction = tf.equal(tf.argmax(final_softmax, 1), tf.argmax(y, 1))\n",
    "accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "gradient = tf.tile((final_softmax-y)[:,None,None,:],[1,6,6,1])/batch_size\n",
    "grad6p,grad6w,grad6_up = l6.backprop(gradient)\n",
    "grad5n = l5n.backprop(grad6p)\n",
    "grad5p,grad5w,grad5_up = l5.backprop(grad6p)\n",
    "grad4n = l4n.backprop(grad5p)\n",
    "grad4p,grad4w,grad4_up = l4.backprop(grad5p,stride=2)\n",
    "\n",
    "grad3n = l3n.backprop(grad4p)\n",
    "grad3p,grad3w,grad3_up = l3.backprop(grad4p,stride=2)\n",
    "grad2n = l2n.backprop(grad3p)\n",
    "grad2p,grad2w,grad2_up = l2.backprop(grad3p,stride=2)\n",
    "grad1n = l1n.backprop(grad2p)\n",
    "grad1p,grad1w,grad1_up = l1.backprop(grad2p,stride=2)\n",
    "\n",
    "update_ops  = update1 + update2 + update3 + update4 + update5\n",
    "gradient_update = grad6_up + grad5_up + grad4_up + grad3_up + grad2_up + grad1_up \n",
    "\n",
    "# train\n",
    "sess.run(tf.global_variables_initializer())\n",
    "avg_acc_train = 0; avg_acc_test  = 0; \n",
    "train_acc = []; test_acc = []\n",
    "for iter in range(num_epoch):\n",
    "\n",
    "    for current_batch_index in range(0,len(train_images),batch_size):\n",
    "        current_data  = train_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = train_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy,gradient_update,update_ops],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(train_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_train = avg_acc_train + sess_results[0]\n",
    "        \n",
    "    # Get weights\n",
    "    save_to_image(sess.run([l1.getw(),l2.getw(),l3.getw(),l4.getw(),l5.getw(),l6.getw()]),'batch Norm/weights/')\n",
    "    save_to_image(sess.run([grad1w,grad2w,grad3w,grad4w,grad5w,grad6w],feed_dict={x:current_data,y:current_label}),'batch Norm/gradientw/')\n",
    "    save_to_image(sess.run([grad1p,grad2p,grad3p,grad4p,grad5p,grad6p],feed_dict={x:current_data,y:current_label}),'batch Norm/gradientp/')\n",
    "    save_to_image(sess.run([grad1_up,grad2_up,grad3_up,grad4_up,grad5_up,grad6_up],feed_dict={x:current_data,y:current_label}),'batch Norm/gradient_update/')\n",
    "        \n",
    "    for current_batch_index in range(0,len(test_images), batch_size):\n",
    "        current_data  = test_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = test_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy],feed_dict={x:current_data,y:current_label,is_train:False})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(test_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_test = avg_acc_test + sess_results[0]   \n",
    "        \n",
    "    print(\"\\n Current : \"+ str(iter) + \" Acc : \" + str(avg_acc_train/(len(train_images)/batch_size)) + \" Test Acc : \" + str(avg_acc_test/(len(test_images)/batch_size)) + '\\n')\n",
    "    \n",
    "    # save the training\n",
    "    train_acc.append(avg_acc_train/(len(train_images)/batch_size))\n",
    "    test_acc .append(avg_acc_test / (len(test_images)/batch_size))\n",
    "    avg_acc_train = 0 ; avg_acc_test  = 0\n",
    "   \n",
    "np.save('batch Norm/train.npy',train_acc)\n",
    "np.save('batch Norm/test.npy', test_acc)\n",
    "sess.close()\n",
    "tf.reset_default_graph();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T22:15:40.774701Z",
     "start_time": "2018-12-20T22:15:40.752771Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T22:15:43.828650Z",
     "start_time": "2018-12-20T22:15:43.552111Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T07:39:51.379493Z",
     "start_time": "2018-12-20T07:39:51.079050Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T07:43:08.033249Z",
     "start_time": "2018-12-20T07:43:06.719715Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T20:25:08.945691Z",
     "start_time": "2018-12-20T20:25:08.918763Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T20:25:16.316488Z",
     "start_time": "2018-12-20T20:25:16.066594Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T07:57:11.376556Z",
     "start_time": "2018-12-20T07:57:11.370596Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "1. mttk/STL10. (2018). GitHub. Retrieved 19 December 2018, from https://github.com/mttk/STL10\n",
    "2. [duplicate], H. (2018). How to display multiple images in one figure correctly?. Stack Overflow. Retrieved 19 December 2018, from https://stackoverflow.com/questions/46615554/how-to-display-multiple-images-in-one-figure-correctly\n",
    "3. plot, H. (2010). How to change the font size on a matplotlib plot. Stack Overflow. Retrieved 20 December 2018, from https://stackoverflow.com/questions/3899980/how-to-change-the-font-size-on-a-matplotlib-plot\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
