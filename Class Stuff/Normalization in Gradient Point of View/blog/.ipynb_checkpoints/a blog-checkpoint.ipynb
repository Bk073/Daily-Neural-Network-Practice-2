{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T06:25:48.211729Z",
     "start_time": "2018-12-20T06:25:44.682125Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# import Library and some random image data set\n",
    "import tensorflow as tf\n",
    "import numpy      as np\n",
    "import seaborn    as sns \n",
    "import pandas     as pd\n",
    "import os,sys\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(78); tf.set_random_seed(78)\n",
    "\n",
    "# get some of the STL data set\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from skimage import util \n",
    "from skimage.transform import resize\n",
    "from skimage.io import imread\n",
    "import warnings\n",
    "from numpy import inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T06:25:53.354955Z",
     "start_time": "2018-12-20T06:25:50.789791Z"
    },
    "code_folding": [
     0,
     2,
     29,
     37
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 96, 96, 3) 1.0 0.0\n",
      "(5000, 10) 1.0 0.0\n",
      "(8000, 96, 96, 3) 1.0 0.0\n",
      "(8000, 10) 1.0 0.0\n"
     ]
    }
   ],
   "source": [
    "# read all of the data\n",
    "# https://github.com/mttk/STL10\n",
    "def read_all_images(path_to_data):\n",
    "    \"\"\"\n",
    "    :param path_to_data: the file containing the binary images from the STL-10 dataset\n",
    "    :return: an array containing all the images\n",
    "    \"\"\"\n",
    "\n",
    "    with open(path_to_data, 'rb') as f:\n",
    "        # read whole file in uint8 chunks\n",
    "        everything = np.fromfile(f, dtype=np.uint8)\n",
    "\n",
    "        # We force the data into 3x96x96 chunks, since the\n",
    "        # images are stored in \"column-major order\", meaning\n",
    "        # that \"the first 96*96 values are the red channel,\n",
    "        # the next 96*96 are green, and the last are blue.\"\n",
    "        # The -1 is since the size of the pictures depends\n",
    "        # on the input file, and this way numpy determines\n",
    "        # the size on its own.\n",
    "\n",
    "        images = np.reshape(everything, (-1, 3, 96, 96))\n",
    "\n",
    "        # Now transpose the images into a standard image format\n",
    "        # readable by, for example, matplotlib.imshow\n",
    "        # You might want to comment this line or reverse the shuffle\n",
    "        # if you will use a learning algorithm like CNN, since they like\n",
    "        # their channels separated.\n",
    "        images = np.transpose(images, (0, 3, 2, 1))\n",
    "        return images\n",
    "def read_labels(path_to_labels):\n",
    "    \"\"\"\n",
    "    :param path_to_labels: path to the binary file containing labels from the STL-10 dataset\n",
    "    :return: an array containing the labels\n",
    "    \"\"\"\n",
    "    with open(path_to_labels, 'rb') as f:\n",
    "        labels = np.fromfile(f, dtype=np.uint8)\n",
    "        return labels\n",
    "def show_images(data,row=1,col=1):\n",
    "    fig=plt.figure(figsize=(10,10))\n",
    "    columns = col; rows = row\n",
    "    for i in range(1, columns*rows +1):\n",
    "        fig.add_subplot(rows, columns, i)\n",
    "        plt.imshow(data[i-1])\n",
    "    plt.show()\n",
    "\n",
    "train_images = read_all_images(\"../../../DataSet/STL10/stl10_binary/train_X.bin\") / 255.0\n",
    "train_labels = read_labels    (\"../../../DataSet/STL10/stl10_binary/train_Y.bin\")\n",
    "test_images  = read_all_images(\"../../../DataSet/STL10/stl10_binary/test_X.bin\")  / 255.0\n",
    "test_labels  = read_labels    (\"../../../DataSet/STL10/stl10_binary/test_y.bin\")\n",
    "\n",
    "label_encoder= OneHotEncoder(sparse=False,categories='auto')\n",
    "train_labels = label_encoder.fit_transform(train_labels.reshape((-1,1)))\n",
    "test_labels  = label_encoder.fit_transform(test_labels.reshape((-1,1)))\n",
    "\n",
    "print(train_images.shape,train_images.max(),train_images.min())\n",
    "print(train_labels.shape,train_labels.max(),train_labels.min())\n",
    "print(test_images.shape,test_images.max(),test_images.min())\n",
    "print(test_labels.shape,test_labels.max(),test_labels.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T07:35:47.657663Z",
     "start_time": "2018-12-20T07:35:47.594783Z"
    },
    "code_folding": [
     15,
     58,
     99,
     140
    ]
   },
   "outputs": [],
   "source": [
    "# create the layers\n",
    "def tf_softmax(x): return tf.nn.softmax(x)\n",
    "\n",
    "def tf_elu(x):   return tf.nn.elu(x)\n",
    "def d_tf_elu(x): return tf.cast(tf.greater(x,0),tf.float32)  + (tf_elu(tf.cast(tf.less_equal(x,0),tf.float32) * x) + 1.0)\n",
    "\n",
    "def tf_relu(x):   return tf.nn.relu(x)\n",
    "def d_tf_relu(x): return tf.cast(tf.greater(x,0),tf.float32)\n",
    "\n",
    "def tf_tanh(x):   return tf.nn.tanh(x)\n",
    "def d_tf_tanh(x): return 1 - tf_tanh(x) ** 2\n",
    "\n",
    "def tf_sigmoid(x):   return tf.nn.sigmoid(x)\n",
    "def d_tf_sigmoid(x): return tf_sigmoid(x) * (1.0-tf_sigmoid(x))\n",
    "\n",
    "class CNN():\n",
    "\n",
    "    def __init__(self,k,inc,out, stddev=0.05,which_reg=0,act=tf_relu,d_act=d_tf_relu):\n",
    "        self.w          = tf.Variable(tf.random_normal([k,k,inc,out],stddev=stddev,seed=2,dtype=tf.float32))\n",
    "        self.m,self.v   = tf.Variable(tf.zeros_like(self.w)),tf.Variable(tf.zeros_like(self.w))\n",
    "        self.act,self.d_act = act,d_act\n",
    "        self.which_reg  = which_reg\n",
    "        \n",
    "    def getw(self): return self.w\n",
    "\n",
    "    def feedforward(self,input,stride=1,padding='SAME'):\n",
    "        self.input  = input\n",
    "        self.layer  = tf.nn.conv2d(input,self.w,strides=[1,stride,stride,1],padding=padding) \n",
    "        self.layerA = self.act(self.layer)\n",
    "        return [self.layer,self.layerA]\n",
    "    \n",
    "    def backprop(self,gradient,stride=1,padding='SAME'):\n",
    "        grad_part_1 = gradient\n",
    "        grad_part_2 = self.d_act(self.layer)\n",
    "        grad_part_3 = self.input\n",
    "\n",
    "        grad_middle = grad_part_1 * grad_part_2\n",
    "        grad        = tf.nn.conv2d_backprop_filter(input = grad_part_3,filter_sizes = tf.shape(self.w),  out_backprop = grad_middle,strides=[1,stride,stride,1],padding=padding) / batch_size\n",
    "        grad_pass   = tf.nn.conv2d_backprop_input (input_sizes = tf.shape(self.input),filter= self.w,out_backprop = grad_middle,strides=[1,stride,stride,1],padding=padding)\n",
    "\n",
    "        if self.which_reg == 0:   grad = grad\n",
    "        if self.which_reg == 0.5: grad = grad + lamda * (tf.sqrt(tf.abs(self.w))) * (1.0/tf.sqrt(tf.abs(self.w)+ 10e-5)) * tf.sign(self.w)\n",
    "        if self.which_reg == 1:   grad = grad + lamda * tf.sign(self.w)\n",
    "        if self.which_reg == 1.5: grad = grad + lamda * 1.0/(tf.sqrt(tf.square(self.w) + 10e-5)) * self.w\n",
    "        if self.which_reg == 2:   grad = grad + lamda * (1.0/tf.sqrt(tf.square(tf.abs(self.w))+ 10e-5)) * tf.abs(self.w) * tf.sign(self.w)\n",
    "        if self.which_reg == 2.5: grad = grad + lamda * 2.0 * self.w\n",
    "        if self.which_reg == 3:   grad = grad + lamda * tf.pow(tf.pow(tf.abs(self.w),3)+ 10e-5,-0.66) * tf.pow(tf.abs(self.w),2) * tf.sign(self.w)\n",
    "        if self.which_reg == 4:   grad = grad + lamda * tf.pow(tf.pow(tf.abs(self.w),4)+ 10e-5,-0.75) * tf.pow(tf.abs(self.w),3) * tf.sign(self.w)\n",
    "\n",
    "        update_w = []\n",
    "        update_w.append(tf.assign( self.m,self.m*beta1 + (1-beta1) * (grad)   ))\n",
    "        update_w.append(tf.assign( self.v,self.v*beta2 + (1-beta2) * (grad ** 2)   ))\n",
    "        m_hat = self.m / (1-beta1) ; v_hat = self.v / (1-beta2)\n",
    "        adam_middle = m_hat * learning_rate/(tf.sqrt(v_hat) + adam_e)\n",
    "        update_w.append(tf.assign(self.w,tf.subtract(self.w,adam_middle  )))\n",
    "        \n",
    "        return grad_pass,grad,update_w\n",
    "    \n",
    "class tf_batch_norm_layer():\n",
    "    \n",
    "    def __init__(self,vector_shape,axis):\n",
    "        self.moving_mean = tf.Variable(tf.zeros(shape=[1,1,1,vector_shape],dtype=tf.float32))\n",
    "        self.moving_vari = tf.Variable(tf.zeros(shape=[1,1,1,vector_shape],dtype=tf.float32))\n",
    "        self.axis        = axis\n",
    "        \n",
    "    def feedforward(self,input,training_phase=True,eps = 1e-8):\n",
    "        self.input = input\n",
    "        self.input_size          = self.input.shape\n",
    "        self.batch,self.h,self.w,self.c = self.input_size[0].value,self.input_size[1].value,self.input_size[2].value,self.input_size[3].value\n",
    "\n",
    "        # Training Moving Average Mean         \n",
    "        def training_fn():\n",
    "            self.mean    = tf.reduce_mean(self.input,axis=self.axis ,keepdims=True)\n",
    "            self.var     = tf.reduce_mean(tf.square(self.input-self.mean),axis=self.axis,keepdims=True)\n",
    "            centered_data= (self.input - self.mean)/tf.sqrt(self.var + eps)\n",
    "            \n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean*0.9 + 0.1 * self.mean ))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari*0.9 + 0.1 * self.var  ))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        # Testing Moving Average Mean        \n",
    "        def  testing_fn():\n",
    "            centered_data   = (self.input - self.moving_mean)/tf.sqrt(self.moving_vari + eps)\n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        self.output,update_variable = tf.cond(training_phase,true_fn=training_fn,false_fn=testing_fn)\n",
    "        return self.output,update_variable\n",
    "    \n",
    "    def backprop(self,grad,eps = 1e-8):\n",
    "        change_parts = 1.0 /(self.batch * self.h * self.w)\n",
    "        grad_sigma   = tf.reduce_sum( grad *  (self.input-self.mean)     ,axis=self.axis,keepdims=True) * -0.5 * (self.var+eps) ** -1.5\n",
    "        grad_mean    = tf.reduce_sum( grad *  (-1./tf.sqrt(self.var+eps)),axis=self.axis,keepdims=True) + grad_sigma * change_parts * 2.0 * tf.reduce_sum((self.input-self.mean),axis=self.axis,keepdims=True) * -1\n",
    "        grad_x       = grad * 1/(tf.sqrt(self.var+eps)) + grad_sigma * change_parts * 2.0 * (self.input-self.mean) + grad_mean * change_parts\n",
    "        return grad_x\n",
    "\n",
    "class tf_layer_norm_layer():\n",
    "    \n",
    "    def __init__(self,vector_shape,axis):\n",
    "        self.moving_mean = tf.Variable(tf.zeros(shape=[vector_shape,1,1,1],dtype=tf.float32))\n",
    "        self.moving_vari = tf.Variable(tf.zeros(shape=[vector_shape,1,1,1],dtype=tf.float32))\n",
    "        self.axis        = axis\n",
    "        \n",
    "    def feedforward(self,input,training_phase=True,eps = 1e-8):\n",
    "        self.input = input\n",
    "        self.input_size          = self.input.shape\n",
    "        self.batch,self.h,self.w,self.c = self.input_size[0].value,self.input_size[1].value,self.input_size[2].value,self.input_size[3].value\n",
    "\n",
    "        # Training Moving Average Mean         \n",
    "        def training_fn():\n",
    "            self.mean    = tf.reduce_mean(self.input,axis=self.axis ,keepdims=True)\n",
    "            self.var     = tf.reduce_mean(tf.square(self.input-self.mean),axis=self.axis,keepdims=True)\n",
    "            centered_data= (self.input - self.mean)/tf.sqrt(self.var + eps)\n",
    "            \n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean*0.9 + 0.1 * self.mean ))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari*0.9 + 0.1 * self.var  ))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        # Testing Moving Average Mean        \n",
    "        def  testing_fn():\n",
    "            centered_data   = (self.input - self.moving_mean)/tf.sqrt(self.moving_vari + eps)\n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        self.output,update_variable = tf.cond(training_phase,true_fn=training_fn,false_fn=testing_fn)\n",
    "        return self.output,update_variable\n",
    "    \n",
    "    def backprop(self,grad,eps = 1e-8):\n",
    "        change_parts = 1.0 /(self.h * self.w * self.c)\n",
    "        grad_sigma   = tf.reduce_sum( grad *  (self.input-self.mean)     ,axis=self.axis,keepdims=True) * -0.5 * (self.var+eps) ** -1.5\n",
    "        grad_mean    = tf.reduce_sum( grad *  (-1./tf.sqrt(self.var+eps)),axis=self.axis,keepdims=True) + grad_sigma * change_parts * 2.0 * tf.reduce_sum((self.input-self.mean),axis=self.axis,keepdims=True) * -1\n",
    "        grad_x       = grad * 1/(tf.sqrt(self.var+eps)) + grad_sigma * change_parts * 2.0 * (self.input-self.mean) + grad_mean * change_parts\n",
    "        return grad_x\n",
    "    \n",
    "class tf_instance_norm_layer():\n",
    "    \n",
    "    def __init__(self,batch_size,vector_shape,axis):\n",
    "        self.moving_mean = tf.Variable(tf.zeros(shape=[batch_size,1,1,vector_shape],dtype=tf.float32))\n",
    "        self.moving_vari = tf.Variable(tf.zeros(shape=[batch_size,1,1,vector_shape],dtype=tf.float32))\n",
    "        self.axis        = axis\n",
    "        \n",
    "    def feedforward(self,input,training_phase=True,eps = 1e-8):\n",
    "        self.input = input\n",
    "        self.input_size          = self.input.shape\n",
    "        self.batch,self.h,self.w,self.c = self.input_size[0].value,self.input_size[1].value,self.input_size[2].value,self.input_size[3].value\n",
    "\n",
    "        # Training Moving Average Mean         \n",
    "        def training_fn():\n",
    "            self.mean    = tf.reduce_mean(self.input,axis=self.axis ,keepdims=True)\n",
    "            self.var     = tf.reduce_mean(tf.square(self.input-self.mean),axis=self.axis,keepdims=True)\n",
    "            centered_data= (self.input - self.mean)/tf.sqrt(self.var + eps)\n",
    "            \n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean*0.9 + 0.1 * self.mean ))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari*0.9 + 0.1 * self.var  ))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        # Testing Moving Average Mean        \n",
    "        def  testing_fn():\n",
    "            centered_data   = (self.input - self.moving_mean)/tf.sqrt(self.moving_vari + eps)\n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        self.output,update_variable = tf.cond(training_phase,true_fn=training_fn,false_fn=testing_fn)\n",
    "        return self.output,update_variable\n",
    "    \n",
    "    def backprop(self,grad,eps = 1e-8):\n",
    "        change_parts = 1.0 /(self.h * self.w)\n",
    "        grad_sigma   = tf.reduce_sum( grad *  (self.input-self.mean)     ,axis=self.axis,keepdims=True) * -0.5 * (self.var+eps) ** -1.5\n",
    "        grad_mean    = tf.reduce_sum( grad *  (-1./tf.sqrt(self.var+eps)),axis=self.axis,keepdims=True) + grad_sigma * change_parts * 2.0 * tf.reduce_sum((self.input-self.mean),axis=self.axis,keepdims=True) * -1\n",
    "        grad_x       = grad * 1/(tf.sqrt(self.var+eps)) + grad_sigma * change_parts * 2.0 * (self.input-self.mean) + grad_mean * change_parts\n",
    "        return grad_x\n",
    "  \n",
    "class tf_box_cox():\n",
    "    \n",
    "    def __init__(self,channel):\n",
    "        self.lmbda    = tf.Variable(2.0) \n",
    "        self.m,self.v = tf.Variable(tf.zeros_like(self.lmbda)),tf.Variable(tf.zeros_like(self.lmbda))\n",
    "    def getw(self): return self.lmbda\n",
    "    \n",
    "    def feedforward(self,data):\n",
    "        self.input = data\n",
    "        self.layer = tf.pow((self.input + 1.0),self.lmbda)\n",
    "        return (self.layer - 1.0)/(self.lmbda + 1e-8)\n",
    "    \n",
    "    def backprop(self,grad):\n",
    "        \n",
    "        # Gradient that gets passed along\n",
    "        grad_pass = tf.pow((self.input + 1),self.lmbda-1.0) * grad\n",
    "        \n",
    "        # Grad respect to the lmbda value (not tested!)\n",
    "        grad_lmbda1 =   (self.layer * tf.log(self.input + 1 ))/(self.lmbda + 1e-8)\n",
    "        grad_lmbda2 = - (self.layer - 1)/(self.lmbda ** 2 + 1e-8)\n",
    "        grad_lmbda  = tf.reduce_mean((grad_lmbda1 + grad_lmbda2)*grad)\n",
    "\n",
    "        update_w = []\n",
    "        update_w.append(tf.assign( self.m,self.m*beta1 + (1-beta1) * (grad_lmbda)   ))\n",
    "        update_w.append(tf.assign( self.v,self.v*beta2 + (1-beta2) * (grad_lmbda ** 2)   ))\n",
    "        m_hat = self.m / (1-beta1) ; v_hat = self.v / (1-beta2)\n",
    "        adam_middle = m_hat * learning_rate/(tf.sqrt(v_hat) + adam_e)\n",
    "        update_w.append(tf.assign(self.lmbda,tf.subtract(self.lmbda,adam_middle  )))\n",
    "        \n",
    "        return grad_pass,grad_lmbda,update_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T06:21:04.871403Z",
     "start_time": "2018-12-20T06:21:04.769676Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# hyper parameter\n",
    "num_epoch = 300; learning_rate = 0.0008; batch_size = 20\n",
    "beta1,beta2,adam_e = 0.9,0.999,1e-9\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T06:07:37.013560Z",
     "start_time": "2018-12-20T06:07:36.830997Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# create layers\n",
    "l1 = CNN(3,3, 16); \n",
    "l2 = CNN(3,16,16); \n",
    "l3 = CNN(3,16,16); \n",
    "\n",
    "l4 = CNN(3,16,32); \n",
    "l5 = CNN(3,32,32); \n",
    "l6 = CNN(3,32,10); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T06:08:47.504461Z",
     "start_time": "2018-12-20T06:08:47.299795Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# create the graph \n",
    "x = tf.placeholder(tf.float32,[batch_size,96,96,3])\n",
    "y = tf.placeholder(tf.float32,[batch_size,10])\n",
    "\n",
    "layer1,layer1a = l1.feedforward(x,stride=2)      ;          \n",
    "layer2,layer2a = l2.feedforward(layer1a,stride=2);         \n",
    "layer3,layer3a = l3.feedforward(layer2a,stride=2); \n",
    "\n",
    "layer4,layer4a = l4.feedforward(layer3a,stride=2);          \n",
    "layer5,layer5a = l5.feedforward(layer4a,stride=1);         \n",
    "layer6,layer6a = l6.feedforward(layer5a,stride=1); \n",
    "\n",
    "final_layer   = tf.reduce_mean(layer6a,(1,2))\n",
    "final_softmax = tf_softmax(final_layer)\n",
    "cost          = -tf.reduce_mean(y * tf.log(final_softmax + 1e-8))\n",
    "correct_prediction = tf.equal(tf.argmax(final_softmax, 1), tf.argmax(y, 1))\n",
    "accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "gradient = tf.tile((final_softmax-y)[:,None,None,:],[1,6,6,1])/batch_size\n",
    "grad6w,grad6p,grad6_up = l6.backprop(gradient)\n",
    "grad5w,grad5p,grad5_up = l5.backprop(grad6p)\n",
    "grad4w,grad4p,grad4_up = l4.backprop(grad5p,stride=2)\n",
    "\n",
    "grad3w,grad3p,grad3_up = l3.backprop(grad4p,stride=2)\n",
    "grad2w,grad2p,grad2_up = l2.backprop(grad3p,stride=2)\n",
    "grad1w,grad1p,grad1_up = l1.backprop(grad2p,stride=2)\n",
    "\n",
    "gradient_update = grad6_up + \\\n",
    "                   grad5_up + \\\n",
    "                   grad4_up + \\\n",
    "                   grad3_up + \\\n",
    "                   grad2_up + \\\n",
    "                   grad1_up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T06:09:14.239248Z",
     "start_time": "2018-12-20T06:08:54.084190Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Iter : 0/300 batch : 7980/8000 acc : 0.35\n",
      " Current : 0 Acc : 0.12840000288188458 Test Acc : 0.21200000332668423\n",
      "\n",
      "Current Iter : 1/300 batch : 7980/8000 acc : 0.45\n",
      " Current : 1 Acc : 0.22900000382959843 Test Acc : 0.25975000286474825\n",
      "\n",
      "Current Iter : 2/300 batch : 7980/8000 acc : 0.55\n",
      " Current : 2 Acc : 0.2868000023066998 Test Acc : 0.3093750014156103\n",
      "\n",
      "Current Iter : 3/300 batch : 900/8000 acc : 0.455\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-fbb428fac211>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mcurrent_data\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mtest_images\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcurrent_batch_index\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mcurrent_batch_index\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mcurrent_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcurrent_batch_index\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mcurrent_batch_index\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0msess_results\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mcurrent_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mcurrent_label\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Current Iter : '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/'\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_epoch\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;33m+\u001b[0m \u001b[1;34m' batch : '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_batch_index\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/'\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' acc : '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'\\r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0mavg_acc_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mavg_acc_test\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msess_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1332\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train\n",
    "sess.run(tf.global_variables_initializer())\n",
    "avg_acc_train = 0; avg_acc_test  = 0; \n",
    "train_acc     = [];test_acc = []\n",
    "for iter in range(num_epoch):\n",
    "\n",
    "    for current_batch_index in range(0,len(train_images),batch_size):\n",
    "        current_data  = train_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = train_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy,gradient_update],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(train_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_train = avg_acc_train + sess_results[0]\n",
    "        \n",
    "    for current_batch_index in range(0,len(test_images), batch_size):\n",
    "        current_data  = test_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = test_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(test_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_test = avg_acc_test + sess_results[0]   \n",
    "        \n",
    "    print(\"\\n Current : \"+ str(iter) + \" Acc : \" + str(avg_acc_train/(len(train_images)/batch_size)) + \" Test Acc : \" + str(avg_acc_test/(len(test_images)/batch_size)) + '\\n')\n",
    "    \n",
    "    # save the training\n",
    "    train_acc.append(avg_acc_train/(len(train_images)/batch_size))\n",
    "    test_acc .append(avg_acc_test/(len(test_images)/batch_size)  )\n",
    "    \n",
    "    \n",
    "    avg_acc_train = 0 ; avg_acc_test  = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T07:25:09.197622Z",
     "start_time": "2018-12-20T07:25:08.961027Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# create layers\n",
    "num_epoch = 150; learning_rate = 0.001; batch_size = 50\n",
    "beta1,beta2,adam_e = 0.9,0.999,1e-8\n",
    "tf.reset_default_graph(); sess.close()\n",
    "sess = tf.InteractiveSession()\n",
    "l1 = CNN(3,3, 16); l1n = tf_box_cox(16)\n",
    "l2 = CNN(3,16,16); l2n = tf_box_cox(16)\n",
    "l3 = CNN(3,16,16); l3n = tf_box_cox(16)\n",
    "\n",
    "l4 = CNN(3,16,32); l4n = tf_box_cox(32)\n",
    "l5 = CNN(3,32,32); l5n = tf_box_cox(32)\n",
    "l6 = CNN(3,32,10); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T07:25:10.526099Z",
     "start_time": "2018-12-20T07:25:09.562414Z"
    }
   },
   "outputs": [],
   "source": [
    "# create the graph \n",
    "x = tf.placeholder(tf.float32,[batch_size,96,96,3])\n",
    "y = tf.placeholder(tf.float32,[batch_size,10])\n",
    "\n",
    "layer1,layer1a = l1.feedforward(x,stride=2)      ;          \n",
    "layer1n = l1n.feedforward(layer1a)\n",
    "layer2,layer2a = l2.feedforward(layer1n,stride=2);          \n",
    "layer2n = l2n.feedforward(layer2a)\n",
    "layer3,layer3a = l3.feedforward(layer2n,stride=2); \n",
    "layer3n = l3n.feedforward(layer3a)\n",
    "\n",
    "layer4,layer4a = l4.feedforward(layer3n,stride=2);          \n",
    "layer4n = l4n.feedforward(layer4a)\n",
    "layer5,layer5a = l5.feedforward(layer4n,stride=1);          \n",
    "layer5n = l5n.feedforward(layer5a)\n",
    "layer6,layer6a = l6.feedforward(layer5n,stride=1); \n",
    "\n",
    "final_layer   = tf.reduce_mean(layer6a,(1,2))\n",
    "final_softmax = tf_softmax(final_layer)\n",
    "cost          = -tf.reduce_mean(y * tf.log(final_softmax + 1e-8))\n",
    "auto_train = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "correct_prediction = tf.equal(tf.argmax(final_softmax, 1), tf.argmax(y, 1))\n",
    "accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "gradient = tf.tile((final_softmax-y)[:,None,None,:],[1,6,6,1])/batch_size\n",
    "grad6p,grad6w,grad6_up  = l6.backprop(gradient)\n",
    "grad5n,grad5l,grad5n_up = l5n.backprop(grad6p); \n",
    "grad5p,grad5w,grad5_up  = l5.backprop(grad5n)\n",
    "grad4n,grad4l,grad4n_up = l4n.backprop(grad5p); \n",
    "grad4p,grad4w,grad4_up  = l4.backprop(grad4n,stride=2)\n",
    "\n",
    "grad3n,grad3l,grad3n_up = l3n.backprop(grad4p);\n",
    "grad3p,grad3w,grad3_up  = l3.backprop(grad3n,stride=2)\n",
    "grad2n,grad2l,grad2n_up = l2n.backprop(grad3p); \n",
    "grad2p,grad2w,grad2_up  = l2.backprop(grad2n,stride=2)\n",
    "grad1n,grad1l,grad1n_up = l1n.backprop(grad2p); \n",
    "grad1p,grad1w,grad1_up  = l1.backprop(grad1n,stride=2)\n",
    "\n",
    "gradient_update = grad6_up + \\\n",
    "                  grad5n_up + grad5_up + \\\n",
    "                  grad4n_up + grad4_up + \\\n",
    "                  grad3n_up + grad3_up + \\\n",
    "                  grad2n_up + grad2_up + \\\n",
    "                  grad1n_up + grad1_up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T07:35:28.790920Z",
     "start_time": "2018-12-20T07:25:10.917789Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Iter : 0/150 batch : 7950/8000 acc : 0.06\n",
      " Current : 0 Acc : 0.10439999990165233 Test Acc : 0.1019999994430691\n",
      "\n",
      "Current Iter : 1/150 batch : 7950/8000 acc : 0.12\n",
      " Current : 1 Acc : 0.12719999946653843 Test Acc : 0.17287500018719584\n",
      "\n",
      "Current Iter : 2/150 batch : 7950/8000 acc : 0.26\n",
      " Current : 2 Acc : 0.17519999973475933 Test Acc : 0.2257499997969717\n",
      "\n",
      "Current Iter : 3/150 batch : 7950/8000 acc : 0.24\n",
      " Current : 3 Acc : 0.23840000063180924 Test Acc : 0.27887499975040553\n",
      "\n",
      "Current Iter : 4/150 batch : 7950/8000 acc : 0.24\n",
      " Current : 4 Acc : 0.25620000038295987 Test Acc : 0.2341250004246831\n",
      "\n",
      "Current Iter : 5/150 batch : 7950/8000 acc : 0.28\n",
      " Current : 5 Acc : 0.2946000012755394 Test Acc : 0.28425000072456896\n",
      "\n",
      "Current Iter : 6/150 batch : 7950/8000 acc : 0.38\n",
      " Current : 6 Acc : 0.31380000039935113 Test Acc : 0.32350000012665986\n",
      "\n",
      "Current Iter : 7/150 batch : 7950/8000 acc : 0.32\n",
      " Current : 7 Acc : 0.3362000006437302 Test Acc : 0.3591250012628734\n",
      "\n",
      "Current Iter : 8/150 batch : 7950/8000 acc : 0.42\n",
      " Current : 8 Acc : 0.35420000046491623 Test Acc : 0.3633750013075769\n",
      "\n",
      "Current Iter : 9/150 batch : 7950/8000 acc : 0.42\n",
      " Current : 9 Acc : 0.35860000148415566 Test Acc : 0.37775000035762785\n",
      "\n",
      "Current Iter : 10/150 batch : 7950/8000 acc : 0.36\n",
      " Current : 10 Acc : 0.3786000004410744 Test Acc : 0.39112500101327896\n",
      "\n",
      "Current Iter : 11/150 batch : 7950/8000 acc : 0.44\n",
      " Current : 11 Acc : 0.3916000008583069 Test Acc : 0.3996249999850988\n",
      "\n",
      "Current Iter : 12/150 batch : 7950/8000 acc : 0.42\n",
      " Current : 12 Acc : 0.3922000002861023 Test Acc : 0.4077500001527369\n",
      "\n",
      "Current Iter : 13/150 batch : 7950/8000 acc : 0.44\n",
      " Current : 13 Acc : 0.4067999988794327 Test Acc : 0.40749999936670067\n",
      "\n",
      "Current Iter : 14/150 batch : 7950/8000 acc : 0.44\n",
      " Current : 14 Acc : 0.41080000057816507 Test Acc : 0.4141249998472631\n",
      "\n",
      "Current Iter : 15/150 batch : 7950/8000 acc : 0.44\n",
      " Current : 15 Acc : 0.41780000016093255 Test Acc : 0.414499999769032\n",
      "\n",
      "Current Iter : 16/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 16 Acc : 0.41999999940395355 Test Acc : 0.4180000012740493\n",
      "\n",
      "Current Iter : 17/150 batch : 7950/8000 acc : 0.44\n",
      " Current : 17 Acc : 0.42799999758601187 Test Acc : 0.42162499884143473\n",
      "\n",
      "Current Iter : 18/150 batch : 7950/8000 acc : 0.42\n",
      " Current : 18 Acc : 0.43239999920129774 Test Acc : 0.4276249992661178\n",
      "\n",
      "Current Iter : 19/150 batch : 7950/8000 acc : 0.46\n",
      " Current : 19 Acc : 0.43820000052452085 Test Acc : 0.42437500040978193\n",
      "\n",
      "Current Iter : 20/150 batch : 7950/8000 acc : 0.46\n",
      " Current : 20 Acc : 0.4455999985337257 Test Acc : 0.43249999843537806\n",
      "\n",
      "Current Iter : 21/150 batch : 7950/8000 acc : 0.44\n",
      " Current : 21 Acc : 0.44799999743700025 Test Acc : 0.4266250001266599\n",
      "\n",
      "Current Iter : 22/150 batch : 7950/8000 acc : 0.42\n",
      " Current : 22 Acc : 0.45519999757409096 Test Acc : 0.42724999943748115\n",
      "\n",
      "Current Iter : 23/150 batch : 7950/8000 acc : 0.44\n",
      " Current : 23 Acc : 0.46099999696016314 Test Acc : 0.4408749993890524\n",
      "\n",
      "Current Iter : 24/150 batch : 7950/8000 acc : 0.44\n",
      " Current : 24 Acc : 0.4639999994635582 Test Acc : 0.43925000075250864\n",
      "\n",
      "Current Iter : 25/150 batch : 7950/8000 acc : 0.44\n",
      " Current : 25 Acc : 0.4713999989628792 Test Acc : 0.4381249978207052\n",
      "\n",
      "Current Iter : 26/150 batch : 7950/8000 acc : 0.44\n",
      " Current : 26 Acc : 0.47439999908208846 Test Acc : 0.4389999996870756\n",
      "\n",
      "Current Iter : 27/150 batch : 7950/8000 acc : 0.44\n",
      " Current : 27 Acc : 0.48099999874830246 Test Acc : 0.4441249996423721\n",
      "\n",
      "Current Iter : 28/150 batch : 7950/8000 acc : 0.46\n",
      " Current : 28 Acc : 0.4821999990940094 Test Acc : 0.4501249989494681\n",
      "\n",
      "Current Iter : 29/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 29 Acc : 0.4875999975204468 Test Acc : 0.45362499784678223\n",
      "\n",
      "Current Iter : 30/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 30 Acc : 0.48999999910593034 Test Acc : 0.4482499996200204\n",
      "\n",
      "Current Iter : 31/150 batch : 7950/8000 acc : 0.46\n",
      " Current : 31 Acc : 0.49099999845027925 Test Acc : 0.46212499905377624\n",
      "\n",
      "Current Iter : 32/150 batch : 7950/8000 acc : 0.56\n",
      " Current : 32 Acc : 0.4971999984979629 Test Acc : 0.46150000002235175\n",
      "\n",
      "Current Iter : 33/150 batch : 7950/8000 acc : 0.54\n",
      " Current : 33 Acc : 0.5015999990701675 Test Acc : 0.46137499967589973\n",
      "\n",
      "Current Iter : 34/150 batch : 7950/8000 acc : 0.56\n",
      " Current : 34 Acc : 0.5087999993562698 Test Acc : 0.46737500093877316\n",
      "\n",
      "Current Iter : 35/150 batch : 7950/8000 acc : 0.56\n",
      " Current : 35 Acc : 0.5125999987125397 Test Acc : 0.4725000007078052\n",
      "\n",
      "Current Iter : 36/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 36 Acc : 0.5169999971985817 Test Acc : 0.4726249987259507\n",
      "\n",
      "Current Iter : 37/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 37 Acc : 0.5159999981522561 Test Acc : 0.45399999925866724\n",
      "\n",
      "Current Iter : 38/150 batch : 7950/8000 acc : 0.58\n",
      " Current : 38 Acc : 0.5215999972820282 Test Acc : 0.4743749987334013\n",
      "\n",
      "Current Iter : 39/150 batch : 7950/8000 acc : 0.54\n",
      " Current : 39 Acc : 0.5245999974012375 Test Acc : 0.4754999982193112\n",
      "\n",
      "Current Iter : 40/150 batch : 7950/8000 acc : 0.56\n",
      " Current : 40 Acc : 0.5289999985694885 Test Acc : 0.4839999994263053\n",
      "\n",
      "Current Iter : 41/150 batch : 7950/8000 acc : 0.62\n",
      " Current : 41 Acc : 0.5306000000238419 Test Acc : 0.4808749988675117\n",
      "\n",
      "Current Iter : 42/150 batch : 7950/8000 acc : 0.56\n",
      " Current : 42 Acc : 0.531999998986721 Test Acc : 0.47437499910593034\n",
      "\n",
      "Current Iter : 43/150 batch : 7950/8000 acc : 0.56\n",
      " Current : 43 Acc : 0.539599997997284 Test Acc : 0.47412499953061343\n",
      "\n",
      "Current Iter : 44/150 batch : 7950/8000 acc : 0.58\n",
      " Current : 44 Acc : 0.5436000031232834 Test Acc : 0.4843750011175871\n",
      "\n",
      "Current Iter : 45/150 batch : 7950/8000 acc : 0.54\n",
      " Current : 45 Acc : 0.5514000010490417 Test Acc : 0.48700000029057267\n",
      "\n",
      "Current Iter : 46/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 46 Acc : 0.5506000027060509 Test Acc : 0.47324999924749134\n",
      "\n",
      "Current Iter : 47/150 batch : 7950/8000 acc : 0.56\n",
      " Current : 47 Acc : 0.5549999997019768 Test Acc : 0.48662500102072953\n",
      "\n",
      "Current Iter : 48/150 batch : 7950/8000 acc : 0.54\n",
      " Current : 48 Acc : 0.5621999999880791 Test Acc : 0.49225000012665987\n",
      "\n",
      "Current Iter : 49/150 batch : 7950/8000 acc : 0.58\n",
      " Current : 49 Acc : 0.5620000019669533 Test Acc : 0.4900000000372529\n",
      "\n",
      "Current Iter : 50/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 50 Acc : 0.5616000044345856 Test Acc : 0.491499999165535\n",
      "\n",
      "Current Iter : 51/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 51 Acc : 0.5658000010251999 Test Acc : 0.47762499898672106\n",
      "\n",
      "Current Iter : 52/150 batch : 7950/8000 acc : 0.54\n",
      " Current : 52 Acc : 0.5666000026464463 Test Acc : 0.4901249999180436\n",
      "\n",
      "Current Iter : 53/150 batch : 7950/8000 acc : 0.54\n",
      " Current : 53 Acc : 0.571200003027916 Test Acc : 0.49637499935925006\n",
      "\n",
      "Current Iter : 54/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 54 Acc : 0.5818000024557114 Test Acc : 0.49312499910593033\n",
      "\n",
      "Current Iter : 55/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 55 Acc : 0.5734000042080879 Test Acc : 0.48187500070780515\n",
      "\n",
      "Current Iter : 56/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 56 Acc : 0.5634000021219253 Test Acc : 0.47749999836087226\n",
      "\n",
      "Current Iter : 57/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 57 Acc : 0.5650000023841858 Test Acc : 0.48962499890476463\n",
      "\n",
      "Current Iter : 58/150 batch : 7950/8000 acc : 0.54\n",
      " Current : 58 Acc : 0.5720000043511391 Test Acc : 0.4747499987483025\n",
      "\n",
      "Current Iter : 59/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 59 Acc : 0.5856000036001205 Test Acc : 0.49462500009685756\n",
      "\n",
      "Current Iter : 60/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 60 Acc : 0.5886000028252601 Test Acc : 0.5012499991804361\n",
      "\n",
      "Current Iter : 61/150 batch : 7950/8000 acc : 0.54\n",
      " Current : 61 Acc : 0.5902000045776368 Test Acc : 0.4902499997988343\n",
      "\n",
      "Current Iter : 62/150 batch : 7950/8000 acc : 0.58\n",
      " Current : 62 Acc : 0.5986000034213066 Test Acc : 0.49212499856948855\n",
      "\n",
      "Current Iter : 63/150 batch : 7950/8000 acc : 0.56\n",
      " Current : 63 Acc : 0.5882000008225441 Test Acc : 0.4823749981820583\n",
      "\n",
      "Current Iter : 64/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 64 Acc : 0.6014000016450882 Test Acc : 0.4953749988228083\n",
      "\n",
      "Current Iter : 65/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 65 Acc : 0.5932000014185905 Test Acc : 0.4896249990910292\n",
      "\n",
      "Current Iter : 66/150 batch : 7950/8000 acc : 0.54\n",
      " Current : 66 Acc : 0.6002000012993812 Test Acc : 0.49412499759346246\n",
      "\n",
      "Current Iter : 67/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 67 Acc : 0.5976000046730041 Test Acc : 0.49350000023841856\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Iter : 68/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 68 Acc : 0.6078000015020371 Test Acc : 0.48449999950826167\n",
      "\n",
      "Current Iter : 69/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 69 Acc : 0.5978000026941299 Test Acc : 0.45149999782443045\n",
      "\n",
      "Current Iter : 70/150 batch : 7950/8000 acc : 0.58\n",
      " Current : 70 Acc : 0.59740000218153 Test Acc : 0.44887499827891586\n",
      "\n",
      "Current Iter : 71/150 batch : 7950/8000 acc : 0.56\n",
      " Current : 71 Acc : 0.6056000003218651 Test Acc : 0.4458749979734421\n",
      "\n",
      "Current Iter : 72/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 72 Acc : 0.6136000043153763 Test Acc : 0.46349999979138373\n",
      "\n",
      "Current Iter : 73/150 batch : 7950/8000 acc : 0.54\n",
      " Current : 73 Acc : 0.6184000030159951 Test Acc : 0.4612499974668026\n",
      "\n",
      "Current Iter : 74/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 74 Acc : 0.6196000021696091 Test Acc : 0.4714999977499247\n",
      "\n",
      "Current Iter : 75/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 75 Acc : 0.6240000051259994 Test Acc : 0.48987499848008154\n",
      "\n",
      "Current Iter : 76/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 76 Acc : 0.636400001347065 Test Acc : 0.49199999812990425\n",
      "\n",
      "Current Iter : 77/150 batch : 7950/8000 acc : 0.56\n",
      " Current : 77 Acc : 0.6382000061869622 Test Acc : 0.4779999991878867\n",
      "\n",
      "Current Iter : 78/150 batch : 7950/8000 acc : 0.54\n",
      " Current : 78 Acc : 0.6436000034213066 Test Acc : 0.5024999987334013\n",
      "\n",
      "Current Iter : 79/150 batch : 7950/8000 acc : 0.54\n",
      " Current : 79 Acc : 0.6482000052928925 Test Acc : 0.5034999988973141\n",
      "\n",
      "Current Iter : 80/150 batch : 7950/8000 acc : 0.54\n",
      " Current : 80 Acc : 0.6476000040769577 Test Acc : 0.48437499776482584\n",
      "\n",
      "Current Iter : 81/150 batch : 7950/8000 acc : 0.44\n",
      " Current : 81 Acc : 0.611000003516674 Test Acc : 0.44562499755993484\n",
      "\n",
      "Current Iter : 82/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 82 Acc : 0.6200000050663949 Test Acc : 0.4888749994337559\n",
      "\n",
      "Current Iter : 83/150 batch : 7950/8000 acc : 0.44\n",
      " Current : 83 Acc : 0.6378000023961067 Test Acc : 0.4282499992288649\n",
      "\n",
      "Current Iter : 84/150 batch : 7950/8000 acc : 0.56\n",
      " Current : 84 Acc : 0.6340000060200691 Test Acc : 0.4491249989718199\n",
      "\n",
      "Current Iter : 85/150 batch : 7950/8000 acc : 0.54\n",
      " Current : 85 Acc : 0.6394000053405762 Test Acc : 0.4711249985732138\n",
      "\n",
      "Current Iter : 86/150 batch : 7950/8000 acc : 0.54\n",
      " Current : 86 Acc : 0.6368000018596649 Test Acc : 0.473999999742955\n",
      "\n",
      "Current Iter : 87/150 batch : 7950/8000 acc : 0.58\n",
      " Current : 87 Acc : 0.6306000033020973 Test Acc : 0.4921249996870756\n",
      "\n",
      "Current Iter : 88/150 batch : 7950/8000 acc : 0.56\n",
      " Current : 88 Acc : 0.6446000036597251 Test Acc : 0.49087499976158144\n",
      "\n",
      "Current Iter : 89/150 batch : 7950/8000 acc : 0.56\n",
      " Current : 89 Acc : 0.6542000052332878 Test Acc : 0.4718749985098839\n",
      "\n",
      "Current Iter : 90/150 batch : 7950/8000 acc : 0.56\n",
      " Current : 90 Acc : 0.6616000041365624 Test Acc : 0.4823749979957938\n",
      "\n",
      "Current Iter : 91/150 batch : 7950/8000 acc : 0.58\n",
      " Current : 91 Acc : 0.6584000054001808 Test Acc : 0.4961249977350235\n",
      "\n",
      "Current Iter : 92/150 batch : 7950/8000 acc : 0.54\n",
      " Current : 92 Acc : 0.645600003004074 Test Acc : 0.4933749994263053\n",
      "\n",
      "Current Iter : 93/150 batch : 7950/8000 acc : 0.54\n",
      " Current : 93 Acc : 0.6386000061035156 Test Acc : 0.4886249991133809\n",
      "\n",
      "Current Iter : 94/150 batch : 7950/8000 acc : 0.58\n",
      " Current : 94 Acc : 0.6714000043272972 Test Acc : 0.49024999924004076\n",
      "\n",
      "Current Iter : 95/150 batch : 7950/8000 acc : 0.54\n",
      " Current : 95 Acc : 0.6708000048995018 Test Acc : 0.4911249985918403\n",
      "\n",
      "Current Iter : 96/150 batch : 7950/8000 acc : 0.62\n",
      " Current : 96 Acc : 0.6562000027298928 Test Acc : 0.5131250003352761\n",
      "\n",
      "Current Iter : 97/150 batch : 7950/8000 acc : 0.58\n",
      " Current : 97 Acc : 0.6640000060200691 Test Acc : 0.4871249977499247\n",
      "\n",
      "Current Iter : 98/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 98 Acc : 0.6522000023722648 Test Acc : 0.45800000000745056\n",
      "\n",
      "Current Iter : 99/150 batch : 7950/8000 acc : 0.62\n",
      " Current : 99 Acc : 0.6696000069379806 Test Acc : 0.4976249994710088\n",
      "\n",
      "Current Iter : 100/150 batch : 7950/8000 acc : 0.68\n",
      " Current : 100 Acc : 0.6800000053644181 Test Acc : 0.4967499990016222\n",
      "\n",
      "Current Iter : 101/150 batch : 7950/8000 acc : 0.68\n",
      " Current : 101 Acc : 0.6826000040769578 Test Acc : 0.48699999824166296\n",
      "\n",
      "Current Iter : 102/150 batch : 7950/8000 acc : 0.64\n",
      " Current : 102 Acc : 0.6848000061511993 Test Acc : 0.486124998703599\n",
      "\n",
      "Current Iter : 103/150 batch : 7950/8000 acc : 0.62\n",
      " Current : 103 Acc : 0.6712000027298928 Test Acc : 0.49587499983608724\n",
      "\n",
      "Current Iter : 104/150 batch : 7950/8000 acc : 0.54\n",
      " Current : 104 Acc : 0.6798000055551529 Test Acc : 0.46474999925121663\n",
      "\n",
      "Current Iter : 105/150 batch : 7950/8000 acc : 0.66\n",
      " Current : 105 Acc : 0.678000003695488 Test Acc : 0.4908749984577298\n",
      "\n",
      "Current Iter : 106/150 batch : 7950/8000 acc : 0.66\n",
      " Current : 106 Acc : 0.6888000047206879 Test Acc : 0.47587500009685757\n",
      "\n",
      "Current Iter : 107/150 batch : 7950/8000 acc : 0.58\n",
      " Current : 107 Acc : 0.6848000049591064 Test Acc : 0.48762499671429393\n",
      "\n",
      "Current Iter : 108/150 batch : 7950/8000 acc : 0.62\n",
      " Current : 108 Acc : 0.7020000016689301 Test Acc : 0.48199999909847974\n",
      "\n",
      "Current Iter : 109/150 batch : 7950/8000 acc : 0.68\n",
      " Current : 109 Acc : 0.705400002002716 Test Acc : 0.495749999396503\n",
      "\n",
      "Current Iter : 110/150 batch : 7950/8000 acc : 0.58\n",
      " Current : 110 Acc : 0.716800001859665 Test Acc : 0.48949999883770945\n",
      "\n",
      "Current Iter : 111/150 batch : 7950/8000 acc : 0.68\n",
      " Current : 111 Acc : 0.7186000031232834 Test Acc : 0.4948749991133809\n",
      "\n",
      "Current Iter : 112/150 batch : 7950/8000 acc : 0.54\n",
      " Current : 112 Acc : 0.6630000025033951 Test Acc : 0.48987499848008154\n",
      "\n",
      "Current Iter : 113/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 113 Acc : 0.6862000048160553 Test Acc : 0.49687499888241293\n",
      "\n",
      "Current Iter : 114/150 batch : 7950/8000 acc : 0.68\n",
      " Current : 114 Acc : 0.7062000036239624 Test Acc : 0.4913749979808927\n",
      "\n",
      "Current Iter : 115/150 batch : 7950/8000 acc : 0.62\n",
      " Current : 115 Acc : 0.7162000036239624 Test Acc : 0.49099999852478504\n",
      "\n",
      "Current Iter : 116/150 batch : 7950/8000 acc : 0.64\n",
      " Current : 116 Acc : 0.7230000030994416 Test Acc : 0.4996249996125698\n",
      "\n",
      "Current Iter : 117/150 batch : 7950/8000 acc : 0.62\n",
      " Current : 117 Acc : 0.7300000017881394 Test Acc : 0.4969999993219972\n",
      "\n",
      "Current Iter : 118/150 batch : 7950/8000 acc : 0.64\n",
      " Current : 118 Acc : 0.7348000025749206 Test Acc : 0.4999999998137355\n",
      "\n",
      "Current Iter : 119/150 batch : 7950/8000 acc : 0.62\n",
      " Current : 119 Acc : 0.7403999990224839 Test Acc : 0.4997499991208315\n",
      "\n",
      "Current Iter : 120/150 batch : 7950/8000 acc : 0.68\n",
      " Current : 120 Acc : 0.7439999997615814 Test Acc : 0.4976249998435378\n",
      "\n",
      "Current Iter : 121/150 batch : 7950/8000 acc : 0.58\n",
      " Current : 121 Acc : 0.7398000013828278 Test Acc : 0.5018749989569187\n",
      "\n",
      "Current Iter : 122/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 122 Acc : 0.7038000041246414 Test Acc : 0.4651249980553985\n",
      "\n",
      "Current Iter : 123/150 batch : 7950/8000 acc : 0.64\n",
      " Current : 123 Acc : 0.7158000016212464 Test Acc : 0.5102500000968575\n",
      "\n",
      "Current Iter : 124/150 batch : 7950/8000 acc : 0.62\n",
      " Current : 124 Acc : 0.7324000006914139 Test Acc : 0.5001250011846423\n",
      "\n",
      "Current Iter : 125/150 batch : 7950/8000 acc : 0.56\n",
      " Current : 125 Acc : 0.7408000004291534 Test Acc : 0.5087499994784593\n",
      "\n",
      "Current Iter : 126/150 batch : 7950/8000 acc : 0.58\n",
      " Current : 126 Acc : 0.7008000040054321 Test Acc : 0.4800000017508864\n",
      "\n",
      "Current Iter : 127/150 batch : 7950/8000 acc : 0.62\n",
      " Current : 127 Acc : 0.7266000008583069 Test Acc : 0.5154999986290931\n",
      "\n",
      "Current Iter : 128/150 batch : 7950/8000 acc : 0.54\n",
      " Current : 128 Acc : 0.7659999984502792 Test Acc : 0.515124999731779\n",
      "\n",
      "Current Iter : 129/150 batch : 7950/8000 acc : 0.58\n",
      " Current : 129 Acc : 0.7643999975919723 Test Acc : 0.5081249982118606\n",
      "\n",
      "Current Iter : 130/150 batch : 7950/8000 acc : 0.58\n",
      " Current : 130 Acc : 0.768400000333786 Test Acc : 0.5067499989643693\n",
      "\n",
      "Current Iter : 131/150 batch : 7950/8000 acc : 0.48\n",
      " Current : 131 Acc : 0.7619999957084655 Test Acc : 0.4902499996125698\n",
      "\n",
      "Current Iter : 132/150 batch : 7950/8000 acc : 0.68\n",
      " Current : 132 Acc : 0.7399999988079071 Test Acc : 0.4941249992698431\n",
      "\n",
      "Current Iter : 133/150 batch : 7950/8000 acc : 0.56\n",
      " Current : 133 Acc : 0.7445999997854232 Test Acc : 0.49087500041350723\n",
      "\n",
      "Current Iter : 134/150 batch : 7950/8000 acc : 0.62\n",
      " Current : 134 Acc : 0.7535999977588653 Test Acc : 0.5013749998062849\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Iter : 135/150 batch : 7950/8000 acc : 0.58\n",
      " Current : 135 Acc : 0.7751999974250794 Test Acc : 0.5045000007376075\n",
      "\n",
      "Current Iter : 136/150 batch : 7950/8000 acc : 0.58\n",
      " Current : 136 Acc : 0.7072000020742416 Test Acc : 0.49300000090152024\n",
      "\n",
      "Current Iter : 137/150 batch : 7950/8000 acc : 0.62\n",
      " Current : 137 Acc : 0.7213999998569488 Test Acc : 0.5087499983608723\n",
      "\n",
      "Current Iter : 138/150 batch : 7950/8000 acc : 0.58\n",
      " Current : 138 Acc : 0.7661999988555909 Test Acc : 0.5132499996572732\n",
      "\n",
      "Current Iter : 139/150 batch : 7950/8000 acc : 0.62\n",
      " Current : 139 Acc : 0.7657999992370605 Test Acc : 0.5074999975040555\n",
      "\n",
      "Current Iter : 140/150 batch : 7950/8000 acc : 0.54\n",
      " Current : 140 Acc : 0.7779999989271164 Test Acc : 0.5098749984055757\n",
      "\n",
      "Current Iter : 141/150 batch : 7950/8000 acc : 0.56\n",
      " Current : 141 Acc : 0.7771999979019165 Test Acc : 0.5071250000968576\n",
      "\n",
      "Current Iter : 142/150 batch : 7950/8000 acc : 0.54\n",
      " Current : 142 Acc : 0.7823999977111816 Test Acc : 0.5102500000968575\n",
      "\n",
      "Current Iter : 143/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 143 Acc : 0.7833999961614608 Test Acc : 0.5027499992400408\n",
      "\n",
      "Current Iter : 144/150 batch : 7950/8000 acc : 0.54\n",
      " Current : 144 Acc : 0.7859999966621399 Test Acc : 0.49437499884516\n",
      "\n",
      "Current Iter : 145/150 batch : 7950/8000 acc : 0.62\n",
      " Current : 145 Acc : 0.7837999993562699 Test Acc : 0.4986249992623925\n",
      "\n",
      "Current Iter : 146/150 batch : 7950/8000 acc : 0.58\n",
      " Current : 146 Acc : 0.7753999966382981 Test Acc : 0.4978749997913837\n",
      "\n",
      "Current Iter : 147/150 batch : 7950/8000 acc : 0.52\n",
      " Current : 147 Acc : 0.7453999990224838 Test Acc : 0.5053749999031425\n",
      "\n",
      "Current Iter : 148/150 batch : 7950/8000 acc : 0.62\n",
      " Current : 148 Acc : 0.7795999956130981 Test Acc : 0.5048750005662441\n",
      "\n",
      "Current Iter : 149/150 batch : 7950/8000 acc : 0.58\n",
      " Current : 149 Acc : 0.7835999947786331 Test Acc : 0.49524999875575304\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "sess.run(tf.global_variables_initializer())\n",
    "avg_acc_train = 0; avg_acc_test  = 0; \n",
    "train_acc     = [];test_acc = []\n",
    "for iter in range(num_epoch):\n",
    "\n",
    "    for current_batch_index in range(0,len(train_images),batch_size):\n",
    "        current_data  = train_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = train_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy,gradient_update],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(train_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_train = avg_acc_train + sess_results[0]\n",
    "        \n",
    "    for current_batch_index in range(0,len(test_images), batch_size):\n",
    "        current_data  = test_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = test_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy],feed_dict={x:current_data,y:current_label})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(test_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_test = avg_acc_test + sess_results[0]   \n",
    "        \n",
    "    print(\"\\n Current : \"+ str(iter) + \" Acc : \" + str(avg_acc_train/(len(train_images)/batch_size)) + \" Test Acc : \" + str(avg_acc_test/(len(test_images)/batch_size)) + '\\n')\n",
    "    \n",
    "    # save the training\n",
    "    train_acc.append(avg_acc_train/(len(train_images)/batch_size))\n",
    "    test_acc .append(avg_acc_test/(len(test_images)/batch_size)  )\n",
    "    \n",
    "    avg_acc_train = 0 ; avg_acc_test  = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T06:52:08.136334Z",
     "start_time": "2018-12-20T06:52:08.051288Z"
    }
   },
   "outputs": [],
   "source": [
    "print(sess.run([l1n.getw(),l2n.getw(),l3n.getw(),l4n.getw(),l5n.getw()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "1. mttk/STL10. (2018). GitHub. Retrieved 19 December 2018, from https://github.com/mttk/STL10\n",
    "2. [duplicate], H. (2018). How to display multiple images in one figure correctly?. Stack Overflow. Retrieved 19 December 2018, from https://stackoverflow.com/questions/46615554/how-to-display-multiple-images-in-one-figure-correctly"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
