{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# import library\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys, os,cv2\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from skimage.transform import resize\n",
    "from scipy.misc import imread\n",
    "from imgaug import augmenters as iaa\n",
    "import nibabel as nib\n",
    "import imgaug as ia\n",
    "from scipy.ndimage import zoom\n",
    "from sklearn.utils import shuffle\n",
    "from skimage import util \n",
    "import matplotlib.animation as animation\n",
    "\n",
    "plt.style.use('seaborn-white')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "np.random.seed(6278)\n",
    "tf.set_random_seed(6728)\n",
    "ia.seed(6278)\n",
    "\n",
    "# Generate training data\n",
    "import tensorflow as tf\n",
    "old_v = tf.logging.get_verbosity()\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# get the data as well as show image function\n",
    "from ipynb.fs.full.z_util  import show_images,get_train,get_test,get_unlabel\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "train_data,train_label = get_train()\n",
    "test_data ,test_label  = get_test()\n",
    "unlabeled              = get_unlabel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# one hot encode the labels\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "encoder = OneHotEncoder(sparse =True)\n",
    "train_label_encoded = encoder.fit_transform(train_label[:,None]).toarray()\n",
    "test__label_encoded = encoder.fit_transform(test_label [:,None]).toarray()\n",
    "print(train_label[:3],'\\n',train_label_encoded[:3])\n",
    "print(test_label[:3], '\\n',test__label_encoded[:3])\n",
    "\n",
    "train_data = (train_data - train_data.min((1,2),keepdims=True))/(train_data.max((1,2),keepdims=True)-train_data.min((1,2),keepdims=True))\n",
    "test_data  = (test_data -  test_data.min((1,2),keepdims=True))/( test_data.max((1,2),keepdims=True)- test_data.min((1,2),keepdims=True))\n",
    "\n",
    "print(train_data.mean(),test_data.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# show some images of train test and unlabeld\n",
    "show_images(train_data,train_label)\n",
    "show_images(test_data,test_label)\n",
    "show_images(unlabeled,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# import layers\n",
    "def tf_relu(x):    return tf.nn.relu(x)\n",
    "def d_tf_relu(x):  return tf.cast(tf.greater(x,0),tf.float64)\n",
    "\n",
    "def tf_elu(x):     return tf.nn.elu(x)\n",
    "def d_tf_elu(x):   return tf.cast(tf.greater(x,0),tf.float64)  + (tf_elu(tf.cast(tf.less_equal(x,0),tf.float64) * x) + 1.0)\n",
    "\n",
    "def tf_tanh(x):   return tf.nn.tanh(x)\n",
    "def d_tf_tanh(x): return 1 - tf_tanh(x) ** 2\n",
    "\n",
    "def tf_sigmoid(x):   return tf.nn.sigmoid(x)\n",
    "def d_tf_sigmoid(x): return tf_sigmoid(x) * (1.0-tf_sigmoid(x))\n",
    "\n",
    "def tf_atan(x):   return tf.atan(x)\n",
    "def d_tf_atan(x): return 1.0/(1.0 + x**2)\n",
    "\n",
    "\n",
    "def tf_softmax(x): return tf.nn.softmax(x)\n",
    "\n",
    "class CNN():\n",
    "\n",
    "    def __init__(self,k,inc,out, stddev=0.05,which_reg=0,act=tf_tanh,d_act=d_tf_tanh):\n",
    "        self.w          = tf.Variable(tf.random_normal([k,k,inc,out],stddev=stddev,seed=2,dtype=tf.float64))\n",
    "        self.b          = tf.Variable(tf.zeros(out,dtype=tf.float64))\n",
    "        self.m,self.v   = tf.Variable(tf.zeros_like(self.w)),tf.Variable(tf.zeros_like(self.w))\n",
    "        self.mb,self.vb = tf.Variable(tf.zeros_like(self.b)),tf.Variable(tf.zeros_like(self.b))\n",
    "        self.act,self.d_act = act,d_act\n",
    "        self.which_reg  = which_reg\n",
    "        \n",
    "    def getw(self): return [self.w,self.b]\n",
    "\n",
    "    def feedforward(self,input,stride=1,padding='VALID'):\n",
    "        self.input  = input\n",
    "        self.layer  = tf.nn.conv2d(input,self.w,strides=[1,stride,stride,1],padding=padding) + self.b \n",
    "        self.layerA = self.act(self.layer)\n",
    "        return self.layerA\n",
    "\n",
    "    def backprop(self,gradient,stride=1,padding='VALID'):\n",
    "        grad_part_1 = gradient\n",
    "        grad_part_2 = self.d_act(self.layer)\n",
    "        grad_part_3 = self.input\n",
    "\n",
    "        grad_middle = grad_part_1 * grad_part_2\n",
    "        grad_b      = tf.reduce_mean(grad_middle,(0,1,2))/batch_size\n",
    "        grad        = tf.nn.conv2d_backprop_filter(input = grad_part_3,filter_sizes = tf.shape(self.w),  out_backprop = grad_middle,strides=[1,stride,stride,1],padding=padding) / batch_size\n",
    "        grad_pass   = tf.nn.conv2d_backprop_input (input_sizes = tf.shape(self.input),filter= self.w,out_backprop = grad_middle,strides=[1,stride,stride,1],padding=padding)\n",
    "\n",
    "        if self.which_reg == 0:   grad = grad\n",
    "        if self.which_reg == 0.5: grad = grad + lamda * (tf.sqrt(tf.abs(self.w))) * (1.0/tf.sqrt(tf.abs(self.w)+ 10e-5)) * tf.sign(self.w)\n",
    "        if self.which_reg == 1:   grad = grad + lamda * tf.sign(self.w)\n",
    "        if self.which_reg == 1.5: grad = grad + lamda * 1.0/(tf.sqrt(tf.square(self.w) + 10e-5)) * self.w\n",
    "        if self.which_reg == 2:   grad = grad + lamda * (1.0/tf.sqrt(tf.square(tf.abs(self.w))+ 10e-5)) * tf.abs(self.w) * tf.sign(self.w)\n",
    "        if self.which_reg == 2.5: grad = grad + lamda * 2.0 * self.w\n",
    "        if self.which_reg == 3:   grad = grad + lamda * tf.pow(tf.pow(tf.abs(self.w),3)+ 10e-5,-0.66) * tf.pow(tf.abs(self.w),2) * tf.sign(self.w)\n",
    "        if self.which_reg == 4:   grad = grad + lamda * tf.pow(tf.pow(tf.abs(self.w),4)+ 10e-5,-0.75) * tf.pow(tf.abs(self.w),3) * tf.sign(self.w)\n",
    "\n",
    "        update_w = []\n",
    "        \n",
    "        update_w.append(tf.assign( self.m,self.m*beta1 + (1-beta1) * (grad)   ))\n",
    "        update_w.append(tf.assign( self.v,self.v*beta2 + (1-beta2) * (grad ** 2)   ))\n",
    "        m_hat = self.m / (1-beta1) ; v_hat = self.v / (1-beta2)\n",
    "        adam_middle = m_hat * learning_rate/(tf.sqrt(v_hat) + adam_e)\n",
    "        update_w.append(tf.assign(self.w,tf.subtract(self.w,adam_middle  )))\n",
    "        \n",
    "        update_w.append(tf.assign( self.mb,self.mb*beta1 + (1-beta1) * (grad_b)   ))\n",
    "        update_w.append(tf.assign( self.vb,self.vb*beta2 + (1-beta2) * (grad_b ** 2)   ))\n",
    "        m_hatb = self.mb / (1-beta1) ; v_hatb = self.vb / (1-beta2)\n",
    "        adam_middleb = m_hatb * learning_rate/(tf.sqrt(v_hatb) + adam_e)\n",
    "        update_w.append(tf.assign(self.b,tf.subtract(self.b,adam_middleb  )))\n",
    "        \n",
    "        return grad_pass,update_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters and layers\n",
    "num_epoch = 100; learning_rate = 0.0008; batch_size = 20 \n",
    "beta1,beta2,adam_e = 0.9,0.999,1e-8\n",
    "\n",
    "train_accu_list = [] ; train_cost_list = [] ; train_accu = 0 ; train_cost = 0 ; \n",
    "test__accu_list = [] ; test__cost_list = [] ; test__accu = 0 ; test__cost = 0 ;\n",
    "\n",
    "l1 = CNN(3,3, 32)\n",
    "l2 = CNN(3,32,32)\n",
    "l3 = CNN(3,32,32)\n",
    "\n",
    "l4 = CNN(3,32,64)\n",
    "l5 = CNN(3,64,64)\n",
    "l6 = CNN(3,64,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create graph\n",
    "x = tf.placeholder(shape=[batch_size,96,96,3],dtype=tf.float64)\n",
    "y = tf.placeholder(shape=[batch_size,10],dtype=tf.float64)\n",
    "temperature    = tf.cast(tf.placeholder_with_default(1.0,shape=[]),tf.float64)\n",
    "training_phase = tf.placeholder_with_default(False,shape=())\n",
    "\n",
    "layer1      = l1.feedforward(x)\n",
    "layer2      = l2.feedforward(layer1)\n",
    "layer3      = l3.feedforward(layer2)\n",
    "layer3_pool = tf.nn.avg_pool(layer3,ksize=[1,2,2,1],strides=[1,2,2,1],padding='VALID')\n",
    "\n",
    "layer4      = l4.feedforward(layer3_pool)\n",
    "layer5      = l5.feedforward(layer4)\n",
    "layer6      = l6.feedforward(layer5)\n",
    "\n",
    "final_squeeze_temp = tf.squeeze(tf.reduce_mean(layer6,(1,2)))/temperature\n",
    "final_prediction   = tf_softmax(final_squeeze_temp) \n",
    "cost               = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=final_squeeze_temp,labels=y))\n",
    "accuracy           = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(y, -1), tf.argmax(final_prediction, -1)),tf.float64))\n",
    "\n",
    "grad        = (final_prediction-y)[:,None,None,:]/batch_size\n",
    "grad_expand = tf.tile(grad,[1,39,39,1])/temperature\n",
    "grad6,grad6_up = l6.backprop(grad_expand)\n",
    "grad5,grad5_up = l5.backprop(grad6)\n",
    "grad4,grad4_up = l4.backprop(grad5)\n",
    "\n",
    "grad3_uppool   = tf.tile(grad4,[1,2,2,1])\n",
    "grad3,grad3_up = l3.backprop(grad3_uppool)\n",
    "grad2,grad2_up = l2.backprop(grad3)\n",
    "grad1,grad1_up = l1.backprop(grad2)\n",
    "\n",
    "grad_update    = grad6_up + grad5_up + grad4_up + grad3_up + grad2_up + grad1_up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# start the session\n",
    "# sess = tf.InteractiveSession()\n",
    "\n",
    "restart_fresh = input('Start with new weightes? ').lower().strip()\n",
    "if restart_fresh=='y':  sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for iter in range(num_epoch):\n",
    "    \n",
    "    # Train Data\n",
    "    for current_data_index in range(0,len(train_data),batch_size):\n",
    "        current_data  = train_data[current_data_index:current_data_index+batch_size]\n",
    "        current_label = train_label_encoded[current_data_index:current_data_index+batch_size]  \n",
    "        sess_results  = sess.run([accuracy,cost,grad_update],feed_dict={x:current_data,y:current_label,temperature:1.5})\n",
    "        sys.stdout.write('\\riter: ' + str(iter) + \" index: \" + str(current_data_index) + '/' + str(len(train_data)) +  \" acc: \" + str(sess_results[0]) + ' cost: ' + str(np.around(sess_results[1],3))); sys.stdout.flush()\n",
    "        train_accu = train_accu + sess_results[0]; train_cost = train_cost + sess_results[1]\n",
    "    \n",
    "    # Test Data\n",
    "    \n",
    "    # Append Results\n",
    "    train_accu_list.append(train_accu/(len(train_data)/batch_size)); train_cost_list.append(train_cost/(len(train_data)/batch_size))\n",
    "    \n",
    "    # Print intermidiate\n",
    "    if iter % 1 == 0 : \n",
    "        print(train_accu_list[-5:])\n",
    "        print(train_cost_list[-5:])\n",
    "        print('\\n-------------')\n",
    "    \n",
    "    # reset variable\n",
    "    train_accu = 0; train_cost = 0; test__accu = 0; test__cost = 0; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mid training and testing section\n",
    "mid_test_data    = train_data[:batch_size,:,:,:]; mid_test_label = train_label_encoded[:batch_size,:]\n",
    "mid_sess_results = sess.run(final_prediction,feed_dict={x:mid_test_data,temperature:0.005}) \n",
    "print(np.around(mid_sess_results,3))\n",
    "print(mid_test_label)\n",
    "print('-------------------')\n",
    "print(np.argmax(np.around(mid_sess_results,3),1))\n",
    "print(np.argmax(mid_test_label  ,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "1. file, i. (2018). import a function from another .ipynb file. Stack Overflow. Retrieved 11 November 2018, from https://stackoverflow.com/questions/44116194/import-a-function-from-another-ipynb-file\n",
    "2. array?, H. (2018). How to print the full NumPy array?. Stack Overflow. Retrieved 11 November 2018, from https://stackoverflow.com/questions/1987694/how-to-print-the-full-numpy-array\n",
    "3. Label Encoder vs. One Hot Encoder in Machine Learning. (2018). Medium. Retrieved 11 November 2018, from https://medium.com/@contactsunny/label-encoder-vs-one-hot-encoder-in-machine-learning-3fc273365621\n",
    "4. API Reference — scikit-learn 0.20.0 documentation. (2018). Scikit-learn.org. Retrieved 11 November 2018, from https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing\n",
    "5. Why is there no support for directly computing cross entropy? · Issue #2462 · tensorflow/tensorflow. (2018). GitHub. Retrieved 11 November 2018, from https://github.com/tensorflow/tensorflow/issues/2462\n",
    "6. argmax?, E. (2018). Evaluating softmax (classification) accuracy in Tensorflow with round or argmax?. Stack Overflow. Retrieved 11 November 2018, from https://stackoverflow.com/questions/41759771/evaluating-softmax-classification-accuracy-in-tensorflow-with-round-or-argmax\n",
    "7. tensorflow.placeholder_with_default Python Example. (2018). Programcreek.com. Retrieved 11 November 2018, from https://www.programcreek.com/python/example/90324/tensorflow.placeholder_with_default\n",
    "8. JaeDukSeo/Daily-Neural-Network-Practice-2. (2018). GitHub. Retrieved 11 November 2018, from https://github.com/JaeDukSeo/Daily-Neural-Network-Practice-2/blob/master/NeuralNetwork/ELU/a%20paper%20adam.py\n",
    "9. Python?, H. (2018). How to get last items of a list in Python?. Stack Overflow. Retrieved 11 November 2018, from https://stackoverflow.com/questions/646644/how-to-get-last-items-of-a-list-in-python\n",
    "10. python?, H. (2018). How to have user true/false input in python?. Stack Overflow. Retrieved 11 November 2018, from https://stackoverflow.com/questions/32616548/how-to-have-user-true-false-input-in-python\n",
    "11. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
