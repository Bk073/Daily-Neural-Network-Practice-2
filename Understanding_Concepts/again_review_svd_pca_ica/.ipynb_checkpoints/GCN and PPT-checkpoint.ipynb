{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# import Library and some random image data set\n",
    "import tensorflow as tf\n",
    "import numpy      as np\n",
    "import seaborn    as sns \n",
    "import pandas     as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(78); tf.set_random_seed(78)\n",
    "\n",
    "# get some of the STL data set\n",
    "# Get the certain STL 10 Data\n",
    "from skimage import util \n",
    "from skimage.transform import resize\n",
    "from skimage.io import imread\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def showimages(x,coloums=30,row=3,col=False):\n",
    "    fig=plt.figure(figsize=(30, 3))\n",
    "    columns = coloums; rows = row\n",
    "    for i in range(1, columns*rows +1):\n",
    "        fig.add_subplot(rows, columns, i)\n",
    "        if col: plt.imshow(np.squeeze(x[i-1]))\n",
    "        else:   plt.imshow(np.squeeze(x[i-1]),cmap='gray')\n",
    "        plt.tick_params(axis='both', left=False, top=False, right=False, bottom=False, labelleft=False, labeltop=False, labelright=False, labelbottom=False)\n",
    "        plt.grid(False)\n",
    "    plt.show()\n",
    "\n",
    "data_location = \"../../Dataset/STL10/img/10/\"\n",
    "train_data = []  # create an empty list\n",
    "for dirName, subdirList, fileList in sorted(os.walk(data_location)):\n",
    "    for filename in fileList:\n",
    "        if \".png\" in filename.lower() :\n",
    "            train_data.append(os.path.join(dirName,filename))\n",
    "\n",
    "image_resize_px = 64\n",
    "train_batch = np.zeros(shape=(len(train_data),image_resize_px,image_resize_px,3))\n",
    "for file_index in range(len(train_data)):\n",
    "    train_batch[file_index] = resize(imread(train_data[file_index]),(image_resize_px,image_resize_px))\n",
    "\n",
    "# print out the data shape and the max and min value\n",
    "print('Train batch, min, max : ',train_batch.shape,train_batch.min((0,1,2)),train_batch.max((0,1,2)))\n",
    "train_batch1 = train_batch.copy()\n",
    "\n",
    "data_location = \"../../Dataset/STL10/img/2/\"\n",
    "train_data = []  # create an empty list\n",
    "for dirName, subdirList, fileList in sorted(os.walk(data_location)):\n",
    "    for filename in fileList:\n",
    "        if \".png\" in filename.lower() :\n",
    "            train_data.append(os.path.join(dirName,filename))\n",
    "\n",
    "image_resize_px = 64\n",
    "train_batch = np.zeros(shape=(len(train_data),image_resize_px,image_resize_px,3))\n",
    "for file_index in range(len(train_data)):\n",
    "    train_batch[file_index] = resize(imread(train_data[file_index]),(image_resize_px,image_resize_px))\n",
    "\n",
    "# print out the data shape and the max and min value\n",
    "print('Train batch, min, max : ',train_batch.shape,train_batch.min((0,1,2)),train_batch.max((0,1,2)))\n",
    "train_batch2 = train_batch.copy()\n",
    "\n",
    "data_location = \"../../Dataset/STL10/img/3/\"\n",
    "train_data = []  # create an empty list\n",
    "for dirName, subdirList, fileList in sorted(os.walk(data_location)):\n",
    "    for filename in fileList:\n",
    "        if \".png\" in filename.lower() :\n",
    "            train_data.append(os.path.join(dirName,filename))\n",
    "\n",
    "image_resize_px = 64\n",
    "train_batch = np.zeros(shape=(len(train_data),image_resize_px,image_resize_px,3))\n",
    "for file_index in range(len(train_data)):\n",
    "    train_batch[file_index] = resize(imread(train_data[file_index]),(image_resize_px,image_resize_px))\n",
    "\n",
    "# print out the data shape and the max and min value\n",
    "print('Train batch, min, max : ',train_batch.shape,train_batch.min((0,1,2)),train_batch.max((0,1,2)))\n",
    "train_batch3 = train_batch.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the random images\n",
    "from sklearn.utils import shuffle\n",
    "train_batch_final = np.vstack((train_batch1,train_batch2,train_batch3))\n",
    "train_batch_final = shuffle(train_batch_final)\n",
    "print(train_batch_final.shape)\n",
    "showimages(shuffle(train_batch_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_iden(x):  return x\n",
    "def d_tf_iden(x):return 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a. Mean Layer - Subtract the Mean of the Data Row Wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# test out the mean layer (reset the graph)\n",
    "tf.reset_default_graph()\n",
    "class tf_mean_layer():\n",
    "    \n",
    "    def __init__(self,vector_shape):\n",
    "        self.moving_mean = tf.Variable(tf.zeros(shape=[vector_shape,1],dtype=tf.float64))\n",
    "        \n",
    "    def feedforward(self,input,training_phase=True):\n",
    "        self.input = input\n",
    "        \n",
    "        # Training Moving Average Mean         \n",
    "        def training_fn():\n",
    "            using_mean         = tf.reduce_mean(input,axis=-1)[:,None]\n",
    "            centered_data      = self.input - using_mean\n",
    "            return centered_data,tf.assign( self.moving_mean,self.moving_mean*0.9 + 0.1 * using_mean )\n",
    "        \n",
    "        # Testing Moving Average Mean        \n",
    "        def  testing_fn():\n",
    "            centered_data      = self.input - self.moving_mean\n",
    "            return centered_data,tf.assign(self.moving_mean,self.moving_mean)\n",
    "        \n",
    "        self.output,update_mean = tf.cond(training_phase,true_fn=training_fn,false_fn=testing_fn)\n",
    "        return self.output,update_mean\n",
    "    \n",
    "    def backprop(self,grad):\n",
    "        return grad - 1.0/grad.shape[1].value\n",
    "    \n",
    "# make the place holder and start the session\n",
    "x     = tf.placeholder(shape=(100,64*64),dtype=tf.float64)\n",
    "phase = tf.placeholder(shape=[],dtype=tf.bool)\n",
    "\n",
    "mean_layer             = tf_mean_layer(100)\n",
    "mean_data,update_mean  = mean_layer.feedforward(x,training_phase=phase)\n",
    "mean_grad              = mean_layer.backprop(tf.ones_like(mean_data))\n",
    "\n",
    "sess = tf.InteractiveSession();  sess.run(tf.global_variables_initializer())\n",
    "mean_result,mean_update,mean_gradient = sess.run([mean_data,update_mean,mean_grad],feed_dict={x:train_batch_final[:100,:,:,0].reshape((100,-1)),phase:True })\n",
    "showimages(mean_result.reshape((100,64,64)),col=False)\n",
    "\n",
    "print('Before Mean Layer : ',train_batch_final[:100,:,:,0].mean(-1).sum());  print('After  Mean Layer : ',mean_result.mean(-1).sum())\n",
    "print('Data    Mean : ',     train_batch_final[:100,:,:,0].mean());          print('Updated Mean : ',   mean_update.mean())\n",
    "print('Gradient of Mean: ',  mean_gradient.mean() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b. Standard Deviation Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "class tf_std_layer():\n",
    "    \n",
    "    def __init__(self,vector_shape):\n",
    "        self.moving_mean = tf.Variable(tf.zeros(shape=(vector_shape,1),dtype=tf.float64))\n",
    "        \n",
    "    def feedforward(self,input,training_phase=True):\n",
    "        self.input = input\n",
    "\n",
    "        self.using_mean     = tf.reduce_mean(input,axis=-1)[:,None]\n",
    "        self.centered_data  = tf.square(self.input - self.using_mean)\n",
    "        self.square_data    = tf.reduce_sum(self.centered_data,-1) \n",
    "        self.sqrt           = tf.sqrt(self.square_data / (self.input.shape[1].value -1 ) ) \n",
    "\n",
    "        # Training Moving Average Mean         \n",
    "        def training_fn():\n",
    "            return self.sqrt,tf.assign( self.moving_mean,self.moving_mean*0.9 + 0.1 * self.using_mean )\n",
    "        \n",
    "        # Testing Moving Average Mean        \n",
    "        def  testing_fn():\n",
    "            centered_data  = tf.square(self.input - self.moving_mean)\n",
    "            square_data    = tf.reduce_sum(centered_data,-1) \n",
    "            sqrt           = tf.sqrt(square_data / (self.input.shape[1].value -1 ) ) \n",
    "            return sqrt,     tf.assign(self.moving_mean,self.moving_mean)\n",
    "        \n",
    "        self.output,update_mean = tf.cond(training_phase,true_fn=training_fn,false_fn=testing_fn)\n",
    "        return self.output,update_mean\n",
    "    \n",
    "    def backprop(self,grad):\n",
    "        grad1 = grad\n",
    "        grad2 = 1.0/(self.sqrt) *(1/ (self.input.shape[1].value -1) )\n",
    "#         grad3 = tf.sqrt(self.centered_data) * (1 - 1.0/self.input.shape[1].value)\n",
    "        \n",
    "        # this grad3 is for gradient check only don't use this         \n",
    "        grad3 = (1 - 1.0/self.input.shape[1].value)\n",
    "        grad_pass = (grad1 * grad2)[:,None] * grad3\n",
    "        return grad_pass      \n",
    "    \n",
    "# make the place holder and start the session\n",
    "x     = tf.placeholder(shape=(5,64*64),dtype=tf.float64)\n",
    "phase = tf.placeholder(shape=[],dtype=tf.bool)\n",
    "\n",
    "current_layer                = tf_std_layer(5)\n",
    "current_data,current_update  = current_layer.feedforward(x,training_phase=phase)\n",
    "current_grad                 = current_layer.backprop(tf.ones_like(current_data))\n",
    "\n",
    "sess = tf.InteractiveSession();  sess.run(tf.global_variables_initializer())\n",
    "current_result,current_r_update,current_gradient = sess.run([current_data,current_update,current_grad],feed_dict={x:train_batch_final[:5,:,:,0].reshape((5,-1)),phase:True })\n",
    "\n",
    "print(\"Data std: \",train_batch_final[:5,:,:,0].reshape((5,-1)).std(-1) )\n",
    "print(\"Updated Moving Mean: \",current_r_update.mean())\n",
    "print('TF   std: ',current_result )\n",
    "print('Gradient of STD: ',current_gradient )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C. Normalization Layer - Range (Min - Max) - User Can Set this Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     3,
     9,
     38
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "class tf_min_max_layer():\n",
    "    \n",
    "    def __init__(self,vector_shape,user_max=1.0,user_min=0.0):\n",
    "        self.moving_min = tf.Variable(tf.zeros(shape=(vector_shape,1),dtype=tf.float64))\n",
    "        self.moving_max = tf.Variable(tf.zeros(shape=(vector_shape,1),dtype=tf.float64))\n",
    "        self.user_min   = tf.Variable(user_min,dtype=tf.float64); \n",
    "        self.user_max   = tf.Variable(user_max,dtype=tf.float64); \n",
    "        \n",
    "    def feedforward(self,input,training_phase):\n",
    "        self.input    = input\n",
    "        self.min_vec  = tf.reduce_min(input,-1)[:,None]\n",
    "        self.min_index= tf.argmin(input,-1)\n",
    "        self.max_vec  = tf.reduce_max(input,-1)[:,None]\n",
    "        self.max_index= tf.argmax(input,-1)\n",
    "        \n",
    "        def training_fn():\n",
    "            normalized_data = (self.user_max-self.user_min)  * \\\n",
    "            ((self.input - self.min_vec)/(self.max_vec - self.min_vec))          + self.user_min\n",
    "            \n",
    "            update_min_max = []\n",
    "            update_min_max.append(tf.assign(self.moving_min,self.moving_min * 0.9 + 0.1 * self.min_vec))\n",
    "            update_min_max.append(tf.assign(self.moving_max,self.moving_max * 0.9 + 0.1 * self.max_vec))\n",
    "            return normalized_data,update_min_max\n",
    "        \n",
    "        # Testing Moving Average Mean        \n",
    "        def  testing_fn():\n",
    "            normalized_data = (self.user_max-self.user_min) * \\\n",
    "            ((self.input - self.moving_min)/(self.moving_max - self.moving_min)) + self.user_min\n",
    "            \n",
    "            update_min_max = []\n",
    "            update_min_max.append(tf.assign(self.moving_min,self.moving_min))\n",
    "            update_min_max.append(tf.assign(self.moving_max,self.moving_max))\n",
    "            return normalized_data,update_min_max\n",
    "        \n",
    "        self.output,update_min_max = tf.cond(training_phase,true_fn=training_fn,false_fn=testing_fn)\n",
    "        return self.output,update_min_max\n",
    "    \n",
    "    def backprop(self,grad):\n",
    "        grad1   = grad\n",
    "        \n",
    "        # Create Mask for min / max value for row\n",
    "        indices = tf.range(0, self.input.shape[0].value,dtype=tf.int64)\n",
    "        min_indices = tf.stack([indices, self.min_index], axis=1)\n",
    "        max_indices = tf.stack([indices, self.max_index], axis=1)\n",
    "        grad_min = tf.cast(tf.sparse_to_dense(min_indices, self.input.shape, sparse_values=1, default_value=0),dtype=tf.float64)\n",
    "        grad_max = tf.cast(tf.sparse_to_dense(max_indices, self.input.shape, sparse_values=1, default_value=0),dtype=tf.float64)\n",
    "        \n",
    "        grad_max_min = 1.0/(self.max_vec-self.min_vec)\n",
    "        grad_pass    = grad1 * (self.user_max-self.user_min) * (\n",
    "            grad_max_min + \\\n",
    "            (self.input - self.max_vec)/tf.square(grad_max_min) * grad_min + \\\n",
    "            (self.min_vec - self.input)/tf.square(grad_max_min) * grad_max\n",
    "        )\n",
    "        \n",
    "        # Again do not RETURN grad_magrad_passx_min correct gradient is grad_pass\n",
    "        return grad_max_min\n",
    "        \n",
    "# make the place holder and start the session\n",
    "x     = tf.placeholder(shape=(5,64*64),dtype=tf.float64)\n",
    "phase = tf.placeholder(shape=[],dtype=tf.bool)\n",
    "\n",
    "current_layer                = tf_min_max_layer(5,user_max=400.0,user_min=0.0)\n",
    "current_data,current_update  = current_layer.feedforward(x,training_phase=phase)\n",
    "current_grad                 = current_layer.backprop(tf.ones_like(current_data))\n",
    "        \n",
    "sess = tf.InteractiveSession();  sess.run(tf.global_variables_initializer())\n",
    "vectorized_train_data = train_batch_final[:5,:,:,0].reshape((5,-1))\n",
    "current_result,current_update,current_gradient = sess.run([current_data,current_update,current_grad],feed_dict={x:vectorized_train_data,phase:True}) \n",
    "\n",
    "\n",
    "print('Original Data Min : ',vectorized_train_data.min(-1))\n",
    "print('Original Data Max : ',vectorized_train_data.max(-1))\n",
    "\n",
    "print('After Layer Min : ',current_result.min(-1))\n",
    "print('After Layer Max : ',current_result.max(-1))\n",
    "\n",
    "print('Moving Min :      ',current_update[0].T)\n",
    "print('Moving Max :      ',current_update[1].T)\n",
    "\n",
    "print('OG Data 1.0(Max - Min) : ',1.0/(vectorized_train_data.max(-1)-vectorized_train_data.min(-1) ) )\n",
    "print('Min Max Gradient: ',current_gradient.T )\n",
    "\n",
    "print('OG    Data Mean and STD: ',vectorized_train_data.mean(),vectorized_train_data.std())\n",
    "print('After Data Mean and STD: ',current_result.mean(),current_result.std())\n",
    "\n",
    "showimages(vectorized_train_data.reshape((5,64,64)),coloums=5,row=1)\n",
    "showimages(current_result.reshape((5,64,64)),coloums=5,row=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D. Global Contrast Normalization Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Modified code from: https://datascience.stackexchange.com/questions/15110/how-to-implement-global-contrast-normalization-in-python\n",
    "def global_contrast_normalization(image):\n",
    "    XX = image\n",
    "\n",
    "    # replacement for the loop\n",
    "    X_average = np.mean(XX)\n",
    "    XX = XX - X_average\n",
    "    \n",
    "    ss   = 1.0\n",
    "    lmda = 10.\n",
    "    # `su` is here the mean, instead of the sum\n",
    "    contrast = np.sqrt(lmda + np.mean(XX**2)).astype(np.float64)\n",
    "    \n",
    "    if contrast > 1e-8:\n",
    "        XX = ss * XX / contrast\n",
    "    else:\n",
    "        XX = ss * XX \n",
    "\n",
    "    return XX\n",
    "\n",
    "vectorized_train_data_color = train_batch_final[:5,:,:,:].reshape((5,-1))\n",
    "gcn_numpy = np.asarray([global_contrast_normalization(x) for x in vectorized_train_data_color]).reshape((5,64,64,3))\n",
    "\n",
    "print('Before GCN Data Mean and STD')\n",
    "print(vectorized_train_data_color.mean(-1).mean(),vectorized_train_data_color.std(-1).mean() )\n",
    "showimages(vectorized_train_data_color.reshape((5,64,64,3)),coloums=5,row=1,col=True)\n",
    "\n",
    "print('After GCN Data Mean and STD - VIEWING IMAGE IS NORMALIZED')\n",
    "print(gcn_numpy.mean(axis=(1,2,3)).mean(),gcn_numpy.std(axis=(1,2,3)).mean() )\n",
    "gcn_numpy = (gcn_numpy - gcn_numpy.min(axis=(1,2))[:,None,None,:] )/(gcn_numpy.max(axis=(1,2))-gcn_numpy.min(axis=(1,2)))[:,None,None,:]\n",
    "showimages(gcn_numpy.reshape((5,64,64,3)),coloums=5,row=1,col=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     17,
     21
    ]
   },
   "outputs": [],
   "source": [
    "# tf layer implementation\n",
    "tf.reset_default_graph()\n",
    "class global_contrast_layer():\n",
    "    \n",
    "    def __init__(self,vector_shape,aimed_std=1.0):\n",
    "        self.moving_mean = tf.Variable(tf.zeros(shape=(vector_shape,1),dtype=tf.float64))\n",
    "        self.aimed_std   = tf.Variable(aimed_std,dtype=tf.float64)\n",
    "        self.reg_lamda   = 10.0; self.EPS         = 1e-8\n",
    "        \n",
    "    def feedforward(self,input,training_phase):\n",
    "        self.input      = input\n",
    "        self.image_mean = tf.reduce_mean(self.input,-1)[:,None]\n",
    "        self.centered   = self.input - self.image_mean\n",
    "        self.square     = tf.square(self.centered)\n",
    "        self.whole_mean = tf.reduce_mean(self.square,-1)[:,None]\n",
    "        self.sqrt       = tf.sqrt(self.reg_lamda + self.whole_mean) + self.EPS\n",
    "        \n",
    "        def training_fn():\n",
    "            gcn_data = self.aimed_std * (self.centered/self.sqrt)\n",
    "            return gcn_data,tf.assign(self.moving_mean,self.moving_mean * 0.9 + 0.1 * self.image_mean)\n",
    "        \n",
    "        def  testing_fn():\n",
    "            centered_data  = self.input - self.moving_mean\n",
    "            squared_data   = tf.square(centered_data)\n",
    "            whole_mean_data= tf.reduce_mean(squared_data,-1)[:,None]\n",
    "            sqrt           = tf.sqrt(self.reg_lamda + whole_mean_data) + self.EPS\n",
    "            gcn_data       = self.aimed_std * (centered_data/self.sqrt)\n",
    "            return gcn_data, tf.assign(self.moving_mean,self.moving_mean)\n",
    "        \n",
    "        self.output,update_gcn_mean = tf.cond(training_phase,true_fn=training_fn,false_fn=testing_fn)\n",
    "        return self.output,tf.assign(self.moving_mean,self.moving_mean)        \n",
    "        \n",
    "    def backprop(self,grad):\n",
    "        grad_1 = grad\n",
    "        grad_2 = self.aimed_std / self.sqrt\n",
    "        grad_3 = -(self.centered ** 2 /self.sqrt ** 2) * (1.0/(self.sqrt-self.EPS)) / (self.input.shape[1].value)\n",
    "        grad_pass = grad_1 * (grad_2 + grad_3) * (1-1./self.input.shape[1].value)\n",
    "        return grad_pass\n",
    "    \n",
    "# make the place holder and start the session\n",
    "x     = tf.placeholder(shape=(5,64*64*3),dtype=tf.float64)\n",
    "phase = tf.placeholder(shape=[],dtype=tf.bool)\n",
    "\n",
    "current_layer                = global_contrast_layer(5)\n",
    "current_data,current_update  = current_layer.feedforward(x,training_phase=phase)\n",
    "current_grad                 = current_layer.backprop(tf.ones_like(current_data))\n",
    "\n",
    "sess = tf.InteractiveSession();  sess.run(tf.global_variables_initializer())\n",
    "current_result,current_update,current_gradient = sess.run([current_data,current_update,current_grad],feed_dict={x:vectorized_train_data_color,phase:True}) \n",
    "\n",
    "print('Before GCN Data Mean and STD',vectorized_train_data_color.mean(-1).mean(),vectorized_train_data_color.std(-1).mean() )\n",
    "showimages(vectorized_train_data_color.reshape((5,64,64,3)),coloums=5,row=1,col=True)\n",
    "\n",
    "print('Data Mean and STD after layer',current_result.mean(-1).mean(),current_result.std(-1).mean())\n",
    "current_result_reshape = current_result.reshape((5,64,64,3))\n",
    "current_result_reshape = (current_result_reshape - current_result_reshape.min(axis=(1,2))[:,None,None,:] )/(current_result_reshape.max(axis=(1,2))-current_result_reshape.min(axis=(1,2)))[:,None,None,:]\n",
    "showimages(current_result_reshape.reshape((5,64,64,3)),coloums=5,row=1,col=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E. Instance Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showimages(train_batch_final[:5,:,:,0].reshape((5,64,64,1)),coloums=5,row=1,col=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F. Reconstructive PCA Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "sess.close(); tf.reset_default_graph()\n",
    "class Reconstructive_PCA():\n",
    "    \n",
    "    def __init__(self,n_components):\n",
    "        self.n_components= tf.Variable(n_components,dtype=tf.int32)\n",
    "        \n",
    "    def feedforward(self,input):\n",
    "        self.input  = input\n",
    "        self.cov    = tf.matmul(self.input,tf.transpose(self.input)) / (self.input.shape[1].value-1)\n",
    "        self.eigenvalues,self.eigenvectors  = tf.linalg.eigh(self.cov)\n",
    "        self.projection_vector              = self.eigenvectors[:,-self.n_components:]\n",
    "        self.reduced= tf.matmul(tf.transpose(self.projection_vector),self.input)\n",
    "        self.reconst= tf.matmul(self.projection_vector,self.reduced)\n",
    "        return self.reconst     \n",
    "    \n",
    "    def backprop(self,grad):\n",
    "        grad1   = grad\n",
    "        grad_A    = grad1 @ tf.transpose(tf.transpose(self.projection_vector) @ self.input) + \\\n",
    "                     tf.transpose(tf.transpose(self.projection_vector)@grad1@tf.transpose(self.input))\n",
    "        diff      = self.input.shape[0].value - self.n_components\n",
    "        added_mat = tf.zeros([self.input.shape[0].value,diff],dtype=tf.float64)\n",
    "        grad_A_add= tf.concat([added_mat,grad_A],1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        grad_B    = self.eigenvectors @ ( (tf.transpose(self.eigenvectors)@ grad_A_add)) @ tf.transpose(self.eigenvectors)\n",
    "        grad_c    = (grad_B @ self.input + tf.transpose(tf.transpose(self.input) @ grad_B))/(self.input.shape[1].value-1)\n",
    "        return self.eigenvalues\n",
    "        \n",
    "# make the place holder and start the session\n",
    "x     = tf.placeholder(shape=(5,64*64),dtype=tf.float64)\n",
    "\n",
    "current_layer     = Reconstructive_PCA(2)\n",
    "current_data      = current_layer.feedforward(x)\n",
    "current_grad      = current_layer.backprop(tf.ones_like(current_data))\n",
    "        \n",
    "sess = tf.InteractiveSession() ; sess.run(tf.global_variables_initializer())\n",
    "vectorized_train_data = train_batch_final[:5,:,:,0].reshape((5,-1))\n",
    "current_result,current_gradient = sess.run([current_data,current_grad],feed_dict={x:vectorized_train_data}) \n",
    "\n",
    "print('Recon Data Mean and STD: ',current_result.mean(),current_result.std())\n",
    "print(\"Gradient of Data : \",      current_gradient.mean())\n",
    "showimages(current_result.reshape((5,64,64,1)),coloums=5,row=1,col=False)\n",
    "showimages(train_batch_final[:5,:,:,0].reshape((5,64,64,1)),coloums=5,row=1,col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Sanity Check using High Level API\n",
    "from sklearn.decomposition import PCA\n",
    "sk_pca = PCA(2)\n",
    "sk_pca_data = sk_pca.fit_transform(vectorized_train_data.T).T\n",
    "sk_pca_inve = sk_pca.inverse_transform(sk_pca_data.T).T\n",
    "print('Mean and STD of High Level API (Inverse): ',sk_pca_inve.mean(),sk_pca_inve.std())\n",
    "showimages(sk_pca_inve.reshape((5,64,64,1)),coloums=5,row=1,col=False)\n",
    "showimages(vectorized_train_data.reshape((5,64,64,1)),coloums=5,row=1,col=False)\n",
    "print('Mean and STD of High Level API (Inverse): ',sk_pca_inve.mean(),sk_pca_inve.std())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
