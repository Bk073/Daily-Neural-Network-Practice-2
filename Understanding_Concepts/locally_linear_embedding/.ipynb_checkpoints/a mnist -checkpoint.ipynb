{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all of the needed\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys, os,cv2\n",
    "from sklearn.utils import shuffle\n",
    "from scipy.misc import imread,imresize\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from skimage.transform import resize\n",
    "from imgaug import augmenters as iaa\n",
    "import imgaug as ia\n",
    "from scipy.ndimage import zoom\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "np.random.seed(0)\n",
    "np.set_printoptions(precision = 3,suppress =True)\n",
    "old_v = tf.logging.get_verbosity()\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0,
     16
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../Dataset/MNIST/train-images-idx3-ubyte.gz\n",
      "Extracting ../../Dataset/MNIST/train-labels-idx1-ubyte.gz\n",
      "Extracting ../../Dataset/MNIST/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../Dataset/MNIST/t10k-labels-idx1-ubyte.gz\n",
      "(55000, 784)\n",
      "0.0 1.0\n",
      "(55000, 10)\n",
      "0.0 1.0\n",
      "(10000, 784)\n",
      "0.0 1.0\n",
      "(10000, 10)\n",
      "0.0 1.0\n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "# read the data\n",
    "mnist = input_data.read_data_sets('../../Dataset/MNIST/',one_hot=True)\n",
    "train_data, train_label, test_data, test_label = mnist.train.images, mnist.train.labels, mnist.test.images, mnist.test.labels\n",
    "\n",
    "# Show some details and vis some of them\n",
    "print(train_data.shape)\n",
    "print(train_data.min(),train_data.max())\n",
    "print(train_label.shape)\n",
    "print(train_label.min(),train_label.max())\n",
    "print(test_data.shape)\n",
    "print(test_data.min(),test_data.max())\n",
    "print(test_label.shape)\n",
    "print(test_label.min(),test_label.max())\n",
    "print('-----------------------')\n",
    "\n",
    "# view the data\n",
    "def view_images(data,row=10,col=10,color=False):\n",
    "    fig=plt.figure(figsize=(10, 10))\n",
    "    columns = col;    rows = row\n",
    "    for i in range(1, columns*rows +1):\n",
    "        fig.add_subplot(rows, columns, i)\n",
    "        if color:plt.imshow(data[i-1])\n",
    "        else:    plt.imshow(data[i-1],cmap='gray')\n",
    "        plt.grid(False)\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "code_folding": [
     152,
     251,
     320,
     453
    ]
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pylab\n",
    "USE_SVD = True\n",
    "\n",
    "######################################################################\n",
    "#  Locally Linear Embedding\n",
    "######################################################################\n",
    "def dimensionality(M,k,v=0.9,quiet=False):\n",
    "    M   = numpy.matrix(M)\n",
    "    d,N = M.shape\n",
    "    assert k<=N\n",
    "    m_estimate = []\n",
    "    var_total = 0\n",
    "    for row in range(N):\n",
    "        if row%500==0:print('finished %s out of %s' % (row,N))\n",
    "        #-----------------------------------------------\n",
    "        #  find k nearest neighbors\n",
    "        #-----------------------------------------------\n",
    "        M_Mi = numpy.array(M-M[:,row])\n",
    "        vec = (M_Mi**2).sum(0)\n",
    "        nbrs = numpy.argsort(vec)[1:k+1]\n",
    "        \n",
    "        #compute distances\n",
    "        x = numpy.matrix(M[:,nbrs] - M[:,row])\n",
    "        #singular values of x give the variance:\n",
    "        # use this to compute intrinsic dimensionality\n",
    "        sig2 = (numpy.linalg.svd(x,compute_uv=0))**2\n",
    "\n",
    "        #sig2 is sorted from large to small\n",
    "        \n",
    "        #use sig2 to compute intrinsic dimensionality of the\n",
    "        # data at this neighborhood.  The dimensionality is the\n",
    "        # number of eigenvalues needed to sum to the total\n",
    "        # desired variance\n",
    "        sig2 /= sig2.sum()\n",
    "        S = sig2.cumsum()\n",
    "        m = S.searchsorted(v)\n",
    "        if m>0:\n",
    "            m += ( (v-S[m-1])/sig2[m] )\n",
    "        else:\n",
    "            m = v/sig2[m]\n",
    "        m_estimate.append(m)\n",
    "        \n",
    "        r = numpy.sum(sig2[int(m):])\n",
    "        var_total += r\n",
    "\n",
    "    if not quiet: print('average variance conserved: %.3g' % (1.0 - var_total/N))\n",
    "\n",
    "    return m_estimate\n",
    "\n",
    "def LLE(M,k,m,quiet=False):\n",
    "    \"\"\"\n",
    "    Perform a Locally Linear Embedding analysis on M\n",
    "    \n",
    "    >> LLE(M,k,d,quiet=False)\n",
    "    \n",
    "     - M is a numpy array of rank (d,N), consisting of N\n",
    "        data points in d dimensions.\n",
    "     - k is the number of neighbors to use in the embedding\n",
    "     - m is the number of dimensions to which the dataset will\n",
    "        be reduced.\n",
    "    Based on the algorithm outlined in\n",
    "     'An Introduction to Locally Linear Embedding'\n",
    "        by L. Saul and S. Roewis\n",
    "    Using imrovements suggested in\n",
    "     'Locally Linear Embedding for Classification'\n",
    "        by D. deRidder and R.P.W. Duin\n",
    "    \"\"\"\n",
    "    M = numpy.matrix(M)\n",
    "    d,N = M.shape\n",
    "    assert k<N\n",
    "    if not quiet:\n",
    "        print('performing LLE on %i points in %i dimensions...' % (N,d))\n",
    "\n",
    "    #build the weight matrix\n",
    "    W = numpy.zeros((N,N))\n",
    "\n",
    "    if not quiet:\n",
    "        print(' - constructing [%i x %i] weight matrix...' % W.shape)\n",
    "\n",
    "    m_estimate = []\n",
    "    var_total = 0.0\n",
    "    \n",
    "    for row in range(N):\n",
    "        #-----------------------------------------------\n",
    "        #  find k nearest neighbors\n",
    "        #-----------------------------------------------\n",
    "        M_Mi = numpy.array(M-M[:,row])\n",
    "        vec = (M_Mi**2).sum(0)\n",
    "        nbrs = numpy.argsort(vec)[1:k+1]\n",
    "        \n",
    "        #-----------------------------------------------\n",
    "        #  compute weight vector based on neighbors\n",
    "        #-----------------------------------------------\n",
    "\n",
    "        #compute covariance matrix of distances\n",
    "        M_Mi = numpy.matrix(M_Mi[:,nbrs])\n",
    "        Q = M_Mi.T * M_Mi\n",
    "\n",
    "        #singular values of M_Mi give the variance:\n",
    "        # use this to compute intrinsic dimensionality\n",
    "        sig2 = (numpy.linalg.svd(M_Mi,compute_uv=0))**2\n",
    "\n",
    "        #use sig2 to compute intrinsic dimensionality of the\n",
    "        # data at this neighborhood.  The dimensionality is the\n",
    "        # number of eigenvalues needed to sum to the total\n",
    "        # desired variance\n",
    "        v=0.9\n",
    "        sig2 /= sig2.sum()\n",
    "        S = sig2.cumsum()\n",
    "        m_est = S.searchsorted(v)\n",
    "        if m_est>0:\n",
    "            m_est += ( (v-S[m_est-1])/sig2[m_est] )\n",
    "        else:\n",
    "            m_est = v/sig2[m_est]\n",
    "        m_estimate.append(m_est)\n",
    "        \n",
    "        #Covariance matrix may be nearly singular:\n",
    "        # add a diagonal correction to prevent numerical errors\n",
    "        # correction is equal to the sum of the (d-m) unused variances\n",
    "        #  (as in deRidder & Duin)\n",
    "        r = numpy.sum(sig2[m:])\n",
    "        var_total += r\n",
    "        Q.flat[::k+1] += r\n",
    "        #Note that Roewis et al instead uses \"a correction that \n",
    "        #   is small compared to the trace\":\n",
    "        #r = 0.001 * float(Q.trace())\n",
    "    \n",
    "        #solve for weight\n",
    "        w = numpy.linalg.solve(Q,numpy.ones(Q.shape[0]))\n",
    "        w /= numpy.sum(w)\n",
    "\n",
    "        #update row of the weight matrix\n",
    "        W[row,nbrs] = w\n",
    "\n",
    "    if not quiet:\n",
    "        print(' - finding [%i x %i] null space of weight matrix...' % (m,N))\n",
    "    #to find the null space, we need the bottom d+1\n",
    "    #  eigenvectors of (W-I).T*(W-I)\n",
    "    #Compute this using the svd of (W-I):\n",
    "    I = numpy.identity(W.shape[0])\n",
    "    U,sig,VT = numpy.linalg.svd(W-I,full_matrices=0)\n",
    "    indices = numpy.argsort(sig)[1:m+1]\n",
    "\n",
    "    print('m_estimate: %.2f +/- %.2f' % (numpy.median(m_estimate),numpy.std(m_estimate)))\n",
    "    print('average variance conserved: %.3g' % (1.0 - var_total/N))\n",
    "    \n",
    "    return numpy.array(VT[indices,:])\n",
    "\n",
    "######################################################################\n",
    "#  Modified Locally Linear Embedding\n",
    "######################################################################\n",
    "def MLLE(X,k,d_out,TOL = 1E-12):\n",
    "    \"\"\"\n",
    "    perfrom Modified LLE on X\n",
    "    \"\"\"\n",
    "    \n",
    "    X = numpy.asarray(X)\n",
    "    d_in,N = X.shape\n",
    "    assert d_out < d_in\n",
    "    assert k >= d_out\n",
    "    assert k < N\n",
    "\n",
    "    #some variables to hold needed values\n",
    "    rho = numpy.zeros(N)\n",
    "    w_reg = numpy.zeros([N,k])\n",
    "    evals = numpy.zeros([N,k])\n",
    "    V = [0 for i in range(N)]\n",
    "    neighbors = numpy.zeros([N,k],dtype=int)\n",
    "\n",
    "    #some functions to simplify the code\n",
    "    column_vector = lambda x: x.reshape( (x.size,1) )\n",
    "    one = lambda d: numpy.ones((d,1))\n",
    "\n",
    "    for i in range(N):\n",
    "        #find neighbors\n",
    "        X_Xi = X - column_vector( X[:,i] )\n",
    "        neighbors[i] = numpy.argsort( (X_Xi**2).sum(0) )[1:k+1]\n",
    "\n",
    "        #find regularized weights: this is like normal LLE\n",
    "        Gi = X_Xi[ : , neighbors[i] ]\n",
    "        Qi = numpy.dot(Gi.T,Gi)\n",
    "\n",
    "        Qi.flat[::k+1] += 1E-3 #* Qi.trace()\n",
    "\n",
    "        y = numpy.linalg.solve(Qi,numpy.ones(k))\n",
    "        w_reg[i] = y/y.sum()\n",
    "\n",
    "        #find the eigenvectors and eigenvalues of Gi.T*Gi\n",
    "        # using SVD\n",
    "        # we want V[i] to be a [k x k] matrix, where the columns\n",
    "        # are eigenvectors of Gi^T * G\n",
    "        V[i],sig,UT = numpy.linalg.svd(Gi.T)\n",
    "        evals[i][:len(sig)] = sig**2\n",
    "\n",
    "        #compute rho_i : this is used to determine eta, the\n",
    "        # cutoff used to determine the size of the \"almost null\"\n",
    "        # space of the local covariance matrices.\n",
    "        rho[i] = (evals[i,d_out:]).sum() / (evals[i,:d_out]).sum()\n",
    "\n",
    "    #find eta - the median of the N rho values\n",
    "    rho.sort()\n",
    "    eta = rho[int(N/2)]\n",
    "\n",
    "    #The next loop calculates Phi.\n",
    "    # This is the [N x N] matrix whose null space is the desired embedding\n",
    "    Phi = numpy.zeros( (N,N) )\n",
    "    for i in range(N):\n",
    "        #determine si - the size of the largest set of eigenvalues\n",
    "        # of Qi such that satisfies:\n",
    "        #    sum(in_set)/sum(not_in_set) < eta\n",
    "        # with the constraint that 0<si<=k-d_out\n",
    "\n",
    "        si = 1\n",
    "        while si < k-d_out:\n",
    "            this_eta = sum( evals[i,k-si:] ) / sum( evals[i,:k-si] )\n",
    "            if this_eta > eta:\n",
    "                if(si!=1): si -= 1\n",
    "                break\n",
    "            else:\n",
    "                si+=1\n",
    "\n",
    "        #select bottom si eigenvectors of Qi\n",
    "        # and calculate alpha\n",
    "        Vi = V[i][:,k-si:]\n",
    "        alpha_i = numpy.linalg.norm( Vi.sum(0) )/numpy.sqrt(si)\n",
    "\n",
    "        #compute Householder matrix which satisfies\n",
    "        #  Hi*Vi.T*one(k) = alpha_i*one(s)\n",
    "        # using proscription from paper\n",
    "        h = alpha_i * one(si) - numpy.dot(Vi.T,one(k))\n",
    "\n",
    "        nh = numpy.linalg.norm(h)\n",
    "        if nh < TOL:\n",
    "            h = numpy.zeros( (si,1) )\n",
    "        else:\n",
    "            h /= nh\n",
    "            \n",
    "        Hi = numpy.identity(si) - 2*numpy.dot(h,h.T)\n",
    "\n",
    "        Wi = numpy.dot(Vi,Hi) + (1-alpha_i) * column_vector(w_reg[i])\n",
    "\n",
    "        W_hat = numpy.zeros( (N,si) )\n",
    "        W_hat[neighbors[i],:] = Wi\n",
    "        W_hat[i]-=1\n",
    "            \n",
    "        Phi += numpy.dot(W_hat,W_hat.T)\n",
    "        \n",
    "    U,sig,VT = numpy.linalg.svd(Phi)\n",
    "    return VT[-d_out-1:-1]\n",
    "\n",
    "def new_LLE_pts(M,M_LLE,k,x):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "       - M: a rank [d * N] data-matrix\n",
    "       - M_LLE: a rank [m * N] matrixwhich is the output of LLE(M,k,m)\n",
    "       - k: the number of neighbors used to produce M_LLE\n",
    "       - x: a length d data vector OR a rank [d * Nx] array\n",
    "    returns:\n",
    "       - y: the LLE reconstruction of x\n",
    "    \"\"\"\n",
    "    M = numpy.matrix(M)\n",
    "    M_LLE = numpy.matrix(M_LLE)\n",
    "\n",
    "    d,N  = M.shape\n",
    "    m,N2 = M_LLE.shape\n",
    "    assert N==N2\n",
    "\n",
    "    #make sure x is a column vector\n",
    "    if numpy.rank(x) == 1:\n",
    "        x = numpy.matrix(x).T\n",
    "    else:\n",
    "        x = numpy.matrix(x)\n",
    "    assert x.shape[0] == d\n",
    "    Nx = x.shape[1]\n",
    "\n",
    "    W = numpy.matrix(numpy.zeros([Nx,N]))\n",
    "\n",
    "    for i in range(x.shape[1]):\n",
    "        #  find k nearest neighbors\n",
    "        M_xi = numpy.array(M-x[:,i])\n",
    "        vec = (M_xi**2).sum(0)\n",
    "        nbrs = numpy.argsort(vec)[1:k+1]\n",
    "        \n",
    "        #compute covariance matrix of distances\n",
    "        M_xi = numpy.matrix(M_xi[:,nbrs])\n",
    "        Q = M_xi.T * M_xi\n",
    "\n",
    "        #singular values of x give the variance:\n",
    "        # use this to compute intrinsic dimensionality\n",
    "        sig2 = (numpy.linalg.svd(M_xi,compute_uv=0))**2\n",
    "    \n",
    "        #Covariance matrix may be nearly singular:\n",
    "        # add a diagonal correction to prevent numerical errors\n",
    "        # correction is equal to the sum of the (d-m) unused variances\n",
    "        #  (as in deRidder & Duin)\n",
    "        r = numpy.sum(sig2[m:])\n",
    "        Q += r*numpy.identity(Q.shape[0])\n",
    "        #Note that Roewis et al instead uses \"a correction that \n",
    "        #   is small compared to the trace\":\n",
    "        #r = 0.001 * float(Q.trace())\n",
    "    \n",
    "        #solve for weight\n",
    "        w = numpy.linalg.solve(Q,numpy.ones((Q.shape[0],1)))[:,0]\n",
    "        w /= numpy.sum(w)\n",
    "\n",
    "        W[i,nbrs] = w\n",
    "        print('x[%i]: variance conserved: %.2f' % (i,1.0- sig2[m:].sum()))\n",
    "\n",
    "    #multiply weights by projections of neighbors to get y\n",
    "    print(M_LLE.shape)\n",
    "    print(W.shape)\n",
    "    print(len(nbrs))\n",
    "    \n",
    "    return numpy.array( M_LLE  * numpy.matrix(W).T )\n",
    "\n",
    "######################################################################\n",
    "#  Hessian Locally Linear Embedding\n",
    "######################################################################\n",
    "\n",
    "def HLLE(M,k,d,quiet=False):\n",
    "    \"\"\"\n",
    "    Perform a Hessian Eigenmapping analysis on M\n",
    "    >> HLLE(M,k,d,quiet=False)\n",
    "    \n",
    "     - M is a numpy array of rank (dim,N), consisting of N\n",
    "        data points in dim dimensions.\n",
    "     - k is the number of neighbors to use in the embedding\n",
    "     - d is the number of dimensions to which the dataset will\n",
    "        be reduced.\n",
    "    \n",
    "    Implementation based on algorithm outlined in\n",
    "     'Hessian Eigenmaps: new locally linear embedding techniques\n",
    "      for high-dimensional data'\n",
    "        by C. Grimes and D. Donoho, March 2003\n",
    "    \"\"\"\n",
    "    M = numpy.matrix(M)\n",
    "    dim,N = M.shape\n",
    "    \n",
    "    if not quiet:\n",
    "        print('performing HLLE on %i points in %i dimensions...' % (N,dim))\n",
    "    \n",
    "    dp = d*(d+1)/2\n",
    "    W = numpy.matrix( numpy.zeros([dp*N,N]) )\n",
    "    \n",
    "    if not quiet:\n",
    "        print(' - constructing [%i x %i] weight matrix...' % W.shape)\n",
    "        \n",
    "    for i in range(N):\n",
    "        #-----------------------------------------------\n",
    "        #  find nearest neighbors\n",
    "        #-----------------------------------------------\n",
    "        M_Mi = numpy.array(M-M[:,i])\n",
    "        vec = sum(M_Mi*M_Mi,0)\n",
    "        nbrs = numpy.argsort(vec)[1:k+1]\n",
    "\n",
    "        #-----------------------------------------------\n",
    "        #  center the neighborhood using the mean\n",
    "        #-----------------------------------------------\n",
    "        nbrhd = M[:,nbrs]\n",
    "        nbrhd -= nbrhd.mean(1)\n",
    "\n",
    "        #-----------------------------------------------\n",
    "        #  compute local coordinates\n",
    "        #   using a singular value decomposition\n",
    "        #-----------------------------------------------\n",
    "        U,vals,VT = numpy.linalg.svd(nbrhd,full_matrices=0)\n",
    "        nbrhd = numpy.matrix( (VT.T)[:,:d] )\n",
    "\n",
    "        #-----------------------------------------------\n",
    "        #  build Hessian estimator\n",
    "        #-----------------------------------------------\n",
    "        ct = 0\n",
    "        Yi = numpy.matrix(numpy.zeros([k,dp]))\n",
    "        \n",
    "        for mm in range(d):\n",
    "            for nn in range(mm,d):\n",
    "                Yi[:,ct] = numpy.multiply(nbrhd[:,mm],nbrhd[:,nn])\n",
    "                ct += 1\n",
    "        Yi = numpy.concatenate( [numpy.tile(1,(k,1)), nbrhd, Yi],1 )\n",
    "\n",
    "        #-----------------------------------------------\n",
    "        #  orthogonalize linear and quadratic forms\n",
    "        #   with QR factorization\n",
    "        #  make the weights sum to 1\n",
    "        #-----------------------------------------------\n",
    "        Q,R = mgs(Yi)\n",
    "        w = numpy.array(Q[:,d+1:].T)\n",
    "        S = w.sum(1) #sum along rows\n",
    "\n",
    "        #if S[i] is too small, set it equal to 1.0\n",
    "        S[numpy.where(numpy.abs(S)<0.0001)] = 1.0\n",
    "        W[ i*dp:(i+1)*dp , nbrs ] = (w.T/S).T\n",
    "\n",
    "    #-----------------------------------------------\n",
    "    # To find the null space, we want the\n",
    "    #  first d+1 eigenvectors of W.T*W\n",
    "    # Compute this using an svd of W\n",
    "    #-----------------------------------------------\n",
    "    if not quiet:\n",
    "        print(' - computing [%i x %i] null space of weight matrix...' % (d,N))\n",
    "\n",
    "    #Fast, but memory intensive\n",
    "    if USE_SVD:\n",
    "        U,sig,VT = numpy.linalg.svd(W,full_matrices=0)\n",
    "        del U\n",
    "        indices = numpy.argsort(sig)[1:d+1]\n",
    "        Y = VT[indices,:] * numpy.sqrt(N)\n",
    "\n",
    "    #Slower, but uses less memory\n",
    "    else:\n",
    "        C = W.T*W\n",
    "        del W\n",
    "        sig2,V = numpy.linalg.eigh(C)\n",
    "        del C\n",
    "        indices = range(1,d+1) #sig2 is sorted in ascending order\n",
    "        Y = V[:,indices].T * numpy.sqrt(N)\n",
    "\n",
    "    #-----------------------------------------------\n",
    "    # Normalize Y\n",
    "    #  we need R = (Y.T*Y)^(-1/2)\n",
    "    #   do this with an SVD of Y\n",
    "    #      Y = U*sig*V.T\n",
    "    #      Y.T*Y = (V*sig.T*U.T) * (U*sig*V.T)\n",
    "    #            = U*(sig*sig.T)*U.T\n",
    "    #   so\n",
    "    #      R = V * sig^-1 * V.T\n",
    "    #-----------------------------------------------\n",
    "    if not quiet:\n",
    "        print(' - normalizing null space via SVD...')\n",
    "\n",
    "    #Fast, but memory intensive\n",
    "    if USE_SVD:\n",
    "        U,sig,VT = numpy.linalg.svd(Y,full_matrices=0)\n",
    "        del U\n",
    "        S = numpy.matrix(numpy.diag(sig**-1))\n",
    "        R = VT.T * S * VT\n",
    "        return numpy.array(Y*R)\n",
    "\n",
    "    #Slower, but uses less memory\n",
    "    else:\n",
    "        C = Y*Y.T\n",
    "        sig2,U = numpy.linalg.eigh(C)\n",
    "        U = U[:,::-1] #eigenvectors should be in descending order\n",
    "        sig2=sig2[::-1]\n",
    "        S = numpy.matrix(numpy.zeros(U.shape))\n",
    "        for i in range(d):\n",
    "            S[i,i] = (1.0*sig2[i])**-1.5\n",
    "        return numpy.array(C * U * S * U.T * Y)\n",
    "\n",
    "######################################################################\n",
    "#  Modified Gram-Schmidt\n",
    "######################################################################\n",
    "def mgs(A):\n",
    "    \"\"\"\n",
    "    Modified Gram-Schmidt version of QR factorization\n",
    "    returns matrices Q,R such that A = Q*R\n",
    "    where Q is an orthogonal matrix,\n",
    "          R is an upper-right triangular matrix\n",
    "    \"\"\"\n",
    "    #copy A and make sure it's a matrix\n",
    "    Q = 1.0*numpy.matrix(A)\n",
    "    m,n = Q.shape\n",
    "    #assume m>=n\n",
    "    R = numpy.matrix(numpy.zeros([n,n]))\n",
    "    for i in range(n):\n",
    "        v = Q[:,i]\n",
    "        R[i,i] = numpy.sqrt(numpy.sum(numpy.multiply(v,v)))\n",
    "        Q[:,i] /= R[i,i]\n",
    "        for j in range(i+1,n):\n",
    "            R[i,j] = Q[:,i].T * Q[:,j]\n",
    "            Q[:,j] -= R[i,j] * Q[:,i]\n",
    "\n",
    "    return Q,R\n",
    "\n",
    "# code from: https://github.com/jakevdp/pyLLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished 0 out of 100\n",
      "average variance conserved: 0.875\n",
      "(100, 784)\n",
      "100\n",
      "5.551718944029661\n",
      "performing LLE on 100 points in 784 dimensions...\n",
      " - constructing [100 x 100] weight matrix...\n",
      " - finding [20 x 100] null space of weight matrix...\n",
      "m_estimate: 15.51 +/- 1.15\n",
      "average variance conserved: 0.937\n",
      "(20, 100)\n"
     ]
    }
   ],
   "source": [
    "# get small data and test\n",
    "small_data    = train_data[:100,:]\n",
    "small_estiate = dimensionality(small_data.T,10)\n",
    "\n",
    "print(small_data.shape)\n",
    "print(len(small_estiate))\n",
    "print(small_estiate[0])\n",
    "\n",
    "small_estiate = LLE(small_data.T,40,20)\n",
    "print(small_estiate.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "1. matplotlib, S. (2018). Seaborn configuration hides default matplotlib. Stack Overflow. Retrieved 12 November 2018, from https://stackoverflow.com/questions/33099348/seaborn-configuration-hides-default-matplotlib\n",
    "2. locally linear embedding - swiss roll example. (2018). Gist. Retrieved 12 November 2018, from https://gist.github.com/fabianp/934363\n",
    "3. Pedregosa, F. (2011). Locally linear embedding and sparse eigensolvers. Fa.bianp.net. Retrieved 12 November 2018, from http://fa.bianp.net/blog/2011/locally-linear-embedding-and-sparse-eigensolvers/\n",
    "4. jakevdp/pyLLE. (2018). GitHub. Retrieved 12 November 2018, from https://github.com/jakevdp/pyLLE\n",
    "5. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
