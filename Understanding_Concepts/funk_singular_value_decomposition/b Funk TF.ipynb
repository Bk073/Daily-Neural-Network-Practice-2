{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T22:22:36.963138Z",
     "start_time": "2018-11-18T22:22:20.327609Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning:\n",
      "\n",
      "This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import lib\n",
    "import numpy      as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go\n",
    "from   plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "\n",
    "from tensorflow import float32\n",
    "\n",
    "# data set \n",
    "import sklearn.datasets as datasets\n",
    "\n",
    "init_notebook_mode(connected=True); np.random.seed(6789); tf.set_random_seed(67890); plt.style.use('seaborn')\n",
    "plt.rcParams['figure.figsize'] = (25.0, 10.0)\n",
    "\n",
    "# ==== fromm the file =====\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from time import gmtime, strftime\n",
    "\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "from pylab import *\n",
    "import pandas as pd\n",
    "import feather\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T22:22:37.036940Z",
     "start_time": "2018-11-18T22:22:37.006023Z"
    },
    "code_folding": [
     2,
     93,
     101,
     119,
     127,
     136,
     154,
     181,
     187
    ]
   },
   "outputs": [],
   "source": [
    "# Given a set of ratings, 2 matrix factors that include one or more\n",
    "# trainable variables, and a regularizer, uses gradient descent to learn the best values of the trainable variables.\n",
    "def mf(ratings_train, ratings_val, W, H, regularizer, mean_rating, max_iter, lr = 0.01, decay_lr = False, log_summaries = False):\n",
    "    # Extract info from training and validation data\n",
    "    rating_values_tr, num_ratings_tr, user_indices_tr, item_indices_tr = extract_rating_info(ratings_train)\n",
    "    rating_values_val, num_ratings_val, user_indices_val, item_indices_val = extract_rating_info(ratings_val)\n",
    "\n",
    "    # Multiply the factors to get our result as a dense matrix\n",
    "    result = tf.matmul(W, H)\n",
    "\n",
    "    # Now we just want the values represented by the pairs of user and item\n",
    "    # indices for which we had known ratings.\n",
    "    result_values_tr = tf.gather(tf.reshape(result, [-1]), user_indices_tr * tf.shape(result)[1] + item_indices_tr, name=\"extract_training_ratings\")\n",
    "    result_values_val = tf.gather(tf.reshape(result, [-1]), user_indices_val * tf.shape(result)[1] + item_indices_val, name=\"extract_validation_ratings\")\n",
    "\n",
    "    # Calculate the difference between the predicted ratings and the actual\n",
    "    # ratings. The predicted ratings are the values obtained form the matrix\n",
    "    # multiplication with the mean rating added on.\n",
    "    diff_op = tf.sub(tf.add(result_values_tr, mean_rating, name=\"add_mean\"), rating_values_tr, name=\"raw_training_error\")\n",
    "    diff_op_val = tf.sub(tf.add(result_values_val, mean_rating, name=\"add_mean_val\"), rating_values_val, name=\"raw_validation_error\")\n",
    "\n",
    "    with tf.name_scope(\"training_cost\") as scope:\n",
    "        base_cost = tf.reduce_sum(tf.square(diff_op, name=\"squared_difference\"), name=\"sum_squared_error\")\n",
    "\n",
    "        cost = tf.div(tf.add(base_cost, regularizer), num_ratings_tr * 2, name=\"average_error\")\n",
    "\n",
    "    with tf.name_scope(\"validation_cost\") as scope:\n",
    "        cost_val = tf.div(tf.reduce_sum(tf.square(diff_op_val, name=\"squared_difference_val\"), name=\"sum_squared_error_val\"), num_ratings_val * 2, name=\"average_error\")\n",
    "\n",
    "    with tf.name_scope(\"train\") as scope:\n",
    "        if decay_lr:\n",
    "            # Use an exponentially decaying learning rate.\n",
    "            global_step = tf.Variable(0, trainable=False)\n",
    "            learning_rate = tf.train.exponential_decay(lr, global_step, 10000, 0.96, staircase=True)\n",
    "            optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "            # Passing global_step to minimize() will increment it at each step \n",
    "            # so that the learning rate will be decayed at the specified \n",
    "            # intervals.\n",
    "            train_step = optimizer.minimize(cost, global_step=global_step)\n",
    "        else:\n",
    "            optimizer = tf.train.GradientDescentOptimizer(lr)\n",
    "            train_step = optimizer.minimize(cost)\n",
    "\n",
    "    with tf.name_scope(\"training_rmse\") as scope:\n",
    "      rmse_tr = tf.sqrt(tf.reduce_sum(tf.square(diff_op)) / num_ratings_tr)\n",
    "\n",
    "    with tf.name_scope(\"validation_rmse\") as scope:\n",
    "      # Validation set rmse:\n",
    "      rmse_val = tf.sqrt(tf.reduce_sum(tf.square(diff_op_val)) / num_ratings_val)\n",
    "\n",
    "    # Create a TensorFlow session and initialize variables.\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "\n",
    "    if log_summaries:\n",
    "        # Make sure summaries get written to the logs.\n",
    "        accuracy_val_summary = tf.scalar_summary(\"accuracy_val\", accuracy_val)\n",
    "        accuracy_tr_summary = tf.scalar_summary(\"accuracy_tr\", accuracy_tr)\n",
    "        summary_op = tf.merge_all_summaries()\n",
    "        writer = tf.train.SummaryWriter(\"/tmp/recommender_logs\", sess.graph_def)\n",
    "    # Keep track of cost difference.\n",
    "    last_cost = 0\n",
    "    diff = 1\n",
    "    # Run the graph and see how we're doing on every 1000th iteration.\n",
    "    for i in range(max_iter):\n",
    "        if i > 0 and i % 1000 == 0:\n",
    "            if diff < 0.000001:\n",
    "                print(\"Converged at iteration %s\" % (i))\n",
    "                break;\n",
    "            if log_summaries:\n",
    "                res = sess.run([rmse_tr, rmse_val, cost, summary_op])\n",
    "                summary_str = res[3]\n",
    "                writer.add_summary(summary_str, i)\n",
    "            else:\n",
    "                res = sess.run([rmse_tr, rmse_val, cost])\n",
    "            acc_tr = res[0]\n",
    "            acc_val = res[1]\n",
    "            cost_ev = res[2]\n",
    "            print(\"Training RMSE at step %s: %s\" % (i, acc_tr))\n",
    "            print(\"Validation RMSE at step %s: %s\" % (i, acc_val))\n",
    "            diff = abs(cost_ev - last_cost)\n",
    "            last_cost = cost_ev\n",
    "        else:\n",
    "            sess.run(train_step)\n",
    "\n",
    "    finalTrain = rmse_tr.eval(session=sess)\n",
    "    finalVal = rmse_val.eval(session=sess)\n",
    "    finalW = W.eval(session=sess)\n",
    "    finalH = H.eval(session=sess)\n",
    "    sess.close()\n",
    "    return finalTrain, finalVal, finalW, finalH\n",
    "\n",
    "# Extracts user indices, item indices, rating values and number of ratings from the ratings triplets.\n",
    "def extract_rating_info(ratings):\n",
    "    rating_values = np.array(ratings[:,2], dtype=float32)\n",
    "    user_indices = ratings[:,0]\n",
    "    item_indices = ratings[:,1]\n",
    "    num_ratings = len(item_indices)\n",
    "    return rating_values, num_ratings, user_indices, item_indices\n",
    "\n",
    "# Creates a trainable tensor representing either user or item bias, and a corresponding tensor of 1's for the other.\n",
    "def create_factors_for_bias(num_users, num_items, lda, user_bias = True):\n",
    "    if user_bias:\n",
    "        # Random normal intialized column for users\n",
    "        W = tf.Variable(tf.truncated_normal([num_users, 1], stddev=0.02, mean=0), name=\"users\")\n",
    "        # Row of 1's for items\n",
    "        H = tf.ones((1, num_items), name=\"items\")\n",
    "        # Add regularization.\n",
    "        regularizer = tf.mul(tf.reduce_sum(tf.square(W)), lda, name=\"regularize\")\n",
    "    else:\n",
    "        # Column of 1's for users\n",
    "        W = tf.ones((num_users, 1), name=\"users\")\n",
    "        # Random normal intialized row for items\n",
    "        H = tf.Variable(tf.truncated_normal([1, num_items], stddev=0.02, mean=0), name=\"items\")\n",
    "        # Add regularization.\n",
    "        regularizer = tf.mul(tf.reduce_sum(tf.square(H)), lda, name=\"regularize\")\n",
    "    return W, H, regularizer\n",
    "\n",
    "# Runs the factorizer for the given number of iterations and with the given regularization parameter to learn item bias on top of provided user bias.\n",
    "def learn_item_bias_from_fixed_user_bias(ratings_tr, ratings_val, user_bias, num_items, lda, global_mean, max_iter):\n",
    "    W = tf.concat(1, [tf.convert_to_tensor(user_bias, dtype=float32, name=\"user_bias\"), tf.ones((user_bias.shape[0],1), dtype=float32, name=\"item_bias_ones\")])\n",
    "    H = tf.Variable(tf.truncated_normal([1, num_items], stddev=0.02, mean=0), name=\"items\")\n",
    "    H_with_user_bias = tf.concat(0, [tf.ones((1, num_items), name=\"user_bias_ones\", dtype=float32), H])\n",
    "    regularizer = tf.mul(tf.reduce_sum(tf.square(H)), lda, name=\"regularize\")\n",
    "    return mf(ratings_tr, ratings_val, W, H_with_user_bias, regularizer, global_mean, max_iter, 0.8)\n",
    "\n",
    "# Learns factors of the given rank with specified regularization parameter.\n",
    "def create_factors_without_biases(num_users, num_items, rank, lda):\n",
    "    # Initialize the matrix factors from random normals with mean 0. W will\n",
    "    # represent users and H will represent items.\n",
    "    W = tf.Variable(tf.truncated_normal([num_users, rank], stddev=0.02, mean=0), name=\"users\")\n",
    "    H = tf.Variable(tf.truncated_normal([rank, num_items], stddev=0.02, mean=0), name=\"items\")\n",
    "    regularizer = tf.mul(tf.add(tf.reduce_sum(tf.square(W)), tf.reduce_sum(tf.square(H))), lda, name=\"regularize\")\n",
    "    return W, H, regularizer\n",
    "\n",
    "# Given previously learned user bias and item bias vectors, creates tensors to learn factors of the given rank (excluding the bias vectors) and a regularizer.\n",
    "def create_factors_with_biases(user_bias, item_bias, rank, lda):\n",
    "    num_users = user_bias.shape[0]\n",
    "    num_items = item_bias.shape[1]\n",
    "    # Initialize the matrix factors from random normals with mean 0. W will\n",
    "    # represent users and H will represent items.\n",
    "    W = tf.Variable(tf.truncated_normal([num_users, rank], stddev=0.02, mean=0), name=\"users\")\n",
    "    H = tf.Variable(tf.truncated_normal([rank, num_items], stddev=0.02, mean=0), name=\"items\")\n",
    "\n",
    "    # To the user matrix we add a bias column holding the bias of each user,\n",
    "    # and another column of 1s to multiply the item bias by.\n",
    "    W_plus_bias = tf.concat(1, [W, tf.convert_to_tensor(user_bias, dtype=float32, name=\"user_bias\"), tf.ones((num_users,1), dtype=float32, name=\"item_bias_ones\")])\n",
    "    # To the item matrix we add a row of 1s to multiply the user bias by, and\n",
    "    # a bias row holding the bias of each item.\n",
    "    H_plus_bias = tf.concat(0, [H, tf.ones((1, num_items), name=\"user_bias_ones\", dtype=float32), tf.convert_to_tensor(item_bias, dtype=float32, name=\"item_bias\")])\n",
    "    regularizer = tf.mul(tf.add(tf.reduce_sum(tf.square(W)), tf.reduce_sum(tf.square(H))), lda, name=\"regularize\")\n",
    "    return W_plus_bias, H_plus_bias, regularizer\n",
    "\n",
    "# Uses k-fold cross-validation to learn the best regularization parameter to use for either user or item bias.\n",
    "def learn_bias_lda(ratings, num_folds, ldas, num_users, num_items, global_mean, max_iter, user_bias = True):\n",
    "    labels = ratings[:,2]\n",
    "    skf = StratifiedKFold(labels, num_folds)\n",
    "    min_lda = None\n",
    "    min_rmse = 0\n",
    "    for lda in ldas:\n",
    "        sum_rmses = 0\n",
    "        W, H, reg = create_factors_for_bias(num_users, num_items, lda, user_bias)\n",
    "        for train, test in skf:\n",
    "            tr, val, finalw, finalh = mf(ratings[train,:], ratings[test,:], W, H, reg, global_mean, max_iter, 0.8)\n",
    "            sum_rmses += val\n",
    "            print(\"Training rmse: %s, val rmse: %s, lda: %s\" % (tr, val, lda))\n",
    "        avg_rmse = sum_rmses / num_folds\n",
    "        if min_lda == None:\n",
    "            # This is our first lambda.\n",
    "            min_lda = lda\n",
    "            min_rmse = avg_rmse\n",
    "        elif avg_rmse < min_rmse:\n",
    "            # We did better than the last lambda.\n",
    "            min_rmse = avg_rmse\n",
    "            min_lda = lda\n",
    "        else:\n",
    "            # It's not going to get any better with the next lambda.\n",
    "            break\n",
    "    return min_lda\n",
    "\n",
    "# Runs the factorizer for the given number of iterations and with the given regularization parameter to learn user bias from the training set.\n",
    "def get_user_bias(ratings_tr, ratings_val, lda, num_users, num_items, global_mean, max_iter):\n",
    "    W, H, reg = create_factors_for_bias(num_users, num_items, lda, True)\n",
    "    tr, val, finalw, finalh = mf(ratings_tr, ratings_val, W, H, reg, global_mean, max_iter, 0.8)\n",
    "    return finalw\n",
    "\n",
    "# Runs the factorizer for the given number of iterations and with the given regularization parameter to learn item bias from the training set.\n",
    "def get_item_bias(ratings_tr, ratings_val, lda, num_users, num_items, global_mean, max_iter):\n",
    "    W, H, reg = create_factors_for_bias(num_users, num_items, lda, False)\n",
    "    tr, val, finalw, finalh = mf(ratings_tr, ratings_val, W, H, reg, global_mean, max_iter, 0.8)\n",
    "    return finalh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T22:24:42.580554Z",
     "start_time": "2018-11-18T22:24:42.487580Z"
    }
   },
   "outputs": [],
   "source": [
    "# read the data and start\n",
    "np.random.seed(12)\n",
    "ratings_df  = feather.read_dataframe('../../DataSet/Movie_Ratings/U_data.feather')\n",
    "num_ratings = ratings_df.shape[0]\n",
    "ratings     = np.concatenate((\n",
    "    np.array(ratings_df['user_id'], dtype=pd.Series).reshape(num_ratings, 1), \n",
    "    np.array(ratings_df['item_id'], dtype=pd.Series).reshape(num_ratings, 1), \n",
    "    np.array(ratings_df['rating'],  dtype=pd.Series).reshape(num_ratings, 1)), axis=1)\n",
    "ratings     = ratings.astype(np.float64)\n",
    "global_mean = np.mean(ratings[:,2])\n",
    "ratings_tr, ratings_val = train_test_split(ratings, train_size=.7)\n",
    "\n",
    "max_iter  = 100\n",
    "to_learn  = \"features-only\"\n",
    "num_users = np.unique(ratings[:,0]).shape[0]; num_items = np.unique(ratings[:,1]).shape[0]\n",
    "lda = float(10); rank = int(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-16T15:14:41.450039Z",
     "start_time": "2018-11-16T15:14:41.444066Z"
    }
   },
   "source": [
    "<img src=\"https://i.imgur.com/OnfUsTo.png\">\n",
    "python factorizer.py ~/Desktop/ratings_triplets.feather 100 features-only 10.0 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-16T15:18:03.090865Z",
     "start_time": "2018-11-16T15:18:02.857438Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
