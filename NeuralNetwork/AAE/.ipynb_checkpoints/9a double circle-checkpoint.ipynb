{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch master\n",
      "Your branch is up to date with 'origin/master'.\n",
      "\n",
      "Changes not staged for commit:\n",
      "\t\u001b[31mmodified:   ../../Understanding_Concepts/.DS_Store\u001b[m\n",
      "\n",
      "no changes added to commit\n",
      "Everything up-to-date\n"
     ]
    }
   ],
   "source": [
    "# update the git\n",
    "! git add  .\n",
    "! git commit -m \"From Mac\"\n",
    "! git push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# import library\n",
    "import matplotlib \n",
    "matplotlib.use('Agg')\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sys, os,cv2\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from skimage.transform import resize\n",
    "from scipy.misc import imread\n",
    "from imgaug import augmenters as iaa\n",
    "import nibabel as nib\n",
    "import imgaug as ia\n",
    "from scipy.ndimage import zoom\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "# Generate training data\n",
    "import tensorflow as tf\n",
    "old_v = tf.logging.get_verbosity()\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "plt.style.use('seaborn-white')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "np.random.seed(6278)\n",
    "tf.set_random_seed(6728)\n",
    "ia.seed(6278)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../Dataset/MNIST/train-images-idx3-ubyte.gz\n",
      "Extracting ../../Dataset/MNIST/train-labels-idx1-ubyte.gz\n",
      "Extracting ../../Dataset/MNIST/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../Dataset/MNIST/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:14: DeprecationWarning: Both axis > a.ndim and axis < -a.ndim - 1 are deprecated and will raise an AxisError in the future.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 28, 28, 1)\n",
      "1.0\n",
      "0.0\n",
      "(60000, 10)\n",
      "1.0\n",
      "0.0\n",
      "(10000, 28, 28, 1)\n",
      "0.0\n",
      "0.0\n",
      "(10000, 10)\n",
      "1.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# get the mnist data set\n",
    "mnist = input_data.read_data_sets('../../Dataset/MNIST/', one_hot=True)\n",
    "x_data, train_label, y_data, test_label = mnist.train.images, mnist.train.labels, mnist.test.images, mnist.test.labels\n",
    "x_data_added,x_data_added_label = mnist.validation.images,mnist.validation.labels\n",
    "x_data = x_data.reshape(-1, 28, 28, 1)  # 28x28x1 input img\n",
    "y_data = y_data.reshape(-1, 28, 28, 1)  # 28x28x1 input img\n",
    "x_data_added = x_data_added.reshape(-1, 28, 28, 1)\n",
    "x_data = np.vstack((x_data,x_data_added))\n",
    "train_label = np.vstack((train_label,x_data_added_label))\n",
    "\n",
    "train_batch = np.zeros((1000,28,28,1))\n",
    "test_batch = np.zeros((10000,28,28,1))\n",
    "for iter in range(1000):\n",
    "    train_batch[iter,:,:,:] = np.expand_dims(resize(x_data[iter,:,:,0],(28,28)),axis=3)\n",
    "\n",
    "# print out the data shape and the max and min value\n",
    "print(train_batch.shape)\n",
    "print(train_batch.max())\n",
    "print(train_batch.min())\n",
    "print(train_label.shape)\n",
    "print(train_label.max())\n",
    "print(train_label.min())\n",
    "print(test_batch.shape)\n",
    "print(test_batch.max())\n",
    "print(test_batch.min())\n",
    "print(test_label.shape)\n",
    "print(test_label.max())\n",
    "print(test_label.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     3
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# show small amount of data\n",
    "fig=plt.figure(figsize=(15, 15))\n",
    "columns = 10 ; rows = 10\n",
    "for i in range(1, columns*rows +1):\n",
    "    fig.add_subplot(rows, columns, i)\n",
    "    plt.imshow(np.squeeze(train_batch[i-1]),cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Label : \"+str(np.argmax(train_label[i-1])))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1,
     17,
     59,
     62,
     110
    ]
   },
   "outputs": [],
   "source": [
    "# import all of the layers\n",
    "def tf_elu(x):\n",
    "    \"\"\" Exponential Linear Unit based on the ICCV 2015 paper\n",
    "    https://arxiv.org/pdf/1511.07289.pdf\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : float\n",
    "        The floating point number that is going to be applied to the ELU activation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Data with the same dimensions as the input after ELU\n",
    "\n",
    "    \"\"\"\n",
    "    return tf.nn.elu(x)\n",
    "def d_tf_elu(x):\n",
    "    \"\"\"Derivative of the Exponential Linear Unit base on the ICCV 2015 paper\n",
    "    https://arxiv.org/pdf/1511.07289.pdf\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : type\n",
    "        Description of parameter `x`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    type\n",
    "        Description of returned object.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    return tf.cast(tf.greater(x,0),tf.float32)  + (tf_elu(tf.cast(tf.less_equal(x,0),tf.float32) * x) + 1.0)\n",
    "\n",
    "def tf_identiy(x)  : return x\n",
    "def d_tf_identiy(x): return tf.ones_like(x,dtype=tf.float32)\n",
    "\n",
    "def tf_relu(x):   return tf.nn.relu(x)\n",
    "def d_tf_relu(x): return tf.cast(tf.greater(x,0.0),tf.float32)\n",
    "\n",
    "def tf_lrelu(x): return tf.nn.leaky_relu(x,alpha=0.2)\n",
    "def d_tf_lrelu(x): return tf.cast(tf.greater(x,0),tf.float32) + tf.cast(tf.less_equal(x,0),tf.float32) * 0.2\n",
    "\n",
    "def tf_tanh(x): return tf.nn.tanh(x)\n",
    "def d_tf_tanh(x): return 1 - tf_tanh(x) ** 2\n",
    "\n",
    "def tf_sigmoid(x):   return tf.nn.sigmoid(x)\n",
    "def d_tf_sigmoid(x): return tf_sigmoid(x) * (1.0-tf_sigmoid(x))\n",
    "\n",
    "def tf_atan(x): return tf.atan(x)\n",
    "def d_tf_atan(x): return 1.0/(1.0 + x**2)\n",
    "\n",
    "def tf_softmax(x): return tf.nn.softmax(x)\n",
    "def softabs(x): return tf.sqrt(x ** 2 + 1e-20)\n",
    "\n",
    "def tf_logcosh(x): return tf.log(tf.cosh(x))\n",
    "def d_tf_logcosh(x): return tf.tanh(x)\n",
    "\n",
    "class FNN():\n",
    "\n",
    "    def __init__(self,inc,outc,stddev=None,act=tf_elu,d_act=d_tf_elu,which_reg=0,init_x=False):\n",
    "        if init_x:\n",
    "            interval = np.sqrt(6.0 / (inc + outc + 1.0))\n",
    "            self.w = tf.Variable(tf.random_uniform(shape=(inc, outc),minval=-interval,maxval=interval,dtype=tf.float32,seed=2))\n",
    "            self.b = tf.Variable(tf.zeros(shape=[outc],dtype=tf.float32))\n",
    "        else:\n",
    "            self.w = tf.Variable(tf.random_normal([inc,outc],stddev=stddev,seed=2,dtype=tf.float32))\n",
    "            self.b = tf.Variable(tf.zeros([outc],dtype=tf.float32))\n",
    "\n",
    "        self.m,  self.v   = tf.Variable(tf.zeros_like(self.w)),tf.Variable(tf.zeros_like(self.w))\n",
    "        self.m_b,self.v_b = tf.Variable(tf.zeros_like(self.b)),tf.Variable(tf.zeros_like(self.b))\n",
    "        self.act,self.d_act = act,d_act\n",
    "        self.which_reg = which_reg\n",
    "\n",
    "    def getw(self): return self.w\n",
    "\n",
    "    def feedforward(self,input=None):\n",
    "        self.input  = input\n",
    "        self.layer  = tf.matmul(input,self.w) + self.b\n",
    "        self.layerA = self.act(self.layer)\n",
    "        return self.layerA\n",
    "\n",
    "    def backprop(self,gradient=None,learning_rate=None):\n",
    "        grad_part_1 = gradient\n",
    "        grad_part_2 = self.d_act(self.layer)\n",
    "        grad_part_3 = self.input\n",
    "\n",
    "        grad_middle = grad_part_1 * grad_part_2\n",
    "        grad  = tf.matmul(tf.transpose(grad_part_3),grad_middle)/batch_size\n",
    "        grad_b= tf.reduce_mean(grad_middle,0)/batch_size\n",
    "        grad_pass = tf.matmul(grad_middle,tf.transpose(self.w))\n",
    "\n",
    "        update_w = []\n",
    "        update_w.append(tf.assign( self.m,self.m*beta1 + (1-beta1) * (grad)   ))\n",
    "        update_w.append(tf.assign( self.v,self.v*beta2 + (1-beta2) * (grad ** 2)   ))\n",
    "        m_hat = self.m / (1.-beta1)\n",
    "        v_hat = self.v / (1.-beta2)\n",
    "        adam_middle  = m_hat     *  learning_rate/(tf.sqrt(v_hat) + adam_e)\n",
    "        update_w.append(tf.assign(self.w,tf.subtract(self.w,adam_middle )))\n",
    "\n",
    "        update_w.append(tf.assign(self.m_b,self.m_b*beta1 + (1-beta1) * (grad_b)   ))\n",
    "        update_w.append(tf.assign(self.v_b,self.v_b*beta2 + (1-beta2) * (grad_b ** 2)   ))\n",
    "        m_hat_b = self.m_b / (1-beta1)\n",
    "        v_hat_b = self.v_b / (1-beta2)\n",
    "        adam_middle_b = m_hat_b *  learning_rate/(tf.sqrt(v_hat_b) + adam_e)\n",
    "        update_w.append(tf.assign(self.b,tf.subtract(self.b,adam_middle_b ))) \n",
    "        \n",
    "        return grad_pass,update_w\n",
    "\n",
    "class CNN():\n",
    "\n",
    "    def __init__(self,k,inc,out,stddev=0.005,which_reg=0,act=tf_elu,d_act=d_tf_elu,residual=False):\n",
    "        self.w = tf.Variable(tf.random_normal([k,k,inc,out],stddev=stddev,seed=2,dtype=tf.float32))\n",
    "        self.m,self.v = tf.Variable(tf.zeros_like(self.w)),tf.Variable(tf.zeros_like(self.w))\n",
    "        self.act,self.d_act = act,d_act\n",
    "        self.which_reg = which_reg\n",
    "        self.residual = residual\n",
    "\n",
    "    def getw(self): return self.w\n",
    "\n",
    "    def feedforward(self,input,stride=1,padding='VALID'):\n",
    "        self.input  = input\n",
    "        if self.residual: \n",
    "            self.layer  = tf.nn.conv2d(input,self.w,strides=[1,stride,stride,1],padding='SAME')\n",
    "            self.layerA = self.act(self.layer) + input\n",
    "        else:\n",
    "            self.layer  = tf.nn.conv2d(input,self.w,strides=[1,stride,stride,1],padding=padding)\n",
    "            self.layerA = self.act(self.layer)\n",
    "        return self.layerA\n",
    "\n",
    "    def backprop(self,gradient,stride=1,padding='VALID'):\n",
    "        grad_part_1 = gradient\n",
    "        grad_part_2 = self.d_act(self.layer)\n",
    "        grad_part_3 = self.input\n",
    "\n",
    "        grad_middle = grad_part_1 * grad_part_2\n",
    "\n",
    "        if self.residual: \n",
    "            grad = tf.nn.conv2d_backprop_filter(input = grad_part_3,filter_sizes = self.w.shape,out_backprop = grad_middle,\n",
    "                strides=[1,stride,stride,1],padding='SAME'\n",
    "            ) / batch_size\n",
    "            \n",
    "            grad_pass = tf.nn.conv2d_backprop_input(input_sizes = [batch_size] + list(grad_part_3.shape[1:]),filter= self.w,out_backprop = grad_middle,\n",
    "                strides=[1,stride,stride,1],padding='SAME'\n",
    "            ) + gradient\n",
    "        else: \n",
    "            grad = tf.nn.conv2d_backprop_filter(input = grad_part_3,filter_sizes = self.w.shape,out_backprop = grad_middle,\n",
    "                strides=[1,stride,stride,1],padding=padding\n",
    "            ) / batch_size\n",
    "        \n",
    "            grad_pass = tf.nn.conv2d_backprop_input(input_sizes = [batch_size] + list(grad_part_3.shape[1:]),filter= self.w,out_backprop = grad_middle,\n",
    "                strides=[1,stride,stride,1],padding=padding\n",
    "            )\n",
    "\n",
    "        # === Reg ===        \n",
    "        if self.which_reg == 0:\n",
    "            grad = grad\n",
    "\n",
    "        if self.which_reg == 0.5:\n",
    "            grad = grad + lamda * (tf.sqrt(tf.abs(self.w))) * (1.0/tf.sqrt(tf.abs(self.w)+ 10e-5)) * tf.sign(self.w)\n",
    "\n",
    "        if self.which_reg == 1:\n",
    "            grad = grad + lamda * tf.sign(self.w)\n",
    "\n",
    "        if self.which_reg == 1.5:\n",
    "            grad = grad + lamda * 1.0/(tf.sqrt(tf.square(self.w) + 10e-5)) * self.w\n",
    "\n",
    "        if self.which_reg == 2:\n",
    "            grad = grad + lamda * (1.0/tf.sqrt(tf.square(tf.abs(self.w))+ 10e-5)) * tf.abs(self.w) * tf.sign(self.w)\n",
    "\n",
    "        if self.which_reg == 2.5:\n",
    "            grad = grad + lamda * 2.0 * self.w\n",
    "\n",
    "        if self.which_reg == 3:\n",
    "            grad = grad + lamda * tf.pow(tf.pow(tf.abs(self.w),3)+ 10e-5,-0.66) * tf.pow(tf.abs(self.w),2) * tf.sign(self.w)\n",
    "\n",
    "        if self.which_reg == 4:\n",
    "            grad = grad + lamda * tf.pow(tf.pow(tf.abs(self.w),4)+ 10e-5,-0.75) * tf.pow(tf.abs(self.w),3) * tf.sign(self.w)\n",
    "\n",
    "        update_w = []\n",
    "        update_w.append(tf.assign( self.m,self.m*beta1 + (1-beta1) * (grad)   ))\n",
    "        update_w.append(tf.assign( self.v,self.v*beta2 + (1-beta2) * (grad ** 2)   ))\n",
    "        m_hat = self.m / (1-beta1)\n",
    "        v_hat = self.v / (1-beta2)\n",
    "        adam_middel = learning_rate/(tf.sqrt(v_hat) + adam_e)\n",
    "        update_w.append(tf.assign(self.w,tf.subtract(self.w,tf.multiply(adam_middel,m_hat)  )))\n",
    "        return grad_pass,update_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# hyper class\n",
    "num_epoch = 10001; batch_size = 50; print_size = 25; lamda = 0.0\n",
    "beta1,beta2,adam_e = 0.9,0.999,1e-8; eps = 1e-10\n",
    "\n",
    "aimed_range = tf.placeholder(shape=[],dtype=tf.float32)\n",
    "\n",
    "e1 = FNN(784,1000,       stddev=0.005,which_reg=0,act=tf_relu,d_act=d_tf_relu)\n",
    "e2 = FNN(1000,1000,      stddev=0.005,which_reg=0,act=tf_relu,d_act=d_tf_relu)\n",
    "e3 = FNN(1000,2,         stddev=0.005,which_reg=0,act=tf_identiy,d_act=d_tf_identiy)\n",
    "\n",
    "d1 = FNN(2,1000,         stddev=0.005,which_reg=0,act=tf_relu,d_act=d_tf_relu)\n",
    "d2 = FNN(1000,1000,      stddev=0.005,which_reg=0,act=tf_relu,d_act=d_tf_relu)\n",
    "d3 = FNN(1000,784,       stddev=0.005,which_reg=0,act=tf_sigmoid,d_act=d_tf_sigmoid)\n",
    "\n",
    "discrim1 = FNN(2,1000   ,stddev=0.005,which_reg=0,act=tf_relu,d_act=d_tf_relu)\n",
    "discrim2 = FNN(1000,1000,stddev=0.005,which_reg=0,act=tf_relu,d_act=d_tf_relu)\n",
    "discrim3 = FNN(1000,1   ,stddev=0.005,which_reg=0,act=tf_sigmoid,d_act=d_tf_sigmoid)\n",
    "\n",
    "# 0.00005\n",
    "def norm_vec(x,aimed_range):\n",
    "    return ((x-tf.reduce_min(x,1)[:,None]) /(tf.reduce_max(x,1)-tf.reduce_min(x,1))[:,None]) *\\\n",
    "           (aimed_range + aimed_range) - aimed_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 0. Update the encoding and decoding\n",
    "x0  = tf.placeholder(shape=[batch_size,784],dtype=tf.float32)\n",
    "lr0 = tf.placeholder(shape=[],dtype=tf.float32)\n",
    "\n",
    "elayer1 = e1.feedforward(x0)\n",
    "elayer2 = e2.feedforward(elayer1)\n",
    "elayer3 = e3.feedforward(elayer2)\n",
    "\n",
    "dlayer1 = d1.feedforward(elayer3)\n",
    "dlayer2 = d2.feedforward(dlayer1)\n",
    "dlayer3 = d3.feedforward(dlayer2)\n",
    "\n",
    "recon_cost = -tf.reduce_mean(x0*tf.log(dlayer3+10e-10)+ (1.0-x0)*tf.log(1.0-dlayer3+10e-10))\n",
    "# auto_train = tf.train.AdamOptimizer(learning_rate=lr0).minimize(recon_cost)\n",
    "recon_ = (-(x0/(dlayer3+10e-10)) + (1.0-x0)/(1.0-dlayer3+10e-10))/(batch_size)\n",
    "\n",
    "\n",
    "dgrad3,dgrad3_up = d3.backprop(norm_vec(recon_,aimed_range)+recon_,learning_rate=lr0)\n",
    "dgrad2,dgrad2_up = d2.backprop(norm_vec(dgrad3,aimed_range)+dgrad3,learning_rate=lr0)\n",
    "dgrad1,dgrad1_up = d1.backprop(norm_vec(dgrad2,aimed_range)+dgrad2,learning_rate=lr0)\n",
    "\n",
    "egrad3,egrad3_up = e3.backprop(norm_vec(dgrad1,aimed_range)+dgrad1,learning_rate=lr0)\n",
    "egrad2,egrad2_up = e2.backprop(norm_vec(egrad3,aimed_range)+egrad3,learning_rate=lr0)\n",
    "egrad1,egrad1_up = e1.backprop(norm_vec(egrad2,aimed_range)+egrad2,learning_rate=lr0)\n",
    "\n",
    "grad0_update = dgrad3_up + dgrad2_up + dgrad1_up + egrad3_up + egrad2_up + egrad1_up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# 1. Update the discrimator (with encode)\n",
    "x1       = tf.placeholder(shape=[batch_size,784],dtype=tf.float32)\n",
    "lr1      = tf.placeholder(shape=[],dtype=tf.float32)\n",
    "x1_label = tf.zeros((batch_size,1),dtype=tf.float32)\n",
    "\n",
    "elayer1_dis = e1.feedforward(x1)\n",
    "elayer2_dis = e2.feedforward(elayer1_dis)\n",
    "elayer3_dis = e3.feedforward(elayer2_dis)\n",
    "\n",
    "dislayer1_dis1= discrim1.feedforward(elayer3_dis)\n",
    "dislayer2_dis1= discrim2.feedforward(dislayer1_dis1)\n",
    "dislayer3_dis1= discrim3.feedforward(dislayer2_dis1)\n",
    "\n",
    "dis_cost1 = - tf.reduce_mean(x1_label * tf.log(dislayer3_dis1 + 10e-10) + (1.-x1_label) * tf.log(1.-dislayer3_dis1+10e-10))\n",
    "dis_grad1 = (-(x1_label/(dislayer3_dis1+10e-10)) + (1.0-x1_label)/(1.0-dislayer3_dis1+10e-10))/(batch_size)\n",
    "\n",
    "disgrad31,disgrad31_up = discrim3.backprop(dis_grad1,learning_rate=lr1)\n",
    "disgrad21,disgrad21_up = discrim2.backprop(disgrad31,learning_rate=lr1)\n",
    "disgrad11,disgrad11_up = discrim1.backprop(disgrad21,learning_rate=lr1)\n",
    "\n",
    "grad1_update = disgrad31_up + disgrad21_up + disgrad11_up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# 2. Update the discrimator (with prior)\n",
    "x2       = tf.placeholder(shape=[batch_size,2],dtype=tf.float32)\n",
    "lr2      = tf.placeholder(shape=[],dtype=tf.float32)\n",
    "x2_label = tf.ones((batch_size,1),dtype=tf.float32)\n",
    "\n",
    "dislayer1_dis2= discrim1.feedforward(x2)\n",
    "dislayer2_dis2= discrim2.feedforward(dislayer1_dis2)\n",
    "dislayer3_dis2= discrim3.feedforward(dislayer2_dis2)\n",
    "\n",
    "dis_cost2 = - tf.reduce_mean(x2_label * tf.log(dislayer3_dis2 + 10e-10) + (1.-x2_label) * tf.log(1.-dislayer3_dis2+10e-10))\n",
    "dis_grad2 = (-(x2_label/(dislayer3_dis2+10e-10)) + (1.0-x2_label)/(1.0-dislayer3_dis2+10e-10))/(batch_size)\n",
    "\n",
    "disgrad32,disgrad32_up = discrim3.backprop(dis_grad2,learning_rate=lr2)\n",
    "disgrad22,disgrad22_up = discrim2.backprop(disgrad32,learning_rate=lr2)\n",
    "disgrad12,disgrad12_up = discrim1.backprop(disgrad22,learning_rate=lr2)\n",
    "\n",
    "grad2_update = disgrad32_up + disgrad22_up + disgrad12_up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# 3. update the generator here \n",
    "x3       = tf.placeholder(shape=[batch_size,784],dtype=tf.float32)\n",
    "lr3      = tf.placeholder(shape=[],dtype=tf.float32)\n",
    "x3_label = tf.ones((batch_size,1),dtype=tf.float32)\n",
    "\n",
    "elayer13 = e1.feedforward(x3)\n",
    "elayer23 = e2.feedforward(elayer13)\n",
    "elayer33 = e3.feedforward(elayer23)\n",
    "\n",
    "dlayer13 = discrim1.feedforward(elayer33)\n",
    "dlayer23 = discrim2.feedforward(dlayer13)\n",
    "dlayer33 = discrim3.feedforward(dlayer23)\n",
    "\n",
    "fake_cost = - tf.reduce_mean(x3_label * tf.log(dlayer33 + 10e-10) + (1.-x3_label) * tf.log(1.-dlayer33+10e-10))\n",
    "fake_grad = (-(x3_label/(dlayer33+10e-10)) + (1.0-x3_label)/(1.0-dlayer33+10e-10))/(batch_size)\n",
    "\n",
    "fake_disgrad3,_ = discrim3.backprop(fake_grad,learning_rate=lr3)\n",
    "fake_disgrad2,_ = discrim2.backprop(fake_disgrad3,learning_rate=lr3)\n",
    "fake_disgrad1,_ = discrim1.backprop(fake_disgrad2,learning_rate=lr3)\n",
    "\n",
    "fake_egrad3,fake_egrad3_up = e3.backprop(norm_vec(fake_disgrad1,aimed_range)+fake_disgrad1,learning_rate=lr3)\n",
    "fake_egrad2,fake_egrad2_up = e2.backprop(norm_vec(fake_egrad3  ,aimed_range)+fake_egrad3  ,learning_rate=lr3)\n",
    "fake_egrad1,fake_egrad1_up = e1.backprop(norm_vec(fake_egrad2  ,aimed_range)+fake_egrad2  ,learning_rate=lr3)\n",
    "\n",
    "fake_grad_update = fake_egrad3_up + fake_egrad2_up + fake_egrad1_up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " iter: 19 batch: 2450 range: 6.754258588364965e-06 recon cost: 0.20232137 zero cost: 0.5938848\t\t\t\t\t\t\t\t one  cost: 0.5876429 fake cost: 1.050293124455"
     ]
    }
   ],
   "source": [
    "# start the session noise=.05\n",
    "try:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "except:\n",
    "    sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "train_batch_shuffled = train_batch.copy()\n",
    "color_dict = {0:'red',1:'blue',2:'green',3:'yellow',4:'purple',5:'grey',6:'black',7:'violet',8:'silver',9:'cyan'}\n",
    "from sklearn import manifold, datasets\n",
    "image_num = 0\n",
    "#                     00005\n",
    "aimed_range_start = 0.00005\n",
    "for iter in range(num_epoch):\n",
    "    for current_data_index in range(0,len(train_batch),batch_size):\n",
    "        prior_z,_    =  datasets.make_circles(n_samples=batch_size, factor=.2)\n",
    "        prior_z = prior_z * 10\n",
    "        current_data = train_batch_shuffled[current_data_index:current_data_index+batch_size].astype(np.float32)\n",
    "    \n",
    "        sess_result0 = sess.run([recon_cost,grad0_update],\n",
    "        feed_dict={x0:current_data.reshape((batch_size,-1)),lr0:0.00005,aimed_range:aimed_range_start\n",
    "        })\n",
    "\n",
    "        sess_result1 = sess.run([dis_cost1,grad1_update],\n",
    "        feed_dict={x1:current_data.reshape((batch_size,-1)),lr1:0.0001})\n",
    "\n",
    "        sess_result2 = sess.run([dis_cost2,grad2_update],\n",
    "        feed_dict={x2:prior_z                              ,lr2:0.0001})\n",
    "        \n",
    "        sess_result3 = sess.run([fake_cost,fake_grad_update],\n",
    "        feed_dict={x3:current_data.reshape((batch_size,-1)),lr3:0.00005,aimed_range:aimed_range_start\n",
    "                  })\n",
    "\n",
    "        sys.stdout.write('\\r iter: ' + str(iter) + ' batch: ' + str(current_data_index) + ' range: ' + str(aimed_range_start) + \\\n",
    "            ' recon cost: ' + str(sess_result0[0]) + \\\n",
    "            ' zero cost: ' + str(sess_result1[0]) + '\\t\\t\\t\\t\\t\\t\\t\\t' + \\\n",
    "            ' one  cost: ' + str(sess_result2[0]) + \\\n",
    "            ' fake cost: ' + str(sess_result3[0]) \n",
    "        ); sys.stdout.flush()\n",
    "        \n",
    "    aimed_range_start = aimed_range_start * 0.9\n",
    "    if iter % print_size == 0:\n",
    "        random_idx = np.random.permutation(len(train_batch_shuffled))\n",
    "        temp_data  = train_batch_shuffled[random_idx][:batch_size].reshape((batch_size,-1))\n",
    "#         sess_result = sess.run(dlayer3,\n",
    "#                       feed_dict={x0:temp_data }).reshape((batch_size,28,28))\n",
    "#         fig=plt.figure(figsize=(15, 15)); columns = 10 ; rows = 1\n",
    "#         for i in range(1, columns*rows +1):\n",
    "#             fig.add_subplot(rows, columns, i)\n",
    "#             if i % 2 ==0 :\n",
    "#                   plt.imshow(np.squeeze(sess_result[i-2]),cmap='gray')\n",
    "#             else:\n",
    "#                 plt.imshow(np.squeeze(temp_data[i-1]).reshape((28,28)),cmap='gray')\n",
    "#             plt.axis('off')\n",
    "#         plt.show()\n",
    "        \n",
    "        # Show the latent space        \n",
    "        all_latent_vectors = sess.run(elayer33,feed_dict={x3:train_batch[:batch_size].reshape((batch_size,-1))})\n",
    "        for current_batch_index in range(batch_size,len(train_batch), batch_size):\n",
    "            current_train_data = train_batch[current_batch_index:current_batch_index+batch_size]\n",
    "            sess_results = sess.run(elayer33,feed_dict={x3:current_train_data.reshape((batch_size,-1))})\n",
    "            all_latent_vectors = np.vstack((all_latent_vectors,sess_results))\n",
    "        fig = plt.figure(figsize=(10,10))\n",
    "        color_mapping = [color_dict[x] for x in np.argmax(train_label[:len(train_batch),:],1) ]\n",
    "        plt.title(str(color_dict))\n",
    "        plt.scatter(all_latent_vectors[:,0],all_latent_vectors[:,1],c=color_mapping)\n",
    "        plt.scatter(prior_z[:,0],prior_z[:,1],marker='^')\n",
    "        plt.xlim(-15,15);plt.ylim(-15,15);\n",
    "        plt.grid(); plt.tight_layout()\n",
    "        plt.savefig('./temp/'+str(image_num)+'.png',)\n",
    "        plt.show()\n",
    "        plt.close('all')\n",
    "        image_num = image_num + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# convert every data into latent to know the statistics\n",
    "all_latent_vectors = sess.run(elayer3,feed_dict={x:train_batch[:batch_size].reshape((batch_size,-1))})\n",
    "print(train_batch.shape)\n",
    "print(all_latent_vectors.shape)\n",
    "for current_batch_index in range(batch_size,len(train_batch), batch_size):\n",
    "    current_train_data = train_batch[current_batch_index:current_batch_index+batch_size]\n",
    "    sess_results = sess.run(elayer3,feed_dict={x:current_train_data.reshape((batch_size,-1))})\n",
    "    all_latent_vectors = np.vstack((all_latent_vectors,sess_results))\n",
    "print(all_latent_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# view the latent vectors\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "color_dict = {\n",
    "    0:'red',\n",
    "    1:'blue',\n",
    "    2:'green',\n",
    "    3:'yellow',\n",
    "    4:'purple',\n",
    "    5:'grey',\n",
    "    6:'black',\n",
    "    7:'violet',\n",
    "    8:'silver',\n",
    "    9:'cyan',\n",
    "}\n",
    "color_mapping = [color_dict[x] for x in np.argmax(train_label[:len(train_batch),:],1) ]\n",
    "plt.title(str(color_dict))\n",
    "plt.scatter(all_latent_vectors[:,0],all_latent_vectors[:,1],c=color_mapping)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
