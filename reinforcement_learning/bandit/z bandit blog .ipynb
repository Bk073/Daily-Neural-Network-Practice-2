{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T01:29:27.640901Z",
     "start_time": "2019-01-13T01:29:26.638907Z"
    }
   },
   "source": [
    "### Compare Listing \n",
    "<ol>\n",
    "<li>a: vector uniform</li>\n",
    "<li>b: greedy</li>\n",
    "<li>c: e - greedy</li>\n",
    "<li>d: decay e - greedy</li>\n",
    "<li>e: Linear Reward Inaction (Pursuit Methods)</li>\n",
    "<li>f: Linear Reward Penalty (Pursuit Methods)</li>\n",
    "<li>g: UBC 1</li>\n",
    "<li>h: UCB 1-Tuned</li>\n",
    "<li>i: Thompson Sampling (beta)</li>\n",
    "<li>j: Thompson Sampling (uniform)</li>\n",
    "<li>k: Neural Network</li>\n",
    "<li>l: softmax </li>\n",
    "<li>m: Gradient Bandits</li>\n",
    "<li>n: Non Stationary</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T05:31:39.383974Z",
     "start_time": "2019-01-14T05:31:26.552073Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "if (!(\"Notification\" in window)) {\n",
       "    alert(\"This browser does not support desktop notifications, so the %%notify magic will not work.\");\n",
       "} else if (Notification.permission !== 'granted' && Notification.permission !== 'denied') {\n",
       "    Notification.requestPermission(function (permission) {\n",
       "        if(!('permission' in Notification)) {\n",
       "            Notification.permission = permission;\n",
       "        }\n",
       "    })\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import lib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import scipy,time,sys\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import beta\n",
    "np.random.seed(5678)\n",
    "np.set_printoptions(3)\n",
    "tf.set_random_seed(678)\n",
    "%load_ext jupyternotify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T05:31:39.403920Z",
     "start_time": "2019-01-14T05:31:39.394945Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736]\n",
      "Best Choice:  11 0.7364685816073836\n"
     ]
    }
   ],
   "source": [
    "# setting the ground truth\n",
    "num_bandit = 12\n",
    "num_ep  = 20\n",
    "num_iter= 1000\n",
    "gt_prob = np.random.uniform(0,1,num_bandit)\n",
    "optimal_choice = np.argmax(gt_prob)\n",
    "print(gt_prob)\n",
    "print('Best Choice: ',optimal_choice,gt_prob[optimal_choice])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T05:32:08.136712Z",
     "start_time": "2019-01-14T05:32:08.110745Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736]\n",
      "Expected \n",
      "[0.481 0.057 0.354 0.528 0.599 0.423 0.17  0.275 0.084 0.182 0.084 0.758]\n"
     ]
    }
   ],
   "source": [
    "# a vectorized\n",
    "a_expect = np.zeros((num_ep,num_bandit))\n",
    "                    \n",
    "for eps in range(num_ep):\n",
    "    temp_expect = np.zeros(num_bandit)\n",
    "    temp_choice = np.zeros(num_bandit)\n",
    "                    \n",
    "    for iter in range(num_iter//10):\n",
    "        temp_choice    = temp_choice + 1\n",
    "        current_reward = np.random.uniform(0,1,num_bandit) < gt_prob\n",
    "        temp_expect    = temp_expect + current_reward\n",
    "\n",
    "    a_expect[eps,:] = temp_expect/temp_choice\n",
    "                    \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(a_expect.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T06:25:36.813205Z",
     "start_time": "2019-01-14T06:25:36.653641Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736]\n",
      "Expected \n",
      "[0.484 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.   ]\n"
     ]
    }
   ],
   "source": [
    "# b greedy\n",
    "b_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "b_estimation   = np.zeros((num_ep,num_bandit))\n",
    "b_reward       = np.zeros((num_ep,num_iter))\n",
    "b_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "b_regret_total = np.zeros((num_ep,num_iter))\n",
    "                    \n",
    "for eps in range(num_ep):\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit)\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "    temp_regret = np.zeros(num_iter)\n",
    "                    \n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        current_choice = np.argmax(temp_estimation)\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        temp_estimation[current_choice] = temp_estimation[current_choice] + (1/(temp_pull_count[current_choice]+1)) * (current_reward-temp_estimation[current_choice])\n",
    "        \n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = current_reward if iter == 0 else temp_reward[iter-1] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        temp_regret[iter] = gt_prob[optimal_choice] - gt_prob[current_choice] if iter == 0 else temp_regret[iter-1] + (gt_prob[optimal_choice] - gt_prob[current_choice])\n",
    "        \n",
    "    b_pull_count[eps,:]   = temp_pull_count\n",
    "    b_estimation[eps,:]   = temp_estimation\n",
    "    b_reward[eps,:]       = temp_reward\n",
    "    b_optimal_pull[eps,:] = temp_optimal_pull\n",
    "    b_regret_total[eps,:] = temp_regret\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(b_estimation.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T06:25:37.435275Z",
     "start_time": "2019-01-14T06:25:37.184025Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736]\n",
      "Expected \n",
      "[0.461 0.049 0.316 0.513 0.577 0.401 0.164 0.244 0.071 0.184 0.074 0.734]\n"
     ]
    }
   ],
   "source": [
    "# c e greedy \n",
    "c_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "c_estimation   = np.zeros((num_ep,num_bandit))\n",
    "c_reward       = np.zeros((num_ep,num_iter))\n",
    "c_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "c_regret_total = np.zeros((num_ep,num_iter))\n",
    "                    \n",
    "for eps in range(num_ep):\n",
    "    epsilon = np.random.uniform(0,1)\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit)\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "    temp_regret = np.zeros(num_iter)\n",
    "  \n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        current_choice = np.argmax(temp_expect) if epsilon < np.random.uniform(0,1) else np.random.choice(np.arange(num_bandit))\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        temp_estimation[current_choice] = temp_estimation[current_choice] + (1/(temp_pull_count[current_choice]+1)) * (current_reward-temp_estimation[current_choice])\n",
    "        \n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = current_reward if iter == 0 else temp_reward[iter-1] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        temp_regret[iter] = gt_prob[optimal_choice] - gt_prob[current_choice] if iter == 0 else temp_regret[iter-1] + (gt_prob[optimal_choice] - gt_prob[current_choice])\n",
    "        \n",
    "    c_pull_count[eps,:]   = temp_pull_count\n",
    "    c_estimation[eps,:]   = temp_estimation\n",
    "    c_reward[eps,:]       = temp_reward\n",
    "    c_optimal_pull[eps,:] = temp_optimal_pull\n",
    "    c_regret_total[eps,:] = temp_regret\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(c_estimation.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T06:27:32.900065Z",
     "start_time": "2019-01-14T06:27:32.701597Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736]\n",
      "Expected \n",
      "[0.436 0.018 0.359 0.438 0.487 0.39  0.155 0.221 0.034 0.118 0.076 0.734]\n"
     ]
    }
   ],
   "source": [
    "# d decy e greedy \n",
    "d_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "d_estimation   = np.zeros((num_ep,num_bandit))\n",
    "d_reward       = np.zeros((num_ep,num_iter))\n",
    "d_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "d_regret_total = np.zeros((num_ep,num_iter))\n",
    "\n",
    "for eps in range(num_ep):\n",
    "    epsilon = 1.0\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit)\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "                    \n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        current_choice = np.argmax(temp_expect) if epsilon < np.random.uniform(0,1) else np.random.choice(np.arange(num_bandit))\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        temp_estimation[current_choice] = temp_estimation[current_choice] + (1/(temp_pull_count[current_choice]+1)) * (current_reward-temp_estimation[current_choice])\n",
    "        \n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = current_reward if iter == 0 else temp_reward[iter-1] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        temp_regret[iter] = gt_prob[optimal_choice] - gt_prob[current_choice] if iter == 0 else temp_regret[iter-1] + (gt_prob[optimal_choice] - gt_prob[current_choice])\n",
    "        \n",
    "        # decay the eps\n",
    "        epsilon = 0.99 * epsilon\n",
    "        \n",
    "    d_pull_count[eps,:]   = temp_pull_count\n",
    "    d_estimation[eps,:]   = temp_estimation\n",
    "    d_reward[eps,:]       = temp_reward\n",
    "    d_optimal_pull[eps,:] = temp_optimal_pull\n",
    "    d_regret_total[eps,:] = temp_regret\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(d_estimation.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T06:33:35.557535Z",
     "start_time": "2019-01-14T06:33:34.744770Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736]\n",
      "Expected \n",
      "[0.   0.   0.   0.1  0.2  0.05 0.   0.   0.   0.   0.   0.65]\n",
      "Expected Normalized\n",
      "[0.    0.    0.    0.401 0.801 0.2   0.    0.    0.    0.    0.    2.604]\n"
     ]
    }
   ],
   "source": [
    "# e Linear Reward Inaction\n",
    "e_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "e_estimation   = np.zeros((num_ep,num_bandit))\n",
    "e_reward       = np.zeros((num_ep,num_iter))\n",
    "e_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "e_regret_total = np.zeros((num_ep,num_iter))\n",
    "      \n",
    "for eps in range(num_ep):\n",
    "    learning_rate = 0.1\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit) + 1.0/num_bandit\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "    \n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        current_choice = np.random.choice(num_bandit, p=temp_estimation)\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        \n",
    "        mask = np.zeros(num_bandit)\n",
    "        mask[current_choice] = 1.0\n",
    "        \n",
    "        if current_reward == 1.0:\n",
    "            temp_estimation = (mask) * (temp_estimation + learning_rate * (1-temp_estimation)) + (1-mask) * ( (1-learning_rate) * temp_estimation)\n",
    "            \n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = current_reward if iter == 0 else temp_reward[iter-1] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        temp_regret[iter] = gt_prob[optimal_choice] - gt_prob[current_choice] if iter == 0 else temp_regret[iter-1] + (gt_prob[optimal_choice] - gt_prob[current_choice])\n",
    "        \n",
    "    e_pull_count[eps,:]   = temp_pull_count\n",
    "    e_estimation[eps,:]   = temp_estimation\n",
    "    e_reward[eps,:]       = temp_reward\n",
    "    e_optimal_pull[eps,:] = temp_optimal_pull\n",
    "    e_regret_total[eps,:] = temp_regret\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(np.around(e_estimation.mean(0),3))\n",
    "print('Expected Normalized')\n",
    "print(np.around(e_estimation.mean(0),3)* gt_prob.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T06:35:57.107981Z",
     "start_time": "2019-01-14T06:35:56.257980Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736]\n",
      "Expected \n",
      "[0.057 0.007 0.031 0.08  0.15  0.037 0.008 0.017 0.007 0.009 0.006 0.591]\n",
      "Expected Normalized\n",
      "[0.229 0.028 0.124 0.319 0.6   0.147 0.034 0.069 0.026 0.036 0.026 2.37 ]\n"
     ]
    }
   ],
   "source": [
    "# f Linear Reward Penalty\n",
    "f_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "f_estimation   = np.zeros((num_ep,num_bandit))\n",
    "f_reward       = np.zeros((num_ep,num_iter))\n",
    "f_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "f_regret_total = np.zeros((num_ep,num_iter))\n",
    "                    \n",
    "for eps in range(num_ep):\n",
    "    alpha = 0.01\n",
    "    beta  = 0.001\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit) + 1.0/num_bandit\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "    \n",
    "    for iter in range(num_iter):\n",
    "\n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        current_choice = np.random.choice(num_bandit, p=temp_estimation)\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "\n",
    "        mask = np.zeros(num_bandit)\n",
    "        mask[current_choice] = 1.0\n",
    "        \n",
    "        if current_reward == 1.0:\n",
    "            temp_estimation = (mask) * (temp_estimation + alpha * (1-temp_estimation)) + (1-mask) * ( (1-alpha) * temp_estimation)\n",
    "        else: \n",
    "            temp_estimation = (mask) * ((1-beta) * temp_estimation) + (1-mask) * ( beta/(num_bandit-1) + (1-beta) * temp_estimation )\n",
    "\n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = current_reward if iter == 0 else temp_reward[iter-1] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        temp_regret[iter] = gt_prob[optimal_choice] - gt_prob[current_choice] if iter == 0 else temp_regret[iter-1] + (gt_prob[optimal_choice] - gt_prob[current_choice])\n",
    "        \n",
    "    f_pull_count[eps,:]   = temp_pull_count\n",
    "    f_estimation[eps,:]   = temp_estimation\n",
    "    f_reward[eps,:]       = temp_reward\n",
    "    f_optimal_pull[eps,:] = temp_optimal_pull\n",
    "    f_regret_total[eps,:] = temp_regret\n",
    "    \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(f_estimation.mean(0))\n",
    "print('Expected Normalized')\n",
    "print(f_estimation.mean(0) * gt_prob.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T06:35:57.403232Z",
     "start_time": "2019-01-14T06:35:57.226670Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(b_regret_total.mean(0),label='greedy')\n",
    "plt.plot(c_regret_total.mean(0),label='e greedy')\n",
    "plt.plot(d_regret_total.mean(0),label='decay e greedy')\n",
    "plt.plot(e_regret_total.mean(0),label='pursit')\n",
    "plt.plot(f_regret_total.mean(0),label='pursit')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T06:17:20.818768Z",
     "start_time": "2019-01-14T06:17:20.548499Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736]\n",
      "Expected \n",
      "[0.497 0.058 0.34  0.492 0.564 0.427 0.134 0.274 0.057 0.139 0.075 0.737]\n"
     ]
    }
   ],
   "source": [
    "# g UBC\n",
    "g_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "g_estimation   = np.zeros((num_ep,num_bandit))\n",
    "g_reward       = np.zeros((num_ep,num_iter))\n",
    "g_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "                    \n",
    "for eps in range(num_ep):\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit)\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "                    \n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        current_choice = np.argmax(temp_estimation + np.sqrt(2*np.log(iter+1)/(temp_pull_count+1)))\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        temp_estimation[current_choice] = temp_estimation[current_choice] + (1/(temp_pull_count[current_choice]+1)) * (current_reward-temp_estimation[current_choice])\n",
    "        \n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = temp_reward[iter] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        \n",
    "    g_pull_count[eps,:]   = temp_pull_count\n",
    "    g_estimation[eps,:]   = temp_estimation\n",
    "    g_reward[eps,:]       = temp_reward\n",
    "    g_optimal_pull[eps,:] = temp_optimal_pull\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(g_estimation.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T06:21:50.280119Z",
     "start_time": "2019-01-14T06:21:49.983217Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736]\n",
      "Expected \n",
      "[0.46  0.047 0.34  0.514 0.568 0.428 0.142 0.244 0.058 0.145 0.066 0.732]\n"
     ]
    }
   ],
   "source": [
    "# h UBC Tuned\n",
    "h_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "h_estimation   = np.zeros((num_ep,num_bandit))\n",
    "h_reward       = np.zeros((num_ep,num_iter))\n",
    "h_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "                    \n",
    "for eps in range(num_ep):\n",
    "    temp_pull_count   = np.zeros(num_bandit) \n",
    "    temp_estimation   = np.zeros(num_bandit) \n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "                    \n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        current_min_value = 1\n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        current_choice = np.argmax(temp_estimation + np.sqrt(np.log(iter+1)/(temp_pull_count+1)*current_min_value))\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        temp_estimation[current_choice] = temp_estimation[current_choice] + (1/(temp_pull_count[current_choice]+1)) * (current_reward-temp_estimation[current_choice])\n",
    "\n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = temp_reward[iter] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        \n",
    "    h_pull_count[eps,:]   = temp_pull_count\n",
    "    h_estimation[eps,:]   = temp_estimation\n",
    "    h_reward[eps,:]       = temp_reward\n",
    "    h_optimal_pull[eps,:] = temp_optimal_pull\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(h_estimation.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T08:56:39.592302Z",
     "start_time": "2019-01-13T08:53:08.806546Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736]\n",
      "Expected \n",
      "[0.412 0.141 0.367 0.409 0.588 0.402 0.194 0.325 0.194 0.233 0.214 0.743]\n"
     ]
    }
   ],
   "source": [
    "# i Thompson Sampling (beta) (slow)\n",
    "k_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "k_estimation   = np.zeros((num_ep,num_bandit))\n",
    "k_reward       = np.zeros((num_ep,num_iter))\n",
    "k_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "                    \n",
    "for eps in range(num_ep):\n",
    "\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit)\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "                    \n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        theta_samples = [stats.beta(a=1+w,b=1+t-w).rvs(size=1) for t, w in zip(temp_pull_count, temp_estimation)]\n",
    "        \n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        current_choice = np.argmax(theta_samples)\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        temp_estimation[current_choice] = temp_estimation[current_choice] + current_reward\n",
    "        \n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = temp_reward[iter] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        \n",
    "    k_pull_count[eps,:]   = temp_pull_count\n",
    "    k_estimation[eps,:]   = theta_samples\n",
    "    k_reward[eps,:]       = temp_reward\n",
    "    k_optimal_pull[eps,:] = temp_optimal_pull\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(k_estimation.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T08:59:43.572283Z",
     "start_time": "2019-01-13T08:56:40.286696Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736]\n",
      "Expected \n",
      "[0.702 0.469 0.715 0.747 0.768 0.701 0.648 0.616 0.503 0.516 0.603 0.885]\n"
     ]
    }
   ],
   "source": [
    "# j Thompson Sampling (uniform) (slow)\n",
    "k_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "k_estimation   = np.zeros((num_ep,num_bandit))\n",
    "k_reward       = np.zeros((num_ep,num_iter))\n",
    "k_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "                    \n",
    "for eps in range(num_ep):\n",
    "\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit)\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "                    \n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        theta_samples = [stats.uniform(w/(t+0.000000001),1-w/(t+0.000000001)).rvs(size=1) for t, w in zip(temp_pull_count, temp_estimation)]\n",
    "        \n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        current_choice = np.argmax(theta_samples)\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        temp_estimation[current_choice] = temp_estimation[current_choice] + current_reward\n",
    "        \n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = temp_reward[iter] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        \n",
    "    k_pull_count[eps,:]   = temp_pull_count\n",
    "    k_estimation[eps,:]   = theta_samples\n",
    "    k_reward[eps,:]       = temp_reward\n",
    "    k_optimal_pull[eps,:] = temp_optimal_pull\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(k_estimation.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T08:59:45.319236Z",
     "start_time": "2019-01-13T08:59:44.170002Z"
    },
    "code_folding": [
     0,
     9
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736]\n",
      "Expected \n",
      "[0.468 0.073 0.311 0.505 0.543 0.373 0.13  0.27  0.071 0.164 0.08  0.7  ]\n",
      "Scaled \n",
      "[0.487 0.061 0.318 0.527 0.568 0.384 0.123 0.273 0.059 0.159 0.069 0.736]\n"
     ]
    }
   ],
   "source": [
    "# k neural network (with adam)\n",
    "k_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "k_estimation   = np.zeros((num_ep,num_bandit))\n",
    "k_reward       = np.zeros((num_ep,num_iter))\n",
    "k_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "            \n",
    "def sigmoid(x): return 1/(1+np.exp(-x))\n",
    "def d_sigmoid(x): return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "for eps in range(num_ep):\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit)\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "    \n",
    "    weights = np.random.randn(num_bandit,1)\n",
    "    moment  = np.zeros_like(weights); \n",
    "    velocity = np.zeros_like(weights);\n",
    "    epsilon  = 1.0 \n",
    "\n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        if np.random.uniform(0,1)>epsilon:\n",
    "            current_choice = np.argmax(weights)\n",
    "            current_input  = np.zeros((1,num_bandit))\n",
    "            current_input[0,current_choice] = 1\n",
    "        else:\n",
    "            current_choice = np.random.choice(np.arange(num_bandit))\n",
    "            current_input  = np.zeros((1,num_bandit))\n",
    "            current_input[0,current_choice] = 1\n",
    "\n",
    "        layer1 = current_input @ weights\n",
    "        layer1a= sigmoid(layer1)\n",
    "\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_estimation[current_choice] = temp_estimation[current_choice] + current_reward\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        \n",
    "        # KL Divergence https://timvieira.github.io/blog/post/2014/10/06/kl-divergence-as-an-objective-function/\n",
    "        grad3 = np.log(layer1a+0.0000001) - np.log(temp_estimation[current_choice]/(temp_pull_count[current_choice])+0.0000001)\n",
    "        grad2 = d_sigmoid(layer1)\n",
    "        grad1 = current_input\n",
    "        grad  = grad1.T @ (grad3 * grad2)\n",
    "        \n",
    "        moment   = 0.9*moment + (1-0.9) * grad\n",
    "        velocity = 0.999*velocity + (1-0.999) * grad**2\n",
    "        moment_hat   = moment/(1-0.9)\n",
    "        velocity_hat = velocity/(1-0.999)\n",
    "        weights  = weights - 0.08 * (moment_hat/(np.sqrt(velocity_hat)+1e-8))\n",
    "\n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = temp_reward[iter] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        \n",
    "        # Decay the learning rate\n",
    "        epsilon = epsilon * 0.999\n",
    "        \n",
    "    k_pull_count[eps,:]   = temp_pull_count\n",
    "    k_estimation[eps,:]   = np.squeeze(sigmoid(weights))\n",
    "    k_reward[eps,:]       = temp_reward\n",
    "    k_optimal_pull[eps,:] = temp_optimal_pull\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(k_estimation.mean(0))\n",
    "print('Scaled ')\n",
    "print((gt_prob.max()-gt_prob.min())*(k_estimation.mean(0)-k_estimation.mean(0).min())/(k_estimation.mean(0).max()-k_estimation.mean(0).min()) + gt_prob.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T08:59:46.606806Z",
     "start_time": "2019-01-13T08:59:46.039910Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736]\n",
      "Expected \n",
      "[0.498 0.062 0.367 0.515 0.609 0.416 0.173 0.272 0.077 0.189 0.088 0.738]\n"
     ]
    }
   ],
   "source": [
    "# l softmax\n",
    "l_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "l_estimation   = np.zeros((num_ep,num_bandit))\n",
    "l_reward       = np.zeros((num_ep,num_iter))\n",
    "l_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "l_regret_total = np.zeros((num_ep,num_iter))\n",
    "                    \n",
    "for eps in range(num_ep):\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit)\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "    temp_regret = np.zeros(num_iter)\n",
    "    tempture = 300\n",
    "                    \n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        pi  = np.exp(temp_estimation/tempture) / np.sum(np.exp(temp_estimation/tempture))\n",
    "        cdf = np.cumsum(pi)\n",
    "        current_choice = np.where(np.random.uniform(0,1) < cdf)[0][0]\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        temp_estimation[current_choice] = temp_estimation[current_choice] + (1/(temp_pull_count[current_choice]+1)) * (current_reward-temp_estimation[current_choice])\n",
    "\n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = current_reward if iter == 0 else temp_reward[iter-1] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        temp_regret[iter] = gt_prob[optimal_choice] - gt_prob[current_choice] if iter == 0 else temp_regret[iter-1] + (gt_prob[optimal_choice] - gt_prob[current_choice])\n",
    "        \n",
    "        tempture = tempture * 0.999999\n",
    "        \n",
    "    l_pull_count[eps,:]   = temp_pull_count\n",
    "    l_estimation[eps,:]   = temp_estimation\n",
    "    l_reward[eps,:]       = temp_reward\n",
    "    l_optimal_pull[eps,:] = temp_optimal_pull\n",
    "    l_regret_total[eps,:] = temp_regret\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(l_estimation.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T06:10:36.712012Z",
     "start_time": "2019-01-14T06:10:36.708024Z"
    }
   },
   "outputs": [],
   "source": [
    "# m gradient base\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n non stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T08:59:47.235732Z",
     "start_time": "2019-01-13T08:59:47.225357Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"e3ef950a-4475-4c9a-8746-7db3cbb41e8d\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"e3ef950a-4475-4c9a-8746-7db3cbb41e8d\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell execution has finished!\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%notify \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T01:50:52.230664Z",
     "start_time": "2019-01-13T01:50:51.987646Z"
    }
   },
   "source": [
    "# Reference \n",
    "1. numpy.set_printoptions — NumPy v1.14 Manual. (2019). Docs.scipy.org. Retrieved 13 January 2019, from https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.set_printoptions.html\n",
    "2. [ Archived Post ] Random Note about Multi-Arm Bandit Problem 2. (2019). Medium. Retrieved 13 January 2019, from https://medium.com/@SeoJaeDuk/archived-post-random-note-about-multi-arm-bandit-problem-2-5c522d1dfbdc\n",
    "3. Vieira, T. (2014). KL-divergence as an objective function — Graduate Descent. Timvieira.github.io. Retrieved 13 January 2019, from https://timvieira.github.io/blog/post/2014/10/06/kl-divergence-as-an-objective-function/\n",
    "4. Some Reinforcement Learning: The Greedy and Explore-Exploit Algorithms for the Multi-Armed Bandit Framework in Python. (2019). Datasciencecentral.com. Retrieved 13 January 2019, from https://www.datasciencecentral.com/profiles/blogs/some-reinforcement-learning-the-greedy-and-explore-exploit\n",
    "5. (2019). Cs.mcgill.ca. Retrieved 13 January 2019, from https://www.cs.mcgill.ca/~vkules/bandits.pdf\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
