{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T01:29:27.640901Z",
     "start_time": "2019-01-13T01:29:26.638907Z"
    }
   },
   "source": [
    "### Compare Listing \n",
    "<ol>\n",
    "<li>a: vector uniform</li>\n",
    "<li>b: greedy</li>\n",
    "<li>c: e - greedy</li>\n",
    "<li>d: decay e - greedy</li>\n",
    "<li>e: Linear Reward Inaction (Pursuit Methods)</li>\n",
    "<li>f: Linear Reward Penalty (Pursuit Methods)</li>\n",
    "<li>g: UBC 1</li>\n",
    "<li>h: UCB 1-Tuned</li>\n",
    "<li>i: Thompson Sampling (beta)</li>\n",
    "<li>j: Thompson Sampling (uniform)</li>\n",
    "<li>k: Neural Network</li>\n",
    "<li>l: softmax </li>\n",
    "<li>m: Gradient Bandits</li>\n",
    "<li>n: Non Stationary</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T05:31:39.383974Z",
     "start_time": "2019-01-14T05:31:26.552073Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "if (!(\"Notification\" in window)) {\n",
       "    alert(\"This browser does not support desktop notifications, so the %%notify magic will not work.\");\n",
       "} else if (Notification.permission !== 'granted' && Notification.permission !== 'denied') {\n",
       "    Notification.requestPermission(function (permission) {\n",
       "        if(!('permission' in Notification)) {\n",
       "            Notification.permission = permission;\n",
       "        }\n",
       "    })\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import lib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import scipy,time,sys\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import beta\n",
    "np.random.seed(5678)\n",
    "np.set_printoptions(3)\n",
    "tf.set_random_seed(678)\n",
    "%load_ext jupyternotify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T05:31:39.403920Z",
     "start_time": "2019-01-14T05:31:39.394945Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736]\n",
      "Best Choice:  11 0.7364685816073836\n"
     ]
    }
   ],
   "source": [
    "# setting the ground truth\n",
    "num_bandit = 12\n",
    "num_ep  = 20\n",
    "num_iter= 1000\n",
    "gt_prob = np.random.uniform(0,1,num_bandit)\n",
    "optimal_choice = np.argmax(gt_prob)\n",
    "print(gt_prob)\n",
    "print('Best Choice: ',optimal_choice,gt_prob[optimal_choice])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T05:32:08.136712Z",
     "start_time": "2019-01-14T05:32:08.110745Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736]\n",
      "Expected \n",
      "[0.481 0.057 0.354 0.528 0.599 0.423 0.17  0.275 0.084 0.182 0.084 0.758]\n"
     ]
    }
   ],
   "source": [
    "# a vectorized\n",
    "a_expect = np.zeros((num_ep,num_bandit))\n",
    "                    \n",
    "for eps in range(num_ep):\n",
    "    temp_expect = np.zeros(num_bandit)\n",
    "    temp_choice = np.zeros(num_bandit)\n",
    "                    \n",
    "    for iter in range(num_iter//10):\n",
    "        temp_choice    = temp_choice + 1\n",
    "        current_reward = np.random.uniform(0,1,num_bandit) < gt_prob\n",
    "        temp_expect    = temp_expect + current_reward\n",
    "\n",
    "    a_expect[eps,:] = temp_expect/temp_choice\n",
    "                    \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(a_expect.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T06:25:36.813205Z",
     "start_time": "2019-01-14T06:25:36.653641Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736]\n",
      "Expected \n",
      "[0.484 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.   ]\n"
     ]
    }
   ],
   "source": [
    "# b greedy\n",
    "b_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "b_estimation   = np.zeros((num_ep,num_bandit))\n",
    "b_reward       = np.zeros((num_ep,num_iter))\n",
    "b_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "b_regret_total = np.zeros((num_ep,num_iter))\n",
    "                    \n",
    "for eps in range(num_ep):\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit)\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "    temp_regret = np.zeros(num_iter)\n",
    "                    \n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        current_choice = np.argmax(temp_estimation)\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        temp_estimation[current_choice] = temp_estimation[current_choice] + (1/(temp_pull_count[current_choice]+1)) * (current_reward-temp_estimation[current_choice])\n",
    "        \n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = current_reward if iter == 0 else temp_reward[iter-1] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        temp_regret[iter] = gt_prob[optimal_choice] - gt_prob[current_choice] if iter == 0 else temp_regret[iter-1] + (gt_prob[optimal_choice] - gt_prob[current_choice])\n",
    "        \n",
    "    b_pull_count[eps,:]   = temp_pull_count\n",
    "    b_estimation[eps,:]   = temp_estimation\n",
    "    b_reward[eps,:]       = temp_reward\n",
    "    b_optimal_pull[eps,:] = temp_optimal_pull\n",
    "    b_regret_total[eps,:] = temp_regret\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(b_estimation.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T06:25:37.435275Z",
     "start_time": "2019-01-14T06:25:37.184025Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736]\n",
      "Expected \n",
      "[0.461 0.049 0.316 0.513 0.577 0.401 0.164 0.244 0.071 0.184 0.074 0.734]\n"
     ]
    }
   ],
   "source": [
    "# c e greedy \n",
    "c_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "c_estimation   = np.zeros((num_ep,num_bandit))\n",
    "c_reward       = np.zeros((num_ep,num_iter))\n",
    "c_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "c_regret_total = np.zeros((num_ep,num_iter))\n",
    "                    \n",
    "for eps in range(num_ep):\n",
    "    epsilon = np.random.uniform(0,1)\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit)\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "    temp_regret = np.zeros(num_iter)\n",
    "  \n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        current_choice = np.argmax(temp_expect) if epsilon < np.random.uniform(0,1) else np.random.choice(np.arange(num_bandit))\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        temp_estimation[current_choice] = temp_estimation[current_choice] + (1/(temp_pull_count[current_choice]+1)) * (current_reward-temp_estimation[current_choice])\n",
    "        \n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = current_reward if iter == 0 else temp_reward[iter-1] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        temp_regret[iter] = gt_prob[optimal_choice] - gt_prob[current_choice] if iter == 0 else temp_regret[iter-1] + (gt_prob[optimal_choice] - gt_prob[current_choice])\n",
    "        \n",
    "    c_pull_count[eps,:]   = temp_pull_count\n",
    "    c_estimation[eps,:]   = temp_estimation\n",
    "    c_reward[eps,:]       = temp_reward\n",
    "    c_optimal_pull[eps,:] = temp_optimal_pull\n",
    "    c_regret_total[eps,:] = temp_regret\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(c_estimation.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T06:27:32.900065Z",
     "start_time": "2019-01-14T06:27:32.701597Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736]\n",
      "Expected \n",
      "[0.436 0.018 0.359 0.438 0.487 0.39  0.155 0.221 0.034 0.118 0.076 0.734]\n"
     ]
    }
   ],
   "source": [
    "# d decy e greedy \n",
    "d_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "d_estimation   = np.zeros((num_ep,num_bandit))\n",
    "d_reward       = np.zeros((num_ep,num_iter))\n",
    "d_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "d_regret_total = np.zeros((num_ep,num_iter))\n",
    "\n",
    "for eps in range(num_ep):\n",
    "    epsilon = 1.0\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit)\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "                    \n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        current_choice = np.argmax(temp_expect) if epsilon < np.random.uniform(0,1) else np.random.choice(np.arange(num_bandit))\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        temp_estimation[current_choice] = temp_estimation[current_choice] + (1/(temp_pull_count[current_choice]+1)) * (current_reward-temp_estimation[current_choice])\n",
    "        \n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = current_reward if iter == 0 else temp_reward[iter-1] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        temp_regret[iter] = gt_prob[optimal_choice] - gt_prob[current_choice] if iter == 0 else temp_regret[iter-1] + (gt_prob[optimal_choice] - gt_prob[current_choice])\n",
    "        \n",
    "        # decay the eps\n",
    "        epsilon = 0.99 * epsilon\n",
    "        \n",
    "    d_pull_count[eps,:]   = temp_pull_count\n",
    "    d_estimation[eps,:]   = temp_estimation\n",
    "    d_reward[eps,:]       = temp_reward\n",
    "    d_optimal_pull[eps,:] = temp_optimal_pull\n",
    "    d_regret_total[eps,:] = temp_regret\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(d_estimation.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T06:33:35.557535Z",
     "start_time": "2019-01-14T06:33:34.744770Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736]\n",
      "Expected \n",
      "[0.   0.   0.   0.1  0.2  0.05 0.   0.   0.   0.   0.   0.65]\n",
      "Expected Normalized\n",
      "[0.    0.    0.    0.401 0.801 0.2   0.    0.    0.    0.    0.    2.604]\n"
     ]
    }
   ],
   "source": [
    "# e Linear Reward Inaction\n",
    "e_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "e_estimation   = np.zeros((num_ep,num_bandit))\n",
    "e_reward       = np.zeros((num_ep,num_iter))\n",
    "e_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "e_regret_total = np.zeros((num_ep,num_iter))\n",
    "      \n",
    "for eps in range(num_ep):\n",
    "    learning_rate = 0.1\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit) + 1.0/num_bandit\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "    \n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        current_choice = np.random.choice(num_bandit, p=temp_estimation)\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        \n",
    "        mask = np.zeros(num_bandit)\n",
    "        mask[current_choice] = 1.0\n",
    "        \n",
    "        if current_reward == 1.0:\n",
    "            temp_estimation = (mask) * (temp_estimation + learning_rate * (1-temp_estimation)) + (1-mask) * ( (1-learning_rate) * temp_estimation)\n",
    "            \n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = current_reward if iter == 0 else temp_reward[iter-1] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        temp_regret[iter] = gt_prob[optimal_choice] - gt_prob[current_choice] if iter == 0 else temp_regret[iter-1] + (gt_prob[optimal_choice] - gt_prob[current_choice])\n",
    "        \n",
    "    e_pull_count[eps,:]   = temp_pull_count\n",
    "    e_estimation[eps,:]   = temp_estimation\n",
    "    e_reward[eps,:]       = temp_reward\n",
    "    e_optimal_pull[eps,:] = temp_optimal_pull\n",
    "    e_regret_total[eps,:] = temp_regret\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(np.around(e_estimation.mean(0),3))\n",
    "print('Expected Normalized')\n",
    "print(np.around(e_estimation.mean(0),3)* gt_prob.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T06:33:35.848763Z",
     "start_time": "2019-01-14T06:33:35.676218Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(b_regret_total.mean(0),label='greedy')\n",
    "plt.plot(c_regret_total.mean(0),label='e greedy')\n",
    "plt.plot(d_regret_total.mean(0),label='decay e greedy')\n",
    "plt.plot(e_regret_total.mean(0),label='pursit')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T08:53:06.356294Z",
     "start_time": "2019-01-13T08:53:05.320152Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736]\n",
      "Expected \n",
      "[0.093 0.063 0.085 0.099 0.105 0.088 0.071 0.078 0.063 0.07  0.065 0.12 ]\n",
      "Expected Normalized\n",
      "[0.374 0.251 0.34  0.396 0.423 0.353 0.283 0.313 0.254 0.28  0.259 0.483]\n"
     ]
    }
   ],
   "source": [
    "# f Linear Reward Penalty\n",
    "f_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "f_estimation   = np.zeros((num_ep,num_bandit))\n",
    "f_reward       = np.zeros((num_ep,num_iter))\n",
    "f_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "f_regret_total = np.zeros((num_ep,num_iter))\n",
    "                    \n",
    "for eps in range(num_ep):\n",
    "    alpha = 0.001\n",
    "    beta  = 0.0001\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit) + 1.0/num_bandit\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "    \n",
    "    for iter in range(num_iter):\n",
    "\n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        current_choice = np.random.choice(num_bandit, p=temp_estimation)\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "\n",
    "        mask = np.zeros(num_bandit)\n",
    "        mask[current_choice] = 1.0\n",
    "        \n",
    "        if current_reward == 1.0:\n",
    "            temp_estimation = (mask) * (temp_estimation + alpha * (1-temp_estimation)) + (1-mask) * ( (1-alpha) * temp_estimation)\n",
    "        else: \n",
    "            temp_estimation = (mask) * ((1-beta) * temp_estimation) + (1-mask) * ( beta/(num_bandit-1) + (1-beta) * temp_estimation )\n",
    "\n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = temp_reward[iter] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        \n",
    "    f_pull_count[eps,:]   = temp_pull_count\n",
    "    f_estimation[eps,:]   = temp_estimation\n",
    "    f_reward[eps,:]       = temp_reward\n",
    "    f_optimal_pull[eps,:] = temp_optimal_pull\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(f_estimation.mean(0))\n",
    "print('Expected Normalized')\n",
    "print(f_estimation.mean(0) * gt_prob.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T06:17:20.818768Z",
     "start_time": "2019-01-14T06:17:20.548499Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736]\n",
      "Expected \n",
      "[0.497 0.058 0.34  0.492 0.564 0.427 0.134 0.274 0.057 0.139 0.075 0.737]\n"
     ]
    }
   ],
   "source": [
    "# g UBC\n",
    "g_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "g_estimation   = np.zeros((num_ep,num_bandit))\n",
    "g_reward       = np.zeros((num_ep,num_iter))\n",
    "g_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "                    \n",
    "for eps in range(num_ep):\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit)\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "                    \n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        current_choice = np.argmax(temp_estimation + np.sqrt(2*np.log(iter+1)/(temp_pull_count+1)))\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        temp_estimation[current_choice] = temp_estimation[current_choice] + (1/(temp_pull_count[current_choice]+1)) * (current_reward-temp_estimation[current_choice])\n",
    "        \n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = temp_reward[iter] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        \n",
    "    g_pull_count[eps,:]   = temp_pull_count\n",
    "    g_estimation[eps,:]   = temp_estimation\n",
    "    g_reward[eps,:]       = temp_reward\n",
    "    g_optimal_pull[eps,:] = temp_optimal_pull\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(g_estimation.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T06:21:50.280119Z",
     "start_time": "2019-01-14T06:21:49.983217Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736]\n",
      "Expected \n",
      "[0.46  0.047 0.34  0.514 0.568 0.428 0.142 0.244 0.058 0.145 0.066 0.732]\n"
     ]
    }
   ],
   "source": [
    "# h UBC Tuned\n",
    "h_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "h_estimation   = np.zeros((num_ep,num_bandit))\n",
    "h_reward       = np.zeros((num_ep,num_iter))\n",
    "h_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "                    \n",
    "for eps in range(num_ep):\n",
    "    temp_pull_count   = np.zeros(num_bandit) \n",
    "    temp_estimation   = np.zeros(num_bandit) \n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "                    \n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        current_min_value = 1\n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        current_choice = np.argmax(temp_estimation + np.sqrt(np.log(iter+1)/(temp_pull_count+1)*current_min_value))\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        temp_estimation[current_choice] = temp_estimation[current_choice] + (1/(temp_pull_count[current_choice]+1)) * (current_reward-temp_estimation[current_choice])\n",
    "\n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = temp_reward[iter] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        \n",
    "    h_pull_count[eps,:]   = temp_pull_count\n",
    "    h_estimation[eps,:]   = temp_estimation\n",
    "    h_reward[eps,:]       = temp_reward\n",
    "    h_optimal_pull[eps,:] = temp_optimal_pull\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(h_estimation.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T08:56:39.592302Z",
     "start_time": "2019-01-13T08:53:08.806546Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736]\n",
      "Expected \n",
      "[0.412 0.141 0.367 0.409 0.588 0.402 0.194 0.325 0.194 0.233 0.214 0.743]\n"
     ]
    }
   ],
   "source": [
    "# i Thompson Sampling (beta) (slow)\n",
    "k_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "k_estimation   = np.zeros((num_ep,num_bandit))\n",
    "k_reward       = np.zeros((num_ep,num_iter))\n",
    "k_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "                    \n",
    "for eps in range(num_ep):\n",
    "\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit)\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "                    \n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        theta_samples = [stats.beta(a=1+w,b=1+t-w).rvs(size=1) for t, w in zip(temp_pull_count, temp_estimation)]\n",
    "        \n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        current_choice = np.argmax(theta_samples)\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        temp_estimation[current_choice] = temp_estimation[current_choice] + current_reward\n",
    "        \n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = temp_reward[iter] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        \n",
    "    k_pull_count[eps,:]   = temp_pull_count\n",
    "    k_estimation[eps,:]   = theta_samples\n",
    "    k_reward[eps,:]       = temp_reward\n",
    "    k_optimal_pull[eps,:] = temp_optimal_pull\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(k_estimation.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T08:59:43.572283Z",
     "start_time": "2019-01-13T08:56:40.286696Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736]\n",
      "Expected \n",
      "[0.702 0.469 0.715 0.747 0.768 0.701 0.648 0.616 0.503 0.516 0.603 0.885]\n"
     ]
    }
   ],
   "source": [
    "# j Thompson Sampling (uniform) (slow)\n",
    "k_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "k_estimation   = np.zeros((num_ep,num_bandit))\n",
    "k_reward       = np.zeros((num_ep,num_iter))\n",
    "k_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "                    \n",
    "for eps in range(num_ep):\n",
    "\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit)\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "                    \n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        theta_samples = [stats.uniform(w/(t+0.000000001),1-w/(t+0.000000001)).rvs(size=1) for t, w in zip(temp_pull_count, temp_estimation)]\n",
    "        \n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        current_choice = np.argmax(theta_samples)\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        temp_estimation[current_choice] = temp_estimation[current_choice] + current_reward\n",
    "        \n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = temp_reward[iter] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        \n",
    "    k_pull_count[eps,:]   = temp_pull_count\n",
    "    k_estimation[eps,:]   = theta_samples\n",
    "    k_reward[eps,:]       = temp_reward\n",
    "    k_optimal_pull[eps,:] = temp_optimal_pull\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(k_estimation.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T08:59:45.319236Z",
     "start_time": "2019-01-13T08:59:44.170002Z"
    },
    "code_folding": [
     0,
     9
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736]\n",
      "Expected \n",
      "[0.468 0.073 0.311 0.505 0.543 0.373 0.13  0.27  0.071 0.164 0.08  0.7  ]\n",
      "Scaled \n",
      "[0.487 0.061 0.318 0.527 0.568 0.384 0.123 0.273 0.059 0.159 0.069 0.736]\n"
     ]
    }
   ],
   "source": [
    "# k neural network (with adam)\n",
    "k_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "k_estimation   = np.zeros((num_ep,num_bandit))\n",
    "k_reward       = np.zeros((num_ep,num_iter))\n",
    "k_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "            \n",
    "def sigmoid(x): return 1/(1+np.exp(-x))\n",
    "def d_sigmoid(x): return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "for eps in range(num_ep):\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit)\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "    \n",
    "    weights = np.random.randn(num_bandit,1)\n",
    "    moment  = np.zeros_like(weights); \n",
    "    velocity = np.zeros_like(weights);\n",
    "    epsilon  = 1.0 \n",
    "\n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        if np.random.uniform(0,1)>epsilon:\n",
    "            current_choice = np.argmax(weights)\n",
    "            current_input  = np.zeros((1,num_bandit))\n",
    "            current_input[0,current_choice] = 1\n",
    "        else:\n",
    "            current_choice = np.random.choice(np.arange(num_bandit))\n",
    "            current_input  = np.zeros((1,num_bandit))\n",
    "            current_input[0,current_choice] = 1\n",
    "\n",
    "        layer1 = current_input @ weights\n",
    "        layer1a= sigmoid(layer1)\n",
    "\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_estimation[current_choice] = temp_estimation[current_choice] + current_reward\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        \n",
    "        # KL Divergence https://timvieira.github.io/blog/post/2014/10/06/kl-divergence-as-an-objective-function/\n",
    "        grad3 = np.log(layer1a+0.0000001) - np.log(temp_estimation[current_choice]/(temp_pull_count[current_choice])+0.0000001)\n",
    "        grad2 = d_sigmoid(layer1)\n",
    "        grad1 = current_input\n",
    "        grad  = grad1.T @ (grad3 * grad2)\n",
    "        \n",
    "        moment   = 0.9*moment + (1-0.9) * grad\n",
    "        velocity = 0.999*velocity + (1-0.999) * grad**2\n",
    "        moment_hat   = moment/(1-0.9)\n",
    "        velocity_hat = velocity/(1-0.999)\n",
    "        weights  = weights - 0.08 * (moment_hat/(np.sqrt(velocity_hat)+1e-8))\n",
    "\n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = temp_reward[iter] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        \n",
    "        # Decay the learning rate\n",
    "        epsilon = epsilon * 0.999\n",
    "        \n",
    "    k_pull_count[eps,:]   = temp_pull_count\n",
    "    k_estimation[eps,:]   = np.squeeze(sigmoid(weights))\n",
    "    k_reward[eps,:]       = temp_reward\n",
    "    k_optimal_pull[eps,:] = temp_optimal_pull\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(k_estimation.mean(0))\n",
    "print('Scaled ')\n",
    "print((gt_prob.max()-gt_prob.min())*(k_estimation.mean(0)-k_estimation.mean(0).min())/(k_estimation.mean(0).max()-k_estimation.mean(0).min()) + gt_prob.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T08:59:46.606806Z",
     "start_time": "2019-01-13T08:59:46.039910Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736]\n",
      "Expected \n",
      "[0.498 0.062 0.367 0.515 0.609 0.416 0.173 0.272 0.077 0.189 0.088 0.738]\n"
     ]
    }
   ],
   "source": [
    "# l softmax\n",
    "l_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "l_estimation   = np.zeros((num_ep,num_bandit))\n",
    "l_reward       = np.zeros((num_ep,num_iter))\n",
    "l_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "l_regret_total = np.zeros((num_ep,num_iter))\n",
    "                    \n",
    "for eps in range(num_ep):\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit)\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "    temp_regret = np.zeros(num_iter)\n",
    "    tempture = 300\n",
    "                    \n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        pi  = np.exp(temp_estimation/tempture) / np.sum(np.exp(temp_estimation/tempture))\n",
    "        cdf = np.cumsum(pi)\n",
    "        current_choice = np.where(np.random.uniform(0,1) < cdf)[0][0]\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        temp_estimation[current_choice] = temp_estimation[current_choice] + (1/(temp_pull_count[current_choice]+1)) * (current_reward-temp_estimation[current_choice])\n",
    "\n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = current_reward if iter == 0 else temp_reward[iter-1] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        temp_regret[iter] = gt_prob[optimal_choice] - gt_prob[current_choice] if iter == 0 else temp_regret[iter-1] + (gt_prob[optimal_choice] - gt_prob[current_choice])\n",
    "        \n",
    "        tempture = tempture * 0.999999\n",
    "        \n",
    "    l_pull_count[eps,:]   = temp_pull_count\n",
    "    l_estimation[eps,:]   = temp_estimation\n",
    "    l_reward[eps,:]       = temp_reward\n",
    "    l_optimal_pull[eps,:] = temp_optimal_pull\n",
    "    l_regret_total[eps,:] = temp_regret\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(l_estimation.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T06:10:36.712012Z",
     "start_time": "2019-01-14T06:10:36.708024Z"
    }
   },
   "outputs": [],
   "source": [
    "# m gradient base\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n non stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T08:59:47.235732Z",
     "start_time": "2019-01-13T08:59:47.225357Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"e3ef950a-4475-4c9a-8746-7db3cbb41e8d\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"e3ef950a-4475-4c9a-8746-7db3cbb41e8d\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell execution has finished!\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%notify \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T01:50:52.230664Z",
     "start_time": "2019-01-13T01:50:51.987646Z"
    }
   },
   "source": [
    "# Reference \n",
    "1. numpy.set_printoptions — NumPy v1.14 Manual. (2019). Docs.scipy.org. Retrieved 13 January 2019, from https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.set_printoptions.html\n",
    "2. [ Archived Post ] Random Note about Multi-Arm Bandit Problem 2. (2019). Medium. Retrieved 13 January 2019, from https://medium.com/@SeoJaeDuk/archived-post-random-note-about-multi-arm-bandit-problem-2-5c522d1dfbdc\n",
    "3. Vieira, T. (2014). KL-divergence as an objective function — Graduate Descent. Timvieira.github.io. Retrieved 13 January 2019, from https://timvieira.github.io/blog/post/2014/10/06/kl-divergence-as-an-objective-function/\n",
    "4. Some Reinforcement Learning: The Greedy and Explore-Exploit Algorithms for the Multi-Armed Bandit Framework in Python. (2019). Datasciencecentral.com. Retrieved 13 January 2019, from https://www.datasciencecentral.com/profiles/blogs/some-reinforcement-learning-the-greedy-and-explore-exploit\n",
    "5. (2019). Cs.mcgill.ca. Retrieved 13 January 2019, from https://www.cs.mcgill.ca/~vkules/bandits.pdf\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
