{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-09T23:04:15.917453Z",
     "start_time": "2019-01-09T23:04:15.913470Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.12798052 0.05257987 0.04168536 0.1013075  0.13220688 0.07774843\n",
      " 0.18022149 0.1258417  0.08837421 0.07205402]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(123)\n",
    "\n",
    "expected_action_value  = np.random.uniform(0 ,1 , 10)\n",
    "expected_action_value = expected_action_value/expected_action_value.sum()\n",
    "num_epoch = 10000\n",
    "alpha = 0.2\n",
    "beta  = 0.8\n",
    "print(expected_action_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-09T23:04:16.527038Z",
     "start_time": "2019-01-09T23:04:16.264140Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.128 0.053 0.042 0.101 0.132 0.078 0.18  0.126 0.088 0.072]\n",
      "[0.111 0.11  0.111 0.111 0.095 0.107 0.111 0.111 0.111 0.022]\n",
      "[1026.  961.  947.  994. 1030.  978. 1092. 1026.  971.  975.]\n",
      "0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "# Linear Reward Inaction \n",
    "estimated = np.zeros(10)+0.1\n",
    "pull_count= np.zeros(10)\n",
    "\n",
    "for iter in range(num_epoch):\n",
    "    \n",
    "    current_choice = np.argmax(estimated) if np.random.uniform(0,1) > 0.3 else np.random.choice(10, p=estimated)\n",
    "    pull_count[current_choice] = pull_count[current_choice] + 1\n",
    "    current_reward = 1 if expected_action_value[current_choice]>np.random.uniform(0,1) else 0\n",
    "    \n",
    "    mask = np.zeros(10) \n",
    "    mask[current_choice] = 1\n",
    "\n",
    "    if current_reward == 1:\n",
    "        estimated = (mask) * (estimated + alpha*(1-estimated)) + (1-mask) * ((1-alpha) *estimated)\n",
    "    else:\n",
    "        estimated = (mask) * ((1-beta)*estimated) + (1-mask) * (beta/9.0 + (1-beta)*estimated)\n",
    "\n",
    "print(np.around(expected_action_value,3))\n",
    "print(np.around(estimated,3))\n",
    "print(np.around(pull_count,3))\n",
    "print(estimated.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-09T23:05:04.946689Z",
     "start_time": "2019-01-09T23:05:04.514141Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.128 0.053 0.042 0.101 0.132 0.078 0.18  0.126 0.088 0.072]\n",
      "[0.1   0.1   0.099 0.1   0.1   0.1   0.102 0.1   0.1   0.1  ]\n",
      "[ 879.  949.  914.  970.  953.  916. 1732.  933.  879.  875.]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Linear Reward Inaction \n",
    "alpha = 0.00001\n",
    "estimated = np.zeros(10)+0.1\n",
    "pull_count= np.zeros(10)\n",
    "\n",
    "for iter in range(num_epoch):\n",
    "    \n",
    "    current_choice = np.argmax(estimated) if np.random.uniform(0,1) > 0.9 else np.random.choice(10, p=estimated)\n",
    "    pull_count[current_choice] = pull_count[current_choice] + 1\n",
    "    current_reward = 1 if expected_action_value[current_choice]>np.random.uniform(0,1) else 0\n",
    "    \n",
    "    mask = np.zeros(10) \n",
    "    mask[current_choice] = 1\n",
    "    \n",
    "    if current_reward == 1:\n",
    "        estimated = (mask) * (estimated + alpha*(1-estimated)) + (1-mask) * ((1-alpha) *estimated)\n",
    "        \n",
    "    estimated = estimated/estimated.sum()\n",
    "        \n",
    "print(np.around(expected_action_value,3))\n",
    "print(np.around(estimated,3))\n",
    "print(np.around(pull_count,3))\n",
    "print(estimated.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-10T05:50:03.731364Z",
     "start_time": "2019-01-10T05:50:03.248232Z"
    },
    "code_folding": [
     3,
     39
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Starting LRI - Linear Reward Inaction Algorithm\n",
      "\n",
      "\n",
      "Run: 1, Optimal Decision: q2 with chance: 0.780948 chosen 1182 times. Average reward: 0.41\n",
      "[0.169 0.781 0.155 0.319 0.064 0.285 0.516 0.496 0.768 0.335] 3.8867135376668\n",
      "[0.079 0.144 0.076 0.09  0.07  0.085 0.112 0.11  0.142 0.092] 1.0000000000000335\n",
      "[0.308 0.561 0.294 0.349 0.273 0.331 0.434 0.426 0.553 0.358]\n",
      "[924, 1182, 848, 935, 924, 856, 1070, 1111, 1174, 976]\n",
      "\n",
      "\n",
      "Starting LRI - Linear Reward Inaction Algorithm\n",
      "\n",
      "\n",
      "Run: 1, Optimal Decision: q6 with chance: 0.984267 chosen 1225 times / 10000\n",
      "[0.95  0.362 0.217 0.685 0.401 0.984 0.843 0.836 0.393 0.347] 6.019174952512265\n",
      "[0.134 0.076 0.064 0.103 0.08  0.143 0.119 0.128 0.079 0.075] 1.0000000000000464\n",
      "[0.808 0.455 0.386 0.622 0.481 0.859 0.714 0.768 0.474 0.454]\n",
      "[1150, 873, 776, 1032, 867, 1225, 1081, 1205, 892, 899]\n"
     ]
    }
   ],
   "source": [
    "import random, datetime, numpy as np, math\n",
    "random.seed(datetime.datetime.now())\n",
    "\n",
    "def lrI(alpha):\n",
    "\tprint(\"\\n\\nStarting LRI - Linear Reward Inaction Algorithm\\n\\n\")\n",
    "\tfor j in range(1):\n",
    "\t\tlevers = [\"q1\", \"q2\", \"q3\", \"q4\", \"q5\", \"q6\", \"q7\", \"q8\", \"q9\", \"q10\"]\n",
    "\t\treward_percent = [random.random() for i in range(10)]\n",
    "\t\tlever_count, reward_count  = ([0 for i in range(10)] for i in range(2))\n",
    "\t\t#probability of choosing a lever - sum equal to 1\n",
    "\t\tlever_probability = [0.1 for i in range(10)]\n",
    "\t\toptimal_decision = 0\n",
    "\t\t#randomly choose a lever and set alpha/beta values\n",
    "\t\tlever = random.randint(0, 9)\n",
    "\t\tbeta = 0\n",
    "\t\t#gets lever that is the most optimal (has the highest chance of giving us a reward)\n",
    "\t\toptimal_decision =reward_percent.index(max(reward_percent))\n",
    "\t\ti = 1\n",
    "\t\twhile i <= 10000:\n",
    "\t\t# if reward = 1\n",
    "\t\t\tif random.random() < reward_percent[lever]:\n",
    "\t\t\t\treward_count[lever]+=1\n",
    "\t\t\t\t#update the current lever\n",
    "\t\t\t\tlever_probability[lever] = lever_probability[lever] + alpha * (1- lever_probability[lever])\n",
    "\t\t\t\t#update other levers\n",
    "\t\t\t\tfor x in range(0, len(lever_probability)):\n",
    "\t\t\t\t\tif x is not lever:\n",
    "\t\t\t\t\t\tlever_probability[x] = (1 - alpha) * lever_probability[x]\n",
    "\t\t\tlever_count[lever] += 1\n",
    "\t\t\ti+=1\n",
    "\t\t\t#choose the lever according to the probability distribution lever_probability\n",
    "\t\t\tlever = np.random.choice(10, p=lever_probability)\n",
    "\n",
    "\t\tprint(\"Run: %d, Optimal Decision: %s with chance: %f chosen %d times. Average reward: %.2f\" % (j+1, levers[optimal_decision], reward_percent[optimal_decision], lever_count[optimal_decision], sum(reward_count)/i))\n",
    "\t\tprint(np.around(reward_percent,3),sum(reward_percent))\n",
    "\t\tprint(np.around(lever_probability,3),sum(lever_probability))\n",
    "\t\tprint(np.around(np.array(lever_probability)*sum(reward_percent),3))\n",
    "\t\tprint(lever_count)\n",
    "        \n",
    "def lrI2(alpha):\n",
    "    print(\"\\n\\nStarting LRI - Linear Reward Inaction Algorithm\\n\\n\")\n",
    "    for j in range(1):\n",
    "        levers = [\"q1\", \"q2\", \"q3\", \"q4\", \"q5\", \"q6\", \"q7\", \"q8\", \"q9\", \"q10\"]\n",
    "        reward_percent = [random.random() for i in range(10)]\n",
    "        lever_count, reward_count  = ([0 for i in range(10)] for i in range(2))\n",
    "        lever_probability = np.zeros(10) + 0.1\n",
    "        optimal_decision = 0\n",
    "        lever = random.randint(0, 9)\n",
    "        optimal_decision = reward_percent.index(max(reward_percent))\n",
    "        i = 1\n",
    "        while i <= 10000:\n",
    "            if random.random() < reward_percent[lever]:\n",
    "                reward_count[lever] = reward_count[lever] + 1\n",
    "                mask = np.zeros(10)\n",
    "                mask[lever] = mask[lever] + 1\n",
    "                lever_probability = (mask) * (lever_probability + alpha*(1-lever_probability)) + (1-mask) *((1 - alpha) * lever_probability)\n",
    "\n",
    "            lever_count[lever] += 1; i+=1\n",
    "            lever = np.random.choice(10, p=lever_probability)\n",
    "\n",
    "        print(\"Run: %d, Optimal Decision: %s with chance: %f chosen %d times / 10000\"% \n",
    "              (j+1, levers[optimal_decision], reward_percent[optimal_decision], lever_count[optimal_decision])  )\n",
    "        print(np.around(reward_percent,3),sum(reward_percent))\n",
    "        print(np.around(lever_probability,3),lever_probability.sum())\n",
    "        print(np.around(lever_probability*sum(reward_percent),3))\n",
    "        print(lever_count)\n",
    "        \n",
    "alpha2 =  0.0001\n",
    "lrI(alpha2)\n",
    "lrI2(alpha2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-10T00:29:26.980418Z",
     "start_time": "2019-01-10T00:29:26.421948Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Starting LRI - Linear Reward Inaction Algorithm\n",
      "\n",
      "\n",
      "Run: 1, Optimal Decision: q1 with chance: 0.289948 chosen 4479 times. Average reward: 0.27\n",
      "[0.29  0.284 0.179 0.    0.104 0.061 0.004 0.    0.078 0.   ]\n",
      "[0.771 0.229 0.    0.    0.    0.    0.    0.    0.    0.   ]\n",
      "[4479, 4634, 329, 69, 75, 101, 62, 81, 101, 69]\n",
      "\n",
      "\n",
      "Starting LRI - Linear Reward Inaction Algorithm\n",
      "\n",
      "\n",
      "Run: 1, Optimal Decision: q3 with chance: 0.394228 chosen 8565 times / 10000\n",
      "[0.132 0.    0.394 0.    0.002 0.    0.119 0.    0.352 0.   ]\n",
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[120, 44, 8565, 40, 57, 51, 77, 49, 954, 43]\n"
     ]
    }
   ],
   "source": [
    "import random, datetime, numpy as np, math\n",
    "random.seed(datetime.datetime.now())\n",
    "\n",
    "def lrI(alpha):\n",
    "\tprint(\"\\n\\nStarting LRI - Linear Reward Inaction Algorithm\\n\\n\")\n",
    "\tfor j in range(1):\n",
    "\t\tlevers = [\"q1\", \"q2\", \"q3\", \"q4\", \"q5\", \"q6\", \"q7\", \"q8\", \"q9\", \"q10\"]\n",
    "\t\treward_percent = np.random.beta(0.1,0.3,size=10)\n",
    "\t\treward_percent = reward_percent/sum(reward_percent)\n",
    "\t\tlever_count, reward_count  = ([0 for i in range(10)] for i in range(2))\n",
    "\t\t#probability of choosing a lever - sum equal to 1\n",
    "\t\tlever_probability = [0.1 for i in range(10)]\n",
    "\t\toptimal_decision = 0\n",
    "\t\t#randomly choose a lever and set alpha/beta values\n",
    "\t\tlever = random.randint(0, 9)\n",
    "\t\tbeta = 0\n",
    "\t\t#gets lever that is the most optimal (has the highest chance of giving us a reward)\n",
    "\t\toptimal_decision = np.argmax(reward_percent)\n",
    "\t\ti = 1\n",
    "\t\twhile i <= 10000:\n",
    "\t\t# if reward = 1\n",
    "\t\t\tif random.random() < reward_percent[lever]:\n",
    "\t\t\t\treward_count[lever]+=1\n",
    "\t\t\t\t#update the current lever\n",
    "\t\t\t\tlever_probability[lever] = lever_probability[lever] + alpha * (1- lever_probability[lever])\n",
    "\t\t\t\t#update other levers\n",
    "\t\t\t\tfor x in range(0, len(lever_probability)):\n",
    "\t\t\t\t\tif x is not lever:\n",
    "\t\t\t\t\t\tlever_probability[x] = (1 - alpha) * lever_probability[x]\n",
    "\t\t\tlever_count[lever] += 1\n",
    "\t\t\ti+=1\n",
    "\t\t\t#choose the lever according to the probability distribution lever_probability\n",
    "\t\t\tlever = np.random.choice(10, p=lever_probability)\n",
    "\n",
    "\t\tprint(\"Run: %d, Optimal Decision: %s with chance: %f chosen %d times. Average reward: %.2f\" % (j+1, levers[optimal_decision], reward_percent[optimal_decision], lever_count[optimal_decision], sum(reward_count)/i))\n",
    "\t\tprint(np.around(reward_percent,3))\n",
    "\t\tprint(np.around(lever_probability,3))\n",
    "\t\tprint(lever_count)\n",
    "        \n",
    "def lrI2(alpha):\n",
    "    print(\"\\n\\nStarting LRI - Linear Reward Inaction Algorithm\\n\\n\")\n",
    "    for j in range(1):\n",
    "        levers = [\"q1\", \"q2\", \"q3\", \"q4\", \"q5\", \"q6\", \"q7\", \"q8\", \"q9\", \"q10\"]\n",
    "        reward_percent = np.random.beta(0.1,0.3,size=10)\n",
    "        reward_percent = reward_percent/reward_percent.sum()\n",
    "        lever_count, reward_count  = ([0 for i in range(10)] for i in range(2))\n",
    "        lever_probability = np.zeros(10) + 0.1\n",
    "        optimal_decision = 0\n",
    "        lever = random.randint(0, 9)\n",
    "        optimal_decision = np.argmax(reward_percent)\n",
    "        i = 1\n",
    "        while i <= 10000:\n",
    "            if random.random() < reward_percent[lever]:\n",
    "                reward_count[lever] = reward_count[lever] + 1\n",
    "                mask = np.zeros(10)\n",
    "                mask[lever] = mask[lever] + 1\n",
    "                lever_probability = (mask) * (lever_probability + alpha*(1-lever_probability)) + (1-mask) *((1 - alpha) * lever_probability)\n",
    "\n",
    "            lever_count[lever] += 1; i+=1\n",
    "            lever = np.random.choice(10, p=lever_probability)\n",
    "\n",
    "        print(\"Run: %d, Optimal Decision: %s with chance: %f chosen %d times / 10000\"% \n",
    "              (j+1, levers[optimal_decision], reward_percent[optimal_decision], lever_count[optimal_decision])  )\n",
    "        print(np.around(reward_percent,3))\n",
    "        print(np.around(lever_probability,3))\n",
    "        print(lever_count)\n",
    "        \n",
    "alpha2 =  0.01\n",
    "lrI(alpha2)\n",
    "lrI2(alpha2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference \n",
    "1. Michael Pacheco. (2019). Michaelpacheco.net. Retrieved 9 January 2019, from https://www.michaelpacheco.net/blog/RL-multi-armed-bandit-2\n",
    "2. numpy.random.choice — NumPy v1.15 Manual. (2019). Docs.scipy.org. Retrieved 9 January 2019, from https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.random.choice.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-09T23:28:19.119522Z",
     "start_time": "2019-01-09T23:28:19.114103Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "lever_probability = np.random.uniform(0,1,10)\n",
    "lever_probability = lever_probability/lever_probability.sum()\n",
    "print(np.random.choice(10, p=lever_probability))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
