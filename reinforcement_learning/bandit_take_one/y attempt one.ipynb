{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-25T09:38:08.225034Z",
     "start_time": "2018-12-25T09:37:53.159644Z"
    }
   },
   "outputs": [],
   "source": [
    "# import lib\n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import threading\n",
    "np.random.seed(78)\n",
    "tf.set_random_seed(678)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-25T09:39:29.438124Z",
     "start_time": "2018-12-25T09:39:29.432138Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.90626164 0.02255411 0.87551925 0.11565788 0.09965873 0.92431365\n",
      " 0.88146685 0.12355341 0.18428255 0.04542723]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# declare\n",
    "num_epoch    = 1000000\n",
    "ground_truth = np.random.uniform(0,1,10)\n",
    "expected     = np.zeros(10)\n",
    "pull_count   = np.zeros(10) \n",
    "print(ground_truth)\n",
    "print(expected)\n",
    "print(pull_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-25T09:39:35.147088Z",
     "start_time": "2018-12-25T09:39:30.013999Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.906 0.023 0.876 0.116 0.1   0.924 0.881 0.124 0.184 0.045]\n",
      "[0.904 0.023 0.876 0.114 0.098 0.924 0.882 0.124 0.186 0.045]\n",
      "[ 99894. 100048.  99708. 100042. 100219.  99885.  99606. 100124. 100277.\n",
      " 100197.]\n"
     ]
    }
   ],
   "source": [
    "# train \n",
    "for iter in range(num_epoch):\n",
    "    current_action = np.random.randint(10)\n",
    "    pull_count[current_action] = pull_count[current_action] + 1\n",
    "    if np.random.uniform(0,1) < ground_truth[current_action]: current_reward = 1\n",
    "    else: current_reward = 0\n",
    "    expected[current_action] = expected[current_action] + (1/pull_count[current_action]) * (current_reward-expected[current_action])\n",
    "    \n",
    "print(np.around(ground_truth,3))\n",
    "print(np.around(expected,3))\n",
    "print(np.around(pull_count,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-25T09:39:41.882690Z",
     "start_time": "2018-12-25T09:39:37.175383Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.90626164 0.02255411 0.87551925 0.11565788 0.09965873 0.92431365\n",
      " 0.88146685 0.12355341 0.18428255 0.04542723]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0.906 0.023 0.876 0.116 0.1   0.924 0.881 0.124 0.184 0.045]\n",
      "[0.906 0.022 0.875 0.115 0.099 0.922 0.882 0.124 0.184 0.045]\n",
      "[ 99787. 100324. 100545.  99759.  99898.  99667.  99909. 100175. 100262.\n",
      "  99674.]\n"
     ]
    }
   ],
   "source": [
    "# multi threaded\n",
    "expected     = np.zeros(10)\n",
    "pull_count   = np.zeros(10) \n",
    "print(ground_truth)\n",
    "print(expected)\n",
    "print(pull_count)\n",
    "\n",
    "def findvalue():\n",
    "    global expected,pull_count\n",
    "    for iter in range(num_epoch//2):\n",
    "        current_action = np.random.randint(10)\n",
    "        pull_count[current_action] = pull_count[current_action] + 1\n",
    "        if np.random.uniform(0,1) < ground_truth[current_action]: current_reward = 1\n",
    "        else: current_reward = 0\n",
    "        expected[current_action] = expected[current_action] + (1/pull_count[current_action]) * (current_reward-expected[current_action])\n",
    "  \n",
    "jobs = []\n",
    "for i in range(0, 2):\n",
    "    thread = threading.Thread(target=findvalue())\n",
    "    jobs.append(thread)\n",
    "\n",
    "for x in jobs:\n",
    "    x.start()\n",
    "\n",
    "for x in jobs:\n",
    "    x.join()\n",
    "\n",
    "print(np.around(ground_truth,3))\n",
    "print(np.around(expected,3))\n",
    "print(np.around(pull_count,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-25T09:44:21.967564Z",
     "start_time": "2018-12-25T09:44:21.327288Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.90626164 0.02255411 0.87551925 0.11565788 0.09965873 0.92431365\n",
      " 0.88146685 0.12355341 0.18428255 0.04542723]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0.906 0.023 0.876 0.116 0.1   0.924 0.881 0.124 0.184 0.045]\n",
      "[0.905 0.023 0.875 0.116 0.1   0.923 0.881 0.124 0.186 0.045]\n",
      "[100000. 100000. 100000. 100000. 100000. 100000. 100000. 100000. 100000.\n",
      " 100000.]\n"
     ]
    }
   ],
   "source": [
    "# vectorized\n",
    "expected     = np.zeros(10)\n",
    "pull_count   = np.zeros(10) \n",
    "print(ground_truth)\n",
    "print(expected)\n",
    "print(pull_count)\n",
    "\n",
    "for iter in range(num_epoch//10):\n",
    "    pull_count = pull_count + 1\n",
    "    current_reward = np.random.uniform(0,1) < ground_truth\n",
    "    expected = expected + (1/pull_count) * (current_reward-expected)\n",
    "    \n",
    "print(np.around(ground_truth,3))\n",
    "print(np.around(expected,3))\n",
    "print(np.around(pull_count,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-25T09:42:27.524334Z",
     "start_time": "2018-12-25T09:42:27.517382Z"
    },
    "code_folding": [
     100,
     133,
     142
    ]
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " code: https://github.com/ankonzoid/LearningX/blob/master/classical_RL/MAB/MAB.py\n",
    " multiarmed_bandits.py  (author: Anson Wong / git: ankonzoid)\n",
    " We solve the multi-armed bandit problem using a classical epsilon-greedy\n",
    " agent with reward-average sampling to estimate the action-value Q.\n",
    " This algorithm follows closely with the notation of Sutton's RL textbook.\n",
    " We set up up bandits with a fixed probability distribution of success,\n",
    " and receive stochastic rewards from the bandits of +1 for success,\n",
    " and 0 reward for failure.\n",
    " The update rule for our action-values Q is:\n",
    "   Q(a) <- Q(a) + 1/(k+1) * (R(a) - Q(a))\n",
    " where\n",
    "   Q(a) = current value estimate of action \"a\"\n",
    "   k = number of times action \"a\" was chosen so far\n",
    "   R(a) = reward of sampling action bandit (bandit) \"a\"\n",
    " The derivation of the above Q incremental implementation update:\n",
    "   Q(a;k+1)\n",
    "   = 1/(k+1) * (R(a_1) + R(a_2) + ... + R(a_k) + R(a))\n",
    "   = 1/(k+1) * (k*Q(a;k) + R(a))\n",
    "   = 1/(k+1) * ((k+1)*Q(a;k) + R(a) - Q(a;k))\n",
    "   = Q(a;k) + 1/(k+1) * (R(a) - Q(a;k))\n",
    "\"\"\"\n",
    "\n",
    "# =========================\n",
    "# Settings\n",
    "# =========================\n",
    "bandit_probs = [0.10, 0.50, 0.60, 0.80, 0.10, 0.25, 0.60, 0.45, 0.75, 0.65]  \n",
    "N_experiments = 100  # number of experiments to perform\n",
    "N_episodes = 10000  # number of episodes per experiment\n",
    "epsilon    = 0.1  # probability of random exploration (fraction)\n",
    "save_fig   = False   # if false -> plot, if true save as file in same directory\n",
    "\n",
    "# =========================\n",
    "# Define Bandit and Agent class\n",
    "# =========================\n",
    "class Bandit:\n",
    "\n",
    "    def __init__(self, bandit_probs):\n",
    "        self.N = len(bandit_probs)  # number of bandits\n",
    "        self.prob = bandit_probs  # success probabilities for each bandit\n",
    "\n",
    "    # Get reward (1 for success, 0 for failure)\n",
    "    def get_reward(self, action):\n",
    "        rand = np.random.random()  # [0.0,1.0)\n",
    "        reward = 1 if (rand < self.prob[action]) else 0\n",
    "        return reward\n",
    "\n",
    "class Agent:\n",
    "\n",
    "    def __init__(self, bandit, epsilon):\n",
    "        self.epsilon = epsilon\n",
    "        self.k = np.zeros(bandit.N, dtype=np.int)  # number of times action was chosen\n",
    "        self.Q = np.zeros(bandit.N, dtype=np.float)  # estimated value\n",
    "\n",
    "    # Update Q action-value using:\n",
    "    # Q(a) <- Q(a) + 1/(k+1) * (r(a) - Q(a))\n",
    "    def update_Q(self, action, reward):\n",
    "        self.k[action] += 1  # update action counter k -> k+1\n",
    "        self.Q[action] += (1./self.k[action]) * (reward - self.Q[action])\n",
    "\n",
    "    # Choose action using an epsilon-greedy agent\n",
    "    def get_action(self, bandit, force_explore=False):\n",
    "        rand = np.random.random()  # [0.0,1.0)\n",
    "        if (rand < self.epsilon) or force_explore:\n",
    "            action_explore = np.random.randint(bandit.N)  # explore random bandit\n",
    "            return action_explore\n",
    "        else:\n",
    "            #action_greedy = np.argmax(self.Q)  # exploit best current bandit\n",
    "            action_greedy = np.random.choice(np.flatnonzero(self.Q == self.Q.max()))\n",
    "            return action_greedy\n",
    "\n",
    "# =========================\n",
    "# Define an experiment\n",
    "# =========================\n",
    "def experiment(agent, bandit, N_episodes):\n",
    "    action_history = []\n",
    "    reward_history = []\n",
    "    for episode in range(N_episodes):\n",
    "        # Choose action from agent (from current Q estimate)\n",
    "        action = agent.get_action(bandit)\n",
    "        # Pick up reward from bandit for chosen action\n",
    "        reward = bandit.get_reward(action)\n",
    "        # Update Q action-value estimates\n",
    "        agent.update_Q(action, reward)\n",
    "        # Append to history\n",
    "        action_history.append(action)\n",
    "        reward_history.append(reward)\n",
    "    return (np.array(action_history), np.array(reward_history))\n",
    "\n",
    "# =========================\n",
    "#\n",
    "# Start multi-armed bandit simulation\n",
    "#\n",
    "# =========================\n",
    "N_bandits = len(bandit_probs)\n",
    "print(\"Running multi-armed bandits with N_bandits = {} and agent epsilon = {}\".format(N_bandits, epsilon))\n",
    "reward_history_avg = np.zeros(N_episodes)  # reward history experiment-averaged\n",
    "action_history_sum = np.zeros((N_episodes, N_bandits))  # sum action history\n",
    "for i in range(N_experiments):\n",
    "    bandit = Bandit(bandit_probs)  # initialize bandits\n",
    "    agent = Agent(bandit, epsilon)  # initialize agent\n",
    "    (action_history, reward_history) = experiment(agent, bandit, N_episodes)  # perform experiment\n",
    "\n",
    "    if (i + 1) % (N_experiments / 100) == 0:\n",
    "        print(\"[Experiment {}/{}]\".format(i + 1, N_experiments))\n",
    "        print(\"  N_episodes = {}\".format(N_episodes))\n",
    "        print(\"  bandit choice history = {}\".format(\n",
    "            action_history + 1))\n",
    "        print(\"  reward history = {}\".format(\n",
    "            reward_history))\n",
    "        print(\"  average reward = {}\".format(np.sum(reward_history) / len(reward_history)))\n",
    "        print(\"\")\n",
    "    # Sum up experiment reward (later to be divided to represent an average)\n",
    "    reward_history_avg += reward_history\n",
    "    # Sum up action history\n",
    "    for j, (a) in enumerate(action_history):\n",
    "        action_history_sum[j][a] += 1\n",
    "\n",
    "reward_history_avg /= np.float(N_experiments)\n",
    "print(\"reward history avg = {}\".format(reward_history_avg))\n",
    "\n",
    "# =========================\n",
    "# Plot reward history results\n",
    "# =========================\n",
    "plt.plot(reward_history_avg)\n",
    "plt.xlabel(\"Episode number\")\n",
    "plt.ylabel(\"Rewards collected\".format(N_experiments))\n",
    "plt.title(\"Bandit reward history averaged over {} experiments (epsilon = {})\".format(N_experiments, epsilon))\n",
    "ax = plt.gca()\n",
    "ax.set_xscale(\"log\", nonposx='clip')\n",
    "plt.xlim([1, N_episodes])\n",
    "if save_fig:\n",
    "    output_file = \"output/rewards.png\"\n",
    "    plt.savefig(output_file, bbox_inches=\"tight\")\n",
    "else: plt.show()\n",
    "\n",
    "# =========================\n",
    "# Plot action history results\n",
    "# =========================\n",
    "plt.figure(figsize=(18, 12))\n",
    "for i in range(N_bandits):\n",
    "    action_history_sum_plot = 100 * action_history_sum[:,i] / N_experiments\n",
    "    plt.plot(list(np.array(range(len(action_history_sum_plot)))+1),action_history_sum_plot,linewidth=5.0,label=\"Bandit #{}\".format(i+1))\n",
    "plt.title(\"Bandit action history averaged over {} experiments (epsilon = {})\".format(N_experiments, epsilon), fontsize=26)\n",
    "plt.xlabel(\"Episode Number\", fontsize=26)\n",
    "plt.ylabel(\"Bandit Action Choices (%)\", fontsize=26)\n",
    "leg = plt.legend(loc='upper left', shadow=True, fontsize=26)\n",
    "ax = plt.gca()\n",
    "ax.set_xscale(\"log\", nonposx='clip')\n",
    "plt.xlim([1, N_episodes])\n",
    "plt.ylim([0, 100])\n",
    "plt.xticks(fontsize=24)\n",
    "plt.yticks(fontsize=24)\n",
    "for legobj in leg.legendHandles:\n",
    "    legobj.set_linewidth(16.0)\n",
    "if save_fig:\n",
    "    output_file = \"output/actions.png\"\n",
    "    plt.savefig(output_file, bbox_inches=\"tight\")\n",
    "else:\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference \n",
    "1. Solving the Multi-Armed Bandit Problem – Towards Data Science. (2017). Towards Data Science. Retrieved 24 December 2018, from https://towardsdatascience.com/solving-the-multi-armed-bandit-problem-b72de40db97c\n",
    "2. Parallelising Python with Threading and Multiprocessing | QuantStart. (2018). Quantstart.com. Retrieved 25 December 2018, from https://www.quantstart.com/articles/Parallelising-Python-with-Threading-and-Multiprocessing\n",
    "3. profile, V. (2014). Python Multiprocessing global variables. Korznikov.com. Retrieved 25 December 2018, from http://www.korznikov.com/2014/07/python-multiprocessing-global-variables.html\n",
    "4. Python Tutorial - 28. Sharing Data Between Processes Using Array and Value. (2018). YouTube. Retrieved 25 December 2018, from https://www.youtube.com/watch?v=uWbSc84he2Q\n",
    "5. Solving the Multi-Armed Bandit Problem – Towards Data Science. (2017). Towards Data Science. Retrieved 25 December 2018, from https://towardsdatascience.com/solving-the-multi-armed-bandit-problem-b72de40db97c"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
